{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLEh-LthYBKv"
   },
   "source": [
    "# Surface Crack Images - OLeNet model\n",
    "This notebook contains the code training the OLeNet model with ReLU (without augmented data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEb3XLxjYBK4"
   },
   "source": [
    "**Load packages/modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200518,
     "status": "ok",
     "timestamp": 1637015442266,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "H-oXyKDyYEED",
    "outputId": "95d5042b-3ac5-4855-a9b4-701ecb459c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27701,
     "status": "ok",
     "timestamp": 1637015499751,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "9AKkpfhtYBK5",
    "outputId": "117d371d-c01d-4ab9-85f2-92545bfbfd35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.10.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print(f'Torch version: {torch .__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3686,
     "status": "ok",
     "timestamp": 1637015503966,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "i_OfDGEmYBK8",
    "outputId": "3b8421d3-e361-468b-d634-b64348e667e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "%pip install torchsummary\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysKmpc9cYBK8"
   },
   "source": [
    "**Load data into tensors first**<br>\n",
    "Then concatenate the tensors with original and augmented data into one for ease of model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1637015834764,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "syPC_CMNYBK_"
   },
   "outputs": [],
   "source": [
    "# load data into tensors first\n",
    "data_dir = '/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/archive/original'\n",
    "# aug_data_dir = '/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/archive/augmented'\n",
    "batch_size = 64 \n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "data = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(data, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True, \n",
    "                                         pin_memory=True)\n",
    "\n",
    "# aug_dataloader = torch.utils.data.DataLoader(aug_data,\n",
    "#                                              batch_size=batch_size*9,  # multiply by 9 since augmented x9 images for each original\n",
    "#                                              shuffle=True,\n",
    "#                                              pin_memory=True)\n",
    "\n",
    "# images, labels = next(iter(dataloader))\n",
    "# aug_images, aug_labels = next(iter(aug_dataloader))\n",
    "\n",
    "# print(f'Original images shape: {images.shape}')\n",
    "# print(f'Augmented images shape: {aug_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9C4r55Yj-Pf"
   },
   "outputs": [],
   "source": [
    "# generate and save test set\n",
    "test_counter = 1\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    if i+1 > 500:\n",
    "        if test_counter == 1:\n",
    "            test_inputs, test_labels = data \n",
    "            test_counter += 1\n",
    "        else:\n",
    "            new_inputs, new_labels = data\n",
    "            test_inputs = torch.concat((test_inputs, new_inputs), 0)\n",
    "            test_labels = torch.concat((test_labels, new_labels), 0)\n",
    "            test_counter += 1\n",
    "\n",
    "        # print(f'Batch {i+1} for test set complete.')\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# print(f'Saving test inputs {test_inputs.shape}, test labels {test_labels.shape}')\n",
    "\n",
    "# save test set at the end of training\n",
    "testset = [test_inputs, test_labels]\n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_testset.pth\"\n",
    "torch.save(testset, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZaeGsiNYBLC"
   },
   "source": [
    "# Construct model\n",
    "**OLeNet model architecture:** <br>\n",
    " Conv2D - 50 x 50 x 32 <br>\n",
    " Conv2D - 50 x 50 x 32 <br>\n",
    " MaxPool2D - 25 x 25 x 32 <br>\n",
    " Drop - 25 x 25 x 32 <br>\n",
    " \n",
    " Conv2D - 50 x 50 x 64 <br>\n",
    " Conv2D - 50 x 50 x 64 <br>\n",
    " MaxPool2D - 12 x 12 x 64 <br>\n",
    " Drop - 12 x 12 x 64 <br>\n",
    "\n",
    " Flatten (9216) <br>\n",
    " Dense (256) <br>\n",
    " Drop (256) <br>\n",
    " Dense (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1637015849706,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "-0gG2OoGYBLD"
   },
   "outputs": [],
   "source": [
    "class OLeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, padding=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 5, padding=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5, padding=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 5, padding=2, stride=2)\n",
    "\n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(64*3*3, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)   # have two classes (has crack/no crack)\n",
    "        \n",
    "        # softmax layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)  # Max pool 2x2\n",
    "\n",
    "        # dropout layer\n",
    "        self.drop = nn.Dropout(p=0.5) # drop out with probability of 0.5\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = F.relu(self.conv1(x)) # layer 1\n",
    "        x = F.relu(self.conv2(x)) # layer 2\n",
    "        x = self.pool(x)          # pool layer\n",
    "        x = self.drop(x)          # dropout layer\n",
    "        \n",
    "        x = F.relu(self.conv3(x)) # layer 5\n",
    "        x = F.relu(self.conv4(x)) # layer 6\n",
    "        x = self.pool(x)          # pool layer\n",
    "        x = self.drop(x)          # dropout layer\n",
    "        \n",
    "        x = torch.flatten(x, 1)   # flatten layer\n",
    "        \n",
    "        x = F.relu(self.fc1(x))   # layer 7\n",
    "        x = self.drop(x)          # dropout layer\n",
    "        logit = self.fc2(x)       # layer \n",
    "        \n",
    "        output = self.softmax(logit) # output layer\n",
    "\n",
    "        return logit, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1636947249491,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "0gbcU53BYBLE",
    "outputId": "abbc08fb-ffc5-4d49-f5cd-b086b3378497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 114, 114]           2,432\n",
      "            Conv2d-2           [-1, 32, 57, 57]          25,632\n",
      "         MaxPool2d-3           [-1, 32, 28, 28]               0\n",
      "           Dropout-4           [-1, 32, 28, 28]               0\n",
      "            Conv2d-5           [-1, 64, 14, 14]          51,264\n",
      "            Conv2d-6             [-1, 64, 7, 7]         102,464\n",
      "         MaxPool2d-7             [-1, 64, 3, 3]               0\n",
      "           Dropout-8             [-1, 64, 3, 3]               0\n",
      "            Linear-9                  [-1, 256]         147,712\n",
      "          Dropout-10                  [-1, 256]               0\n",
      "           Linear-11                    [-1, 2]             514\n",
      "          Softmax-12                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 330,018\n",
      "Trainable params: 330,018\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 4.48\n",
      "Params size (MB): 1.26\n",
      "Estimated Total Size (MB): 6.33\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print model summary\n",
    "\n",
    "model = OLeNet()\n",
    "\n",
    "channels = 3\n",
    "H = 227\n",
    "W = 227\n",
    "\n",
    "summary(model, (channels, H, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp89_6KnjowJ"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14198943,
     "status": "ok",
     "timestamp": 1636943797940,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "0acPeyXXsDdk",
    "outputId": "aba04bd2-815d-4bf5-b478-67d03093613d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Training iteration 21 loss: 0.7005048990249634, ACC:0.40625\n",
      "Training iteration 22 loss: 0.6990957260131836, ACC:0.40625\n",
      "Training iteration 23 loss: 0.7038742303848267, ACC:0.359375\n",
      "Training iteration 24 loss: 0.69583660364151, ACC:0.53125\n",
      "Training iteration 25 loss: 0.683174192905426, ACC:0.640625\n",
      "Training iteration 26 loss: 0.6889878511428833, ACC:0.578125\n",
      "Training iteration 27 loss: 0.6991180777549744, ACC:0.390625\n",
      "Training iteration 28 loss: 0.6910980343818665, ACC:0.5625\n",
      "Training iteration 29 loss: 0.6895067691802979, ACC:0.546875\n",
      "Training iteration 30 loss: 0.6836745142936707, ACC:0.59375\n",
      "Training iteration 31 loss: 0.6822362542152405, ACC:0.53125\n",
      "Training iteration 32 loss: 0.6839014887809753, ACC:0.640625\n",
      "Training iteration 33 loss: 0.6749911904335022, ACC:0.625\n",
      "Training iteration 34 loss: 0.7088664174079895, ACC:0.4375\n",
      "Training iteration 35 loss: 0.6343264579772949, ACC:0.59375\n",
      "Training iteration 36 loss: 0.6618523597717285, ACC:0.546875\n",
      "Training iteration 37 loss: 0.6965069770812988, ACC:0.484375\n",
      "Training iteration 38 loss: 0.6591402292251587, ACC:0.625\n",
      "Training iteration 39 loss: 0.6686155200004578, ACC:0.578125\n",
      "Training iteration 40 loss: 0.7260185480117798, ACC:0.46875\n",
      "Training iteration 41 loss: 0.704662561416626, ACC:0.453125\n",
      "Training iteration 42 loss: 0.6548915505409241, ACC:0.703125\n",
      "Training iteration 43 loss: 0.6799637079238892, ACC:0.515625\n",
      "Training iteration 44 loss: 0.6886948347091675, ACC:0.4375\n",
      "Training iteration 45 loss: 0.6393659710884094, ACC:0.796875\n",
      "Training iteration 46 loss: 0.6592789888381958, ACC:0.546875\n",
      "Training iteration 47 loss: 0.6575700044631958, ACC:0.625\n",
      "Training iteration 48 loss: 0.6088600158691406, ACC:0.875\n",
      "Training iteration 49 loss: 0.5859630703926086, ACC:0.796875\n",
      "Training iteration 50 loss: 0.5728738903999329, ACC:0.890625\n",
      "Training iteration 51 loss: 0.5955927968025208, ACC:0.734375\n",
      "Training iteration 52 loss: 0.5166594386100769, ACC:0.84375\n",
      "Training iteration 53 loss: 1.0902140140533447, ACC:0.46875\n",
      "Training iteration 54 loss: 0.4379720091819763, ACC:0.921875\n",
      "Training iteration 55 loss: 0.5536790490150452, ACC:0.71875\n",
      "Training iteration 56 loss: 0.5909610390663147, ACC:0.640625\n",
      "Training iteration 57 loss: 0.5166809558868408, ACC:0.796875\n",
      "Training iteration 58 loss: 0.5619763135910034, ACC:0.53125\n",
      "Training iteration 59 loss: 0.5961092114448547, ACC:0.484375\n",
      "Training iteration 60 loss: 0.5508982539176941, ACC:0.546875\n",
      "Training iteration 61 loss: 0.47874635457992554, ACC:0.9375\n",
      "Training iteration 62 loss: 0.4638303220272064, ACC:0.921875\n",
      "Training iteration 63 loss: 0.45093730092048645, ACC:0.828125\n",
      "Training iteration 64 loss: 0.5344205498695374, ACC:0.71875\n",
      "Training iteration 65 loss: 0.3360235095024109, ACC:0.90625\n",
      "Training iteration 66 loss: 0.45913150906562805, ACC:0.765625\n",
      "Training iteration 67 loss: 0.4147154688835144, ACC:0.8125\n",
      "Training iteration 68 loss: 0.2814476490020752, ACC:0.9375\n",
      "Training iteration 69 loss: 1.9625910520553589, ACC:0.453125\n",
      "Training iteration 70 loss: 0.2885124981403351, ACC:0.921875\n",
      "Training iteration 71 loss: 0.5939731001853943, ACC:0.6875\n",
      "Training iteration 72 loss: 0.4549221992492676, ACC:0.796875\n",
      "Training iteration 73 loss: 0.47868505120277405, ACC:0.8125\n",
      "Training iteration 74 loss: 0.5064002871513367, ACC:0.765625\n",
      "Training iteration 75 loss: 0.4433569610118866, ACC:0.90625\n",
      "Training iteration 76 loss: 0.4473712742328644, ACC:0.796875\n",
      "Training iteration 77 loss: 0.3468773066997528, ACC:0.90625\n",
      "Training iteration 78 loss: 0.2512211799621582, ACC:0.90625\n",
      "Training iteration 79 loss: 0.3547373116016388, ACC:0.859375\n",
      "Training iteration 80 loss: 0.29858365654945374, ACC:0.90625\n",
      "Training iteration 81 loss: 0.439492404460907, ACC:0.765625\n",
      "Training iteration 82 loss: 0.2100742757320404, ACC:0.921875\n",
      "Training iteration 83 loss: 0.5935806632041931, ACC:0.8125\n",
      "Training iteration 84 loss: 0.4093147814273834, ACC:0.828125\n",
      "Training iteration 85 loss: 0.2714044749736786, ACC:0.90625\n",
      "Training iteration 86 loss: 0.2669409513473511, ACC:0.96875\n",
      "Training iteration 87 loss: 0.29288312792778015, ACC:0.984375\n",
      "Training iteration 88 loss: 0.2908251881599426, ACC:0.96875\n",
      "Training iteration 89 loss: 0.40796056389808655, ACC:0.90625\n",
      "Training iteration 90 loss: 0.3975495994091034, ACC:0.90625\n",
      "Training iteration 91 loss: 0.30970537662506104, ACC:0.9375\n",
      "Training iteration 92 loss: 0.1792023926973343, ACC:0.984375\n",
      "Training iteration 93 loss: 0.19374537467956543, ACC:0.96875\n",
      "Training iteration 94 loss: 0.18649879097938538, ACC:0.953125\n",
      "Training iteration 95 loss: 0.23866154253482819, ACC:0.921875\n",
      "Training iteration 96 loss: 0.19804880023002625, ACC:0.90625\n",
      "Training iteration 97 loss: 0.04346069321036339, ACC:0.984375\n",
      "Training iteration 98 loss: 0.17534691095352173, ACC:0.9375\n",
      "Training iteration 99 loss: 0.32922473549842834, ACC:0.90625\n",
      "Training iteration 100 loss: 0.17818912863731384, ACC:0.953125\n",
      "Training iteration 101 loss: 0.07408996671438217, ACC:0.984375\n",
      "Training iteration 102 loss: 0.1753007024526596, ACC:0.921875\n",
      "Training iteration 103 loss: 0.23839791119098663, ACC:0.921875\n",
      "Training iteration 104 loss: 0.15653103590011597, ACC:0.96875\n",
      "Training iteration 105 loss: 0.19473575055599213, ACC:0.9375\n",
      "Training iteration 106 loss: 0.15306997299194336, ACC:0.953125\n",
      "Training iteration 107 loss: 0.043908607214689255, ACC:1.0\n",
      "Training iteration 108 loss: 0.3745918869972229, ACC:0.9375\n",
      "Training iteration 109 loss: 0.12321554124355316, ACC:0.984375\n",
      "Training iteration 110 loss: 0.04350106418132782, ACC:0.984375\n",
      "Training iteration 111 loss: 0.04485687240958214, ACC:1.0\n",
      "Training iteration 112 loss: 0.2571689784526825, ACC:0.921875\n",
      "Training iteration 113 loss: 0.0437023751437664, ACC:1.0\n",
      "Training iteration 114 loss: 0.13179239630699158, ACC:0.953125\n",
      "Training iteration 115 loss: 0.2594393193721771, ACC:0.90625\n",
      "Training iteration 116 loss: 0.10436111688613892, ACC:0.984375\n",
      "Training iteration 117 loss: 0.11685268580913544, ACC:0.96875\n",
      "Training iteration 118 loss: 0.16442202031612396, ACC:0.9375\n",
      "Training iteration 119 loss: 0.15184976160526276, ACC:0.9375\n",
      "Training iteration 120 loss: 0.07754380255937576, ACC:0.984375\n",
      "Training iteration 121 loss: 0.13637395203113556, ACC:0.953125\n",
      "Training iteration 122 loss: 0.09098123013973236, ACC:0.953125\n",
      "Training iteration 123 loss: 0.13546591997146606, ACC:0.9375\n",
      "Training iteration 124 loss: 0.06292954832315445, ACC:0.984375\n",
      "Training iteration 125 loss: 0.1286647915840149, ACC:0.984375\n",
      "Training iteration 126 loss: 0.0908166766166687, ACC:0.984375\n",
      "Training iteration 127 loss: 0.08046012371778488, ACC:0.96875\n",
      "Training iteration 128 loss: 0.0710759088397026, ACC:0.953125\n",
      "Training iteration 129 loss: 0.33691126108169556, ACC:0.953125\n",
      "Training iteration 130 loss: 0.24471700191497803, ACC:0.921875\n",
      "Training iteration 131 loss: 0.11262793838977814, ACC:0.984375\n",
      "Training iteration 132 loss: 0.05439629405736923, ACC:0.984375\n",
      "Training iteration 133 loss: 0.122764952480793, ACC:0.953125\n",
      "Training iteration 134 loss: 0.1706818789243698, ACC:0.953125\n",
      "Training iteration 135 loss: 0.14245489239692688, ACC:0.96875\n",
      "Training iteration 136 loss: 0.133546382188797, ACC:0.96875\n",
      "Training iteration 137 loss: 0.16096976399421692, ACC:0.953125\n",
      "Training iteration 138 loss: 0.0818881168961525, ACC:0.984375\n",
      "Training iteration 139 loss: 0.03277382627129555, ACC:1.0\n",
      "Training iteration 140 loss: 0.043475426733493805, ACC:0.984375\n",
      "Training iteration 141 loss: 0.08405923843383789, ACC:0.96875\n",
      "Training iteration 142 loss: 0.04651257395744324, ACC:0.96875\n",
      "Training iteration 143 loss: 0.054922208189964294, ACC:0.96875\n",
      "Training iteration 144 loss: 0.046297986060380936, ACC:0.984375\n",
      "Training iteration 145 loss: 0.09690289199352264, ACC:0.984375\n",
      "Training iteration 146 loss: 0.15465927124023438, ACC:0.953125\n",
      "Training iteration 147 loss: 0.5087311267852783, ACC:0.90625\n",
      "Training iteration 148 loss: 0.07193366438150406, ACC:0.984375\n",
      "Training iteration 149 loss: 0.02057013474404812, ACC:1.0\n",
      "Training iteration 150 loss: 0.053337451070547104, ACC:0.984375\n",
      "Training iteration 151 loss: 0.0327051505446434, ACC:1.0\n",
      "Training iteration 152 loss: 0.07652681320905685, ACC:0.984375\n",
      "Training iteration 153 loss: 0.22059598565101624, ACC:0.984375\n",
      "Training iteration 154 loss: 0.18347948789596558, ACC:0.90625\n",
      "Training iteration 155 loss: 0.12726186215877533, ACC:0.96875\n",
      "Training iteration 156 loss: 0.17635585367679596, ACC:0.96875\n",
      "Training iteration 157 loss: 0.04729834944009781, ACC:0.984375\n",
      "Training iteration 158 loss: 0.0634150356054306, ACC:0.984375\n",
      "Training iteration 159 loss: 0.12147538363933563, ACC:0.96875\n",
      "Training iteration 160 loss: 0.05580277740955353, ACC:0.96875\n",
      "Training iteration 161 loss: 0.0682041347026825, ACC:0.984375\n",
      "Training iteration 162 loss: 0.10257025063037872, ACC:0.96875\n",
      "Training iteration 163 loss: 0.2307644635438919, ACC:0.921875\n",
      "Training iteration 164 loss: 0.07378392666578293, ACC:0.984375\n",
      "Training iteration 165 loss: 0.037911005318164825, ACC:1.0\n",
      "Training iteration 166 loss: 0.08594389259815216, ACC:0.953125\n",
      "Training iteration 167 loss: 0.12716032564640045, ACC:0.96875\n",
      "Training iteration 168 loss: 0.03631637245416641, ACC:0.984375\n",
      "Training iteration 169 loss: 0.03157268464565277, ACC:1.0\n",
      "Training iteration 170 loss: 0.041061125695705414, ACC:0.984375\n",
      "Training iteration 171 loss: 0.13975751399993896, ACC:0.9375\n",
      "Training iteration 172 loss: 0.08239791542291641, ACC:0.96875\n",
      "Training iteration 173 loss: 0.03518231213092804, ACC:0.984375\n",
      "Training iteration 174 loss: 0.10592559725046158, ACC:0.96875\n",
      "Training iteration 175 loss: 0.02162196859717369, ACC:1.0\n",
      "Training iteration 176 loss: 0.057452961802482605, ACC:0.984375\n",
      "Training iteration 177 loss: 0.09176129102706909, ACC:0.96875\n",
      "Training iteration 178 loss: 0.0787898525595665, ACC:0.9375\n",
      "Training iteration 179 loss: 0.19307473301887512, ACC:0.9375\n",
      "Training iteration 180 loss: 0.08864277601242065, ACC:0.96875\n",
      "Training iteration 181 loss: 0.09495417773723602, ACC:0.9375\n",
      "Training iteration 182 loss: 0.09328062832355499, ACC:0.96875\n",
      "Training iteration 183 loss: 0.033893339335918427, ACC:0.984375\n",
      "Training iteration 184 loss: 0.02117224782705307, ACC:1.0\n",
      "Training iteration 185 loss: 0.08940847963094711, ACC:0.984375\n",
      "Training iteration 186 loss: 0.07967492192983627, ACC:0.96875\n",
      "Training iteration 187 loss: 0.06235571578145027, ACC:0.984375\n",
      "Training iteration 188 loss: 0.12376183271408081, ACC:0.953125\n",
      "Training iteration 189 loss: 0.07496897131204605, ACC:0.96875\n",
      "Training iteration 190 loss: 0.05828456953167915, ACC:0.984375\n",
      "Training iteration 191 loss: 0.03906652703881264, ACC:1.0\n",
      "Training iteration 192 loss: 0.06571902334690094, ACC:0.96875\n",
      "Training iteration 193 loss: 0.06778465956449509, ACC:0.96875\n",
      "Training iteration 194 loss: 0.07860694825649261, ACC:0.96875\n",
      "Training iteration 195 loss: 0.021253172308206558, ACC:1.0\n",
      "Training iteration 196 loss: 0.007988980039954185, ACC:1.0\n",
      "Training iteration 197 loss: 0.0536469966173172, ACC:0.984375\n",
      "Training iteration 198 loss: 0.19840319454669952, ACC:0.9375\n",
      "Training iteration 199 loss: 0.013337084092199802, ACC:1.0\n",
      "Training iteration 200 loss: 0.02764109894633293, ACC:1.0\n",
      "Training iteration 201 loss: 0.08013219386339188, ACC:0.953125\n",
      "Training iteration 202 loss: 0.11181887239217758, ACC:0.96875\n",
      "Training iteration 203 loss: 0.163697749376297, ACC:0.953125\n",
      "Training iteration 204 loss: 0.06099865213036537, ACC:0.984375\n",
      "Training iteration 205 loss: 0.027122529223561287, ACC:0.984375\n",
      "Training iteration 206 loss: 0.07156260311603546, ACC:0.96875\n",
      "Training iteration 207 loss: 0.033002082258462906, ACC:1.0\n",
      "Training iteration 208 loss: 0.029888560995459557, ACC:0.984375\n",
      "Training iteration 209 loss: 0.029435250908136368, ACC:1.0\n",
      "Training iteration 210 loss: 0.026661401614546776, ACC:0.984375\n",
      "Training iteration 211 loss: 0.1643659621477127, ACC:0.90625\n",
      "Training iteration 212 loss: 0.024468909949064255, ACC:0.984375\n",
      "Training iteration 213 loss: 0.04670706018805504, ACC:0.96875\n",
      "Training iteration 214 loss: 0.025348788127303123, ACC:0.984375\n",
      "Training iteration 215 loss: 0.011500016786158085, ACC:1.0\n",
      "Training iteration 216 loss: 0.19451184570789337, ACC:0.96875\n",
      "Training iteration 217 loss: 0.013435369357466698, ACC:1.0\n",
      "Training iteration 218 loss: 0.2767563462257385, ACC:0.9375\n",
      "Training iteration 219 loss: 0.011548020876944065, ACC:1.0\n",
      "Training iteration 220 loss: 0.040462516248226166, ACC:1.0\n",
      "Training iteration 221 loss: 0.07237722724676132, ACC:0.984375\n",
      "Training iteration 222 loss: 0.045961566269397736, ACC:0.96875\n",
      "Training iteration 223 loss: 0.03628858923912048, ACC:1.0\n",
      "Training iteration 224 loss: 0.015343694016337395, ACC:1.0\n",
      "Training iteration 225 loss: 0.1010790541768074, ACC:0.984375\n",
      "Training iteration 226 loss: 0.046427011489868164, ACC:0.984375\n",
      "Training iteration 227 loss: 0.027163546532392502, ACC:0.96875\n",
      "Training iteration 228 loss: 0.07950687408447266, ACC:0.953125\n",
      "Training iteration 229 loss: 0.11822742968797684, ACC:0.921875\n",
      "Training iteration 230 loss: 0.3437063694000244, ACC:0.828125\n",
      "Training iteration 231 loss: 0.21853238344192505, ACC:0.875\n",
      "Training iteration 232 loss: 0.045045435428619385, ACC:0.984375\n",
      "Training iteration 233 loss: 0.0201974306255579, ACC:0.984375\n",
      "Training iteration 234 loss: 0.010298214852809906, ACC:1.0\n",
      "Training iteration 235 loss: 0.11616936326026917, ACC:0.96875\n",
      "Training iteration 236 loss: 0.09676038473844528, ACC:0.96875\n",
      "Training iteration 237 loss: 0.2415827363729477, ACC:0.921875\n",
      "Training iteration 238 loss: 0.029209595173597336, ACC:1.0\n",
      "Training iteration 239 loss: 0.1334841549396515, ACC:0.96875\n",
      "Training iteration 240 loss: 0.08416927605867386, ACC:0.984375\n",
      "Training iteration 241 loss: 0.06787659972906113, ACC:0.96875\n",
      "Training iteration 242 loss: 0.026877736672759056, ACC:1.0\n",
      "Training iteration 243 loss: 0.03889039158821106, ACC:0.984375\n",
      "Training iteration 244 loss: 0.10515882819890976, ACC:0.9375\n",
      "Training iteration 245 loss: 0.020543385297060013, ACC:1.0\n",
      "Training iteration 246 loss: 0.06767487525939941, ACC:0.96875\n",
      "Training iteration 247 loss: 0.04003101959824562, ACC:1.0\n",
      "Training iteration 248 loss: 0.15545056760311127, ACC:0.984375\n",
      "Training iteration 249 loss: 0.01160519476979971, ACC:1.0\n",
      "Training iteration 250 loss: 0.018694370985031128, ACC:1.0\n",
      "Training iteration 251 loss: 0.03198447450995445, ACC:1.0\n",
      "Training iteration 252 loss: 0.21452321112155914, ACC:0.953125\n",
      "Training iteration 253 loss: 0.052968185395002365, ACC:0.984375\n",
      "Training iteration 254 loss: 0.0748799741268158, ACC:0.96875\n",
      "Training iteration 255 loss: 0.04969434812664986, ACC:0.984375\n",
      "Training iteration 256 loss: 0.10979583114385605, ACC:0.96875\n",
      "Training iteration 257 loss: 0.09177745133638382, ACC:0.96875\n",
      "Training iteration 258 loss: 0.09941910207271576, ACC:0.96875\n",
      "Training iteration 259 loss: 0.11599580198526382, ACC:0.96875\n",
      "Training iteration 260 loss: 0.1489105522632599, ACC:0.953125\n",
      "Training iteration 261 loss: 0.1347326934337616, ACC:0.96875\n",
      "Training iteration 262 loss: 0.010043531656265259, ACC:1.0\n",
      "Training iteration 263 loss: 0.07951010018587112, ACC:0.96875\n",
      "Training iteration 264 loss: 0.03602316230535507, ACC:1.0\n",
      "Training iteration 265 loss: 0.021929187700152397, ACC:1.0\n",
      "Training iteration 266 loss: 0.07584428042173386, ACC:0.96875\n",
      "Training iteration 267 loss: 0.16954943537712097, ACC:0.953125\n",
      "Training iteration 268 loss: 0.10816767811775208, ACC:0.984375\n",
      "Training iteration 269 loss: 0.05305126681923866, ACC:0.984375\n",
      "Training iteration 270 loss: 0.049687471240758896, ACC:0.984375\n",
      "Training iteration 271 loss: 0.0640554279088974, ACC:0.984375\n",
      "Training iteration 272 loss: 0.02607790380716324, ACC:1.0\n",
      "Training iteration 273 loss: 0.04606739431619644, ACC:0.96875\n",
      "Training iteration 274 loss: 0.07961726188659668, ACC:0.96875\n",
      "Training iteration 275 loss: 0.1429806351661682, ACC:0.96875\n",
      "Training iteration 276 loss: 0.019052712246775627, ACC:1.0\n",
      "Training iteration 277 loss: 0.0350433848798275, ACC:0.984375\n",
      "Training iteration 278 loss: 0.024158064275979996, ACC:0.984375\n",
      "Training iteration 279 loss: 0.16633154451847076, ACC:0.96875\n",
      "Training iteration 280 loss: 0.05725323408842087, ACC:0.96875\n",
      "Training iteration 281 loss: 0.014131670817732811, ACC:1.0\n",
      "Training iteration 282 loss: 0.06870774179697037, ACC:0.984375\n",
      "Training iteration 283 loss: 0.01587745174765587, ACC:1.0\n",
      "Training iteration 284 loss: 0.01899767853319645, ACC:1.0\n",
      "Training iteration 285 loss: 0.039749663323163986, ACC:0.984375\n",
      "Training iteration 286 loss: 0.05764918401837349, ACC:0.984375\n",
      "Training iteration 287 loss: 0.1503041535615921, ACC:0.96875\n",
      "Training iteration 288 loss: 0.13977175951004028, ACC:0.953125\n",
      "Training iteration 289 loss: 0.024980178102850914, ACC:1.0\n",
      "Training iteration 290 loss: 0.050829168409109116, ACC:0.96875\n",
      "Training iteration 291 loss: 0.030197469517588615, ACC:0.984375\n",
      "Training iteration 292 loss: 0.047265421599149704, ACC:0.984375\n",
      "Training iteration 293 loss: 0.008442951366305351, ACC:1.0\n",
      "Training iteration 294 loss: 0.1324617862701416, ACC:0.96875\n",
      "Training iteration 295 loss: 0.14813581109046936, ACC:0.96875\n",
      "Training iteration 296 loss: 0.02603854611515999, ACC:0.984375\n",
      "Training iteration 297 loss: 0.010142403654754162, ACC:1.0\n",
      "Training iteration 298 loss: 0.07366751879453659, ACC:0.96875\n",
      "Training iteration 299 loss: 0.08988219499588013, ACC:0.96875\n",
      "Training iteration 300 loss: 0.12762783467769623, ACC:0.953125\n",
      "Training iteration 301 loss: 0.02078850567340851, ACC:0.984375\n",
      "Training iteration 302 loss: 0.02747422829270363, ACC:1.0\n",
      "Training iteration 303 loss: 0.13081279397010803, ACC:0.96875\n",
      "Training iteration 304 loss: 0.0513627789914608, ACC:0.96875\n",
      "Training iteration 305 loss: 0.045943982899188995, ACC:0.984375\n",
      "Training iteration 306 loss: 0.050987690687179565, ACC:0.984375\n",
      "Training iteration 307 loss: 0.07651681452989578, ACC:0.96875\n",
      "Training iteration 308 loss: 0.025721397250890732, ACC:1.0\n",
      "Training iteration 309 loss: 0.07714274525642395, ACC:0.96875\n",
      "Training iteration 310 loss: 0.22020989656448364, ACC:0.875\n",
      "Training iteration 311 loss: 0.07824569940567017, ACC:0.96875\n",
      "Training iteration 312 loss: 0.006145150400698185, ACC:1.0\n",
      "Training iteration 313 loss: 0.08188597112894058, ACC:0.984375\n",
      "Training iteration 314 loss: 0.02151390165090561, ACC:1.0\n",
      "Training iteration 315 loss: 0.10767553746700287, ACC:0.96875\n",
      "Training iteration 316 loss: 0.011207804083824158, ACC:1.0\n",
      "Training iteration 317 loss: 0.016698909923434258, ACC:1.0\n",
      "Training iteration 318 loss: 0.09044323861598969, ACC:0.984375\n",
      "Training iteration 319 loss: 0.006534987594932318, ACC:1.0\n",
      "Training iteration 320 loss: 0.10227950662374496, ACC:0.984375\n",
      "Training iteration 321 loss: 0.15258783102035522, ACC:0.953125\n",
      "Training iteration 322 loss: 0.14197954535484314, ACC:0.953125\n",
      "Training iteration 323 loss: 0.015319482423365116, ACC:1.0\n",
      "Training iteration 324 loss: 0.08222296833992004, ACC:0.96875\n",
      "Training iteration 325 loss: 0.026861414313316345, ACC:1.0\n",
      "Training iteration 326 loss: 0.019159873947501183, ACC:0.984375\n",
      "Training iteration 327 loss: 0.028531013056635857, ACC:1.0\n",
      "Training iteration 328 loss: 0.03450176119804382, ACC:1.0\n",
      "Training iteration 329 loss: 0.03940987586975098, ACC:1.0\n",
      "Training iteration 330 loss: 0.05490102991461754, ACC:0.984375\n",
      "Training iteration 331 loss: 0.061995238065719604, ACC:0.96875\n",
      "Training iteration 332 loss: 0.013778132386505604, ACC:1.0\n",
      "Training iteration 333 loss: 0.016102319583296776, ACC:1.0\n",
      "Training iteration 334 loss: 0.018477169796824455, ACC:1.0\n",
      "Training iteration 335 loss: 0.06732472032308578, ACC:0.984375\n",
      "Training iteration 336 loss: 0.11191567778587341, ACC:0.96875\n",
      "Training iteration 337 loss: 0.05312880873680115, ACC:0.984375\n",
      "Training iteration 338 loss: 0.18648949265480042, ACC:0.9375\n",
      "Training iteration 339 loss: 0.017698951065540314, ACC:1.0\n",
      "Training iteration 340 loss: 0.07782715559005737, ACC:0.96875\n",
      "Training iteration 341 loss: 0.07241014391183853, ACC:0.984375\n",
      "Training iteration 342 loss: 0.041495878249406815, ACC:0.984375\n",
      "Training iteration 343 loss: 0.03076891601085663, ACC:1.0\n",
      "Training iteration 344 loss: 0.1341630220413208, ACC:0.96875\n",
      "Training iteration 345 loss: 0.01893940381705761, ACC:1.0\n",
      "Training iteration 346 loss: 0.06301587074995041, ACC:0.984375\n",
      "Training iteration 347 loss: 0.047762103378772736, ACC:0.984375\n",
      "Training iteration 348 loss: 0.05140642449259758, ACC:0.984375\n",
      "Training iteration 349 loss: 0.04597702994942665, ACC:0.984375\n",
      "Training iteration 350 loss: 0.05491161346435547, ACC:0.984375\n",
      "Training iteration 351 loss: 0.13115784525871277, ACC:0.96875\n",
      "Training iteration 352 loss: 0.0380626916885376, ACC:0.984375\n",
      "Training iteration 353 loss: 0.041519250720739365, ACC:0.96875\n",
      "Training iteration 354 loss: 0.06966321170330048, ACC:0.96875\n",
      "Training iteration 355 loss: 0.005684248171746731, ACC:1.0\n",
      "Training iteration 356 loss: 0.06347589194774628, ACC:0.984375\n",
      "Training iteration 357 loss: 0.012464114464819431, ACC:1.0\n",
      "Training iteration 358 loss: 0.015196872875094414, ACC:1.0\n",
      "Training iteration 359 loss: 0.12557700276374817, ACC:0.96875\n",
      "Training iteration 360 loss: 0.04445032775402069, ACC:0.984375\n",
      "Training iteration 361 loss: 0.022242531180381775, ACC:0.984375\n",
      "Training iteration 362 loss: 0.05896434187889099, ACC:0.984375\n",
      "Training iteration 363 loss: 0.10345160961151123, ACC:0.96875\n",
      "Training iteration 364 loss: 0.20840205252170563, ACC:0.921875\n",
      "Training iteration 365 loss: 0.06948314607143402, ACC:0.984375\n",
      "Training iteration 366 loss: 0.018932726234197617, ACC:1.0\n",
      "Training iteration 367 loss: 0.0946543887257576, ACC:0.96875\n",
      "Training iteration 368 loss: 0.04093291983008385, ACC:0.96875\n",
      "Training iteration 369 loss: 0.05643301457166672, ACC:0.96875\n",
      "Training iteration 370 loss: 0.05836891755461693, ACC:0.96875\n",
      "Training iteration 371 loss: 0.1462174504995346, ACC:0.953125\n",
      "Training iteration 372 loss: 0.13338567316532135, ACC:0.96875\n",
      "Training iteration 373 loss: 0.8632855415344238, ACC:0.953125\n",
      "Training iteration 374 loss: 0.03915746882557869, ACC:1.0\n",
      "Training iteration 375 loss: 0.07453379034996033, ACC:0.984375\n",
      "Training iteration 376 loss: 0.11888080090284348, ACC:0.984375\n",
      "Training iteration 377 loss: 0.0719524547457695, ACC:0.984375\n",
      "Training iteration 378 loss: 0.45131948590278625, ACC:0.875\n",
      "Training iteration 379 loss: 0.1716919094324112, ACC:0.953125\n",
      "Training iteration 380 loss: 0.1576874554157257, ACC:0.9375\n",
      "Training iteration 381 loss: 0.20540325343608856, ACC:0.9375\n",
      "Training iteration 382 loss: 0.15562677383422852, ACC:0.96875\n",
      "Training iteration 383 loss: 0.13858607411384583, ACC:0.984375\n",
      "Training iteration 384 loss: 0.16794192790985107, ACC:0.953125\n",
      "Training iteration 385 loss: 0.10232464969158173, ACC:0.984375\n",
      "Training iteration 386 loss: 0.06470510363578796, ACC:1.0\n",
      "Training iteration 387 loss: 0.1334962099790573, ACC:0.96875\n",
      "Training iteration 388 loss: 0.07977192848920822, ACC:0.984375\n",
      "Training iteration 389 loss: 0.04757041111588478, ACC:1.0\n",
      "Training iteration 390 loss: 0.09960740059614182, ACC:0.96875\n",
      "Training iteration 391 loss: 0.05566229671239853, ACC:0.984375\n",
      "Training iteration 392 loss: 0.09463299065828323, ACC:0.96875\n",
      "Training iteration 393 loss: 0.062242329120635986, ACC:0.984375\n",
      "Training iteration 394 loss: 0.023875193670392036, ACC:0.984375\n",
      "Training iteration 395 loss: 0.034356191754341125, ACC:0.984375\n",
      "Training iteration 396 loss: 0.12664762139320374, ACC:0.96875\n",
      "Training iteration 397 loss: 0.032002560794353485, ACC:0.984375\n",
      "Training iteration 398 loss: 0.0826452299952507, ACC:0.984375\n",
      "Training iteration 399 loss: 0.017877278849482536, ACC:0.984375\n",
      "Training iteration 400 loss: 0.10564973950386047, ACC:0.984375\n",
      "Training iteration 401 loss: 0.04396878927946091, ACC:0.984375\n",
      "Training iteration 402 loss: 0.14745312929153442, ACC:0.96875\n",
      "Training iteration 403 loss: 0.048928402364254, ACC:0.984375\n",
      "Training iteration 404 loss: 0.019768361002206802, ACC:1.0\n",
      "Training iteration 405 loss: 0.06207431107759476, ACC:0.953125\n",
      "Training iteration 406 loss: 0.14136597514152527, ACC:0.9375\n",
      "Training iteration 407 loss: 0.023932337760925293, ACC:0.984375\n",
      "Training iteration 408 loss: 0.06205546110868454, ACC:0.953125\n",
      "Training iteration 409 loss: 0.007482215296477079, ACC:1.0\n",
      "Training iteration 410 loss: 0.03519195318222046, ACC:0.984375\n",
      "Training iteration 411 loss: 0.08485229313373566, ACC:0.9375\n",
      "Training iteration 412 loss: 0.07211561501026154, ACC:0.96875\n",
      "Training iteration 413 loss: 0.007706714794039726, ACC:1.0\n",
      "Training iteration 414 loss: 0.07432060688734055, ACC:0.96875\n",
      "Training iteration 415 loss: 0.0119235310703516, ACC:1.0\n",
      "Training iteration 416 loss: 0.0647466704249382, ACC:0.96875\n",
      "Training iteration 417 loss: 0.07709001749753952, ACC:0.984375\n",
      "Training iteration 418 loss: 0.017451684921979904, ACC:0.984375\n",
      "Training iteration 419 loss: 0.12568548321723938, ACC:0.953125\n",
      "Training iteration 420 loss: 0.20291109383106232, ACC:0.9375\n",
      "Training iteration 421 loss: 0.25229158997535706, ACC:0.90625\n",
      "Training iteration 422 loss: 0.172366201877594, ACC:0.90625\n",
      "Training iteration 423 loss: 0.10273910313844681, ACC:0.9375\n",
      "Training iteration 424 loss: 0.04327471926808357, ACC:0.984375\n",
      "Training iteration 425 loss: 0.014304633252322674, ACC:1.0\n",
      "Training iteration 426 loss: 0.10326326638460159, ACC:0.984375\n",
      "Training iteration 427 loss: 0.04978806897997856, ACC:0.984375\n",
      "Training iteration 428 loss: 0.0037111318670213223, ACC:1.0\n",
      "Training iteration 429 loss: 0.10890256613492966, ACC:0.96875\n",
      "Training iteration 430 loss: 0.027250412851572037, ACC:0.984375\n",
      "Training iteration 431 loss: 0.35598471760749817, ACC:0.921875\n",
      "Training iteration 432 loss: 0.09574784338474274, ACC:0.984375\n",
      "Training iteration 433 loss: 0.05808740854263306, ACC:0.984375\n",
      "Training iteration 434 loss: 0.09731356799602509, ACC:0.984375\n",
      "Training iteration 435 loss: 0.09540235251188278, ACC:0.96875\n",
      "Training iteration 436 loss: 0.05331684648990631, ACC:1.0\n",
      "Training iteration 437 loss: 0.051068197935819626, ACC:0.984375\n",
      "Training iteration 438 loss: 0.18063074350357056, ACC:0.9375\n",
      "Training iteration 439 loss: 0.12378400564193726, ACC:0.953125\n",
      "Training iteration 440 loss: 0.06931786239147186, ACC:0.984375\n",
      "Training iteration 441 loss: 0.06428258121013641, ACC:0.984375\n",
      "Training iteration 442 loss: 0.029370712116360664, ACC:1.0\n",
      "Training iteration 443 loss: 0.1498897224664688, ACC:0.953125\n",
      "Training iteration 444 loss: 0.17711371183395386, ACC:0.953125\n",
      "Training iteration 445 loss: 0.014454036951065063, ACC:1.0\n",
      "Training iteration 446 loss: 0.07327597588300705, ACC:0.96875\n",
      "Training iteration 447 loss: 0.03161676228046417, ACC:0.984375\n",
      "Training iteration 448 loss: 0.06134200841188431, ACC:0.984375\n",
      "Training iteration 449 loss: 0.009473615325987339, ACC:1.0\n",
      "Training iteration 450 loss: 0.022644933313131332, ACC:1.0\n",
      "Validation iteration 451 loss: 0.04978077858686447, ACC: 0.984375\n",
      "Validation iteration 452 loss: 0.03785528987646103, ACC: 0.984375\n",
      "Validation iteration 453 loss: 0.07120789587497711, ACC: 0.984375\n",
      "Validation iteration 454 loss: 0.12836448848247528, ACC: 0.96875\n",
      "Validation iteration 455 loss: 0.16930261254310608, ACC: 0.953125\n",
      "Validation iteration 456 loss: 0.007279256824404001, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.011840442195534706, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.00805121473968029, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.06358451396226883, ACC: 0.96875\n",
      "Validation iteration 460 loss: 0.08950795233249664, ACC: 0.953125\n",
      "Validation iteration 461 loss: 0.05546092242002487, ACC: 0.984375\n",
      "Validation iteration 462 loss: 0.0855024978518486, ACC: 0.96875\n",
      "Validation iteration 463 loss: 0.016790620982646942, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.14551182091236115, ACC: 0.953125\n",
      "Validation iteration 465 loss: 0.21090424060821533, ACC: 0.9375\n",
      "Validation iteration 466 loss: 0.07855266332626343, ACC: 0.96875\n",
      "Validation iteration 467 loss: 0.04368329048156738, ACC: 0.984375\n",
      "Validation iteration 468 loss: 0.06363359838724136, ACC: 0.984375\n",
      "Validation iteration 469 loss: 0.22638818621635437, ACC: 0.921875\n",
      "Validation iteration 470 loss: 0.09399513900279999, ACC: 0.984375\n",
      "Validation iteration 471 loss: 0.04431469738483429, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.03727377578616142, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.010201624594628811, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.04123390465974808, ACC: 0.984375\n",
      "Validation iteration 475 loss: 0.09541677683591843, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.015369558706879616, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.058979135006666183, ACC: 0.984375\n",
      "Validation iteration 478 loss: 0.02300920896232128, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.057181525975465775, ACC: 0.984375\n",
      "Validation iteration 480 loss: 0.023663805797696114, ACC: 0.984375\n",
      "Validation iteration 481 loss: 0.12727344036102295, ACC: 0.96875\n",
      "Validation iteration 482 loss: 0.09111849963665009, ACC: 0.96875\n",
      "Validation iteration 483 loss: 0.10481321066617966, ACC: 0.953125\n",
      "Validation iteration 484 loss: 0.07744105905294418, ACC: 0.96875\n",
      "Validation iteration 485 loss: 0.015389973297715187, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.10327067971229553, ACC: 0.96875\n",
      "Validation iteration 487 loss: 0.06300622969865799, ACC: 0.984375\n",
      "Validation iteration 488 loss: 0.01961962878704071, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.24089959263801575, ACC: 0.9375\n",
      "Validation iteration 490 loss: 0.12512525916099548, ACC: 0.9375\n",
      "Validation iteration 491 loss: 0.10397309064865112, ACC: 0.953125\n",
      "Validation iteration 492 loss: 0.080438531935215, ACC: 0.96875\n",
      "Validation iteration 493 loss: 0.013861937448382378, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.0540241077542305, ACC: 0.984375\n",
      "Validation iteration 495 loss: 0.15360912680625916, ACC: 0.96875\n",
      "Validation iteration 496 loss: 0.06322163343429565, ACC: 0.984375\n",
      "Validation iteration 497 loss: 0.012125029228627682, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.03647137060761452, ACC: 0.984375\n",
      "Validation iteration 499 loss: 0.019051996991038322, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.1293077915906906, ACC: 0.953125\n",
      "-- Epoch 1 done -- Train loss: 0.191282651077749, train ACC: 0.9114930555555556, val loss: 0.07395767257548869, val ACC: 0.976875\n",
      "<--- 6914.179730176926 seconds --->\n",
      "Training iteration 1 loss: 0.10819391161203384, ACC:0.984375\n",
      "Training iteration 2 loss: 0.11481013894081116, ACC:0.953125\n",
      "Training iteration 3 loss: 0.0966765359044075, ACC:0.96875\n",
      "Training iteration 4 loss: 0.02095649391412735, ACC:1.0\n",
      "Training iteration 5 loss: 0.009102403186261654, ACC:1.0\n",
      "Training iteration 6 loss: 0.021804746240377426, ACC:0.984375\n",
      "Training iteration 7 loss: 0.01857038028538227, ACC:1.0\n",
      "Training iteration 8 loss: 0.03417236730456352, ACC:0.984375\n",
      "Training iteration 9 loss: 0.1203208938241005, ACC:0.96875\n",
      "Training iteration 10 loss: 0.011841454543173313, ACC:1.0\n",
      "Training iteration 11 loss: 0.09698312729597092, ACC:0.96875\n",
      "Training iteration 12 loss: 0.07918953895568848, ACC:0.984375\n",
      "Training iteration 13 loss: 0.11727012693881989, ACC:0.9375\n",
      "Training iteration 14 loss: 0.04701923578977585, ACC:0.96875\n",
      "Training iteration 15 loss: 0.007346234284341335, ACC:1.0\n",
      "Training iteration 16 loss: 0.12179093807935715, ACC:0.984375\n",
      "Training iteration 17 loss: 0.09333036094903946, ACC:0.984375\n",
      "Training iteration 18 loss: 0.17350545525550842, ACC:0.953125\n",
      "Training iteration 19 loss: 0.03424815833568573, ACC:0.984375\n",
      "Training iteration 20 loss: 0.08075527101755142, ACC:0.984375\n",
      "Training iteration 21 loss: 0.010260575450956821, ACC:1.0\n",
      "Training iteration 22 loss: 0.06818760186433792, ACC:0.96875\n",
      "Training iteration 23 loss: 0.06597502529621124, ACC:0.96875\n",
      "Training iteration 24 loss: 0.016511697322130203, ACC:1.0\n",
      "Training iteration 25 loss: 0.06373929977416992, ACC:0.96875\n",
      "Training iteration 26 loss: 0.015769997611641884, ACC:1.0\n",
      "Training iteration 27 loss: 0.04510646313428879, ACC:0.96875\n",
      "Training iteration 28 loss: 0.011081565171480179, ACC:1.0\n",
      "Training iteration 29 loss: 0.03707123547792435, ACC:1.0\n",
      "Training iteration 30 loss: 0.027411814779043198, ACC:0.984375\n",
      "Training iteration 31 loss: 0.01825641840696335, ACC:1.0\n",
      "Training iteration 32 loss: 0.026872873306274414, ACC:0.984375\n",
      "Training iteration 33 loss: 0.004978037439286709, ACC:1.0\n",
      "Training iteration 34 loss: 0.07653262466192245, ACC:0.953125\n",
      "Training iteration 35 loss: 0.07655618339776993, ACC:0.984375\n",
      "Training iteration 36 loss: 0.004863441921770573, ACC:1.0\n",
      "Training iteration 37 loss: 0.026722433045506477, ACC:1.0\n",
      "Training iteration 38 loss: 0.03848521038889885, ACC:0.96875\n",
      "Training iteration 39 loss: 0.060444772243499756, ACC:0.984375\n",
      "Training iteration 40 loss: 0.030674239620566368, ACC:0.984375\n",
      "Training iteration 41 loss: 0.03426358476281166, ACC:0.984375\n",
      "Training iteration 42 loss: 0.06387246400117874, ACC:0.984375\n",
      "Training iteration 43 loss: 0.0024610760156065226, ACC:1.0\n",
      "Training iteration 44 loss: 0.005883697886019945, ACC:1.0\n",
      "Training iteration 45 loss: 0.06488998234272003, ACC:0.984375\n",
      "Training iteration 46 loss: 0.07035551965236664, ACC:0.984375\n",
      "Training iteration 47 loss: 0.057379502803087234, ACC:0.96875\n",
      "Training iteration 48 loss: 0.03126431629061699, ACC:0.984375\n",
      "Training iteration 49 loss: 0.10145878791809082, ACC:0.953125\n",
      "Training iteration 50 loss: 0.03397157043218613, ACC:1.0\n",
      "Training iteration 51 loss: 0.047481782734394073, ACC:0.96875\n",
      "Training iteration 52 loss: 0.016884736716747284, ACC:1.0\n",
      "Training iteration 53 loss: 0.027289552614092827, ACC:0.984375\n",
      "Training iteration 54 loss: 0.004382078070193529, ACC:1.0\n",
      "Training iteration 55 loss: 0.02618514932692051, ACC:0.984375\n",
      "Training iteration 56 loss: 0.14511306583881378, ACC:0.96875\n",
      "Training iteration 57 loss: 0.031024998053908348, ACC:0.984375\n",
      "Training iteration 58 loss: 0.07900051772594452, ACC:0.984375\n",
      "Training iteration 59 loss: 0.04456141218543053, ACC:0.984375\n",
      "Training iteration 60 loss: 0.007361524738371372, ACC:1.0\n",
      "Training iteration 61 loss: 0.09624142944812775, ACC:0.96875\n",
      "Training iteration 62 loss: 0.0359390489757061, ACC:0.984375\n",
      "Training iteration 63 loss: 0.08446573466062546, ACC:0.953125\n",
      "Training iteration 64 loss: 0.012899641878902912, ACC:1.0\n",
      "Training iteration 65 loss: 0.04166536405682564, ACC:0.984375\n",
      "Training iteration 66 loss: 0.11642202734947205, ACC:0.96875\n",
      "Training iteration 67 loss: 0.07866507768630981, ACC:0.953125\n",
      "Training iteration 68 loss: 0.04721389710903168, ACC:0.96875\n",
      "Training iteration 69 loss: 0.05395248904824257, ACC:0.984375\n",
      "Training iteration 70 loss: 0.03750676289200783, ACC:0.984375\n",
      "Training iteration 71 loss: 0.05930175632238388, ACC:0.984375\n",
      "Training iteration 72 loss: 0.011592762544751167, ACC:1.0\n",
      "Training iteration 73 loss: 0.02720983885228634, ACC:0.984375\n",
      "Training iteration 74 loss: 0.04813455417752266, ACC:0.984375\n",
      "Training iteration 75 loss: 0.006770925596356392, ACC:1.0\n",
      "Training iteration 76 loss: 0.016034185886383057, ACC:1.0\n",
      "Training iteration 77 loss: 0.05985890328884125, ACC:0.96875\n",
      "Training iteration 78 loss: 0.004468273371458054, ACC:1.0\n",
      "Training iteration 79 loss: 0.06183002516627312, ACC:0.984375\n",
      "Training iteration 80 loss: 0.01964043453335762, ACC:0.984375\n",
      "Training iteration 81 loss: 0.03778534010052681, ACC:1.0\n",
      "Training iteration 82 loss: 0.055144309997558594, ACC:0.984375\n",
      "Training iteration 83 loss: 0.11793629825115204, ACC:0.984375\n",
      "Training iteration 84 loss: 0.02895917370915413, ACC:0.984375\n",
      "Training iteration 85 loss: 0.04539185389876366, ACC:0.96875\n",
      "Training iteration 86 loss: 0.048430006951093674, ACC:0.984375\n",
      "Training iteration 87 loss: 0.11996719241142273, ACC:0.953125\n",
      "Training iteration 88 loss: 0.08937399834394455, ACC:0.96875\n",
      "Training iteration 89 loss: 0.027367256581783295, ACC:0.984375\n",
      "Training iteration 90 loss: 0.02909659966826439, ACC:0.984375\n",
      "Training iteration 91 loss: 0.0181124210357666, ACC:0.984375\n",
      "Training iteration 92 loss: 0.035422902554273605, ACC:0.984375\n",
      "Training iteration 93 loss: 0.03775038942694664, ACC:0.984375\n",
      "Training iteration 94 loss: 0.10743903368711472, ACC:0.953125\n",
      "Training iteration 95 loss: 0.024234924465417862, ACC:0.984375\n",
      "Training iteration 96 loss: 0.005625632591545582, ACC:1.0\n",
      "Training iteration 97 loss: 0.04177999496459961, ACC:0.96875\n",
      "Training iteration 98 loss: 0.014887028373777866, ACC:1.0\n",
      "Training iteration 99 loss: 0.03641415014863014, ACC:1.0\n",
      "Training iteration 100 loss: 0.0409468375146389, ACC:0.984375\n",
      "Training iteration 101 loss: 0.013155106455087662, ACC:1.0\n",
      "Training iteration 102 loss: 0.0035731494426727295, ACC:1.0\n",
      "Training iteration 103 loss: 0.07528500258922577, ACC:0.96875\n",
      "Training iteration 104 loss: 0.0687284991145134, ACC:0.96875\n",
      "Training iteration 105 loss: 0.019604099914431572, ACC:1.0\n",
      "Training iteration 106 loss: 0.009925799444317818, ACC:1.0\n",
      "Training iteration 107 loss: 0.06280654668807983, ACC:0.984375\n",
      "Training iteration 108 loss: 0.13044635951519012, ACC:0.96875\n",
      "Training iteration 109 loss: 0.027277523651719093, ACC:1.0\n",
      "Training iteration 110 loss: 0.024019023403525352, ACC:1.0\n",
      "Training iteration 111 loss: 0.019281478598713875, ACC:1.0\n",
      "Training iteration 112 loss: 0.02884759195148945, ACC:1.0\n",
      "Training iteration 113 loss: 0.06572161614894867, ACC:0.96875\n",
      "Training iteration 114 loss: 0.01464622188359499, ACC:0.984375\n",
      "Training iteration 115 loss: 0.03834537789225578, ACC:0.984375\n",
      "Training iteration 116 loss: 0.00902631226927042, ACC:1.0\n",
      "Training iteration 117 loss: 0.07956705242395401, ACC:0.984375\n",
      "Training iteration 118 loss: 0.024849195033311844, ACC:1.0\n",
      "Training iteration 119 loss: 0.09556686878204346, ACC:0.984375\n",
      "Training iteration 120 loss: 0.11508502066135406, ACC:0.96875\n",
      "Training iteration 121 loss: 0.04600056633353233, ACC:0.984375\n",
      "Training iteration 122 loss: 0.005828564520925283, ACC:1.0\n",
      "Training iteration 123 loss: 0.023698560893535614, ACC:0.984375\n",
      "Training iteration 124 loss: 0.14537794888019562, ACC:0.984375\n",
      "Training iteration 125 loss: 0.011811547912657261, ACC:1.0\n",
      "Training iteration 126 loss: 0.02894309163093567, ACC:0.96875\n",
      "Training iteration 127 loss: 0.028542116284370422, ACC:1.0\n",
      "Training iteration 128 loss: 0.04256633669137955, ACC:0.984375\n",
      "Training iteration 129 loss: 0.05005250871181488, ACC:0.984375\n",
      "Training iteration 130 loss: 0.02171783521771431, ACC:1.0\n",
      "Training iteration 131 loss: 0.014149002730846405, ACC:1.0\n",
      "Training iteration 132 loss: 0.015785029157996178, ACC:1.0\n",
      "Training iteration 133 loss: 0.059115562587976456, ACC:0.96875\n",
      "Training iteration 134 loss: 0.0030996755231171846, ACC:1.0\n",
      "Training iteration 135 loss: 0.10872092097997665, ACC:0.96875\n",
      "Training iteration 136 loss: 0.09346774220466614, ACC:0.96875\n",
      "Training iteration 137 loss: 0.09887408465147018, ACC:0.984375\n",
      "Training iteration 138 loss: 0.03719981759786606, ACC:0.984375\n",
      "Training iteration 139 loss: 0.03902672976255417, ACC:0.984375\n",
      "Training iteration 140 loss: 0.06835320591926575, ACC:0.96875\n",
      "Training iteration 141 loss: 0.048659369349479675, ACC:0.96875\n",
      "Training iteration 142 loss: 0.0598968081176281, ACC:0.984375\n",
      "Training iteration 143 loss: 0.09405934810638428, ACC:0.96875\n",
      "Training iteration 144 loss: 0.09045133739709854, ACC:1.0\n",
      "Training iteration 145 loss: 0.10577717423439026, ACC:0.96875\n",
      "Training iteration 146 loss: 0.04744551703333855, ACC:1.0\n",
      "Training iteration 147 loss: 0.038467250764369965, ACC:0.96875\n",
      "Training iteration 148 loss: 0.07867074012756348, ACC:0.984375\n",
      "Training iteration 149 loss: 0.02055647410452366, ACC:1.0\n",
      "Training iteration 150 loss: 0.013350400142371655, ACC:1.0\n",
      "Training iteration 151 loss: 0.03503384441137314, ACC:0.984375\n",
      "Training iteration 152 loss: 0.16607220470905304, ACC:0.953125\n",
      "Training iteration 153 loss: 0.03717369586229324, ACC:0.984375\n",
      "Training iteration 154 loss: 0.07188963145017624, ACC:0.96875\n",
      "Training iteration 155 loss: 0.004962548613548279, ACC:1.0\n",
      "Training iteration 156 loss: 0.044818390160799026, ACC:0.984375\n",
      "Training iteration 157 loss: 0.12943625450134277, ACC:0.984375\n",
      "Training iteration 158 loss: 0.014327846467494965, ACC:1.0\n",
      "Training iteration 159 loss: 0.03482877463102341, ACC:0.984375\n",
      "Training iteration 160 loss: 0.002806626260280609, ACC:1.0\n",
      "Training iteration 161 loss: 0.02238389477133751, ACC:1.0\n",
      "Training iteration 162 loss: 0.08273770660161972, ACC:0.984375\n",
      "Training iteration 163 loss: 0.05610136315226555, ACC:0.96875\n",
      "Training iteration 164 loss: 0.018779130652546883, ACC:1.0\n",
      "Training iteration 165 loss: 0.025225941091775894, ACC:1.0\n",
      "Training iteration 166 loss: 0.09890525788068771, ACC:0.984375\n",
      "Training iteration 167 loss: 0.03411028906702995, ACC:0.984375\n",
      "Training iteration 168 loss: 0.030699100345373154, ACC:0.984375\n",
      "Training iteration 169 loss: 0.022253895178437233, ACC:0.984375\n",
      "Training iteration 170 loss: 0.01130596362054348, ACC:1.0\n",
      "Training iteration 171 loss: 0.011651457287371159, ACC:1.0\n",
      "Training iteration 172 loss: 0.05895436182618141, ACC:0.984375\n",
      "Training iteration 173 loss: 0.016303589567542076, ACC:1.0\n",
      "Training iteration 174 loss: 0.048469625413417816, ACC:0.984375\n",
      "Training iteration 175 loss: 0.03284933418035507, ACC:0.984375\n",
      "Training iteration 176 loss: 0.008085910230875015, ACC:1.0\n",
      "Training iteration 177 loss: 0.05310700461268425, ACC:0.984375\n",
      "Training iteration 178 loss: 0.019463811069726944, ACC:1.0\n",
      "Training iteration 179 loss: 0.02242795005440712, ACC:0.984375\n",
      "Training iteration 180 loss: 0.031211555004119873, ACC:0.984375\n",
      "Training iteration 181 loss: 0.012331158854067326, ACC:1.0\n",
      "Training iteration 182 loss: 0.029519177973270416, ACC:0.984375\n",
      "Training iteration 183 loss: 0.010459915734827518, ACC:1.0\n",
      "Training iteration 184 loss: 0.008144989609718323, ACC:1.0\n",
      "Training iteration 185 loss: 0.0817435011267662, ACC:0.984375\n",
      "Training iteration 186 loss: 0.0727660208940506, ACC:0.953125\n",
      "Training iteration 187 loss: 0.0674213394522667, ACC:0.984375\n",
      "Training iteration 188 loss: 0.03808077797293663, ACC:0.984375\n",
      "Training iteration 189 loss: 0.06507082283496857, ACC:0.96875\n",
      "Training iteration 190 loss: 0.20689281821250916, ACC:0.953125\n",
      "Training iteration 191 loss: 0.012064137496054173, ACC:1.0\n",
      "Training iteration 192 loss: 0.04308074712753296, ACC:0.984375\n",
      "Training iteration 193 loss: 0.022601205855607986, ACC:1.0\n",
      "Training iteration 194 loss: 0.04377484694123268, ACC:0.953125\n",
      "Training iteration 195 loss: 0.03214253857731819, ACC:1.0\n",
      "Training iteration 196 loss: 0.02956988848745823, ACC:1.0\n",
      "Training iteration 197 loss: 0.040390655398368835, ACC:1.0\n",
      "Training iteration 198 loss: 0.07362262904644012, ACC:0.953125\n",
      "Training iteration 199 loss: 0.023238010704517365, ACC:1.0\n",
      "Training iteration 200 loss: 0.025468112900853157, ACC:1.0\n",
      "Training iteration 201 loss: 0.008359573781490326, ACC:1.0\n",
      "Training iteration 202 loss: 0.019001688808202744, ACC:1.0\n",
      "Training iteration 203 loss: 0.021920712664723396, ACC:1.0\n",
      "Training iteration 204 loss: 0.008864331990480423, ACC:1.0\n",
      "Training iteration 205 loss: 0.002654343144968152, ACC:1.0\n",
      "Training iteration 206 loss: 0.039698269218206406, ACC:0.984375\n",
      "Training iteration 207 loss: 0.1404145509004593, ACC:0.96875\n",
      "Training iteration 208 loss: 0.011197533458471298, ACC:1.0\n",
      "Training iteration 209 loss: 0.02729465253651142, ACC:0.984375\n",
      "Training iteration 210 loss: 0.009876394644379616, ACC:1.0\n",
      "Training iteration 211 loss: 0.08788882941007614, ACC:0.984375\n",
      "Training iteration 212 loss: 0.022628432139754295, ACC:0.984375\n",
      "Training iteration 213 loss: 0.013789934106171131, ACC:0.984375\n",
      "Training iteration 214 loss: 0.07523449510335922, ACC:0.96875\n",
      "Training iteration 215 loss: 0.0878986120223999, ACC:0.96875\n",
      "Training iteration 216 loss: 0.017335208132863045, ACC:1.0\n",
      "Training iteration 217 loss: 0.052359651774168015, ACC:0.984375\n",
      "Training iteration 218 loss: 0.04500710591673851, ACC:0.96875\n",
      "Training iteration 219 loss: 0.03633465990424156, ACC:0.984375\n",
      "Training iteration 220 loss: 0.02060503140091896, ACC:1.0\n",
      "Training iteration 221 loss: 0.002331920899450779, ACC:1.0\n",
      "Training iteration 222 loss: 0.09649249911308289, ACC:0.984375\n",
      "Training iteration 223 loss: 0.050631094723939896, ACC:0.984375\n",
      "Training iteration 224 loss: 0.03876786306500435, ACC:0.984375\n",
      "Training iteration 225 loss: 0.11007063835859299, ACC:0.953125\n",
      "Training iteration 226 loss: 0.02039301209151745, ACC:1.0\n",
      "Training iteration 227 loss: 0.06288959085941315, ACC:0.984375\n",
      "Training iteration 228 loss: 0.014072677120566368, ACC:0.984375\n",
      "Training iteration 229 loss: 0.008213446475565434, ACC:1.0\n",
      "Training iteration 230 loss: 0.06241095811128616, ACC:0.953125\n",
      "Training iteration 231 loss: 0.006217075511813164, ACC:1.0\n",
      "Training iteration 232 loss: 0.07745885103940964, ACC:0.984375\n",
      "Training iteration 233 loss: 0.07027047127485275, ACC:0.96875\n",
      "Training iteration 234 loss: 0.024513129144906998, ACC:1.0\n",
      "Training iteration 235 loss: 0.0674256756901741, ACC:0.984375\n",
      "Training iteration 236 loss: 0.060008808970451355, ACC:0.984375\n",
      "Training iteration 237 loss: 0.01435589138418436, ACC:1.0\n",
      "Training iteration 238 loss: 0.009149659425020218, ACC:1.0\n",
      "Training iteration 239 loss: 0.013813003897666931, ACC:1.0\n",
      "Training iteration 240 loss: 0.10153163969516754, ACC:0.96875\n",
      "Training iteration 241 loss: 0.09934848546981812, ACC:0.96875\n",
      "Training iteration 242 loss: 0.03205239772796631, ACC:0.984375\n",
      "Training iteration 243 loss: 0.13770127296447754, ACC:0.96875\n",
      "Training iteration 244 loss: 0.017439480870962143, ACC:1.0\n",
      "Training iteration 245 loss: 0.013981172814965248, ACC:1.0\n",
      "Training iteration 246 loss: 0.04454997181892395, ACC:0.984375\n",
      "Training iteration 247 loss: 0.06485845893621445, ACC:0.984375\n",
      "Training iteration 248 loss: 0.006771543994545937, ACC:1.0\n",
      "Training iteration 249 loss: 0.07953407615423203, ACC:0.953125\n",
      "Training iteration 250 loss: 0.003753119148313999, ACC:1.0\n",
      "Training iteration 251 loss: 0.005843710154294968, ACC:1.0\n",
      "Training iteration 252 loss: 0.0024584224447607994, ACC:1.0\n",
      "Training iteration 253 loss: 0.011046738363802433, ACC:1.0\n",
      "Training iteration 254 loss: 0.012116155587136745, ACC:1.0\n",
      "Training iteration 255 loss: 0.01409677229821682, ACC:0.984375\n",
      "Training iteration 256 loss: 0.07133322954177856, ACC:0.953125\n",
      "Training iteration 257 loss: 0.03929445520043373, ACC:0.96875\n",
      "Training iteration 258 loss: 0.02719186060130596, ACC:0.984375\n",
      "Training iteration 259 loss: 0.13762667775154114, ACC:0.984375\n",
      "Training iteration 260 loss: 0.0008075283840298653, ACC:1.0\n",
      "Training iteration 261 loss: 0.03738890588283539, ACC:0.984375\n",
      "Training iteration 262 loss: 0.015398665331304073, ACC:1.0\n",
      "Training iteration 263 loss: 0.00706943916156888, ACC:1.0\n",
      "Training iteration 264 loss: 0.07985027879476547, ACC:0.96875\n",
      "Training iteration 265 loss: 0.0740862712264061, ACC:0.984375\n",
      "Training iteration 266 loss: 0.061076145619153976, ACC:0.96875\n",
      "Training iteration 267 loss: 0.01928255707025528, ACC:1.0\n",
      "Training iteration 268 loss: 0.03246617689728737, ACC:0.984375\n",
      "Training iteration 269 loss: 0.03515607491135597, ACC:0.984375\n",
      "Training iteration 270 loss: 0.03390844166278839, ACC:1.0\n",
      "Training iteration 271 loss: 0.025062819942831993, ACC:1.0\n",
      "Training iteration 272 loss: 0.03178168088197708, ACC:0.984375\n",
      "Training iteration 273 loss: 0.007165177725255489, ACC:1.0\n",
      "Training iteration 274 loss: 0.018265631049871445, ACC:1.0\n",
      "Training iteration 275 loss: 0.013822821900248528, ACC:1.0\n",
      "Training iteration 276 loss: 0.02583502046763897, ACC:0.984375\n",
      "Training iteration 277 loss: 0.02024594508111477, ACC:0.984375\n",
      "Training iteration 278 loss: 0.006493468768894672, ACC:1.0\n",
      "Training iteration 279 loss: 0.2286398559808731, ACC:0.9375\n",
      "Training iteration 280 loss: 0.019679730758070946, ACC:0.984375\n",
      "Training iteration 281 loss: 0.012117759324610233, ACC:1.0\n",
      "Training iteration 282 loss: 0.03204590082168579, ACC:0.984375\n",
      "Training iteration 283 loss: 0.021248428151011467, ACC:1.0\n",
      "Training iteration 284 loss: 0.02797423116862774, ACC:0.984375\n",
      "Training iteration 285 loss: 0.006376337260007858, ACC:1.0\n",
      "Training iteration 286 loss: 0.018093759194016457, ACC:1.0\n",
      "Training iteration 287 loss: 0.007493051700294018, ACC:1.0\n",
      "Training iteration 288 loss: 0.05082166567444801, ACC:0.96875\n",
      "Training iteration 289 loss: 0.014974997378885746, ACC:1.0\n",
      "Training iteration 290 loss: 0.08439014106988907, ACC:0.984375\n",
      "Training iteration 291 loss: 0.020506910979747772, ACC:1.0\n",
      "Training iteration 292 loss: 0.02479841187596321, ACC:0.984375\n",
      "Training iteration 293 loss: 0.00994431134313345, ACC:1.0\n",
      "Training iteration 294 loss: 0.019505413249135017, ACC:1.0\n",
      "Training iteration 295 loss: 0.03413156047463417, ACC:0.96875\n",
      "Training iteration 296 loss: 0.0645163282752037, ACC:0.96875\n",
      "Training iteration 297 loss: 0.016524886712431908, ACC:1.0\n",
      "Training iteration 298 loss: 0.14126069843769073, ACC:0.953125\n",
      "Training iteration 299 loss: 0.020820004865527153, ACC:0.984375\n",
      "Training iteration 300 loss: 0.011431989260017872, ACC:1.0\n",
      "Training iteration 301 loss: 0.04520554095506668, ACC:0.984375\n",
      "Training iteration 302 loss: 0.026597963646054268, ACC:0.984375\n",
      "Training iteration 303 loss: 0.1199115589261055, ACC:0.984375\n",
      "Training iteration 304 loss: 0.002806795295327902, ACC:1.0\n",
      "Training iteration 305 loss: 0.01652516983449459, ACC:1.0\n",
      "Training iteration 306 loss: 0.00307845463976264, ACC:1.0\n",
      "Training iteration 307 loss: 0.002725837752223015, ACC:1.0\n",
      "Training iteration 308 loss: 0.02644454315304756, ACC:1.0\n",
      "Training iteration 309 loss: 0.008051702752709389, ACC:1.0\n",
      "Training iteration 310 loss: 0.18159513175487518, ACC:0.953125\n",
      "Training iteration 311 loss: 0.00856123398989439, ACC:1.0\n",
      "Training iteration 312 loss: 0.03477161005139351, ACC:0.984375\n",
      "Training iteration 313 loss: 0.145396888256073, ACC:0.984375\n",
      "Training iteration 314 loss: 0.04238217696547508, ACC:0.984375\n",
      "Training iteration 315 loss: 0.006646533962339163, ACC:1.0\n",
      "Training iteration 316 loss: 0.08957413583993912, ACC:0.984375\n",
      "Training iteration 317 loss: 0.023311369121074677, ACC:0.984375\n",
      "Training iteration 318 loss: 0.028465794399380684, ACC:0.984375\n",
      "Training iteration 319 loss: 0.0506657212972641, ACC:0.984375\n",
      "Training iteration 320 loss: 0.04038039594888687, ACC:0.984375\n",
      "Training iteration 321 loss: 0.014033101499080658, ACC:1.0\n",
      "Training iteration 322 loss: 0.010088576003909111, ACC:1.0\n",
      "Training iteration 323 loss: 0.014881877228617668, ACC:1.0\n",
      "Training iteration 324 loss: 0.010768930427730083, ACC:1.0\n",
      "Training iteration 325 loss: 0.2364400327205658, ACC:0.9375\n",
      "Training iteration 326 loss: 0.025475097820162773, ACC:1.0\n",
      "Training iteration 327 loss: 0.05977534130215645, ACC:0.984375\n",
      "Training iteration 328 loss: 0.016791081055998802, ACC:1.0\n",
      "Training iteration 329 loss: 0.032481323927640915, ACC:0.984375\n",
      "Training iteration 330 loss: 0.03080661967396736, ACC:0.984375\n",
      "Training iteration 331 loss: 0.008726687170565128, ACC:1.0\n",
      "Training iteration 332 loss: 0.06310570240020752, ACC:0.984375\n",
      "Training iteration 333 loss: 0.082264743745327, ACC:0.96875\n",
      "Training iteration 334 loss: 0.08060146868228912, ACC:0.953125\n",
      "Training iteration 335 loss: 0.058413147926330566, ACC:0.96875\n",
      "Training iteration 336 loss: 0.017824044451117516, ACC:1.0\n",
      "Training iteration 337 loss: 0.004925081040710211, ACC:1.0\n",
      "Training iteration 338 loss: 0.009767312556505203, ACC:1.0\n",
      "Training iteration 339 loss: 0.02003125660121441, ACC:0.984375\n",
      "Training iteration 340 loss: 0.0440400056540966, ACC:0.984375\n",
      "Training iteration 341 loss: 0.04386201500892639, ACC:0.984375\n",
      "Training iteration 342 loss: 0.1516665369272232, ACC:0.96875\n",
      "Training iteration 343 loss: 0.04606581851840019, ACC:0.96875\n",
      "Training iteration 344 loss: 0.04585951566696167, ACC:0.96875\n",
      "Training iteration 345 loss: 0.1997116357088089, ACC:0.953125\n",
      "Training iteration 346 loss: 0.019846724346280098, ACC:1.0\n",
      "Training iteration 347 loss: 0.03219907358288765, ACC:0.984375\n",
      "Training iteration 348 loss: 0.003934331703931093, ACC:1.0\n",
      "Training iteration 349 loss: 0.029950041323900223, ACC:1.0\n",
      "Training iteration 350 loss: 0.04201798513531685, ACC:0.984375\n",
      "Training iteration 351 loss: 0.18727080523967743, ACC:0.953125\n",
      "Training iteration 352 loss: 0.08723240345716476, ACC:0.96875\n",
      "Training iteration 353 loss: 0.038649577647447586, ACC:0.984375\n",
      "Training iteration 354 loss: 0.03590003401041031, ACC:1.0\n",
      "Training iteration 355 loss: 0.04243814945220947, ACC:1.0\n",
      "Training iteration 356 loss: 0.07676628232002258, ACC:0.984375\n",
      "Training iteration 357 loss: 0.12042693048715591, ACC:0.96875\n",
      "Training iteration 358 loss: 0.021864227950572968, ACC:1.0\n",
      "Training iteration 359 loss: 0.0162935983389616, ACC:1.0\n",
      "Training iteration 360 loss: 0.05969884991645813, ACC:0.96875\n",
      "Training iteration 361 loss: 0.043105583637952805, ACC:0.984375\n",
      "Training iteration 362 loss: 0.010950310155749321, ACC:1.0\n",
      "Training iteration 363 loss: 0.1124088242650032, ACC:0.984375\n",
      "Training iteration 364 loss: 0.05237681418657303, ACC:0.984375\n",
      "Training iteration 365 loss: 0.010346912778913975, ACC:1.0\n",
      "Training iteration 366 loss: 0.1037057563662529, ACC:0.984375\n",
      "Training iteration 367 loss: 0.03936028108000755, ACC:0.984375\n",
      "Training iteration 368 loss: 0.04142773523926735, ACC:0.984375\n",
      "Training iteration 369 loss: 0.06129075214266777, ACC:0.984375\n",
      "Training iteration 370 loss: 0.027874993160367012, ACC:0.984375\n",
      "Training iteration 371 loss: 0.005932684056460857, ACC:1.0\n",
      "Training iteration 372 loss: 0.012867801822721958, ACC:1.0\n",
      "Training iteration 373 loss: 0.006087964866310358, ACC:1.0\n",
      "Training iteration 374 loss: 0.018074100837111473, ACC:1.0\n",
      "Training iteration 375 loss: 0.004377765581011772, ACC:1.0\n",
      "Training iteration 376 loss: 0.004520791117101908, ACC:1.0\n",
      "Training iteration 377 loss: 0.048809681087732315, ACC:0.984375\n",
      "Training iteration 378 loss: 0.007936338894069195, ACC:1.0\n",
      "Training iteration 379 loss: 0.005331350024789572, ACC:1.0\n",
      "Training iteration 380 loss: 0.008691973052918911, ACC:1.0\n",
      "Training iteration 381 loss: 0.006008686497807503, ACC:1.0\n",
      "Training iteration 382 loss: 0.0034160250797867775, ACC:1.0\n",
      "Training iteration 383 loss: 0.039005130529403687, ACC:0.984375\n",
      "Training iteration 384 loss: 0.21576696634292603, ACC:0.953125\n",
      "Training iteration 385 loss: 0.007884294725954533, ACC:1.0\n",
      "Training iteration 386 loss: 0.09888152033090591, ACC:0.984375\n",
      "Training iteration 387 loss: 0.024983691051602364, ACC:0.984375\n",
      "Training iteration 388 loss: 0.07658065110445023, ACC:0.984375\n",
      "Training iteration 389 loss: 0.11610020697116852, ACC:0.953125\n",
      "Training iteration 390 loss: 0.03880176320672035, ACC:0.984375\n",
      "Training iteration 391 loss: 0.03743533045053482, ACC:0.984375\n",
      "Training iteration 392 loss: 0.019048402085900307, ACC:1.0\n",
      "Training iteration 393 loss: 0.05003252625465393, ACC:0.984375\n",
      "Training iteration 394 loss: 0.1542452722787857, ACC:0.984375\n",
      "Training iteration 395 loss: 0.004655920900404453, ACC:1.0\n",
      "Training iteration 396 loss: 0.02032351680099964, ACC:1.0\n",
      "Training iteration 397 loss: 0.02054072543978691, ACC:1.0\n",
      "Training iteration 398 loss: 0.04022228717803955, ACC:0.984375\n",
      "Training iteration 399 loss: 0.013036024756729603, ACC:1.0\n",
      "Training iteration 400 loss: 0.31987085938453674, ACC:0.96875\n",
      "Training iteration 401 loss: 0.00431481096893549, ACC:1.0\n",
      "Training iteration 402 loss: 0.015830371528863907, ACC:1.0\n",
      "Training iteration 403 loss: 0.08963172137737274, ACC:0.96875\n",
      "Training iteration 404 loss: 0.04420637711882591, ACC:0.96875\n",
      "Training iteration 405 loss: 0.07038655132055283, ACC:0.984375\n",
      "Training iteration 406 loss: 0.014502930454909801, ACC:1.0\n",
      "Training iteration 407 loss: 0.012764636427164078, ACC:1.0\n",
      "Training iteration 408 loss: 0.04190712422132492, ACC:0.984375\n",
      "Training iteration 409 loss: 0.011799579486250877, ACC:1.0\n",
      "Training iteration 410 loss: 0.0285017192363739, ACC:0.984375\n",
      "Training iteration 411 loss: 0.02045316994190216, ACC:1.0\n",
      "Training iteration 412 loss: 0.05363994836807251, ACC:0.96875\n",
      "Training iteration 413 loss: 0.01084912195801735, ACC:1.0\n",
      "Training iteration 414 loss: 0.03617720678448677, ACC:0.984375\n",
      "Training iteration 415 loss: 0.017918480560183525, ACC:0.984375\n",
      "Training iteration 416 loss: 0.02397748827934265, ACC:1.0\n",
      "Training iteration 417 loss: 0.015716716647148132, ACC:1.0\n",
      "Training iteration 418 loss: 0.048033133149147034, ACC:0.96875\n",
      "Training iteration 419 loss: 0.04805220291018486, ACC:0.984375\n",
      "Training iteration 420 loss: 0.018644580617547035, ACC:1.0\n",
      "Training iteration 421 loss: 0.005512971896678209, ACC:1.0\n",
      "Training iteration 422 loss: 0.034027911722660065, ACC:0.984375\n",
      "Training iteration 423 loss: 0.03299706429243088, ACC:0.984375\n",
      "Training iteration 424 loss: 0.15528951585292816, ACC:0.9375\n",
      "Training iteration 425 loss: 0.017596418038010597, ACC:1.0\n",
      "Training iteration 426 loss: 0.06204076483845711, ACC:0.984375\n",
      "Training iteration 427 loss: 0.0326315276324749, ACC:1.0\n",
      "Training iteration 428 loss: 0.03276063874363899, ACC:0.984375\n",
      "Training iteration 429 loss: 0.037194736301898956, ACC:0.96875\n",
      "Training iteration 430 loss: 0.07968363910913467, ACC:0.96875\n",
      "Training iteration 431 loss: 0.030830997973680496, ACC:0.984375\n",
      "Training iteration 432 loss: 0.017496079206466675, ACC:1.0\n",
      "Training iteration 433 loss: 0.03752949833869934, ACC:0.984375\n",
      "Training iteration 434 loss: 0.012753275223076344, ACC:1.0\n",
      "Training iteration 435 loss: 0.027903415262699127, ACC:1.0\n",
      "Training iteration 436 loss: 0.06520217657089233, ACC:0.953125\n",
      "Training iteration 437 loss: 0.04734857380390167, ACC:0.984375\n",
      "Training iteration 438 loss: 0.0017817594343796372, ACC:1.0\n",
      "Training iteration 439 loss: 0.08744752407073975, ACC:0.96875\n",
      "Training iteration 440 loss: 0.06749439239501953, ACC:0.96875\n",
      "Training iteration 441 loss: 0.07195627689361572, ACC:0.984375\n",
      "Training iteration 442 loss: 0.025589626282453537, ACC:1.0\n",
      "Training iteration 443 loss: 0.0026018228381872177, ACC:1.0\n",
      "Training iteration 444 loss: 0.04566529020667076, ACC:0.96875\n",
      "Training iteration 445 loss: 0.039542436599731445, ACC:0.984375\n",
      "Training iteration 446 loss: 0.026755761355161667, ACC:0.984375\n",
      "Training iteration 447 loss: 0.06499350070953369, ACC:0.96875\n",
      "Training iteration 448 loss: 0.008561762981116772, ACC:1.0\n",
      "Training iteration 449 loss: 0.002686966909095645, ACC:1.0\n",
      "Training iteration 450 loss: 0.030709512531757355, ACC:0.984375\n",
      "Validation iteration 451 loss: 0.0026133188512176275, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.0035146852023899555, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.008696859702467918, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.006500613410025835, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.005977003835141659, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.011914080008864403, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.012080127373337746, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.015384658239781857, ACC: 0.984375\n",
      "Validation iteration 459 loss: 0.010596769861876965, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.029232200235128403, ACC: 0.984375\n",
      "Validation iteration 461 loss: 0.011669694446027279, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.07953488081693649, ACC: 0.984375\n",
      "Validation iteration 463 loss: 0.02152162976562977, ACC: 0.984375\n",
      "Validation iteration 464 loss: 0.0014925498981028795, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.004997492302209139, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.06312092393636703, ACC: 0.984375\n",
      "Validation iteration 467 loss: 0.032896481454372406, ACC: 0.984375\n",
      "Validation iteration 468 loss: 0.12562526762485504, ACC: 0.96875\n",
      "Validation iteration 469 loss: 0.014441924169659615, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.010145178064703941, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.026469258591532707, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.0036535970866680145, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.008171334862709045, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.023465152829885483, ACC: 0.984375\n",
      "Validation iteration 475 loss: 0.07852821052074432, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.005642446223646402, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.025710726156830788, ACC: 0.984375\n",
      "Validation iteration 478 loss: 0.008642071858048439, ACC: 1.0\n",
      "Validation iteration 479 loss: 0.02594921924173832, ACC: 0.984375\n",
      "Validation iteration 480 loss: 0.09235920011997223, ACC: 0.96875\n",
      "Validation iteration 481 loss: 0.03618181496858597, ACC: 0.984375\n",
      "Validation iteration 482 loss: 0.005290200002491474, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.004981032107025385, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.010660300962626934, ACC: 1.0\n",
      "Validation iteration 485 loss: 0.01774304360151291, ACC: 0.984375\n",
      "Validation iteration 486 loss: 0.08764562010765076, ACC: 0.984375\n",
      "Validation iteration 487 loss: 0.006076694931834936, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.08096759021282196, ACC: 0.984375\n",
      "Validation iteration 489 loss: 0.04848995804786682, ACC: 0.984375\n",
      "Validation iteration 490 loss: 0.1329740434885025, ACC: 0.984375\n",
      "Validation iteration 491 loss: 0.03914453834295273, ACC: 0.984375\n",
      "Validation iteration 492 loss: 0.0074291471391916275, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.05496242642402649, ACC: 0.984375\n",
      "Validation iteration 494 loss: 0.010655427351593971, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.01662844978272915, ACC: 0.984375\n",
      "Validation iteration 496 loss: 0.014596624299883842, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.020574398338794708, ACC: 0.984375\n",
      "Validation iteration 498 loss: 0.1654304414987564, ACC: 0.953125\n",
      "Validation iteration 499 loss: 0.005909760948270559, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.0024206829257309437, ACC: 1.0\n",
      "-- Epoch 2 done -- Train loss: 0.04507462721033436, train ACC: 0.9861458333333334, val loss: 0.030786195043474435, val ACC: 0.99125\n",
      "<--- 8764.182784318924 seconds --->\n",
      "Training iteration 1 loss: 0.004723048768937588, ACC:1.0\n",
      "Training iteration 2 loss: 0.00617444422096014, ACC:1.0\n",
      "Training iteration 3 loss: 0.010047480463981628, ACC:1.0\n",
      "Training iteration 4 loss: 0.004566891118884087, ACC:1.0\n",
      "Training iteration 5 loss: 0.022342966869473457, ACC:0.984375\n",
      "Training iteration 6 loss: 0.014122102409601212, ACC:1.0\n",
      "Training iteration 7 loss: 0.06892648339271545, ACC:0.96875\n",
      "Training iteration 8 loss: 0.13225838541984558, ACC:0.96875\n",
      "Training iteration 9 loss: 0.06656866520643234, ACC:0.96875\n",
      "Training iteration 10 loss: 0.040301613509655, ACC:0.984375\n",
      "Training iteration 11 loss: 0.11133220791816711, ACC:0.984375\n",
      "Training iteration 12 loss: 0.053158778697252274, ACC:0.984375\n",
      "Training iteration 13 loss: 0.05267294868826866, ACC:0.96875\n",
      "Training iteration 14 loss: 0.02809402346611023, ACC:1.0\n",
      "Training iteration 15 loss: 0.018129663541913033, ACC:1.0\n",
      "Training iteration 16 loss: 0.03656301274895668, ACC:0.984375\n",
      "Training iteration 17 loss: 0.010406994260847569, ACC:1.0\n",
      "Training iteration 18 loss: 0.041552044451236725, ACC:0.96875\n",
      "Training iteration 19 loss: 0.021665146574378014, ACC:0.984375\n",
      "Training iteration 20 loss: 0.008367598056793213, ACC:1.0\n",
      "Training iteration 21 loss: 0.020494263619184494, ACC:1.0\n",
      "Training iteration 22 loss: 0.006624490953981876, ACC:1.0\n",
      "Training iteration 23 loss: 0.010112857446074486, ACC:1.0\n",
      "Training iteration 24 loss: 0.14276030659675598, ACC:0.96875\n",
      "Training iteration 25 loss: 0.02548344060778618, ACC:0.984375\n",
      "Training iteration 26 loss: 0.00645627873018384, ACC:1.0\n",
      "Training iteration 27 loss: 0.06998627632856369, ACC:0.96875\n",
      "Training iteration 28 loss: 0.09248337149620056, ACC:0.96875\n",
      "Training iteration 29 loss: 0.02062138170003891, ACC:0.984375\n",
      "Training iteration 30 loss: 0.006075580604374409, ACC:1.0\n",
      "Training iteration 31 loss: 0.03238194063305855, ACC:0.984375\n",
      "Training iteration 32 loss: 0.016798190772533417, ACC:1.0\n",
      "Training iteration 33 loss: 0.004524377640336752, ACC:1.0\n",
      "Training iteration 34 loss: 0.06820940226316452, ACC:0.984375\n",
      "Training iteration 35 loss: 0.011485057882964611, ACC:1.0\n",
      "Training iteration 36 loss: 0.008782914839684963, ACC:1.0\n",
      "Training iteration 37 loss: 0.004003215581178665, ACC:1.0\n",
      "Training iteration 38 loss: 0.0020003875251859426, ACC:1.0\n",
      "Training iteration 39 loss: 0.010219665244221687, ACC:1.0\n",
      "Training iteration 40 loss: 0.002391673857346177, ACC:1.0\n",
      "Training iteration 41 loss: 0.051543932408094406, ACC:0.984375\n",
      "Training iteration 42 loss: 0.0031641803216189146, ACC:1.0\n",
      "Training iteration 43 loss: 0.04505529627203941, ACC:0.984375\n",
      "Training iteration 44 loss: 0.00491554569453001, ACC:1.0\n",
      "Training iteration 45 loss: 0.015856409445405006, ACC:1.0\n",
      "Training iteration 46 loss: 0.05232632905244827, ACC:0.96875\n",
      "Training iteration 47 loss: 0.05546707287430763, ACC:0.984375\n",
      "Training iteration 48 loss: 0.02776860073208809, ACC:0.984375\n",
      "Training iteration 49 loss: 0.01527596078813076, ACC:1.0\n",
      "Training iteration 50 loss: 0.040321044623851776, ACC:0.984375\n",
      "Training iteration 51 loss: 0.0005577529082074761, ACC:1.0\n",
      "Training iteration 52 loss: 0.006723302416503429, ACC:1.0\n",
      "Training iteration 53 loss: 0.01132533885538578, ACC:1.0\n",
      "Training iteration 54 loss: 0.21392934024333954, ACC:0.9375\n",
      "Training iteration 55 loss: 0.002297253580763936, ACC:1.0\n",
      "Training iteration 56 loss: 0.019367998465895653, ACC:0.984375\n",
      "Training iteration 57 loss: 0.0427996926009655, ACC:0.984375\n",
      "Training iteration 58 loss: 0.017136134207248688, ACC:1.0\n",
      "Training iteration 59 loss: 0.03318367898464203, ACC:0.984375\n",
      "Training iteration 60 loss: 0.05677418038249016, ACC:0.984375\n",
      "Training iteration 61 loss: 0.006602567620575428, ACC:1.0\n",
      "Training iteration 62 loss: 0.022601978853344917, ACC:0.984375\n",
      "Training iteration 63 loss: 0.00859316997230053, ACC:1.0\n",
      "Training iteration 64 loss: 0.09787045419216156, ACC:0.984375\n",
      "Training iteration 65 loss: 0.0611007884144783, ACC:0.96875\n",
      "Training iteration 66 loss: 0.0077455961145460606, ACC:1.0\n",
      "Training iteration 67 loss: 0.005383193027228117, ACC:1.0\n",
      "Training iteration 68 loss: 0.04478573799133301, ACC:0.984375\n",
      "Training iteration 69 loss: 0.025807954370975494, ACC:0.984375\n",
      "Training iteration 70 loss: 0.027485528960824013, ACC:0.984375\n",
      "Training iteration 71 loss: 0.15667101740837097, ACC:0.953125\n",
      "Training iteration 72 loss: 0.01190243661403656, ACC:1.0\n",
      "Training iteration 73 loss: 0.0038341940380632877, ACC:1.0\n",
      "Training iteration 74 loss: 0.02420807257294655, ACC:0.984375\n",
      "Training iteration 75 loss: 0.024575138464570045, ACC:1.0\n",
      "Training iteration 76 loss: 0.038320913910865784, ACC:0.984375\n",
      "Training iteration 77 loss: 0.0666593387722969, ACC:0.984375\n",
      "Training iteration 78 loss: 0.04484796151518822, ACC:0.984375\n",
      "Training iteration 79 loss: 0.03157884255051613, ACC:0.984375\n",
      "Training iteration 80 loss: 0.027890417724847794, ACC:1.0\n",
      "Training iteration 81 loss: 0.04251767322421074, ACC:0.96875\n",
      "Training iteration 82 loss: 0.04209978133440018, ACC:0.984375\n",
      "Training iteration 83 loss: 0.01504326332360506, ACC:0.984375\n",
      "Training iteration 84 loss: 0.011152703315019608, ACC:1.0\n",
      "Training iteration 85 loss: 0.0034706187434494495, ACC:1.0\n",
      "Training iteration 86 loss: 0.01421580370515585, ACC:1.0\n",
      "Training iteration 87 loss: 0.03524116054177284, ACC:0.984375\n",
      "Training iteration 88 loss: 0.00791248120367527, ACC:1.0\n",
      "Training iteration 89 loss: 0.0005237069563008845, ACC:1.0\n",
      "Training iteration 90 loss: 0.17579349875450134, ACC:0.96875\n",
      "Training iteration 91 loss: 0.010799180716276169, ACC:1.0\n",
      "Training iteration 92 loss: 0.10122249275445938, ACC:0.96875\n",
      "Training iteration 93 loss: 0.009724035859107971, ACC:1.0\n",
      "Training iteration 94 loss: 0.008034011349081993, ACC:1.0\n",
      "Training iteration 95 loss: 0.024768320843577385, ACC:1.0\n",
      "Training iteration 96 loss: 0.03510923683643341, ACC:0.984375\n",
      "Training iteration 97 loss: 0.0995640829205513, ACC:0.953125\n",
      "Training iteration 98 loss: 0.013030767440795898, ACC:1.0\n",
      "Training iteration 99 loss: 0.02954960986971855, ACC:0.984375\n",
      "Training iteration 100 loss: 0.005722941365092993, ACC:1.0\n",
      "Training iteration 101 loss: 0.019741784781217575, ACC:1.0\n",
      "Training iteration 102 loss: 0.011711875908076763, ACC:1.0\n",
      "Training iteration 103 loss: 0.04380526393651962, ACC:0.96875\n",
      "Training iteration 104 loss: 0.008970950730144978, ACC:1.0\n",
      "Training iteration 105 loss: 0.10669989138841629, ACC:0.96875\n",
      "Training iteration 106 loss: 0.020175278186798096, ACC:1.0\n",
      "Training iteration 107 loss: 0.05474692955613136, ACC:0.984375\n",
      "Training iteration 108 loss: 0.03055541031062603, ACC:1.0\n",
      "Training iteration 109 loss: 0.022878305986523628, ACC:0.984375\n",
      "Training iteration 110 loss: 0.057661302387714386, ACC:0.96875\n",
      "Training iteration 111 loss: 0.011788350529968739, ACC:1.0\n",
      "Training iteration 112 loss: 0.04808276519179344, ACC:0.984375\n",
      "Training iteration 113 loss: 0.06657800823450089, ACC:0.984375\n",
      "Training iteration 114 loss: 0.03252483904361725, ACC:0.984375\n",
      "Training iteration 115 loss: 0.02145279571413994, ACC:1.0\n",
      "Training iteration 116 loss: 0.011481666937470436, ACC:1.0\n",
      "Training iteration 117 loss: 0.002673132112249732, ACC:1.0\n",
      "Training iteration 118 loss: 0.029845738783478737, ACC:0.96875\n",
      "Training iteration 119 loss: 0.006989925168454647, ACC:1.0\n",
      "Training iteration 120 loss: 0.01404398214071989, ACC:1.0\n",
      "Training iteration 121 loss: 0.05511641129851341, ACC:0.96875\n",
      "Training iteration 122 loss: 0.08045457303524017, ACC:0.984375\n",
      "Training iteration 123 loss: 0.05513659492135048, ACC:0.96875\n",
      "Training iteration 124 loss: 0.039162199944257736, ACC:0.984375\n",
      "Training iteration 125 loss: 0.017570126801729202, ACC:0.984375\n",
      "Training iteration 126 loss: 0.002073852811008692, ACC:1.0\n",
      "Training iteration 127 loss: 0.10028735548257828, ACC:0.953125\n",
      "Training iteration 128 loss: 0.04871981590986252, ACC:0.96875\n",
      "Training iteration 129 loss: 0.034046970307826996, ACC:0.984375\n",
      "Training iteration 130 loss: 0.04113399237394333, ACC:0.984375\n",
      "Training iteration 131 loss: 0.027988553047180176, ACC:0.984375\n",
      "Training iteration 132 loss: 0.0017446174751967192, ACC:1.0\n",
      "Training iteration 133 loss: 0.04033925011754036, ACC:0.984375\n",
      "Training iteration 134 loss: 0.03209218755364418, ACC:0.984375\n",
      "Training iteration 135 loss: 0.05650591850280762, ACC:0.96875\n",
      "Training iteration 136 loss: 0.013386744074523449, ACC:1.0\n",
      "Training iteration 137 loss: 0.011386619880795479, ACC:1.0\n",
      "Training iteration 138 loss: 0.04015062376856804, ACC:0.984375\n",
      "Training iteration 139 loss: 0.010552252642810345, ACC:1.0\n",
      "Training iteration 140 loss: 0.013011139817535877, ACC:1.0\n",
      "Training iteration 141 loss: 0.02920423448085785, ACC:0.984375\n",
      "Training iteration 142 loss: 0.06594029068946838, ACC:0.984375\n",
      "Training iteration 143 loss: 0.0036469982005655766, ACC:1.0\n",
      "Training iteration 144 loss: 0.007294601295143366, ACC:1.0\n",
      "Training iteration 145 loss: 0.05045783147215843, ACC:0.984375\n",
      "Training iteration 146 loss: 0.06264828145503998, ACC:0.984375\n",
      "Training iteration 147 loss: 0.04398170858621597, ACC:0.984375\n",
      "Training iteration 148 loss: 0.024694882333278656, ACC:0.984375\n",
      "Training iteration 149 loss: 0.07362543046474457, ACC:0.984375\n",
      "Training iteration 150 loss: 0.004206480924040079, ACC:1.0\n",
      "Training iteration 151 loss: 0.08730251342058182, ACC:0.984375\n",
      "Training iteration 152 loss: 0.05054913088679314, ACC:0.984375\n",
      "Training iteration 153 loss: 0.09549681842327118, ACC:0.984375\n",
      "Training iteration 154 loss: 0.049621738493442535, ACC:0.984375\n",
      "Training iteration 155 loss: 0.04768270254135132, ACC:0.96875\n",
      "Training iteration 156 loss: 0.012641656212508678, ACC:1.0\n",
      "Training iteration 157 loss: 0.013176821172237396, ACC:1.0\n",
      "Training iteration 158 loss: 0.02929757907986641, ACC:0.984375\n",
      "Training iteration 159 loss: 0.017860552296042442, ACC:1.0\n",
      "Training iteration 160 loss: 0.01051677018404007, ACC:1.0\n",
      "Training iteration 161 loss: 0.0067160483449697495, ACC:1.0\n",
      "Training iteration 162 loss: 0.022012121975421906, ACC:0.984375\n",
      "Training iteration 163 loss: 0.04279462993144989, ACC:0.96875\n",
      "Training iteration 164 loss: 0.0038345337379723787, ACC:1.0\n",
      "Training iteration 165 loss: 0.006912984419614077, ACC:1.0\n",
      "Training iteration 166 loss: 0.00726044038310647, ACC:1.0\n",
      "Training iteration 167 loss: 0.09671801328659058, ACC:0.984375\n",
      "Training iteration 168 loss: 0.003554661525413394, ACC:1.0\n",
      "Training iteration 169 loss: 0.017740078270435333, ACC:1.0\n",
      "Training iteration 170 loss: 0.017966315150260925, ACC:1.0\n",
      "Training iteration 171 loss: 0.00587059510871768, ACC:1.0\n",
      "Training iteration 172 loss: 0.007776173762977123, ACC:1.0\n",
      "Training iteration 173 loss: 0.020183313637971878, ACC:0.984375\n",
      "Training iteration 174 loss: 0.0982956737279892, ACC:0.984375\n",
      "Training iteration 175 loss: 0.005781642626971006, ACC:1.0\n",
      "Training iteration 176 loss: 0.06326399743556976, ACC:0.96875\n",
      "Training iteration 177 loss: 0.0045846919529139996, ACC:1.0\n",
      "Training iteration 178 loss: 0.05836401507258415, ACC:0.984375\n",
      "Training iteration 179 loss: 0.029589666053652763, ACC:0.96875\n",
      "Training iteration 180 loss: 0.059425972402095795, ACC:0.984375\n",
      "Training iteration 181 loss: 0.008398964069783688, ACC:1.0\n",
      "Training iteration 182 loss: 0.002368495799601078, ACC:1.0\n",
      "Training iteration 183 loss: 0.10622680932283401, ACC:0.984375\n",
      "Training iteration 184 loss: 0.005977686028927565, ACC:1.0\n",
      "Training iteration 185 loss: 0.0356961153447628, ACC:0.96875\n",
      "Training iteration 186 loss: 0.11084064841270447, ACC:0.984375\n",
      "Training iteration 187 loss: 0.09147725254297256, ACC:0.984375\n",
      "Training iteration 188 loss: 0.06609109044075012, ACC:0.984375\n",
      "Training iteration 189 loss: 0.005068883765488863, ACC:1.0\n",
      "Training iteration 190 loss: 0.020049529150128365, ACC:0.984375\n",
      "Training iteration 191 loss: 0.00458531454205513, ACC:1.0\n",
      "Training iteration 192 loss: 0.02824064902961254, ACC:1.0\n",
      "Training iteration 193 loss: 0.19376319646835327, ACC:0.953125\n",
      "Training iteration 194 loss: 0.015397371724247932, ACC:1.0\n",
      "Training iteration 195 loss: 0.029905516654253006, ACC:1.0\n",
      "Training iteration 196 loss: 0.013982861302793026, ACC:1.0\n",
      "Training iteration 197 loss: 0.04656802490353584, ACC:0.984375\n",
      "Training iteration 198 loss: 0.023475445806980133, ACC:1.0\n",
      "Training iteration 199 loss: 0.015006115660071373, ACC:1.0\n",
      "Training iteration 200 loss: 0.041296251118183136, ACC:0.984375\n",
      "Training iteration 201 loss: 0.016954313963651657, ACC:1.0\n",
      "Training iteration 202 loss: 0.022874988615512848, ACC:1.0\n",
      "Training iteration 203 loss: 0.07461829483509064, ACC:0.96875\n",
      "Training iteration 204 loss: 0.0578596368432045, ACC:0.96875\n",
      "Training iteration 205 loss: 0.0168160293251276, ACC:0.984375\n",
      "Training iteration 206 loss: 0.09462155401706696, ACC:0.96875\n",
      "Training iteration 207 loss: 0.0818951278924942, ACC:0.984375\n",
      "Training iteration 208 loss: 0.020353861153125763, ACC:0.984375\n",
      "Training iteration 209 loss: 0.013375898823142052, ACC:1.0\n",
      "Training iteration 210 loss: 0.03166516125202179, ACC:0.984375\n",
      "Training iteration 211 loss: 0.016521241515874863, ACC:1.0\n",
      "Training iteration 212 loss: 0.016994643956422806, ACC:1.0\n",
      "Training iteration 213 loss: 0.06897885352373123, ACC:0.984375\n",
      "Training iteration 214 loss: 0.010602811351418495, ACC:1.0\n",
      "Training iteration 215 loss: 0.02242887392640114, ACC:1.0\n",
      "Training iteration 216 loss: 0.004419425036758184, ACC:1.0\n",
      "Training iteration 217 loss: 0.09040344506502151, ACC:0.96875\n",
      "Training iteration 218 loss: 0.04855727404356003, ACC:0.96875\n",
      "Training iteration 219 loss: 0.003228111658245325, ACC:1.0\n",
      "Training iteration 220 loss: 0.023541094735264778, ACC:0.984375\n",
      "Training iteration 221 loss: 0.016042347997426987, ACC:1.0\n",
      "Training iteration 222 loss: 0.00458152312785387, ACC:1.0\n",
      "Training iteration 223 loss: 0.020385973155498505, ACC:0.984375\n",
      "Training iteration 224 loss: 0.036432281136512756, ACC:0.96875\n",
      "Training iteration 225 loss: 0.003648181678727269, ACC:1.0\n",
      "Training iteration 226 loss: 0.0030183112248778343, ACC:1.0\n",
      "Training iteration 227 loss: 0.16675019264221191, ACC:0.96875\n",
      "Training iteration 228 loss: 0.013294216245412827, ACC:1.0\n",
      "Training iteration 229 loss: 0.001463946420699358, ACC:1.0\n",
      "Training iteration 230 loss: 0.007603102829307318, ACC:1.0\n",
      "Training iteration 231 loss: 0.036397822201251984, ACC:0.984375\n",
      "Training iteration 232 loss: 0.024648362770676613, ACC:0.984375\n",
      "Training iteration 233 loss: 0.023127399384975433, ACC:0.984375\n",
      "Training iteration 234 loss: 0.058785416185855865, ACC:0.984375\n",
      "Training iteration 235 loss: 0.005109262652695179, ACC:1.0\n",
      "Training iteration 236 loss: 0.008687627501785755, ACC:1.0\n",
      "Training iteration 237 loss: 0.0024136179126799107, ACC:1.0\n",
      "Training iteration 238 loss: 0.023737655952572823, ACC:0.984375\n",
      "Training iteration 239 loss: 0.0030325278639793396, ACC:1.0\n",
      "Training iteration 240 loss: 0.05135878548026085, ACC:0.984375\n",
      "Training iteration 241 loss: 0.0034222183749079704, ACC:1.0\n",
      "Training iteration 242 loss: 0.0914938822388649, ACC:0.984375\n",
      "Training iteration 243 loss: 0.05835789814591408, ACC:0.984375\n",
      "Training iteration 244 loss: 0.036102816462516785, ACC:0.984375\n",
      "Training iteration 245 loss: 0.04236070066690445, ACC:0.984375\n",
      "Training iteration 246 loss: 0.015300181694328785, ACC:1.0\n",
      "Training iteration 247 loss: 0.15525689721107483, ACC:0.953125\n",
      "Training iteration 248 loss: 0.04852340370416641, ACC:0.96875\n",
      "Training iteration 249 loss: 0.03660421073436737, ACC:0.984375\n",
      "Training iteration 250 loss: 0.10636761784553528, ACC:0.96875\n",
      "Training iteration 251 loss: 0.036683499813079834, ACC:0.984375\n",
      "Training iteration 252 loss: 0.006486670579761267, ACC:1.0\n",
      "Training iteration 253 loss: 0.008090839721262455, ACC:1.0\n",
      "Training iteration 254 loss: 0.06585562974214554, ACC:0.96875\n",
      "Training iteration 255 loss: 0.0022056179586797953, ACC:1.0\n",
      "Training iteration 256 loss: 0.12281379103660583, ACC:0.96875\n",
      "Training iteration 257 loss: 0.007162660825997591, ACC:1.0\n",
      "Training iteration 258 loss: 0.007180775050073862, ACC:1.0\n",
      "Training iteration 259 loss: 0.06611035019159317, ACC:0.96875\n",
      "Training iteration 260 loss: 0.012049136683344841, ACC:1.0\n",
      "Training iteration 261 loss: 0.042184192687273026, ACC:0.984375\n",
      "Training iteration 262 loss: 0.054952964186668396, ACC:0.96875\n",
      "Training iteration 263 loss: 0.03303838521242142, ACC:0.984375\n",
      "Training iteration 264 loss: 0.006248664576560259, ACC:1.0\n",
      "Training iteration 265 loss: 0.29992884397506714, ACC:0.96875\n",
      "Training iteration 266 loss: 0.004000126384198666, ACC:1.0\n",
      "Training iteration 267 loss: 0.012371683493256569, ACC:1.0\n",
      "Training iteration 268 loss: 0.00792316347360611, ACC:1.0\n",
      "Training iteration 269 loss: 0.007537761237472296, ACC:1.0\n",
      "Training iteration 270 loss: 0.013791341334581375, ACC:1.0\n",
      "Training iteration 271 loss: 0.0073721641674637794, ACC:1.0\n",
      "Training iteration 272 loss: 0.013085763901472092, ACC:1.0\n",
      "Training iteration 273 loss: 0.016188640147447586, ACC:1.0\n",
      "Training iteration 274 loss: 0.04715034365653992, ACC:0.984375\n",
      "Training iteration 275 loss: 0.0028960835188627243, ACC:1.0\n",
      "Training iteration 276 loss: 0.012904576025903225, ACC:1.0\n",
      "Training iteration 277 loss: 0.005088183097541332, ACC:1.0\n",
      "Training iteration 278 loss: 0.006165854632854462, ACC:1.0\n",
      "Training iteration 279 loss: 0.0021402041893452406, ACC:1.0\n",
      "Training iteration 280 loss: 0.010003294795751572, ACC:1.0\n",
      "Training iteration 281 loss: 0.02531403861939907, ACC:0.984375\n",
      "Training iteration 282 loss: 0.0038960918318480253, ACC:1.0\n",
      "Training iteration 283 loss: 0.036060065031051636, ACC:0.984375\n",
      "Training iteration 284 loss: 0.0017245751805603504, ACC:1.0\n",
      "Training iteration 285 loss: 0.006748533342033625, ACC:1.0\n",
      "Training iteration 286 loss: 0.009062396362423897, ACC:1.0\n",
      "Training iteration 287 loss: 0.004561400040984154, ACC:1.0\n",
      "Training iteration 288 loss: 0.03163500130176544, ACC:0.984375\n",
      "Training iteration 289 loss: 0.009572464041411877, ACC:1.0\n",
      "Training iteration 290 loss: 0.027230991050601006, ACC:0.984375\n",
      "Training iteration 291 loss: 0.023857494816184044, ACC:0.984375\n",
      "Training iteration 292 loss: 0.0010178543161600828, ACC:1.0\n",
      "Training iteration 293 loss: 0.007787508424371481, ACC:1.0\n",
      "Training iteration 294 loss: 0.0027230011764913797, ACC:1.0\n",
      "Training iteration 295 loss: 0.004695015959441662, ACC:1.0\n",
      "Training iteration 296 loss: 0.06391145288944244, ACC:0.96875\n",
      "Training iteration 297 loss: 0.004342175088822842, ACC:1.0\n",
      "Training iteration 298 loss: 0.07339250296354294, ACC:0.96875\n",
      "Training iteration 299 loss: 0.02621118165552616, ACC:0.984375\n",
      "Training iteration 300 loss: 0.009122154675424099, ACC:1.0\n",
      "Training iteration 301 loss: 0.016291877254843712, ACC:0.984375\n",
      "Training iteration 302 loss: 0.04866892844438553, ACC:0.984375\n",
      "Training iteration 303 loss: 0.026078274473547935, ACC:0.984375\n",
      "Training iteration 304 loss: 0.07070183753967285, ACC:0.96875\n",
      "Training iteration 305 loss: 0.010209786705672741, ACC:1.0\n",
      "Training iteration 306 loss: 0.005877879448235035, ACC:1.0\n",
      "Training iteration 307 loss: 0.007968783378601074, ACC:1.0\n",
      "Training iteration 308 loss: 0.02589893527328968, ACC:0.984375\n",
      "Training iteration 309 loss: 0.014635690487921238, ACC:1.0\n",
      "Training iteration 310 loss: 0.0007171116885729134, ACC:1.0\n",
      "Training iteration 311 loss: 0.018642157316207886, ACC:1.0\n",
      "Training iteration 312 loss: 0.004778633825480938, ACC:1.0\n",
      "Training iteration 313 loss: 0.009639834985136986, ACC:1.0\n",
      "Training iteration 314 loss: 0.0011415531625971198, ACC:1.0\n",
      "Training iteration 315 loss: 0.008866634219884872, ACC:1.0\n",
      "Training iteration 316 loss: 0.001127264928072691, ACC:1.0\n",
      "Training iteration 317 loss: 0.015281625092029572, ACC:1.0\n",
      "Training iteration 318 loss: 0.011038446798920631, ACC:1.0\n",
      "Training iteration 319 loss: 0.007738866843283176, ACC:1.0\n",
      "Training iteration 320 loss: 0.0025498114991933107, ACC:1.0\n",
      "Training iteration 321 loss: 0.01989283412694931, ACC:1.0\n",
      "Training iteration 322 loss: 0.027994079515337944, ACC:0.984375\n",
      "Training iteration 323 loss: 0.0004620957188308239, ACC:1.0\n",
      "Training iteration 324 loss: 0.004786554258316755, ACC:1.0\n",
      "Training iteration 325 loss: 0.12937568128108978, ACC:0.96875\n",
      "Training iteration 326 loss: 0.12963858246803284, ACC:0.953125\n",
      "Training iteration 327 loss: 0.006383226253092289, ACC:1.0\n",
      "Training iteration 328 loss: 0.06436817348003387, ACC:0.984375\n",
      "Training iteration 329 loss: 0.018361594527959824, ACC:1.0\n",
      "Training iteration 330 loss: 0.0330667607486248, ACC:0.984375\n",
      "Training iteration 331 loss: 0.053875163197517395, ACC:0.984375\n",
      "Training iteration 332 loss: 0.018127283081412315, ACC:0.984375\n",
      "Training iteration 333 loss: 0.12646275758743286, ACC:0.984375\n",
      "Training iteration 334 loss: 0.015015474520623684, ACC:1.0\n",
      "Training iteration 335 loss: 0.16963303089141846, ACC:0.96875\n",
      "Training iteration 336 loss: 0.08277791738510132, ACC:0.984375\n",
      "Training iteration 337 loss: 0.0756714791059494, ACC:0.96875\n",
      "Training iteration 338 loss: 0.03286797180771828, ACC:0.984375\n",
      "Training iteration 339 loss: 0.008100184611976147, ACC:1.0\n",
      "Training iteration 340 loss: 0.06999588012695312, ACC:0.96875\n",
      "Training iteration 341 loss: 0.007756329141557217, ACC:1.0\n",
      "Training iteration 342 loss: 0.020023798570036888, ACC:1.0\n",
      "Training iteration 343 loss: 0.017154091969132423, ACC:1.0\n",
      "Training iteration 344 loss: 0.08928955346345901, ACC:0.984375\n",
      "Training iteration 345 loss: 0.07894538342952728, ACC:0.984375\n",
      "Training iteration 346 loss: 0.015565416775643826, ACC:1.0\n",
      "Training iteration 347 loss: 0.10194382816553116, ACC:0.96875\n",
      "Training iteration 348 loss: 0.00888118613511324, ACC:1.0\n",
      "Training iteration 349 loss: 0.053500838577747345, ACC:0.96875\n",
      "Training iteration 350 loss: 0.16201701760292053, ACC:0.953125\n",
      "Training iteration 351 loss: 0.01730838418006897, ACC:1.0\n",
      "Training iteration 352 loss: 0.021500473842024803, ACC:0.984375\n",
      "Training iteration 353 loss: 0.06118383631110191, ACC:0.984375\n",
      "Training iteration 354 loss: 0.029267190024256706, ACC:0.984375\n",
      "Training iteration 355 loss: 0.03462141379714012, ACC:0.984375\n",
      "Training iteration 356 loss: 0.0835639089345932, ACC:0.984375\n",
      "Training iteration 357 loss: 0.014837979339063168, ACC:1.0\n",
      "Training iteration 358 loss: 0.07977135479450226, ACC:0.953125\n",
      "Training iteration 359 loss: 0.03251445293426514, ACC:0.984375\n",
      "Training iteration 360 loss: 0.10262707620859146, ACC:0.953125\n",
      "Training iteration 361 loss: 0.19930601119995117, ACC:0.984375\n",
      "Training iteration 362 loss: 0.1505448818206787, ACC:0.96875\n",
      "Training iteration 363 loss: 0.0030202209018170834, ACC:1.0\n",
      "Training iteration 364 loss: 0.01525064930319786, ACC:1.0\n",
      "Training iteration 365 loss: 0.008411809802055359, ACC:1.0\n",
      "Training iteration 366 loss: 0.0076933978125452995, ACC:1.0\n",
      "Training iteration 367 loss: 0.0032101476099342108, ACC:1.0\n",
      "Training iteration 368 loss: 0.12168057262897491, ACC:0.984375\n",
      "Training iteration 369 loss: 0.007595213130116463, ACC:1.0\n",
      "Training iteration 370 loss: 0.012646439485251904, ACC:1.0\n",
      "Training iteration 371 loss: 0.011271171271800995, ACC:1.0\n",
      "Training iteration 372 loss: 0.12486008554697037, ACC:0.953125\n",
      "Training iteration 373 loss: 0.045002445578575134, ACC:0.984375\n",
      "Training iteration 374 loss: 0.05785500258207321, ACC:0.96875\n",
      "Training iteration 375 loss: 0.11320296674966812, ACC:0.96875\n",
      "Training iteration 376 loss: 0.06730952113866806, ACC:0.96875\n",
      "Training iteration 377 loss: 0.11947520077228546, ACC:0.953125\n",
      "Training iteration 378 loss: 0.046069931238889694, ACC:0.96875\n",
      "Training iteration 379 loss: 0.04719659313559532, ACC:0.96875\n",
      "Training iteration 380 loss: 0.023583266884088516, ACC:1.0\n",
      "Training iteration 381 loss: 0.02033071592450142, ACC:1.0\n",
      "Training iteration 382 loss: 0.05581103265285492, ACC:0.984375\n",
      "Training iteration 383 loss: 0.014817513525485992, ACC:1.0\n",
      "Training iteration 384 loss: 0.06931592524051666, ACC:0.96875\n",
      "Training iteration 385 loss: 0.0398520864546299, ACC:0.984375\n",
      "Training iteration 386 loss: 0.02679159864783287, ACC:1.0\n",
      "Training iteration 387 loss: 0.00913061760365963, ACC:1.0\n",
      "Training iteration 388 loss: 0.07718595862388611, ACC:0.984375\n",
      "Training iteration 389 loss: 0.03957851231098175, ACC:0.984375\n",
      "Training iteration 390 loss: 0.006870059296488762, ACC:1.0\n",
      "Training iteration 391 loss: 0.029603024944663048, ACC:0.984375\n",
      "Training iteration 392 loss: 0.040704742074012756, ACC:0.984375\n",
      "Training iteration 393 loss: 0.010236909613013268, ACC:1.0\n",
      "Training iteration 394 loss: 0.06243128329515457, ACC:0.984375\n",
      "Training iteration 395 loss: 0.03781270980834961, ACC:0.984375\n",
      "Training iteration 396 loss: 0.004863126203417778, ACC:1.0\n",
      "Training iteration 397 loss: 0.06599237769842148, ACC:0.984375\n",
      "Training iteration 398 loss: 0.006299261469393969, ACC:1.0\n",
      "Training iteration 399 loss: 0.006119551137089729, ACC:1.0\n",
      "Training iteration 400 loss: 0.020036224275827408, ACC:0.984375\n",
      "Training iteration 401 loss: 0.020642058923840523, ACC:0.984375\n",
      "Training iteration 402 loss: 0.021469280123710632, ACC:1.0\n",
      "Training iteration 403 loss: 0.0375216081738472, ACC:0.984375\n",
      "Training iteration 404 loss: 0.006326751783490181, ACC:1.0\n",
      "Training iteration 405 loss: 0.11551174521446228, ACC:0.984375\n",
      "Training iteration 406 loss: 0.008076386526226997, ACC:1.0\n",
      "Training iteration 407 loss: 0.007306491490453482, ACC:1.0\n",
      "Training iteration 408 loss: 0.039180170744657516, ACC:0.984375\n",
      "Training iteration 409 loss: 0.009111952967941761, ACC:1.0\n",
      "Training iteration 410 loss: 0.13245360553264618, ACC:0.984375\n",
      "Training iteration 411 loss: 0.005798098631203175, ACC:1.0\n",
      "Training iteration 412 loss: 0.0032096332870423794, ACC:1.0\n",
      "Training iteration 413 loss: 0.009803395718336105, ACC:1.0\n",
      "Training iteration 414 loss: 0.005260993726551533, ACC:1.0\n",
      "Training iteration 415 loss: 0.014203889295458794, ACC:1.0\n",
      "Training iteration 416 loss: 0.01152011938393116, ACC:1.0\n",
      "Training iteration 417 loss: 0.002100615296512842, ACC:1.0\n",
      "Training iteration 418 loss: 0.029437873512506485, ACC:0.984375\n",
      "Training iteration 419 loss: 0.0222565159201622, ACC:1.0\n",
      "Training iteration 420 loss: 0.044584378600120544, ACC:0.984375\n",
      "Training iteration 421 loss: 0.024579772725701332, ACC:1.0\n",
      "Training iteration 422 loss: 0.07166612148284912, ACC:0.984375\n",
      "Training iteration 423 loss: 0.02178329788148403, ACC:1.0\n",
      "Training iteration 424 loss: 0.007847744040191174, ACC:1.0\n",
      "Training iteration 425 loss: 0.03446834534406662, ACC:0.984375\n",
      "Training iteration 426 loss: 0.0033649993129074574, ACC:1.0\n",
      "Training iteration 427 loss: 0.018254579976201057, ACC:0.984375\n",
      "Training iteration 428 loss: 0.016047492623329163, ACC:1.0\n",
      "Training iteration 429 loss: 0.0002623446052893996, ACC:1.0\n",
      "Training iteration 430 loss: 0.12779279053211212, ACC:0.984375\n",
      "Training iteration 431 loss: 0.0023070652969181538, ACC:1.0\n",
      "Training iteration 432 loss: 0.2075478732585907, ACC:0.96875\n",
      "Training iteration 433 loss: 0.0031324985902756453, ACC:1.0\n",
      "Training iteration 434 loss: 0.005737324710935354, ACC:1.0\n",
      "Training iteration 435 loss: 0.03718139976263046, ACC:0.984375\n",
      "Training iteration 436 loss: 0.0034004012122750282, ACC:1.0\n",
      "Training iteration 437 loss: 0.018512148410081863, ACC:1.0\n",
      "Training iteration 438 loss: 0.05991542339324951, ACC:0.96875\n",
      "Training iteration 439 loss: 0.005581772420555353, ACC:1.0\n",
      "Training iteration 440 loss: 0.04001518711447716, ACC:0.984375\n",
      "Training iteration 441 loss: 0.01960514299571514, ACC:1.0\n",
      "Training iteration 442 loss: 0.03322076052427292, ACC:0.984375\n",
      "Training iteration 443 loss: 0.01060143020004034, ACC:1.0\n",
      "Training iteration 444 loss: 0.01995786651968956, ACC:0.984375\n",
      "Training iteration 445 loss: 0.06273919343948364, ACC:0.96875\n",
      "Training iteration 446 loss: 0.07926994562149048, ACC:0.984375\n",
      "Training iteration 447 loss: 0.005921680014580488, ACC:1.0\n",
      "Training iteration 448 loss: 0.022041330114006996, ACC:1.0\n",
      "Training iteration 449 loss: 0.05434415489435196, ACC:0.984375\n",
      "Training iteration 450 loss: 0.010149026289582253, ACC:1.0\n",
      "Validation iteration 451 loss: 0.013469364494085312, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.03967500105500221, ACC: 0.984375\n",
      "Validation iteration 453 loss: 0.012125808745622635, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.006628192961215973, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.10718228667974472, ACC: 0.96875\n",
      "Validation iteration 456 loss: 0.0041033984161913395, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.02654583379626274, ACC: 0.984375\n",
      "Validation iteration 458 loss: 0.021021660417318344, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.013703062199056149, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.04492029920220375, ACC: 0.984375\n",
      "Validation iteration 461 loss: 0.005790503695607185, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.0008631604723632336, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.042356472462415695, ACC: 0.984375\n",
      "Validation iteration 464 loss: 0.06232854723930359, ACC: 0.96875\n",
      "Validation iteration 465 loss: 0.005686298944056034, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.09446641057729721, ACC: 0.96875\n",
      "Validation iteration 467 loss: 0.008684699423611164, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.1068718433380127, ACC: 0.953125\n",
      "Validation iteration 469 loss: 0.007623812183737755, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.008486147038638592, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.01017695665359497, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.007792285177856684, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.020977342501282692, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.02753414399921894, ACC: 0.984375\n",
      "Validation iteration 475 loss: 0.027536198496818542, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.004807617049664259, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.02291516587138176, ACC: 0.984375\n",
      "Validation iteration 478 loss: 0.06950666010379791, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.07073976844549179, ACC: 0.984375\n",
      "Validation iteration 480 loss: 0.06939979642629623, ACC: 0.96875\n",
      "Validation iteration 481 loss: 0.02182302623987198, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.008513853885233402, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.018002141267061234, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.004579897038638592, ACC: 1.0\n",
      "Validation iteration 485 loss: 0.03554710000753403, ACC: 0.984375\n",
      "Validation iteration 486 loss: 0.006667749956250191, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.008535110391676426, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.013873630203306675, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.01464839093387127, ACC: 1.0\n",
      "Validation iteration 490 loss: 0.02617424540221691, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.023847082629799843, ACC: 0.984375\n",
      "Validation iteration 492 loss: 0.010689115151762962, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.015135997906327248, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.01518760621547699, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.01653086394071579, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.012560045346617699, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.040388282388448715, ACC: 0.984375\n",
      "Validation iteration 498 loss: 0.008594741113483906, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.009205855429172516, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.030291259288787842, ACC: 0.984375\n",
      "-- Epoch 3 done -- Train loss: 0.03556059964725541, train ACC: 0.989375, val loss: 0.026094294656068086, val ACC: 0.9925\n",
      "<--- 9647.171295404434 seconds --->\n",
      "Training iteration 1 loss: 0.01252797245979309, ACC:1.0\n",
      "Training iteration 2 loss: 0.003908946178853512, ACC:1.0\n",
      "Training iteration 3 loss: 0.002246418036520481, ACC:1.0\n",
      "Training iteration 4 loss: 0.1658773422241211, ACC:0.9375\n",
      "Training iteration 5 loss: 0.006829784717410803, ACC:1.0\n",
      "Training iteration 6 loss: 0.012987950816750526, ACC:1.0\n",
      "Training iteration 7 loss: 0.0314909927546978, ACC:0.984375\n",
      "Training iteration 8 loss: 0.047372959554195404, ACC:0.984375\n",
      "Training iteration 9 loss: 0.008238698355853558, ACC:1.0\n",
      "Training iteration 10 loss: 0.015479999594390392, ACC:0.984375\n",
      "Training iteration 11 loss: 0.0018963267793878913, ACC:1.0\n",
      "Training iteration 12 loss: 0.01871103048324585, ACC:1.0\n",
      "Training iteration 13 loss: 0.0009421418653801084, ACC:1.0\n",
      "Training iteration 14 loss: 0.008661692030727863, ACC:1.0\n",
      "Training iteration 15 loss: 0.06560322642326355, ACC:0.953125\n",
      "Training iteration 16 loss: 0.010407709516584873, ACC:1.0\n",
      "Training iteration 17 loss: 0.0067301541566848755, ACC:1.0\n",
      "Training iteration 18 loss: 0.03816961869597435, ACC:0.984375\n",
      "Training iteration 19 loss: 0.023832805454730988, ACC:0.984375\n",
      "Training iteration 20 loss: 0.018469244241714478, ACC:1.0\n",
      "Training iteration 21 loss: 0.09421537071466446, ACC:0.984375\n",
      "Training iteration 22 loss: 0.0209023579955101, ACC:0.984375\n",
      "Training iteration 23 loss: 0.020992938429117203, ACC:0.984375\n",
      "Training iteration 24 loss: 0.006392584182322025, ACC:1.0\n",
      "Training iteration 25 loss: 0.0038443836383521557, ACC:1.0\n",
      "Training iteration 26 loss: 0.002019940409809351, ACC:1.0\n",
      "Training iteration 27 loss: 0.022490384057164192, ACC:0.984375\n",
      "Training iteration 28 loss: 0.006598455365747213, ACC:1.0\n",
      "Training iteration 29 loss: 0.07647211849689484, ACC:0.984375\n",
      "Training iteration 30 loss: 0.06694300472736359, ACC:0.96875\n",
      "Training iteration 31 loss: 0.009330792352557182, ACC:1.0\n",
      "Training iteration 32 loss: 0.13079069554805756, ACC:0.96875\n",
      "Training iteration 33 loss: 0.010344145819544792, ACC:1.0\n",
      "Training iteration 34 loss: 0.007539446000009775, ACC:1.0\n",
      "Training iteration 35 loss: 0.10673830658197403, ACC:0.984375\n",
      "Training iteration 36 loss: 0.04459018260240555, ACC:0.984375\n",
      "Training iteration 37 loss: 0.012896224856376648, ACC:1.0\n",
      "Training iteration 38 loss: 0.002798464149236679, ACC:1.0\n",
      "Training iteration 39 loss: 0.0036607859656214714, ACC:1.0\n",
      "Training iteration 40 loss: 0.03648628294467926, ACC:0.984375\n",
      "Training iteration 41 loss: 0.05363026261329651, ACC:0.96875\n",
      "Training iteration 42 loss: 0.011679844930768013, ACC:1.0\n",
      "Training iteration 43 loss: 0.007932494394481182, ACC:1.0\n",
      "Training iteration 44 loss: 0.04280569776892662, ACC:0.96875\n",
      "Training iteration 45 loss: 0.05408339574933052, ACC:0.96875\n",
      "Training iteration 46 loss: 0.004402500111609697, ACC:1.0\n",
      "Training iteration 47 loss: 0.008843200281262398, ACC:1.0\n",
      "Training iteration 48 loss: 0.07496662437915802, ACC:0.984375\n",
      "Training iteration 49 loss: 0.006402876693755388, ACC:1.0\n",
      "Training iteration 50 loss: 0.017865579575300217, ACC:1.0\n",
      "Training iteration 51 loss: 0.013252723962068558, ACC:1.0\n",
      "Training iteration 52 loss: 0.0018002921715378761, ACC:1.0\n",
      "Training iteration 53 loss: 0.07646188139915466, ACC:0.953125\n",
      "Training iteration 54 loss: 0.07105116546154022, ACC:0.984375\n",
      "Training iteration 55 loss: 0.03518006205558777, ACC:0.984375\n",
      "Training iteration 56 loss: 0.054965753108263016, ACC:0.96875\n",
      "Training iteration 57 loss: 0.018379904329776764, ACC:1.0\n",
      "Training iteration 58 loss: 0.01248595304787159, ACC:1.0\n",
      "Training iteration 59 loss: 0.08317957818508148, ACC:0.984375\n",
      "Training iteration 60 loss: 0.008798344060778618, ACC:1.0\n",
      "Training iteration 61 loss: 0.02879592403769493, ACC:0.984375\n",
      "Training iteration 62 loss: 0.006398824043571949, ACC:1.0\n",
      "Training iteration 63 loss: 0.025885924696922302, ACC:0.984375\n",
      "Training iteration 64 loss: 0.010499860160052776, ACC:1.0\n",
      "Training iteration 65 loss: 0.015687042847275734, ACC:0.984375\n",
      "Training iteration 66 loss: 0.008300959132611752, ACC:1.0\n",
      "Training iteration 67 loss: 0.010409679263830185, ACC:1.0\n",
      "Training iteration 68 loss: 0.011659652926027775, ACC:1.0\n",
      "Training iteration 69 loss: 0.0065573230385780334, ACC:1.0\n",
      "Training iteration 70 loss: 0.006515356246381998, ACC:1.0\n",
      "Training iteration 71 loss: 0.0821535736322403, ACC:0.984375\n",
      "Training iteration 72 loss: 0.0033845840953290462, ACC:1.0\n",
      "Training iteration 73 loss: 0.009573041461408138, ACC:1.0\n",
      "Training iteration 74 loss: 0.0033981390297412872, ACC:1.0\n",
      "Training iteration 75 loss: 0.002290321746841073, ACC:1.0\n",
      "Training iteration 76 loss: 0.0054778484627604485, ACC:1.0\n",
      "Training iteration 77 loss: 0.008269788697361946, ACC:1.0\n",
      "Training iteration 78 loss: 0.0009415935492143035, ACC:1.0\n",
      "Training iteration 79 loss: 0.003500245977193117, ACC:1.0\n",
      "Training iteration 80 loss: 0.006073347758501768, ACC:1.0\n",
      "Training iteration 81 loss: 0.0015197002794593573, ACC:1.0\n",
      "Training iteration 82 loss: 0.007959790527820587, ACC:1.0\n",
      "Training iteration 83 loss: 0.00834390427917242, ACC:1.0\n",
      "Training iteration 84 loss: 0.010445378720760345, ACC:1.0\n",
      "Training iteration 85 loss: 0.14712977409362793, ACC:0.984375\n",
      "Training iteration 86 loss: 0.024205375462770462, ACC:0.984375\n",
      "Training iteration 87 loss: 0.005855521187186241, ACC:1.0\n",
      "Training iteration 88 loss: 0.006314295809715986, ACC:1.0\n",
      "Training iteration 89 loss: 0.004386706277728081, ACC:1.0\n",
      "Training iteration 90 loss: 0.0486823171377182, ACC:0.984375\n",
      "Training iteration 91 loss: 0.004808314144611359, ACC:1.0\n",
      "Training iteration 92 loss: 0.02040080912411213, ACC:0.984375\n",
      "Training iteration 93 loss: 0.011781114153563976, ACC:1.0\n",
      "Training iteration 94 loss: 0.016642967239022255, ACC:1.0\n",
      "Training iteration 95 loss: 0.012564344331622124, ACC:1.0\n",
      "Training iteration 96 loss: 0.00459580821916461, ACC:1.0\n",
      "Training iteration 97 loss: 0.03061586059629917, ACC:0.984375\n",
      "Training iteration 98 loss: 0.002889123046770692, ACC:1.0\n",
      "Training iteration 99 loss: 0.01657225377857685, ACC:1.0\n",
      "Training iteration 100 loss: 0.0020717165898531675, ACC:1.0\n",
      "Training iteration 101 loss: 0.02342485450208187, ACC:0.984375\n",
      "Training iteration 102 loss: 0.019449984654784203, ACC:1.0\n",
      "Training iteration 103 loss: 0.0023123633582144976, ACC:1.0\n",
      "Training iteration 104 loss: 0.014492472633719444, ACC:1.0\n",
      "Training iteration 105 loss: 0.002834709594026208, ACC:1.0\n",
      "Training iteration 106 loss: 0.005483080632984638, ACC:1.0\n",
      "Training iteration 107 loss: 0.0029506513383239508, ACC:1.0\n",
      "Training iteration 108 loss: 0.03973096236586571, ACC:0.984375\n",
      "Training iteration 109 loss: 0.002148750238120556, ACC:1.0\n",
      "Training iteration 110 loss: 0.0035730379167944193, ACC:1.0\n",
      "Training iteration 111 loss: 0.009801721200346947, ACC:1.0\n",
      "Training iteration 112 loss: 0.009842213243246078, ACC:1.0\n",
      "Training iteration 113 loss: 0.0019063943764194846, ACC:1.0\n",
      "Training iteration 114 loss: 0.0067718420177698135, ACC:1.0\n",
      "Training iteration 115 loss: 0.0018069633515551686, ACC:1.0\n",
      "Training iteration 116 loss: 0.15237580239772797, ACC:0.96875\n",
      "Training iteration 117 loss: 0.028517022728919983, ACC:0.984375\n",
      "Training iteration 118 loss: 0.10083553194999695, ACC:0.96875\n",
      "Training iteration 119 loss: 0.031245745718479156, ACC:0.984375\n",
      "Training iteration 120 loss: 0.050790317356586456, ACC:0.96875\n",
      "Training iteration 121 loss: 0.04693290963768959, ACC:0.984375\n",
      "Training iteration 122 loss: 0.18239262700080872, ACC:0.96875\n",
      "Training iteration 123 loss: 0.08010674268007278, ACC:0.984375\n",
      "Training iteration 124 loss: 0.047051120549440384, ACC:0.984375\n",
      "Training iteration 125 loss: 0.04761695861816406, ACC:0.984375\n",
      "Training iteration 126 loss: 0.22389629483222961, ACC:0.984375\n",
      "Training iteration 127 loss: 0.15060830116271973, ACC:0.953125\n",
      "Training iteration 128 loss: 0.04186270758509636, ACC:0.96875\n",
      "Training iteration 129 loss: 0.15633681416511536, ACC:0.9375\n",
      "Training iteration 130 loss: 0.023112235590815544, ACC:1.0\n",
      "Training iteration 131 loss: 0.019300976768136024, ACC:0.984375\n",
      "Training iteration 132 loss: 0.03504481539130211, ACC:0.984375\n",
      "Training iteration 133 loss: 0.00886897649616003, ACC:1.0\n",
      "Training iteration 134 loss: 0.009881416335701942, ACC:1.0\n",
      "Training iteration 135 loss: 0.017153706401586533, ACC:1.0\n",
      "Training iteration 136 loss: 0.0040923478081822395, ACC:1.0\n",
      "Training iteration 137 loss: 0.10507664084434509, ACC:0.984375\n",
      "Training iteration 138 loss: 0.0031757971737533808, ACC:1.0\n",
      "Training iteration 139 loss: 0.2541191279888153, ACC:0.9375\n",
      "Training iteration 140 loss: 0.022641776129603386, ACC:0.984375\n",
      "Training iteration 141 loss: 0.07403142750263214, ACC:0.984375\n",
      "Training iteration 142 loss: 0.0292649082839489, ACC:0.984375\n",
      "Training iteration 143 loss: 0.0905739963054657, ACC:0.953125\n",
      "Training iteration 144 loss: 0.05736420676112175, ACC:0.984375\n",
      "Training iteration 145 loss: 0.06776700168848038, ACC:0.984375\n",
      "Training iteration 146 loss: 0.0828862190246582, ACC:0.96875\n",
      "Training iteration 147 loss: 0.04569368064403534, ACC:1.0\n",
      "Training iteration 148 loss: 0.05987036973237991, ACC:0.96875\n",
      "Training iteration 149 loss: 0.03316540643572807, ACC:0.984375\n",
      "Training iteration 150 loss: 0.08868380635976791, ACC:0.984375\n",
      "Training iteration 151 loss: 0.08513574302196503, ACC:0.984375\n",
      "Training iteration 152 loss: 0.05585062876343727, ACC:0.96875\n",
      "Training iteration 153 loss: 0.055898748338222504, ACC:0.96875\n",
      "Training iteration 154 loss: 0.005863660480827093, ACC:1.0\n",
      "Training iteration 155 loss: 0.003930339589715004, ACC:1.0\n",
      "Training iteration 156 loss: 0.017891772091388702, ACC:1.0\n",
      "Training iteration 157 loss: 0.013478470966219902, ACC:1.0\n",
      "Training iteration 158 loss: 0.005220170132815838, ACC:1.0\n",
      "Training iteration 159 loss: 0.006375430151820183, ACC:1.0\n",
      "Training iteration 160 loss: 0.06184956431388855, ACC:0.984375\n",
      "Training iteration 161 loss: 0.033867526799440384, ACC:0.984375\n",
      "Training iteration 162 loss: 0.034320373088121414, ACC:0.984375\n",
      "Training iteration 163 loss: 0.009905824437737465, ACC:1.0\n",
      "Training iteration 164 loss: 0.002765235025435686, ACC:1.0\n",
      "Training iteration 165 loss: 0.04280773922801018, ACC:0.984375\n",
      "Training iteration 166 loss: 0.12391834706068039, ACC:0.984375\n",
      "Training iteration 167 loss: 0.04575374722480774, ACC:0.96875\n",
      "Training iteration 168 loss: 0.0014818316558375955, ACC:1.0\n",
      "Training iteration 169 loss: 0.09085316956043243, ACC:0.984375\n",
      "Training iteration 170 loss: 0.09944372624158859, ACC:0.984375\n",
      "Training iteration 171 loss: 0.05927412956953049, ACC:0.984375\n",
      "Training iteration 172 loss: 0.002057489939033985, ACC:1.0\n",
      "Training iteration 173 loss: 0.043845731765031815, ACC:0.96875\n",
      "Training iteration 174 loss: 0.09118971973657608, ACC:0.984375\n",
      "Training iteration 175 loss: 0.003461414948105812, ACC:1.0\n",
      "Training iteration 176 loss: 0.02451261505484581, ACC:1.0\n",
      "Training iteration 177 loss: 0.027026642113924026, ACC:1.0\n",
      "Training iteration 178 loss: 0.023450767621397972, ACC:1.0\n",
      "Training iteration 179 loss: 0.021561771631240845, ACC:1.0\n",
      "Training iteration 180 loss: 0.023009948432445526, ACC:1.0\n",
      "Training iteration 181 loss: 0.023542819544672966, ACC:1.0\n",
      "Training iteration 182 loss: 0.012618235312402248, ACC:1.0\n",
      "Training iteration 183 loss: 0.1010424867272377, ACC:0.96875\n",
      "Training iteration 184 loss: 0.02341117523610592, ACC:0.984375\n",
      "Training iteration 185 loss: 0.017207926139235497, ACC:1.0\n",
      "Training iteration 186 loss: 0.0029808166436851025, ACC:1.0\n",
      "Training iteration 187 loss: 0.0371830090880394, ACC:0.984375\n",
      "Training iteration 188 loss: 0.13759826123714447, ACC:0.96875\n",
      "Training iteration 189 loss: 0.0014728385722264647, ACC:1.0\n",
      "Training iteration 190 loss: 0.015173847787082195, ACC:1.0\n",
      "Training iteration 191 loss: 0.006868061143904924, ACC:1.0\n",
      "Training iteration 192 loss: 0.007210923358798027, ACC:1.0\n",
      "Training iteration 193 loss: 0.017228927463293076, ACC:0.984375\n",
      "Training iteration 194 loss: 0.12104418128728867, ACC:0.953125\n",
      "Training iteration 195 loss: 0.013027339242398739, ACC:1.0\n",
      "Training iteration 196 loss: 0.11464748531579971, ACC:0.96875\n",
      "Training iteration 197 loss: 0.05963308364152908, ACC:0.984375\n",
      "Training iteration 198 loss: 0.03104284778237343, ACC:0.984375\n",
      "Training iteration 199 loss: 0.05609175190329552, ACC:0.96875\n",
      "Training iteration 200 loss: 0.020528076216578484, ACC:1.0\n",
      "Training iteration 201 loss: 0.0051477765664458275, ACC:1.0\n",
      "Training iteration 202 loss: 0.05238015577197075, ACC:0.96875\n",
      "Training iteration 203 loss: 0.030213143676519394, ACC:0.984375\n",
      "Training iteration 204 loss: 0.007810045965015888, ACC:1.0\n",
      "Training iteration 205 loss: 0.00542411906644702, ACC:1.0\n",
      "Training iteration 206 loss: 0.09810992330312729, ACC:0.96875\n",
      "Training iteration 207 loss: 0.0021860217675566673, ACC:1.0\n",
      "Training iteration 208 loss: 0.0778908059000969, ACC:0.984375\n",
      "Training iteration 209 loss: 0.011700418777763844, ACC:1.0\n",
      "Training iteration 210 loss: 0.023925380781292915, ACC:0.984375\n",
      "Training iteration 211 loss: 0.03794256970286369, ACC:0.984375\n",
      "Training iteration 212 loss: 0.014506365172564983, ACC:1.0\n",
      "Training iteration 213 loss: 0.003773642936721444, ACC:1.0\n",
      "Training iteration 214 loss: 0.014224689453840256, ACC:1.0\n",
      "Training iteration 215 loss: 0.02047806605696678, ACC:0.984375\n",
      "Training iteration 216 loss: 0.0668887048959732, ACC:0.96875\n",
      "Training iteration 217 loss: 0.013952543027698994, ACC:0.984375\n",
      "Training iteration 218 loss: 0.0074103525839746, ACC:1.0\n",
      "Training iteration 219 loss: 0.003367273136973381, ACC:1.0\n",
      "Training iteration 220 loss: 0.0030124103650450706, ACC:1.0\n",
      "Training iteration 221 loss: 0.07059463858604431, ACC:0.984375\n",
      "Training iteration 222 loss: 0.001528038177639246, ACC:1.0\n",
      "Training iteration 223 loss: 0.011843929998576641, ACC:1.0\n",
      "Training iteration 224 loss: 0.008085012435913086, ACC:1.0\n",
      "Training iteration 225 loss: 0.01705974154174328, ACC:1.0\n",
      "Training iteration 226 loss: 0.06214582547545433, ACC:0.984375\n",
      "Training iteration 227 loss: 0.021280691027641296, ACC:0.984375\n",
      "Training iteration 228 loss: 0.007816310971975327, ACC:1.0\n",
      "Training iteration 229 loss: 0.007262479979544878, ACC:1.0\n",
      "Training iteration 230 loss: 0.012170108035206795, ACC:1.0\n",
      "Training iteration 231 loss: 0.003979266155511141, ACC:1.0\n",
      "Training iteration 232 loss: 0.004052122589200735, ACC:1.0\n",
      "Training iteration 233 loss: 0.07835724949836731, ACC:0.96875\n",
      "Training iteration 234 loss: 0.01031402125954628, ACC:1.0\n",
      "Training iteration 235 loss: 0.010712986811995506, ACC:1.0\n",
      "Training iteration 236 loss: 0.018632588908076286, ACC:0.984375\n",
      "Training iteration 237 loss: 0.020083606243133545, ACC:1.0\n",
      "Training iteration 238 loss: 0.03440513834357262, ACC:0.984375\n",
      "Training iteration 239 loss: 0.016757620498538017, ACC:0.984375\n",
      "Training iteration 240 loss: 0.02925458736717701, ACC:1.0\n",
      "Training iteration 241 loss: 0.001060509355738759, ACC:1.0\n",
      "Training iteration 242 loss: 0.06018953397870064, ACC:0.953125\n",
      "Training iteration 243 loss: 0.012833175249397755, ACC:1.0\n",
      "Training iteration 244 loss: 0.010584361851215363, ACC:1.0\n",
      "Training iteration 245 loss: 0.03291604295372963, ACC:0.984375\n",
      "Training iteration 246 loss: 0.007295368239283562, ACC:1.0\n",
      "Training iteration 247 loss: 0.01436696294695139, ACC:1.0\n",
      "Training iteration 248 loss: 0.012706038542091846, ACC:1.0\n",
      "Training iteration 249 loss: 0.03356830030679703, ACC:0.984375\n",
      "Training iteration 250 loss: 0.006532521918416023, ACC:1.0\n",
      "Training iteration 251 loss: 0.12149829417467117, ACC:0.984375\n",
      "Training iteration 252 loss: 0.016018569469451904, ACC:1.0\n",
      "Training iteration 253 loss: 0.06900594383478165, ACC:0.984375\n",
      "Training iteration 254 loss: 0.07405958324670792, ACC:0.984375\n",
      "Training iteration 255 loss: 0.016634173691272736, ACC:1.0\n",
      "Training iteration 256 loss: 0.01517981942743063, ACC:1.0\n",
      "Training iteration 257 loss: 0.03519628942012787, ACC:0.984375\n",
      "Training iteration 258 loss: 0.012092274613678455, ACC:1.0\n",
      "Training iteration 259 loss: 0.1308872252702713, ACC:0.984375\n",
      "Training iteration 260 loss: 0.007715491112321615, ACC:1.0\n",
      "Training iteration 261 loss: 0.007275765296071768, ACC:1.0\n",
      "Training iteration 262 loss: 0.018644139170646667, ACC:0.984375\n",
      "Training iteration 263 loss: 0.030800452455878258, ACC:0.984375\n",
      "Training iteration 264 loss: 0.052592430263757706, ACC:0.984375\n",
      "Training iteration 265 loss: 0.02168484963476658, ACC:0.984375\n",
      "Training iteration 266 loss: 0.0554005391895771, ACC:0.984375\n",
      "Training iteration 267 loss: 0.0025174403563141823, ACC:1.0\n",
      "Training iteration 268 loss: 0.0027102716267108917, ACC:1.0\n",
      "Training iteration 269 loss: 0.05809057131409645, ACC:0.96875\n",
      "Training iteration 270 loss: 0.008167644962668419, ACC:1.0\n",
      "Training iteration 271 loss: 0.017941895872354507, ACC:1.0\n",
      "Training iteration 272 loss: 0.014615838415920734, ACC:1.0\n",
      "Training iteration 273 loss: 0.027127403765916824, ACC:0.984375\n",
      "Training iteration 274 loss: 0.02923126518726349, ACC:0.984375\n",
      "Training iteration 275 loss: 0.029757341369986534, ACC:0.984375\n",
      "Training iteration 276 loss: 0.07077158242464066, ACC:0.96875\n",
      "Training iteration 277 loss: 0.007260564714670181, ACC:1.0\n",
      "Training iteration 278 loss: 0.007742941379547119, ACC:1.0\n",
      "Training iteration 279 loss: 0.0022532856091856956, ACC:1.0\n",
      "Training iteration 280 loss: 0.0009938015136867762, ACC:1.0\n",
      "Training iteration 281 loss: 0.05225959047675133, ACC:0.984375\n",
      "Training iteration 282 loss: 0.005182710941880941, ACC:1.0\n",
      "Training iteration 283 loss: 0.000865835347212851, ACC:1.0\n",
      "Training iteration 284 loss: 0.06780935078859329, ACC:0.984375\n",
      "Training iteration 285 loss: 0.0014564215671271086, ACC:1.0\n",
      "Training iteration 286 loss: 0.0005670159589499235, ACC:1.0\n",
      "Training iteration 287 loss: 0.018509352579712868, ACC:0.984375\n",
      "Training iteration 288 loss: 0.004015292506664991, ACC:1.0\n",
      "Training iteration 289 loss: 0.17602255940437317, ACC:0.953125\n",
      "Training iteration 290 loss: 0.03092137910425663, ACC:0.984375\n",
      "Training iteration 291 loss: 0.025802433490753174, ACC:0.984375\n",
      "Training iteration 292 loss: 0.001789382309652865, ACC:1.0\n",
      "Training iteration 293 loss: 0.05311908572912216, ACC:0.984375\n",
      "Training iteration 294 loss: 0.020675335079431534, ACC:0.984375\n",
      "Training iteration 295 loss: 0.0030448997858911753, ACC:1.0\n",
      "Training iteration 296 loss: 0.06362826377153397, ACC:0.984375\n",
      "Training iteration 297 loss: 0.011104241013526917, ACC:1.0\n",
      "Training iteration 298 loss: 0.006948333233594894, ACC:1.0\n",
      "Training iteration 299 loss: 0.016099318861961365, ACC:1.0\n",
      "Training iteration 300 loss: 0.01596829667687416, ACC:1.0\n",
      "Training iteration 301 loss: 0.01583768054842949, ACC:1.0\n",
      "Training iteration 302 loss: 0.02827204018831253, ACC:0.984375\n",
      "Training iteration 303 loss: 0.06008131429553032, ACC:0.984375\n",
      "Training iteration 304 loss: 0.008234594948589802, ACC:1.0\n",
      "Training iteration 305 loss: 0.011659489944577217, ACC:1.0\n",
      "Training iteration 306 loss: 0.02515585348010063, ACC:0.984375\n",
      "Training iteration 307 loss: 0.0442940928041935, ACC:0.984375\n",
      "Training iteration 308 loss: 0.11999912559986115, ACC:0.984375\n",
      "Training iteration 309 loss: 0.00999465398490429, ACC:1.0\n",
      "Training iteration 310 loss: 0.03561116382479668, ACC:0.96875\n",
      "Training iteration 311 loss: 0.020193882286548615, ACC:0.984375\n",
      "Training iteration 312 loss: 0.013615298084914684, ACC:1.0\n",
      "Training iteration 313 loss: 0.04789590463042259, ACC:0.984375\n",
      "Training iteration 314 loss: 0.047671105712652206, ACC:0.984375\n",
      "Training iteration 315 loss: 0.03544945269823074, ACC:0.984375\n",
      "Training iteration 316 loss: 0.0027889260090887547, ACC:1.0\n",
      "Training iteration 317 loss: 0.0768391415476799, ACC:0.984375\n",
      "Training iteration 318 loss: 0.01843966357409954, ACC:0.984375\n",
      "Training iteration 319 loss: 0.016515661031007767, ACC:1.0\n",
      "Training iteration 320 loss: 0.015248492360115051, ACC:1.0\n",
      "Training iteration 321 loss: 0.010656789876520634, ACC:1.0\n",
      "Training iteration 322 loss: 0.008547919802367687, ACC:1.0\n",
      "Training iteration 323 loss: 0.01860129088163376, ACC:1.0\n",
      "Training iteration 324 loss: 0.0038226612377911806, ACC:1.0\n",
      "Training iteration 325 loss: 0.000828209740575403, ACC:1.0\n",
      "Training iteration 326 loss: 0.0030994624830782413, ACC:1.0\n",
      "Training iteration 327 loss: 0.08575727045536041, ACC:0.984375\n",
      "Training iteration 328 loss: 0.13398626446723938, ACC:0.984375\n",
      "Training iteration 329 loss: 0.11790327727794647, ACC:0.984375\n",
      "Training iteration 330 loss: 0.02449052780866623, ACC:1.0\n",
      "Training iteration 331 loss: 0.02132463827729225, ACC:0.984375\n",
      "Training iteration 332 loss: 0.04038231074810028, ACC:0.96875\n",
      "Training iteration 333 loss: 0.06134965270757675, ACC:0.984375\n",
      "Training iteration 334 loss: 0.012575088068842888, ACC:1.0\n",
      "Training iteration 335 loss: 0.008455988951027393, ACC:1.0\n",
      "Training iteration 336 loss: 0.022549865767359734, ACC:0.984375\n",
      "Training iteration 337 loss: 0.014663646928966045, ACC:1.0\n",
      "Training iteration 338 loss: 0.047872837632894516, ACC:0.984375\n",
      "Training iteration 339 loss: 0.02725641429424286, ACC:0.984375\n",
      "Training iteration 340 loss: 0.046805236488580704, ACC:0.984375\n",
      "Training iteration 341 loss: 0.042082469910383224, ACC:0.984375\n",
      "Training iteration 342 loss: 0.04580167308449745, ACC:0.984375\n",
      "Training iteration 343 loss: 0.008415142074227333, ACC:1.0\n",
      "Training iteration 344 loss: 0.005032516550272703, ACC:1.0\n",
      "Training iteration 345 loss: 0.01893506944179535, ACC:1.0\n",
      "Training iteration 346 loss: 0.003875348251312971, ACC:1.0\n",
      "Training iteration 347 loss: 0.00376674672588706, ACC:1.0\n",
      "Training iteration 348 loss: 0.005644447170197964, ACC:1.0\n",
      "Training iteration 349 loss: 0.007902706041932106, ACC:1.0\n",
      "Training iteration 350 loss: 0.010194739326834679, ACC:1.0\n",
      "Training iteration 351 loss: 0.09106294065713882, ACC:0.96875\n",
      "Training iteration 352 loss: 0.047056861221790314, ACC:0.96875\n",
      "Training iteration 353 loss: 0.06304033100605011, ACC:0.984375\n",
      "Training iteration 354 loss: 0.00992362666875124, ACC:1.0\n",
      "Training iteration 355 loss: 0.004878242965787649, ACC:1.0\n",
      "Training iteration 356 loss: 0.0989055186510086, ACC:0.96875\n",
      "Training iteration 357 loss: 0.004191742278635502, ACC:1.0\n",
      "Training iteration 358 loss: 0.010200632736086845, ACC:1.0\n",
      "Training iteration 359 loss: 0.0034526176750659943, ACC:1.0\n",
      "Training iteration 360 loss: 0.030066553503274918, ACC:0.984375\n",
      "Training iteration 361 loss: 0.0962539091706276, ACC:0.953125\n",
      "Training iteration 362 loss: 0.024403586983680725, ACC:0.984375\n",
      "Training iteration 363 loss: 0.0025446913205087185, ACC:1.0\n",
      "Training iteration 364 loss: 0.02540665678679943, ACC:0.984375\n",
      "Training iteration 365 loss: 0.07099293172359467, ACC:0.96875\n",
      "Training iteration 366 loss: 0.012406004592776299, ACC:1.0\n",
      "Training iteration 367 loss: 0.1428893506526947, ACC:0.96875\n",
      "Training iteration 368 loss: 0.005541251972317696, ACC:1.0\n",
      "Training iteration 369 loss: 0.018839336931705475, ACC:1.0\n",
      "Training iteration 370 loss: 0.1193719208240509, ACC:0.96875\n",
      "Training iteration 371 loss: 0.019373388960957527, ACC:1.0\n",
      "Training iteration 372 loss: 0.04215423762798309, ACC:0.984375\n",
      "Training iteration 373 loss: 0.07564163208007812, ACC:0.953125\n",
      "Training iteration 374 loss: 0.01733904704451561, ACC:1.0\n",
      "Training iteration 375 loss: 0.016286974772810936, ACC:1.0\n",
      "Training iteration 376 loss: 0.026905423030257225, ACC:0.984375\n",
      "Training iteration 377 loss: 0.024886732921004295, ACC:0.984375\n",
      "Training iteration 378 loss: 0.08605846762657166, ACC:0.96875\n",
      "Training iteration 379 loss: 0.04883551970124245, ACC:0.984375\n",
      "Training iteration 380 loss: 0.010174401104450226, ACC:1.0\n",
      "Training iteration 381 loss: 0.00803951732814312, ACC:1.0\n",
      "Training iteration 382 loss: 0.017685938626527786, ACC:0.984375\n",
      "Training iteration 383 loss: 0.013823541812598705, ACC:1.0\n",
      "Training iteration 384 loss: 0.004782828502357006, ACC:1.0\n",
      "Training iteration 385 loss: 0.20973575115203857, ACC:0.96875\n",
      "Training iteration 386 loss: 0.01104341447353363, ACC:1.0\n",
      "Training iteration 387 loss: 0.0026681232266128063, ACC:1.0\n",
      "Training iteration 388 loss: 0.007682452909648418, ACC:1.0\n",
      "Training iteration 389 loss: 0.013286675326526165, ACC:1.0\n",
      "Training iteration 390 loss: 0.00979022216051817, ACC:1.0\n",
      "Training iteration 391 loss: 0.018764710053801537, ACC:0.984375\n",
      "Training iteration 392 loss: 0.022139303386211395, ACC:0.984375\n",
      "Training iteration 393 loss: 0.023233499377965927, ACC:0.984375\n",
      "Training iteration 394 loss: 0.007581330370157957, ACC:1.0\n",
      "Training iteration 395 loss: 0.10055604577064514, ACC:0.96875\n",
      "Training iteration 396 loss: 0.03582548722624779, ACC:0.984375\n",
      "Training iteration 397 loss: 0.015895739197731018, ACC:1.0\n",
      "Training iteration 398 loss: 0.0731874629855156, ACC:0.984375\n",
      "Training iteration 399 loss: 0.031230732798576355, ACC:1.0\n",
      "Training iteration 400 loss: 0.02502097375690937, ACC:0.984375\n",
      "Training iteration 401 loss: 0.0702812522649765, ACC:0.96875\n",
      "Training iteration 402 loss: 0.03216230124235153, ACC:0.96875\n",
      "Training iteration 403 loss: 0.020695626735687256, ACC:0.984375\n",
      "Training iteration 404 loss: 0.01399972103536129, ACC:1.0\n",
      "Training iteration 405 loss: 0.043395206332206726, ACC:0.984375\n",
      "Training iteration 406 loss: 0.01243082620203495, ACC:1.0\n",
      "Training iteration 407 loss: 0.04345784708857536, ACC:0.96875\n",
      "Training iteration 408 loss: 0.015376790426671505, ACC:1.0\n",
      "Training iteration 409 loss: 0.027920447289943695, ACC:0.984375\n",
      "Training iteration 410 loss: 0.10707048326730728, ACC:0.953125\n",
      "Training iteration 411 loss: 0.049618884921073914, ACC:0.984375\n",
      "Training iteration 412 loss: 0.05347314476966858, ACC:0.96875\n",
      "Training iteration 413 loss: 0.017143502831459045, ACC:1.0\n",
      "Training iteration 414 loss: 0.016118770465254784, ACC:1.0\n",
      "Training iteration 415 loss: 0.07311967760324478, ACC:0.984375\n",
      "Training iteration 416 loss: 0.025295289233326912, ACC:1.0\n",
      "Training iteration 417 loss: 0.02492506429553032, ACC:1.0\n",
      "Training iteration 418 loss: 0.10111577808856964, ACC:0.984375\n",
      "Training iteration 419 loss: 0.045379266142845154, ACC:0.984375\n",
      "Training iteration 420 loss: 0.008612744510173798, ACC:1.0\n",
      "Training iteration 421 loss: 0.009480494074523449, ACC:1.0\n",
      "Training iteration 422 loss: 0.008245735429227352, ACC:1.0\n",
      "Training iteration 423 loss: 0.007953625172376633, ACC:1.0\n",
      "Training iteration 424 loss: 0.0068761310540139675, ACC:1.0\n",
      "Training iteration 425 loss: 0.02855081669986248, ACC:0.984375\n",
      "Training iteration 426 loss: 0.015530604869127274, ACC:1.0\n",
      "Training iteration 427 loss: 0.0699704959988594, ACC:0.984375\n",
      "Training iteration 428 loss: 0.04202426224946976, ACC:0.96875\n",
      "Training iteration 429 loss: 0.0070564523339271545, ACC:1.0\n",
      "Training iteration 430 loss: 0.002563198795542121, ACC:1.0\n",
      "Training iteration 431 loss: 0.00634761992841959, ACC:1.0\n",
      "Training iteration 432 loss: 0.10232227295637131, ACC:0.984375\n",
      "Training iteration 433 loss: 0.08914290368556976, ACC:0.96875\n",
      "Training iteration 434 loss: 0.0005592243396677077, ACC:1.0\n",
      "Training iteration 435 loss: 0.006552490405738354, ACC:1.0\n",
      "Training iteration 436 loss: 0.0038606980815529823, ACC:1.0\n",
      "Training iteration 437 loss: 0.006456505507230759, ACC:1.0\n",
      "Training iteration 438 loss: 0.007048367522656918, ACC:1.0\n",
      "Training iteration 439 loss: 0.012006429955363274, ACC:1.0\n",
      "Training iteration 440 loss: 0.021602477878332138, ACC:0.984375\n",
      "Training iteration 441 loss: 0.034072551876306534, ACC:0.984375\n",
      "Training iteration 442 loss: 0.006495542358607054, ACC:1.0\n",
      "Training iteration 443 loss: 0.005459512583911419, ACC:1.0\n",
      "Training iteration 444 loss: 0.014399748295545578, ACC:0.984375\n",
      "Training iteration 445 loss: 0.04553227126598358, ACC:0.984375\n",
      "Training iteration 446 loss: 0.11316093057394028, ACC:0.9375\n",
      "Training iteration 447 loss: 0.011672222055494785, ACC:1.0\n",
      "Training iteration 448 loss: 0.0005593063542619348, ACC:1.0\n",
      "Training iteration 449 loss: 0.14982837438583374, ACC:0.984375\n",
      "Training iteration 450 loss: 0.006972113624215126, ACC:1.0\n",
      "Validation iteration 451 loss: 0.0024999231100082397, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.01891954056918621, ACC: 0.984375\n",
      "Validation iteration 453 loss: 0.039609573781490326, ACC: 0.984375\n",
      "Validation iteration 454 loss: 0.003605973208323121, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.08193706721067429, ACC: 0.96875\n",
      "Validation iteration 456 loss: 0.007942918688058853, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.015820549800992012, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.0035561597906053066, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.003991146571934223, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.07742970436811447, ACC: 0.984375\n",
      "Validation iteration 461 loss: 0.004635679069906473, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.014635739848017693, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.010187605395913124, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.010014375671744347, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.06821874529123306, ACC: 0.984375\n",
      "Validation iteration 466 loss: 0.002334682736545801, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.003875080030411482, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.028928354382514954, ACC: 0.984375\n",
      "Validation iteration 469 loss: 0.0035576527006924152, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.008613956160843372, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.005355099681764841, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.010350126773118973, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.02615523897111416, ACC: 0.984375\n",
      "Validation iteration 474 loss: 0.08540332317352295, ACC: 0.96875\n",
      "Validation iteration 475 loss: 0.008079838007688522, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.006622219458222389, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.024720270186662674, ACC: 0.984375\n",
      "Validation iteration 478 loss: 0.10410651564598083, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.0030286249238997698, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.024285297840833664, ACC: 0.984375\n",
      "Validation iteration 481 loss: 0.0017809809651225805, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.0074007026851177216, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.023340260609984398, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.021830813959240913, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.011987835168838501, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.013479691930115223, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.028554338961839676, ACC: 0.984375\n",
      "Validation iteration 488 loss: 0.05006757751107216, ACC: 0.96875\n",
      "Validation iteration 489 loss: 0.08888310194015503, ACC: 0.984375\n",
      "Validation iteration 490 loss: 0.014941450208425522, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.006154317408800125, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.00789823941886425, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.06661543250083923, ACC: 0.96875\n",
      "Validation iteration 494 loss: 0.008613929152488708, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.005674014799296856, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.009008916094899178, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.0008274237043224275, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.004105300176888704, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.030667003244161606, ACC: 0.984375\n",
      "Validation iteration 500 loss: 0.018912337720394135, ACC: 0.984375\n",
      "-- Epoch 4 done -- Train loss: 0.03284956498588953, train ACC: 0.9902430555555556, val loss: 0.02258329302421771, val ACC: 0.993125\n",
      "<--- 10331.029798030853 seconds --->\n",
      "Training iteration 1 loss: 0.02058723010122776, ACC:0.984375\n",
      "Training iteration 2 loss: 0.03610292822122574, ACC:0.984375\n",
      "Training iteration 3 loss: 0.027760379016399384, ACC:0.984375\n",
      "Training iteration 4 loss: 0.08843227475881577, ACC:0.984375\n",
      "Training iteration 5 loss: 0.007593265734612942, ACC:1.0\n",
      "Training iteration 6 loss: 0.033299341797828674, ACC:0.984375\n",
      "Training iteration 7 loss: 0.016599349677562714, ACC:0.984375\n",
      "Training iteration 8 loss: 0.006124443840235472, ACC:1.0\n",
      "Training iteration 9 loss: 0.07725536823272705, ACC:0.984375\n",
      "Training iteration 10 loss: 0.01899564079940319, ACC:1.0\n",
      "Training iteration 11 loss: 0.02084209769964218, ACC:1.0\n",
      "Training iteration 12 loss: 0.039614446461200714, ACC:0.984375\n",
      "Training iteration 13 loss: 0.0014876623172312975, ACC:1.0\n",
      "Training iteration 14 loss: 0.00586326327174902, ACC:1.0\n",
      "Training iteration 15 loss: 0.001654885709285736, ACC:1.0\n",
      "Training iteration 16 loss: 0.012163868173956871, ACC:1.0\n",
      "Training iteration 17 loss: 0.028758559376001358, ACC:0.984375\n",
      "Training iteration 18 loss: 0.024842575192451477, ACC:0.984375\n",
      "Training iteration 19 loss: 0.043786562979221344, ACC:0.984375\n",
      "Training iteration 20 loss: 0.0029545966535806656, ACC:1.0\n",
      "Training iteration 21 loss: 0.028640594333410263, ACC:0.984375\n",
      "Training iteration 22 loss: 0.10822002589702606, ACC:0.984375\n",
      "Training iteration 23 loss: 0.006835654843598604, ACC:1.0\n",
      "Training iteration 24 loss: 0.022464094683527946, ACC:0.984375\n",
      "Training iteration 25 loss: 0.0020949942991137505, ACC:1.0\n",
      "Training iteration 26 loss: 0.003958195447921753, ACC:1.0\n",
      "Training iteration 27 loss: 0.04588741436600685, ACC:0.984375\n",
      "Training iteration 28 loss: 0.010025573894381523, ACC:1.0\n",
      "Training iteration 29 loss: 0.013869521208107471, ACC:1.0\n",
      "Training iteration 30 loss: 0.07750377058982849, ACC:0.984375\n",
      "Training iteration 31 loss: 0.005297909490764141, ACC:1.0\n",
      "Training iteration 32 loss: 0.0020665687043219805, ACC:1.0\n",
      "Training iteration 33 loss: 0.007410179823637009, ACC:1.0\n",
      "Training iteration 34 loss: 0.0012570273829624057, ACC:1.0\n",
      "Training iteration 35 loss: 0.1094675287604332, ACC:0.984375\n",
      "Training iteration 36 loss: 0.06346579641103745, ACC:0.984375\n",
      "Training iteration 37 loss: 0.004823398310691118, ACC:1.0\n",
      "Training iteration 38 loss: 0.012114262208342552, ACC:1.0\n",
      "Training iteration 39 loss: 0.005881978664547205, ACC:1.0\n",
      "Training iteration 40 loss: 0.0084987822920084, ACC:1.0\n",
      "Training iteration 41 loss: 0.004644202999770641, ACC:1.0\n",
      "Training iteration 42 loss: 0.011369114741683006, ACC:1.0\n",
      "Training iteration 43 loss: 0.013751830905675888, ACC:1.0\n",
      "Training iteration 44 loss: 0.0050414493307471275, ACC:1.0\n",
      "Training iteration 45 loss: 0.0017461845418438315, ACC:1.0\n",
      "Training iteration 46 loss: 0.009426390752196312, ACC:1.0\n",
      "Training iteration 47 loss: 0.0038265264593064785, ACC:1.0\n",
      "Training iteration 48 loss: 0.009208062663674355, ACC:1.0\n",
      "Training iteration 49 loss: 0.027093790471553802, ACC:0.984375\n",
      "Training iteration 50 loss: 0.01972617395222187, ACC:1.0\n",
      "Training iteration 51 loss: 0.021096130833029747, ACC:1.0\n",
      "Training iteration 52 loss: 0.019929250702261925, ACC:1.0\n",
      "Training iteration 53 loss: 0.06596256792545319, ACC:0.984375\n",
      "Training iteration 54 loss: 0.0006570827099494636, ACC:1.0\n",
      "Training iteration 55 loss: 0.036427173763513565, ACC:0.984375\n",
      "Training iteration 56 loss: 0.002115100622177124, ACC:1.0\n",
      "Training iteration 57 loss: 0.0005253998097032309, ACC:1.0\n",
      "Training iteration 58 loss: 0.002040620194748044, ACC:1.0\n",
      "Training iteration 59 loss: 0.02590520679950714, ACC:0.984375\n",
      "Training iteration 60 loss: 0.08720841258764267, ACC:0.984375\n",
      "Training iteration 61 loss: 0.13331075012683868, ACC:0.953125\n",
      "Training iteration 62 loss: 0.004839891102164984, ACC:1.0\n",
      "Training iteration 63 loss: 0.05765046551823616, ACC:0.984375\n",
      "Training iteration 64 loss: 0.013735960237681866, ACC:1.0\n",
      "Training iteration 65 loss: 0.03729932755231857, ACC:0.96875\n",
      "Training iteration 66 loss: 0.07326170057058334, ACC:0.984375\n",
      "Training iteration 67 loss: 0.032284967601299286, ACC:0.984375\n",
      "Training iteration 68 loss: 0.018320230767130852, ACC:1.0\n",
      "Training iteration 69 loss: 0.003013881854712963, ACC:1.0\n",
      "Training iteration 70 loss: 0.010982787236571312, ACC:1.0\n",
      "Training iteration 71 loss: 0.007331206928938627, ACC:1.0\n",
      "Training iteration 72 loss: 0.005132016260176897, ACC:1.0\n",
      "Training iteration 73 loss: 0.002871384844183922, ACC:1.0\n",
      "Training iteration 74 loss: 0.009907854720950127, ACC:1.0\n",
      "Training iteration 75 loss: 0.005433035083115101, ACC:1.0\n",
      "Training iteration 76 loss: 0.002964387647807598, ACC:1.0\n",
      "Training iteration 77 loss: 0.055398013442754745, ACC:0.984375\n",
      "Training iteration 78 loss: 0.04316746070981026, ACC:0.984375\n",
      "Training iteration 79 loss: 0.0026697281282395124, ACC:1.0\n",
      "Training iteration 80 loss: 0.0047725108452141285, ACC:1.0\n",
      "Training iteration 81 loss: 0.0013535339385271072, ACC:1.0\n",
      "Training iteration 82 loss: 0.009774456731975079, ACC:1.0\n",
      "Training iteration 83 loss: 0.02503201924264431, ACC:0.984375\n",
      "Training iteration 84 loss: 0.08841902762651443, ACC:0.96875\n",
      "Training iteration 85 loss: 0.0991794764995575, ACC:0.984375\n",
      "Training iteration 86 loss: 0.002064879285171628, ACC:1.0\n",
      "Training iteration 87 loss: 0.008355686441063881, ACC:1.0\n",
      "Training iteration 88 loss: 0.12847644090652466, ACC:0.953125\n",
      "Training iteration 89 loss: 0.008718692697584629, ACC:1.0\n",
      "Training iteration 90 loss: 0.17220164835453033, ACC:0.96875\n",
      "Training iteration 91 loss: 0.009304795414209366, ACC:1.0\n",
      "Training iteration 92 loss: 0.03682680055499077, ACC:0.984375\n",
      "Training iteration 93 loss: 0.032237276434898376, ACC:0.984375\n",
      "Training iteration 94 loss: 0.010846114717423916, ACC:1.0\n",
      "Training iteration 95 loss: 0.00206656358204782, ACC:1.0\n",
      "Training iteration 96 loss: 0.002050310606136918, ACC:1.0\n",
      "Training iteration 97 loss: 0.0029606460593640804, ACC:1.0\n",
      "Training iteration 98 loss: 0.19464074075222015, ACC:0.96875\n",
      "Training iteration 99 loss: 0.06988994032144547, ACC:0.96875\n",
      "Training iteration 100 loss: 0.016765832901000977, ACC:0.984375\n",
      "Training iteration 101 loss: 0.004930915776640177, ACC:1.0\n",
      "Training iteration 102 loss: 0.005628777667880058, ACC:1.0\n",
      "Training iteration 103 loss: 0.12182973325252533, ACC:0.96875\n",
      "Training iteration 104 loss: 0.06289972364902496, ACC:0.984375\n",
      "Training iteration 105 loss: 0.045315325260162354, ACC:1.0\n",
      "Training iteration 106 loss: 0.087893545627594, ACC:0.96875\n",
      "Training iteration 107 loss: 0.09615015983581543, ACC:0.96875\n",
      "Training iteration 108 loss: 0.009253020398318768, ACC:1.0\n",
      "Training iteration 109 loss: 0.017994854599237442, ACC:1.0\n",
      "Training iteration 110 loss: 0.0037485866341739893, ACC:1.0\n",
      "Training iteration 111 loss: 0.0063501340337097645, ACC:1.0\n",
      "Training iteration 112 loss: 0.022415073588490486, ACC:0.984375\n",
      "Training iteration 113 loss: 0.020629139617085457, ACC:0.984375\n",
      "Training iteration 114 loss: 0.018036510795354843, ACC:1.0\n",
      "Training iteration 115 loss: 0.028497494757175446, ACC:0.984375\n",
      "Training iteration 116 loss: 0.011873397044837475, ACC:1.0\n",
      "Training iteration 117 loss: 0.05784427747130394, ACC:0.96875\n",
      "Training iteration 118 loss: 0.03164944425225258, ACC:0.96875\n",
      "Training iteration 119 loss: 0.02679816447198391, ACC:0.984375\n",
      "Training iteration 120 loss: 0.10830461978912354, ACC:0.984375\n",
      "Training iteration 121 loss: 0.009326441213488579, ACC:1.0\n",
      "Training iteration 122 loss: 0.02637019008398056, ACC:0.984375\n",
      "Training iteration 123 loss: 0.008246345445513725, ACC:1.0\n",
      "Training iteration 124 loss: 0.0008593680104240775, ACC:1.0\n",
      "Training iteration 125 loss: 0.028231069445610046, ACC:0.984375\n",
      "Training iteration 126 loss: 0.010682196356356144, ACC:1.0\n",
      "Training iteration 127 loss: 0.129889115691185, ACC:0.96875\n",
      "Training iteration 128 loss: 0.09736087918281555, ACC:0.984375\n",
      "Training iteration 129 loss: 0.08273620158433914, ACC:0.96875\n",
      "Training iteration 130 loss: 0.015621185302734375, ACC:1.0\n",
      "Training iteration 131 loss: 0.012637815438210964, ACC:1.0\n",
      "Training iteration 132 loss: 0.16718322038650513, ACC:0.984375\n",
      "Training iteration 133 loss: 0.05723607912659645, ACC:0.984375\n",
      "Training iteration 134 loss: 0.019819924607872963, ACC:1.0\n",
      "Training iteration 135 loss: 0.10544395446777344, ACC:0.953125\n",
      "Training iteration 136 loss: 0.0033716028556227684, ACC:1.0\n",
      "Training iteration 137 loss: 0.0284869484603405, ACC:0.984375\n",
      "Training iteration 138 loss: 0.06252402067184448, ACC:0.96875\n",
      "Training iteration 139 loss: 0.03466983884572983, ACC:0.984375\n",
      "Training iteration 140 loss: 0.07056081295013428, ACC:0.953125\n",
      "Training iteration 141 loss: 0.06163056567311287, ACC:0.96875\n",
      "Training iteration 142 loss: 0.025719093158841133, ACC:1.0\n",
      "Training iteration 143 loss: 0.019337063655257225, ACC:1.0\n",
      "Training iteration 144 loss: 0.004778813570737839, ACC:1.0\n",
      "Training iteration 145 loss: 0.027870111167430878, ACC:0.984375\n",
      "Training iteration 146 loss: 0.018018705770373344, ACC:1.0\n",
      "Training iteration 147 loss: 0.004054136108607054, ACC:1.0\n",
      "Training iteration 148 loss: 0.0041520921513438225, ACC:1.0\n",
      "Training iteration 149 loss: 0.00252278964035213, ACC:1.0\n",
      "Training iteration 150 loss: 0.0257556214928627, ACC:0.984375\n",
      "Training iteration 151 loss: 0.018046202138066292, ACC:1.0\n",
      "Training iteration 152 loss: 0.024751901626586914, ACC:0.984375\n",
      "Training iteration 153 loss: 0.0014672029064968228, ACC:1.0\n",
      "Training iteration 154 loss: 0.0038430606946349144, ACC:1.0\n",
      "Training iteration 155 loss: 0.0011560664279386401, ACC:1.0\n",
      "Training iteration 156 loss: 0.002669283654540777, ACC:1.0\n",
      "Training iteration 157 loss: 0.02997259423136711, ACC:0.984375\n",
      "Training iteration 158 loss: 0.02275118976831436, ACC:0.984375\n",
      "Training iteration 159 loss: 0.013971736654639244, ACC:0.984375\n",
      "Training iteration 160 loss: 0.11089473962783813, ACC:0.984375\n",
      "Training iteration 161 loss: 0.016399042680859566, ACC:0.984375\n",
      "Training iteration 162 loss: 0.008222537115216255, ACC:1.0\n",
      "Training iteration 163 loss: 0.0010970885632559657, ACC:1.0\n",
      "Training iteration 164 loss: 0.08392884582281113, ACC:0.984375\n",
      "Training iteration 165 loss: 0.003268084954470396, ACC:1.0\n",
      "Training iteration 166 loss: 0.022225957363843918, ACC:0.984375\n",
      "Training iteration 167 loss: 0.20307762920856476, ACC:0.953125\n",
      "Training iteration 168 loss: 0.11938764154911041, ACC:0.984375\n",
      "Training iteration 169 loss: 0.0038018105551600456, ACC:1.0\n",
      "Training iteration 170 loss: 0.007555121555924416, ACC:1.0\n",
      "Training iteration 171 loss: 0.06676287949085236, ACC:0.984375\n",
      "Training iteration 172 loss: 0.003080381080508232, ACC:1.0\n",
      "Training iteration 173 loss: 0.25035417079925537, ACC:0.984375\n",
      "Training iteration 174 loss: 0.0577964149415493, ACC:0.984375\n",
      "Training iteration 175 loss: 0.016072988510131836, ACC:1.0\n",
      "Training iteration 176 loss: 0.01734062284231186, ACC:1.0\n",
      "Training iteration 177 loss: 0.03381044417619705, ACC:1.0\n",
      "Training iteration 178 loss: 0.10664111375808716, ACC:0.953125\n",
      "Training iteration 179 loss: 0.03554336726665497, ACC:0.984375\n",
      "Training iteration 180 loss: 0.03996237367391586, ACC:1.0\n",
      "Training iteration 181 loss: 0.024781446903944016, ACC:1.0\n",
      "Training iteration 182 loss: 0.07445528358221054, ACC:0.96875\n",
      "Training iteration 183 loss: 0.03441701456904411, ACC:0.984375\n",
      "Training iteration 184 loss: 0.06044558063149452, ACC:0.984375\n",
      "Training iteration 185 loss: 0.04986531659960747, ACC:0.984375\n",
      "Training iteration 186 loss: 0.06900301575660706, ACC:0.96875\n",
      "Training iteration 187 loss: 0.02731897681951523, ACC:1.0\n",
      "Training iteration 188 loss: 0.09732609987258911, ACC:0.96875\n",
      "Training iteration 189 loss: 0.10555972158908844, ACC:0.953125\n",
      "Training iteration 190 loss: 0.05780563876032829, ACC:0.96875\n",
      "Training iteration 191 loss: 0.07343020290136337, ACC:0.96875\n",
      "Training iteration 192 loss: 0.03401504084467888, ACC:0.984375\n",
      "Training iteration 193 loss: 0.08236965537071228, ACC:0.96875\n",
      "Training iteration 194 loss: 0.011768958531320095, ACC:1.0\n",
      "Training iteration 195 loss: 0.08430338650941849, ACC:0.953125\n",
      "Training iteration 196 loss: 0.009966582991182804, ACC:1.0\n",
      "Training iteration 197 loss: 0.14135809242725372, ACC:0.96875\n",
      "Training iteration 198 loss: 0.05234270915389061, ACC:0.984375\n",
      "Training iteration 199 loss: 0.08248195797204971, ACC:0.984375\n",
      "Training iteration 200 loss: 0.03655315190553665, ACC:0.984375\n",
      "Training iteration 201 loss: 0.03440982475876808, ACC:0.984375\n",
      "Training iteration 202 loss: 0.02688872255384922, ACC:1.0\n",
      "Training iteration 203 loss: 0.13009417057037354, ACC:0.9375\n",
      "Training iteration 204 loss: 0.02728605456650257, ACC:1.0\n",
      "Training iteration 205 loss: 0.01958082988858223, ACC:1.0\n",
      "Training iteration 206 loss: 0.01345414575189352, ACC:1.0\n",
      "Training iteration 207 loss: 0.02449258230626583, ACC:1.0\n",
      "Training iteration 208 loss: 0.07859158515930176, ACC:0.984375\n",
      "Training iteration 209 loss: 0.051187366247177124, ACC:0.984375\n",
      "Training iteration 210 loss: 0.1923253834247589, ACC:0.984375\n",
      "Training iteration 211 loss: 0.04017800837755203, ACC:0.984375\n",
      "Training iteration 212 loss: 0.043926797807216644, ACC:0.984375\n",
      "Training iteration 213 loss: 0.053806859999895096, ACC:0.984375\n",
      "Training iteration 214 loss: 0.03363271802663803, ACC:1.0\n",
      "Training iteration 215 loss: 0.025723114609718323, ACC:0.984375\n",
      "Training iteration 216 loss: 0.11742747575044632, ACC:0.984375\n",
      "Training iteration 217 loss: 0.0604843832552433, ACC:0.96875\n",
      "Training iteration 218 loss: 0.03778475895524025, ACC:1.0\n",
      "Training iteration 219 loss: 0.008170416578650475, ACC:1.0\n",
      "Training iteration 220 loss: 0.0026520504616200924, ACC:1.0\n",
      "Training iteration 221 loss: 0.002354628872126341, ACC:1.0\n",
      "Training iteration 222 loss: 0.07425291836261749, ACC:0.96875\n",
      "Training iteration 223 loss: 0.09824412316083908, ACC:0.96875\n",
      "Training iteration 224 loss: 0.07171536237001419, ACC:0.96875\n",
      "Training iteration 225 loss: 0.11044219881296158, ACC:0.984375\n",
      "Training iteration 226 loss: 0.01038613636046648, ACC:1.0\n",
      "Training iteration 227 loss: 0.005958051420748234, ACC:1.0\n",
      "Training iteration 228 loss: 0.034027304500341415, ACC:1.0\n",
      "Training iteration 229 loss: 0.032741185277700424, ACC:0.984375\n",
      "Training iteration 230 loss: 0.015652110800147057, ACC:1.0\n",
      "Training iteration 231 loss: 0.022570868954062462, ACC:1.0\n",
      "Training iteration 232 loss: 0.03667927160859108, ACC:1.0\n",
      "Training iteration 233 loss: 0.056757159531116486, ACC:0.984375\n",
      "Training iteration 234 loss: 0.0058295708149671555, ACC:1.0\n",
      "Training iteration 235 loss: 0.019246960058808327, ACC:0.984375\n",
      "Training iteration 236 loss: 0.02376021444797516, ACC:0.984375\n",
      "Training iteration 237 loss: 0.06998630613088608, ACC:0.984375\n",
      "Training iteration 238 loss: 0.010437549091875553, ACC:1.0\n",
      "Training iteration 239 loss: 0.07495097815990448, ACC:0.96875\n",
      "Training iteration 240 loss: 0.08890895545482635, ACC:0.984375\n",
      "Training iteration 241 loss: 0.003559232223778963, ACC:1.0\n",
      "Training iteration 242 loss: 0.003898920025676489, ACC:1.0\n",
      "Training iteration 243 loss: 0.0010419562458992004, ACC:1.0\n",
      "Training iteration 244 loss: 0.0957149937748909, ACC:0.984375\n",
      "Training iteration 245 loss: 0.09458567202091217, ACC:0.984375\n",
      "Training iteration 246 loss: 0.0009536189027130604, ACC:1.0\n",
      "Training iteration 247 loss: 0.09397900849580765, ACC:0.984375\n",
      "Training iteration 248 loss: 0.011194538325071335, ACC:1.0\n",
      "Training iteration 249 loss: 0.011518917046487331, ACC:1.0\n",
      "Training iteration 250 loss: 0.007588834036141634, ACC:1.0\n",
      "Training iteration 251 loss: 0.007033202797174454, ACC:1.0\n",
      "Training iteration 252 loss: 0.03102308139204979, ACC:0.984375\n",
      "Training iteration 253 loss: 0.021192435175180435, ACC:1.0\n",
      "Training iteration 254 loss: 0.013065085746347904, ACC:1.0\n",
      "Training iteration 255 loss: 0.04938102513551712, ACC:0.984375\n",
      "Training iteration 256 loss: 0.007447900716215372, ACC:1.0\n",
      "Training iteration 257 loss: 0.008219130337238312, ACC:1.0\n",
      "Training iteration 258 loss: 0.031088821589946747, ACC:0.984375\n",
      "Training iteration 259 loss: 0.017984965816140175, ACC:1.0\n",
      "Training iteration 260 loss: 0.0028297293465584517, ACC:1.0\n",
      "Training iteration 261 loss: 0.017294928431510925, ACC:0.984375\n",
      "Training iteration 262 loss: 0.03627489507198334, ACC:0.984375\n",
      "Training iteration 263 loss: 0.004848983138799667, ACC:1.0\n",
      "Training iteration 264 loss: 0.004978665150702, ACC:1.0\n",
      "Training iteration 265 loss: 0.08401886373758316, ACC:0.984375\n",
      "Training iteration 266 loss: 0.03019755333662033, ACC:0.984375\n",
      "Training iteration 267 loss: 0.002900410443544388, ACC:1.0\n",
      "Training iteration 268 loss: 0.07799474149942398, ACC:0.984375\n",
      "Training iteration 269 loss: 0.009387688711285591, ACC:1.0\n",
      "Training iteration 270 loss: 0.027671564370393753, ACC:0.984375\n",
      "Training iteration 271 loss: 0.01595202647149563, ACC:1.0\n",
      "Training iteration 272 loss: 0.06729307770729065, ACC:0.984375\n",
      "Training iteration 273 loss: 0.011666295118629932, ACC:1.0\n",
      "Training iteration 274 loss: 0.0032559740357100964, ACC:1.0\n",
      "Training iteration 275 loss: 0.04256350174546242, ACC:0.96875\n",
      "Training iteration 276 loss: 0.033940598368644714, ACC:0.984375\n",
      "Training iteration 277 loss: 0.11518537998199463, ACC:0.96875\n",
      "Training iteration 278 loss: 0.03158476948738098, ACC:1.0\n",
      "Training iteration 279 loss: 0.051153119653463364, ACC:0.984375\n",
      "Training iteration 280 loss: 0.030516769737005234, ACC:0.984375\n",
      "Training iteration 281 loss: 0.001675883075222373, ACC:1.0\n",
      "Training iteration 282 loss: 0.0021079746074974537, ACC:1.0\n",
      "Training iteration 283 loss: 0.011994421482086182, ACC:1.0\n",
      "Training iteration 284 loss: 0.01051329355686903, ACC:1.0\n",
      "Training iteration 285 loss: 0.043367642909288406, ACC:0.984375\n",
      "Training iteration 286 loss: 0.008748713880777359, ACC:1.0\n",
      "Training iteration 287 loss: 0.14696599543094635, ACC:0.984375\n",
      "Training iteration 288 loss: 0.1699276566505432, ACC:0.96875\n",
      "Training iteration 289 loss: 0.003028822597116232, ACC:1.0\n",
      "Training iteration 290 loss: 0.003422568552196026, ACC:1.0\n",
      "Training iteration 291 loss: 0.002006933093070984, ACC:1.0\n",
      "Training iteration 292 loss: 0.02450547367334366, ACC:0.984375\n",
      "Training iteration 293 loss: 0.03847802057862282, ACC:0.984375\n",
      "Training iteration 294 loss: 0.017307525500655174, ACC:1.0\n",
      "Training iteration 295 loss: 0.007575821131467819, ACC:1.0\n",
      "Training iteration 296 loss: 0.08341290056705475, ACC:0.984375\n",
      "Training iteration 297 loss: 0.02319064550101757, ACC:1.0\n",
      "Training iteration 298 loss: 0.008171692490577698, ACC:1.0\n",
      "Training iteration 299 loss: 0.020053477957844734, ACC:1.0\n",
      "Training iteration 300 loss: 0.026513561606407166, ACC:0.984375\n",
      "Training iteration 301 loss: 0.058694709092378616, ACC:0.984375\n",
      "Training iteration 302 loss: 0.004654307384043932, ACC:1.0\n",
      "Training iteration 303 loss: 0.016939489170908928, ACC:0.984375\n",
      "Training iteration 304 loss: 0.003746171947568655, ACC:1.0\n",
      "Training iteration 305 loss: 0.05592435970902443, ACC:0.96875\n",
      "Training iteration 306 loss: 0.012316069565713406, ACC:1.0\n",
      "Training iteration 307 loss: 0.025555189698934555, ACC:1.0\n",
      "Training iteration 308 loss: 0.02411629632115364, ACC:0.984375\n",
      "Training iteration 309 loss: 0.015292677097022533, ACC:1.0\n",
      "Training iteration 310 loss: 0.016102353110909462, ACC:0.984375\n",
      "Training iteration 311 loss: 0.009464525617659092, ACC:1.0\n",
      "Training iteration 312 loss: 0.030223801732063293, ACC:0.984375\n",
      "Training iteration 313 loss: 0.010775547474622726, ACC:1.0\n",
      "Training iteration 314 loss: 0.0044400948099792, ACC:1.0\n",
      "Training iteration 315 loss: 0.07289409637451172, ACC:0.953125\n",
      "Training iteration 316 loss: 0.006856297142803669, ACC:1.0\n",
      "Training iteration 317 loss: 0.0537932887673378, ACC:0.984375\n",
      "Training iteration 318 loss: 0.008140710182487965, ACC:1.0\n",
      "Training iteration 319 loss: 0.01454948727041483, ACC:1.0\n",
      "Training iteration 320 loss: 0.007659901864826679, ACC:1.0\n",
      "Training iteration 321 loss: 0.02353118173778057, ACC:1.0\n",
      "Training iteration 322 loss: 0.15259648859500885, ACC:0.96875\n",
      "Training iteration 323 loss: 0.003684326773509383, ACC:1.0\n",
      "Training iteration 324 loss: 0.021501369774341583, ACC:1.0\n",
      "Training iteration 325 loss: 0.011715443804860115, ACC:1.0\n",
      "Training iteration 326 loss: 0.006361185107380152, ACC:1.0\n",
      "Training iteration 327 loss: 0.01734853722155094, ACC:1.0\n",
      "Training iteration 328 loss: 0.02897062711417675, ACC:0.984375\n",
      "Training iteration 329 loss: 0.033785734325647354, ACC:0.96875\n",
      "Training iteration 330 loss: 0.00638177152723074, ACC:1.0\n",
      "Training iteration 331 loss: 0.0008733886061236262, ACC:1.0\n",
      "Training iteration 332 loss: 0.0007803287007845938, ACC:1.0\n",
      "Training iteration 333 loss: 0.004349085036665201, ACC:1.0\n",
      "Training iteration 334 loss: 0.024938784539699554, ACC:0.984375\n",
      "Training iteration 335 loss: 0.0005967561737634242, ACC:1.0\n",
      "Training iteration 336 loss: 0.046188101172447205, ACC:0.96875\n",
      "Training iteration 337 loss: 0.0294206403195858, ACC:0.984375\n",
      "Training iteration 338 loss: 0.017618419602513313, ACC:0.984375\n",
      "Training iteration 339 loss: 0.058580562472343445, ACC:0.984375\n",
      "Training iteration 340 loss: 0.01994105987250805, ACC:1.0\n",
      "Training iteration 341 loss: 0.0030711116269230843, ACC:1.0\n",
      "Training iteration 342 loss: 0.02596243843436241, ACC:0.984375\n",
      "Training iteration 343 loss: 0.016704048961400986, ACC:0.984375\n",
      "Training iteration 344 loss: 0.010696372017264366, ACC:1.0\n",
      "Training iteration 345 loss: 0.03521215170621872, ACC:0.984375\n",
      "Training iteration 346 loss: 0.06475938111543655, ACC:0.984375\n",
      "Training iteration 347 loss: 0.009045736864209175, ACC:1.0\n",
      "Training iteration 348 loss: 0.002680922392755747, ACC:1.0\n",
      "Training iteration 349 loss: 0.01980496011674404, ACC:0.984375\n",
      "Training iteration 350 loss: 0.031060876324772835, ACC:0.984375\n",
      "Training iteration 351 loss: 0.0020917414221912622, ACC:1.0\n",
      "Training iteration 352 loss: 0.04864763095974922, ACC:0.96875\n",
      "Training iteration 353 loss: 0.025353269651532173, ACC:0.984375\n",
      "Training iteration 354 loss: 0.08438606560230255, ACC:0.984375\n",
      "Training iteration 355 loss: 0.0012227189727127552, ACC:1.0\n",
      "Training iteration 356 loss: 0.04386288300156593, ACC:0.984375\n",
      "Training iteration 357 loss: 0.03650028631091118, ACC:0.984375\n",
      "Training iteration 358 loss: 0.1384154111146927, ACC:0.96875\n",
      "Training iteration 359 loss: 0.00815375056117773, ACC:1.0\n",
      "Training iteration 360 loss: 0.0057700686156749725, ACC:1.0\n",
      "Training iteration 361 loss: 0.0083236675709486, ACC:1.0\n",
      "Training iteration 362 loss: 0.07677879184484482, ACC:0.96875\n",
      "Training iteration 363 loss: 0.012420205399394035, ACC:1.0\n",
      "Training iteration 364 loss: 0.03241591900587082, ACC:0.984375\n",
      "Training iteration 365 loss: 0.023922743275761604, ACC:0.984375\n",
      "Training iteration 366 loss: 0.008577330969274044, ACC:1.0\n",
      "Training iteration 367 loss: 0.005328265950083733, ACC:1.0\n",
      "Training iteration 368 loss: 0.00867456290870905, ACC:1.0\n",
      "Training iteration 369 loss: 0.07176511734724045, ACC:0.984375\n",
      "Training iteration 370 loss: 0.0037714780773967505, ACC:1.0\n",
      "Training iteration 371 loss: 0.008832477033138275, ACC:1.0\n",
      "Training iteration 372 loss: 0.007450388744473457, ACC:1.0\n",
      "Training iteration 373 loss: 0.009106442332267761, ACC:1.0\n",
      "Training iteration 374 loss: 0.014430289156734943, ACC:1.0\n",
      "Training iteration 375 loss: 0.004221189767122269, ACC:1.0\n",
      "Training iteration 376 loss: 0.045382946729660034, ACC:0.96875\n",
      "Training iteration 377 loss: 0.05394246056675911, ACC:0.96875\n",
      "Training iteration 378 loss: 0.09642033278942108, ACC:0.984375\n",
      "Training iteration 379 loss: 0.0021408656612038612, ACC:1.0\n",
      "Training iteration 380 loss: 0.0026896637864410877, ACC:1.0\n",
      "Training iteration 381 loss: 0.04475601017475128, ACC:0.984375\n",
      "Training iteration 382 loss: 0.014911677688360214, ACC:1.0\n",
      "Training iteration 383 loss: 0.013586940243840218, ACC:1.0\n",
      "Training iteration 384 loss: 0.004402636084705591, ACC:1.0\n",
      "Training iteration 385 loss: 0.027634378522634506, ACC:0.96875\n",
      "Training iteration 386 loss: 0.004578645806759596, ACC:1.0\n",
      "Training iteration 387 loss: 0.015514236874878407, ACC:1.0\n",
      "Training iteration 388 loss: 0.032264288514852524, ACC:0.984375\n",
      "Training iteration 389 loss: 0.009040996432304382, ACC:1.0\n",
      "Training iteration 390 loss: 0.0138041777536273, ACC:1.0\n",
      "Training iteration 391 loss: 0.01923765242099762, ACC:1.0\n",
      "Training iteration 392 loss: 0.1017669290304184, ACC:0.96875\n",
      "Training iteration 393 loss: 0.0463496670126915, ACC:0.984375\n",
      "Training iteration 394 loss: 0.02334996499121189, ACC:0.984375\n",
      "Training iteration 395 loss: 0.019283821806311607, ACC:0.984375\n",
      "Training iteration 396 loss: 0.10225991159677505, ACC:0.96875\n",
      "Training iteration 397 loss: 0.0383150652050972, ACC:0.984375\n",
      "Training iteration 398 loss: 0.02048884704709053, ACC:0.984375\n",
      "Training iteration 399 loss: 0.003951785620301962, ACC:1.0\n",
      "Training iteration 400 loss: 0.007019387558102608, ACC:1.0\n",
      "Training iteration 401 loss: 0.1527780294418335, ACC:0.96875\n",
      "Training iteration 402 loss: 0.0023094178177416325, ACC:1.0\n",
      "Training iteration 403 loss: 0.029400436207652092, ACC:0.984375\n",
      "Training iteration 404 loss: 0.013177474960684776, ACC:1.0\n",
      "Training iteration 405 loss: 0.027764229103922844, ACC:1.0\n",
      "Training iteration 406 loss: 0.08277229219675064, ACC:0.984375\n",
      "Training iteration 407 loss: 0.006103462539613247, ACC:1.0\n",
      "Training iteration 408 loss: 0.00194226682651788, ACC:1.0\n",
      "Training iteration 409 loss: 0.030544273555278778, ACC:0.984375\n",
      "Training iteration 410 loss: 0.058523427695035934, ACC:0.984375\n",
      "Training iteration 411 loss: 0.0062075392343103886, ACC:1.0\n",
      "Training iteration 412 loss: 0.0496659055352211, ACC:0.96875\n",
      "Training iteration 413 loss: 0.00850678887218237, ACC:1.0\n",
      "Training iteration 414 loss: 0.006637429818511009, ACC:1.0\n",
      "Training iteration 415 loss: 0.007860270328819752, ACC:1.0\n",
      "Training iteration 416 loss: 0.004852009005844593, ACC:1.0\n",
      "Training iteration 417 loss: 0.022652974352240562, ACC:0.984375\n",
      "Training iteration 418 loss: 0.013845439068973064, ACC:1.0\n",
      "Training iteration 419 loss: 0.002506459364667535, ACC:1.0\n",
      "Training iteration 420 loss: 0.11849917471408844, ACC:0.984375\n",
      "Training iteration 421 loss: 0.007923115976154804, ACC:1.0\n",
      "Training iteration 422 loss: 0.0047726863995194435, ACC:1.0\n",
      "Training iteration 423 loss: 0.06771901994943619, ACC:0.96875\n",
      "Training iteration 424 loss: 0.03654204308986664, ACC:0.984375\n",
      "Training iteration 425 loss: 0.06630003452301025, ACC:0.984375\n",
      "Training iteration 426 loss: 0.11044714599847794, ACC:0.984375\n",
      "Training iteration 427 loss: 0.0028884313069283962, ACC:1.0\n",
      "Training iteration 428 loss: 0.005787523463368416, ACC:1.0\n",
      "Training iteration 429 loss: 0.0023356005549430847, ACC:1.0\n",
      "Training iteration 430 loss: 0.0581798329949379, ACC:0.96875\n",
      "Training iteration 431 loss: 0.006173178553581238, ACC:1.0\n",
      "Training iteration 432 loss: 0.009126529097557068, ACC:1.0\n",
      "Training iteration 433 loss: 0.045001812279224396, ACC:0.984375\n",
      "Training iteration 434 loss: 0.011729407124221325, ACC:1.0\n",
      "Training iteration 435 loss: 0.013285609893500805, ACC:1.0\n",
      "Training iteration 436 loss: 0.05958892032504082, ACC:0.984375\n",
      "Training iteration 437 loss: 0.04471341893076897, ACC:0.984375\n",
      "Training iteration 438 loss: 0.025177206844091415, ACC:0.984375\n",
      "Training iteration 439 loss: 0.007175596430897713, ACC:1.0\n",
      "Training iteration 440 loss: 0.016633588820695877, ACC:1.0\n",
      "Training iteration 441 loss: 0.003976292442530394, ACC:1.0\n",
      "Training iteration 442 loss: 0.022546200081706047, ACC:0.984375\n",
      "Training iteration 443 loss: 0.0053568026050925255, ACC:1.0\n",
      "Training iteration 444 loss: 0.0038364604115486145, ACC:1.0\n",
      "Training iteration 445 loss: 0.004537289496511221, ACC:1.0\n",
      "Training iteration 446 loss: 0.011634495109319687, ACC:1.0\n",
      "Training iteration 447 loss: 0.005057434551417828, ACC:1.0\n",
      "Training iteration 448 loss: 0.14210650324821472, ACC:0.96875\n",
      "Training iteration 449 loss: 0.007465837057679892, ACC:1.0\n",
      "Training iteration 450 loss: 0.05815748870372772, ACC:0.984375\n",
      "Validation iteration 451 loss: 0.12580606341362, ACC: 0.984375\n",
      "Validation iteration 452 loss: 0.018754253163933754, ACC: 0.984375\n",
      "Validation iteration 453 loss: 0.13320614397525787, ACC: 0.96875\n",
      "Validation iteration 454 loss: 0.12830908596515656, ACC: 0.96875\n",
      "Validation iteration 455 loss: 0.0030815983191132545, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.00994836539030075, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.003326300997287035, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.05256442353129387, ACC: 0.984375\n",
      "Validation iteration 459 loss: 0.0750560387969017, ACC: 0.953125\n",
      "Validation iteration 460 loss: 0.020989803597331047, ACC: 0.984375\n",
      "Validation iteration 461 loss: 0.03576058894395828, ACC: 0.984375\n",
      "Validation iteration 462 loss: 0.046783655881881714, ACC: 0.984375\n",
      "Validation iteration 463 loss: 0.0033263645600527525, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.021224308758974075, ACC: 0.984375\n",
      "Validation iteration 465 loss: 0.001751309959217906, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.009992365725338459, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.013455769047141075, ACC: 0.984375\n",
      "Validation iteration 468 loss: 0.06269503384828568, ACC: 0.984375\n",
      "Validation iteration 469 loss: 0.05631115287542343, ACC: 0.96875\n",
      "Validation iteration 470 loss: 0.0055711944587528706, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.01742163859307766, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.04083718731999397, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.08391223102807999, ACC: 0.984375\n",
      "Validation iteration 474 loss: 0.12165114283561707, ACC: 0.953125\n",
      "Validation iteration 475 loss: 0.02266041748225689, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.12809042632579803, ACC: 0.984375\n",
      "Validation iteration 477 loss: 0.19739742577075958, ACC: 0.9375\n",
      "Validation iteration 478 loss: 0.00457010930404067, ACC: 1.0\n",
      "Validation iteration 479 loss: 0.0030612454283982515, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.0040680826641619205, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.047801677137613297, ACC: 0.984375\n",
      "Validation iteration 482 loss: 0.014453778974711895, ACC: 0.984375\n",
      "Validation iteration 483 loss: 0.08309820294380188, ACC: 0.96875\n",
      "Validation iteration 484 loss: 0.023361803963780403, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.04374346509575844, ACC: 0.984375\n",
      "Validation iteration 486 loss: 0.06859972327947617, ACC: 0.984375\n",
      "Validation iteration 487 loss: 0.036803796887397766, ACC: 0.984375\n",
      "Validation iteration 488 loss: 0.0367993526160717, ACC: 0.984375\n",
      "Validation iteration 489 loss: 0.11113851517438889, ACC: 0.953125\n",
      "Validation iteration 490 loss: 0.0015323227271437645, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.03709622472524643, ACC: 0.984375\n",
      "Validation iteration 492 loss: 0.05572265759110451, ACC: 0.984375\n",
      "Validation iteration 493 loss: 0.003280774923041463, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.17816734313964844, ACC: 0.953125\n",
      "Validation iteration 495 loss: 0.005894924979656935, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.002345132175832987, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.12545862793922424, ACC: 0.984375\n",
      "Validation iteration 498 loss: 0.009311174042522907, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.11286739259958267, ACC: 0.96875\n",
      "Validation iteration 500 loss: 0.004618728067725897, ACC: 1.0\n",
      "-- Epoch 5 done -- Train loss: 0.03389947893162672, train ACC: 0.9904166666666666, val loss: 0.049073586938902736, val ACC: 0.984375\n",
      "<--- 10966.647348880768 seconds --->\n",
      "Training iteration 1 loss: 0.021868785843253136, ACC:0.984375\n",
      "Training iteration 2 loss: 0.0392313078045845, ACC:0.984375\n",
      "Training iteration 3 loss: 0.01390650775283575, ACC:1.0\n",
      "Training iteration 4 loss: 0.0020609241910278797, ACC:1.0\n",
      "Training iteration 5 loss: 0.005084501579403877, ACC:1.0\n",
      "Training iteration 6 loss: 0.005763046443462372, ACC:1.0\n",
      "Training iteration 7 loss: 0.03813692927360535, ACC:0.96875\n",
      "Training iteration 8 loss: 0.015156292356550694, ACC:0.984375\n",
      "Training iteration 9 loss: 0.04657023772597313, ACC:0.984375\n",
      "Training iteration 10 loss: 0.008666296489536762, ACC:1.0\n",
      "Training iteration 11 loss: 0.011335860937833786, ACC:1.0\n",
      "Training iteration 12 loss: 0.006926024332642555, ACC:1.0\n",
      "Training iteration 13 loss: 0.003917289432138205, ACC:1.0\n",
      "Training iteration 14 loss: 0.00392184080556035, ACC:1.0\n",
      "Training iteration 15 loss: 0.0031031640246510506, ACC:1.0\n",
      "Training iteration 16 loss: 0.018066035583615303, ACC:0.984375\n",
      "Training iteration 17 loss: 0.029907723888754845, ACC:0.984375\n",
      "Training iteration 18 loss: 0.002859942615032196, ACC:1.0\n",
      "Training iteration 19 loss: 0.006069061812013388, ACC:1.0\n",
      "Training iteration 20 loss: 0.004498905036598444, ACC:1.0\n",
      "Training iteration 21 loss: 0.003739476203918457, ACC:1.0\n",
      "Training iteration 22 loss: 0.05406464636325836, ACC:0.96875\n",
      "Training iteration 23 loss: 0.05780128389596939, ACC:0.984375\n",
      "Training iteration 24 loss: 0.08772752434015274, ACC:0.984375\n",
      "Training iteration 25 loss: 0.07642094045877457, ACC:0.96875\n",
      "Training iteration 26 loss: 0.02733747474849224, ACC:0.984375\n",
      "Training iteration 27 loss: 0.012808460742235184, ACC:1.0\n",
      "Training iteration 28 loss: 0.011729832738637924, ACC:1.0\n",
      "Training iteration 29 loss: 0.003584059188142419, ACC:1.0\n",
      "Training iteration 30 loss: 0.019713105633854866, ACC:0.984375\n",
      "Training iteration 31 loss: 0.02088470384478569, ACC:1.0\n",
      "Training iteration 32 loss: 0.044917032122612, ACC:0.96875\n",
      "Training iteration 33 loss: 0.01854602061212063, ACC:0.984375\n",
      "Training iteration 34 loss: 0.06630251556634903, ACC:0.984375\n",
      "Training iteration 35 loss: 0.03902355581521988, ACC:0.984375\n",
      "Training iteration 36 loss: 0.009033235721290112, ACC:1.0\n",
      "Training iteration 37 loss: 0.0017808959819376469, ACC:1.0\n",
      "Training iteration 38 loss: 0.02625441551208496, ACC:0.984375\n",
      "Training iteration 39 loss: 0.018779389560222626, ACC:0.984375\n",
      "Training iteration 40 loss: 0.0037217633798718452, ACC:1.0\n",
      "Training iteration 41 loss: 0.016536831855773926, ACC:0.984375\n",
      "Training iteration 42 loss: 0.01432323269546032, ACC:1.0\n",
      "Training iteration 43 loss: 0.02147788181900978, ACC:0.984375\n",
      "Training iteration 44 loss: 0.0028197169303894043, ACC:1.0\n",
      "Training iteration 45 loss: 0.013448763638734818, ACC:1.0\n",
      "Training iteration 46 loss: 0.006862413603812456, ACC:1.0\n",
      "Training iteration 47 loss: 0.018498513847589493, ACC:0.984375\n",
      "Training iteration 48 loss: 0.007392903324216604, ACC:1.0\n",
      "Training iteration 49 loss: 0.003315222915261984, ACC:1.0\n",
      "Training iteration 50 loss: 0.03966868668794632, ACC:0.984375\n",
      "Training iteration 51 loss: 0.04563911259174347, ACC:0.984375\n",
      "Training iteration 52 loss: 0.003162290435284376, ACC:1.0\n",
      "Training iteration 53 loss: 0.002787100151181221, ACC:1.0\n",
      "Training iteration 54 loss: 0.0012675959151238203, ACC:1.0\n",
      "Training iteration 55 loss: 0.01313679851591587, ACC:1.0\n",
      "Training iteration 56 loss: 0.004016571678221226, ACC:1.0\n",
      "Training iteration 57 loss: 0.02324521914124489, ACC:0.984375\n",
      "Training iteration 58 loss: 0.0011738493340089917, ACC:1.0\n",
      "Training iteration 59 loss: 0.006270675919950008, ACC:1.0\n",
      "Training iteration 60 loss: 0.027615757659077644, ACC:0.984375\n",
      "Training iteration 61 loss: 0.000959513708949089, ACC:1.0\n",
      "Training iteration 62 loss: 0.026357105001807213, ACC:0.984375\n",
      "Training iteration 63 loss: 0.07446234673261642, ACC:0.984375\n",
      "Training iteration 64 loss: 0.01426976453512907, ACC:0.984375\n",
      "Training iteration 65 loss: 0.007632903754711151, ACC:1.0\n",
      "Training iteration 66 loss: 0.0010927184484899044, ACC:1.0\n",
      "Training iteration 67 loss: 0.0022038614843040705, ACC:1.0\n",
      "Training iteration 68 loss: 0.025318568572402, ACC:0.984375\n",
      "Training iteration 69 loss: 0.017957530915737152, ACC:1.0\n",
      "Training iteration 70 loss: 0.12922541797161102, ACC:0.984375\n",
      "Training iteration 71 loss: 0.013679280877113342, ACC:1.0\n",
      "Training iteration 72 loss: 0.0037464494816958904, ACC:1.0\n",
      "Training iteration 73 loss: 0.011051764711737633, ACC:1.0\n",
      "Training iteration 74 loss: 0.0014114833902567625, ACC:1.0\n",
      "Training iteration 75 loss: 0.0007817612495273352, ACC:1.0\n",
      "Training iteration 76 loss: 0.0009502462926320732, ACC:1.0\n",
      "Training iteration 77 loss: 0.001871966291218996, ACC:1.0\n",
      "Training iteration 78 loss: 0.019613519310951233, ACC:0.984375\n",
      "Training iteration 79 loss: 0.0012979020830243826, ACC:1.0\n",
      "Training iteration 80 loss: 0.0032674940302968025, ACC:1.0\n",
      "Training iteration 81 loss: 0.15163490176200867, ACC:0.96875\n",
      "Training iteration 82 loss: 0.0006122536142356694, ACC:1.0\n",
      "Training iteration 83 loss: 0.04471107944846153, ACC:0.96875\n",
      "Training iteration 84 loss: 0.0030593974515795708, ACC:1.0\n",
      "Training iteration 85 loss: 0.014145242050290108, ACC:1.0\n",
      "Training iteration 86 loss: 0.04079447686672211, ACC:1.0\n",
      "Training iteration 87 loss: 0.03252675011754036, ACC:1.0\n",
      "Training iteration 88 loss: 0.014630748890340328, ACC:1.0\n",
      "Training iteration 89 loss: 0.023612983524799347, ACC:0.984375\n",
      "Training iteration 90 loss: 0.02958574704825878, ACC:0.984375\n",
      "Training iteration 91 loss: 0.11648368835449219, ACC:0.953125\n",
      "Training iteration 92 loss: 0.008890694938600063, ACC:1.0\n",
      "Training iteration 93 loss: 0.05197316035628319, ACC:0.984375\n",
      "Training iteration 94 loss: 0.011927309446036816, ACC:1.0\n",
      "Training iteration 95 loss: 0.007813707925379276, ACC:1.0\n",
      "Training iteration 96 loss: 0.003659556619822979, ACC:1.0\n",
      "Training iteration 97 loss: 0.01930423267185688, ACC:0.984375\n",
      "Training iteration 98 loss: 0.012163366191089153, ACC:1.0\n",
      "Training iteration 99 loss: 0.03750775009393692, ACC:0.984375\n",
      "Training iteration 100 loss: 0.010085628367960453, ACC:1.0\n",
      "Training iteration 101 loss: 0.020511263981461525, ACC:0.984375\n",
      "Training iteration 102 loss: 0.030629612505435944, ACC:0.984375\n",
      "Training iteration 103 loss: 0.017933417111635208, ACC:0.984375\n",
      "Training iteration 104 loss: 0.008762186393141747, ACC:1.0\n",
      "Training iteration 105 loss: 0.05267466977238655, ACC:0.984375\n",
      "Training iteration 106 loss: 0.009626680985093117, ACC:1.0\n",
      "Training iteration 107 loss: 0.001423242618329823, ACC:1.0\n",
      "Training iteration 108 loss: 0.021212048828601837, ACC:1.0\n",
      "Training iteration 109 loss: 0.015369798056781292, ACC:1.0\n",
      "Training iteration 110 loss: 0.014008559286594391, ACC:0.984375\n",
      "Training iteration 111 loss: 0.010049265809357166, ACC:1.0\n",
      "Training iteration 112 loss: 0.046073608100414276, ACC:0.984375\n",
      "Training iteration 113 loss: 0.017789265140891075, ACC:0.984375\n",
      "Training iteration 114 loss: 0.019857576116919518, ACC:1.0\n",
      "Training iteration 115 loss: 0.0021404477301985025, ACC:1.0\n",
      "Training iteration 116 loss: 0.007276329677551985, ACC:1.0\n",
      "Training iteration 117 loss: 0.01994451880455017, ACC:1.0\n",
      "Training iteration 118 loss: 0.0007050428539514542, ACC:1.0\n",
      "Training iteration 119 loss: 0.055876001715660095, ACC:0.984375\n",
      "Training iteration 120 loss: 0.02073557674884796, ACC:1.0\n",
      "Training iteration 121 loss: 0.002540882444009185, ACC:1.0\n",
      "Training iteration 122 loss: 0.0033504373859614134, ACC:1.0\n",
      "Training iteration 123 loss: 0.001670965226367116, ACC:1.0\n",
      "Training iteration 124 loss: 0.0007224795408546925, ACC:1.0\n",
      "Training iteration 125 loss: 0.017701245844364166, ACC:0.984375\n",
      "Training iteration 126 loss: 0.09793060272932053, ACC:0.984375\n",
      "Training iteration 127 loss: 0.000699907133821398, ACC:1.0\n",
      "Training iteration 128 loss: 0.006385364104062319, ACC:1.0\n",
      "Training iteration 129 loss: 0.06301739811897278, ACC:0.984375\n",
      "Training iteration 130 loss: 0.11517546325922012, ACC:0.96875\n",
      "Training iteration 131 loss: 0.01732565648853779, ACC:0.984375\n",
      "Training iteration 132 loss: 0.010537042282521725, ACC:1.0\n",
      "Training iteration 133 loss: 0.049959152936935425, ACC:0.984375\n",
      "Training iteration 134 loss: 0.0031921006739139557, ACC:1.0\n",
      "Training iteration 135 loss: 0.0037713712081313133, ACC:1.0\n",
      "Training iteration 136 loss: 0.01533010695129633, ACC:1.0\n",
      "Training iteration 137 loss: 0.005194368306547403, ACC:1.0\n",
      "Training iteration 138 loss: 0.0013435953296720982, ACC:1.0\n",
      "Training iteration 139 loss: 0.01715518906712532, ACC:0.984375\n",
      "Training iteration 140 loss: 0.02697710320353508, ACC:0.984375\n",
      "Training iteration 141 loss: 0.002155433176085353, ACC:1.0\n",
      "Training iteration 142 loss: 0.01601351797580719, ACC:1.0\n",
      "Training iteration 143 loss: 0.019118202850222588, ACC:0.984375\n",
      "Training iteration 144 loss: 0.0005530049675144255, ACC:1.0\n",
      "Training iteration 145 loss: 0.023995602503418922, ACC:0.984375\n",
      "Training iteration 146 loss: 0.016537213698029518, ACC:1.0\n",
      "Training iteration 147 loss: 0.008416488766670227, ACC:1.0\n",
      "Training iteration 148 loss: 0.010666870512068272, ACC:1.0\n",
      "Training iteration 149 loss: 0.012967633083462715, ACC:1.0\n",
      "Training iteration 150 loss: 0.03324580937623978, ACC:0.984375\n",
      "Training iteration 151 loss: 0.03559250012040138, ACC:0.96875\n",
      "Training iteration 152 loss: 0.012797408737242222, ACC:0.984375\n",
      "Training iteration 153 loss: 0.023089809343218803, ACC:0.984375\n",
      "Training iteration 154 loss: 0.15734140574932098, ACC:0.96875\n",
      "Training iteration 155 loss: 0.020058292895555496, ACC:0.984375\n",
      "Training iteration 156 loss: 0.06630390137434006, ACC:0.984375\n",
      "Training iteration 157 loss: 0.10063906013965607, ACC:0.96875\n",
      "Training iteration 158 loss: 0.002978917211294174, ACC:1.0\n",
      "Training iteration 159 loss: 0.0186317041516304, ACC:0.984375\n",
      "Training iteration 160 loss: 0.0162879079580307, ACC:1.0\n",
      "Training iteration 161 loss: 0.04346504062414169, ACC:0.984375\n",
      "Training iteration 162 loss: 0.008267564699053764, ACC:1.0\n",
      "Training iteration 163 loss: 0.0199532862752676, ACC:1.0\n",
      "Training iteration 164 loss: 0.006601568311452866, ACC:1.0\n",
      "Training iteration 165 loss: 0.08149073272943497, ACC:0.984375\n",
      "Training iteration 166 loss: 0.2164042443037033, ACC:0.96875\n",
      "Training iteration 167 loss: 0.11575771868228912, ACC:0.96875\n",
      "Training iteration 168 loss: 0.10801525413990021, ACC:0.984375\n",
      "Training iteration 169 loss: 0.055068306624889374, ACC:0.984375\n",
      "Training iteration 170 loss: 0.009762539528310299, ACC:1.0\n",
      "Training iteration 171 loss: 0.01975102908909321, ACC:0.984375\n",
      "Training iteration 172 loss: 0.018021903932094574, ACC:1.0\n",
      "Training iteration 173 loss: 0.03799954056739807, ACC:0.984375\n",
      "Training iteration 174 loss: 0.002222898416221142, ACC:1.0\n",
      "Training iteration 175 loss: 0.004835814703255892, ACC:1.0\n",
      "Training iteration 176 loss: 0.02066691964864731, ACC:1.0\n",
      "Training iteration 177 loss: 0.03277317434549332, ACC:0.984375\n",
      "Training iteration 178 loss: 0.011826814152300358, ACC:1.0\n",
      "Training iteration 179 loss: 0.04933574050664902, ACC:0.96875\n",
      "Training iteration 180 loss: 0.01339103002101183, ACC:0.984375\n",
      "Training iteration 181 loss: 0.00942942500114441, ACC:1.0\n",
      "Training iteration 182 loss: 0.0017023925902321935, ACC:1.0\n",
      "Training iteration 183 loss: 0.008648860268294811, ACC:1.0\n",
      "Training iteration 184 loss: 0.02657635137438774, ACC:0.984375\n",
      "Training iteration 185 loss: 0.002935466356575489, ACC:1.0\n",
      "Training iteration 186 loss: 0.03412448242306709, ACC:0.984375\n",
      "Training iteration 187 loss: 0.02229764312505722, ACC:0.984375\n",
      "Training iteration 188 loss: 0.009428092278540134, ACC:1.0\n",
      "Training iteration 189 loss: 0.09469608962535858, ACC:0.984375\n",
      "Training iteration 190 loss: 0.006547239609062672, ACC:1.0\n",
      "Training iteration 191 loss: 0.04785219207406044, ACC:0.953125\n",
      "Training iteration 192 loss: 0.029258012771606445, ACC:0.984375\n",
      "Training iteration 193 loss: 0.048237111419439316, ACC:0.984375\n",
      "Training iteration 194 loss: 0.006926318164914846, ACC:1.0\n",
      "Training iteration 195 loss: 0.03599289432168007, ACC:0.984375\n",
      "Training iteration 196 loss: 0.0033939077984541655, ACC:1.0\n",
      "Training iteration 197 loss: 0.009395740926265717, ACC:1.0\n",
      "Training iteration 198 loss: 0.0381346233189106, ACC:0.984375\n",
      "Training iteration 199 loss: 0.02221333421766758, ACC:0.984375\n",
      "Training iteration 200 loss: 0.01284183468669653, ACC:1.0\n",
      "Training iteration 201 loss: 0.006356723606586456, ACC:1.0\n",
      "Training iteration 202 loss: 0.023718837648630142, ACC:0.984375\n",
      "Training iteration 203 loss: 0.005981921683996916, ACC:1.0\n",
      "Training iteration 204 loss: 0.011512164026498795, ACC:1.0\n",
      "Training iteration 205 loss: 0.01421527098864317, ACC:1.0\n",
      "Training iteration 206 loss: 0.010613897815346718, ACC:1.0\n",
      "Training iteration 207 loss: 0.018109401687979698, ACC:0.984375\n",
      "Training iteration 208 loss: 0.002588809235021472, ACC:1.0\n",
      "Training iteration 209 loss: 0.1347704827785492, ACC:0.953125\n",
      "Training iteration 210 loss: 0.022729435935616493, ACC:0.984375\n",
      "Training iteration 211 loss: 0.00362629652954638, ACC:1.0\n",
      "Training iteration 212 loss: 0.015553309582173824, ACC:1.0\n",
      "Training iteration 213 loss: 0.017641419544816017, ACC:0.984375\n",
      "Training iteration 214 loss: 0.01896163821220398, ACC:0.984375\n",
      "Training iteration 215 loss: 0.017394857481122017, ACC:0.984375\n",
      "Training iteration 216 loss: 0.19795936346054077, ACC:0.9375\n",
      "Training iteration 217 loss: 0.009507188573479652, ACC:1.0\n",
      "Training iteration 218 loss: 0.003947362303733826, ACC:1.0\n",
      "Training iteration 219 loss: 0.004978877492249012, ACC:1.0\n",
      "Training iteration 220 loss: 0.004761049058288336, ACC:1.0\n",
      "Training iteration 221 loss: 0.23037487268447876, ACC:0.96875\n",
      "Training iteration 222 loss: 0.005636308342218399, ACC:1.0\n",
      "Training iteration 223 loss: 0.011153643019497395, ACC:1.0\n",
      "Training iteration 224 loss: 0.013969840481877327, ACC:1.0\n",
      "Training iteration 225 loss: 0.052648790180683136, ACC:0.96875\n",
      "Training iteration 226 loss: 0.0037005438935011625, ACC:1.0\n",
      "Training iteration 227 loss: 0.014157249592244625, ACC:1.0\n",
      "Training iteration 228 loss: 0.12391000241041183, ACC:0.953125\n",
      "Training iteration 229 loss: 0.03660095855593681, ACC:0.984375\n",
      "Training iteration 230 loss: 0.060351066291332245, ACC:0.984375\n",
      "Training iteration 231 loss: 0.07286646217107773, ACC:0.984375\n",
      "Training iteration 232 loss: 0.023260222747921944, ACC:0.984375\n",
      "Training iteration 233 loss: 0.03134940564632416, ACC:0.984375\n",
      "Training iteration 234 loss: 0.004661856684833765, ACC:1.0\n",
      "Training iteration 235 loss: 0.026335543021559715, ACC:1.0\n",
      "Training iteration 236 loss: 0.012062391266226768, ACC:1.0\n",
      "Training iteration 237 loss: 0.011140371672809124, ACC:1.0\n",
      "Training iteration 238 loss: 0.0070749372243881226, ACC:1.0\n",
      "Training iteration 239 loss: 0.0029244176112115383, ACC:1.0\n",
      "Training iteration 240 loss: 0.0029745204374194145, ACC:1.0\n",
      "Training iteration 241 loss: 0.01114477775990963, ACC:1.0\n",
      "Training iteration 242 loss: 0.008437542244791985, ACC:1.0\n",
      "Training iteration 243 loss: 0.013648318126797676, ACC:1.0\n",
      "Training iteration 244 loss: 0.07774347066879272, ACC:0.984375\n",
      "Training iteration 245 loss: 0.00630585104227066, ACC:1.0\n",
      "Training iteration 246 loss: 0.06707896292209625, ACC:0.96875\n",
      "Training iteration 247 loss: 0.016431158408522606, ACC:1.0\n",
      "Training iteration 248 loss: 0.005325422156602144, ACC:1.0\n",
      "Training iteration 249 loss: 0.011893506161868572, ACC:1.0\n",
      "Training iteration 250 loss: 0.002853770274668932, ACC:1.0\n",
      "Training iteration 251 loss: 0.05844277888536453, ACC:0.96875\n",
      "Training iteration 252 loss: 0.023569457232952118, ACC:0.984375\n",
      "Training iteration 253 loss: 0.019150985404849052, ACC:0.984375\n",
      "Training iteration 254 loss: 0.10320498049259186, ACC:0.953125\n",
      "Training iteration 255 loss: 0.001311406958848238, ACC:1.0\n",
      "Training iteration 256 loss: 0.0021106433123350143, ACC:1.0\n",
      "Training iteration 257 loss: 0.019992312416434288, ACC:1.0\n",
      "Training iteration 258 loss: 0.010736441239714622, ACC:1.0\n",
      "Training iteration 259 loss: 0.013959967531263828, ACC:1.0\n",
      "Training iteration 260 loss: 0.01673620566725731, ACC:1.0\n",
      "Training iteration 261 loss: 0.031241627410054207, ACC:0.984375\n",
      "Training iteration 262 loss: 0.0073809572495520115, ACC:1.0\n",
      "Training iteration 263 loss: 0.08639749884605408, ACC:0.984375\n",
      "Training iteration 264 loss: 0.014227375388145447, ACC:1.0\n",
      "Training iteration 265 loss: 0.06598662585020065, ACC:0.984375\n",
      "Training iteration 266 loss: 0.14698289334774017, ACC:0.96875\n",
      "Training iteration 267 loss: 0.018981508910655975, ACC:1.0\n",
      "Training iteration 268 loss: 0.010560822673141956, ACC:1.0\n",
      "Training iteration 269 loss: 0.006480516400188208, ACC:1.0\n",
      "Training iteration 270 loss: 0.020342225208878517, ACC:0.984375\n",
      "Training iteration 271 loss: 0.017158791422843933, ACC:1.0\n",
      "Training iteration 272 loss: 0.007368146441876888, ACC:1.0\n",
      "Training iteration 273 loss: 0.06839170306921005, ACC:0.96875\n",
      "Training iteration 274 loss: 0.05811315029859543, ACC:0.984375\n",
      "Training iteration 275 loss: 0.027286969125270844, ACC:0.984375\n",
      "Training iteration 276 loss: 0.05768464505672455, ACC:0.984375\n",
      "Training iteration 277 loss: 0.018153445795178413, ACC:1.0\n",
      "Training iteration 278 loss: 0.031510476022958755, ACC:0.984375\n",
      "Training iteration 279 loss: 0.043995004147291183, ACC:0.96875\n",
      "Training iteration 280 loss: 0.028253398835659027, ACC:0.984375\n",
      "Training iteration 281 loss: 0.08871553093194962, ACC:0.984375\n",
      "Training iteration 282 loss: 0.004793174564838409, ACC:1.0\n",
      "Training iteration 283 loss: 0.005041593685746193, ACC:1.0\n",
      "Training iteration 284 loss: 0.019839905202388763, ACC:1.0\n",
      "Training iteration 285 loss: 0.008093670941889286, ACC:1.0\n",
      "Training iteration 286 loss: 0.008344423957169056, ACC:1.0\n",
      "Training iteration 287 loss: 0.006156801246106625, ACC:1.0\n",
      "Training iteration 288 loss: 0.008644470013678074, ACC:1.0\n",
      "Training iteration 289 loss: 0.004589634016156197, ACC:1.0\n",
      "Training iteration 290 loss: 0.045599330216646194, ACC:0.984375\n",
      "Training iteration 291 loss: 0.002716987393796444, ACC:1.0\n",
      "Training iteration 292 loss: 0.008966277353465557, ACC:1.0\n",
      "Training iteration 293 loss: 0.002747648861259222, ACC:1.0\n",
      "Training iteration 294 loss: 0.012151355855166912, ACC:1.0\n",
      "Training iteration 295 loss: 0.004857167601585388, ACC:1.0\n",
      "Training iteration 296 loss: 0.006048917304724455, ACC:1.0\n",
      "Training iteration 297 loss: 0.005775780417025089, ACC:1.0\n",
      "Training iteration 298 loss: 0.005583929363638163, ACC:1.0\n",
      "Training iteration 299 loss: 0.03901924565434456, ACC:0.984375\n",
      "Training iteration 300 loss: 0.05539371818304062, ACC:0.984375\n",
      "Training iteration 301 loss: 0.0021811602637171745, ACC:1.0\n",
      "Training iteration 302 loss: 0.014294921420514584, ACC:1.0\n",
      "Training iteration 303 loss: 0.0013912172289565206, ACC:1.0\n",
      "Training iteration 304 loss: 0.0030453945510089397, ACC:1.0\n",
      "Training iteration 305 loss: 0.021631907671689987, ACC:0.984375\n",
      "Training iteration 306 loss: 0.05711464211344719, ACC:0.96875\n",
      "Training iteration 307 loss: 0.0026990296319127083, ACC:1.0\n",
      "Training iteration 308 loss: 0.009574854746460915, ACC:1.0\n",
      "Training iteration 309 loss: 0.001674421364441514, ACC:1.0\n",
      "Training iteration 310 loss: 0.001100029214285314, ACC:1.0\n",
      "Training iteration 311 loss: 0.003643448930233717, ACC:1.0\n",
      "Training iteration 312 loss: 0.03279756009578705, ACC:0.984375\n",
      "Training iteration 313 loss: 0.03645828366279602, ACC:0.984375\n",
      "Training iteration 314 loss: 0.06207411363720894, ACC:0.984375\n",
      "Training iteration 315 loss: 0.05567598715424538, ACC:0.984375\n",
      "Training iteration 316 loss: 0.0018094840925186872, ACC:1.0\n",
      "Training iteration 317 loss: 0.010362785309553146, ACC:1.0\n",
      "Training iteration 318 loss: 0.009698950685560703, ACC:1.0\n",
      "Training iteration 319 loss: 0.009334067814052105, ACC:1.0\n",
      "Training iteration 320 loss: 0.0031279416289180517, ACC:1.0\n",
      "Training iteration 321 loss: 0.008093207143247128, ACC:1.0\n",
      "Training iteration 322 loss: 0.07283266633749008, ACC:0.953125\n",
      "Training iteration 323 loss: 0.021549513563513756, ACC:1.0\n",
      "Training iteration 324 loss: 0.020144814625382423, ACC:0.984375\n",
      "Training iteration 325 loss: 0.010136355645954609, ACC:1.0\n",
      "Training iteration 326 loss: 0.00042682987987063825, ACC:1.0\n",
      "Training iteration 327 loss: 0.04463193938136101, ACC:0.984375\n",
      "Training iteration 328 loss: 0.0010724278399720788, ACC:1.0\n",
      "Training iteration 329 loss: 0.06596679985523224, ACC:0.984375\n",
      "Training iteration 330 loss: 0.010403581894934177, ACC:1.0\n",
      "Training iteration 331 loss: 0.004636596422642469, ACC:1.0\n",
      "Training iteration 332 loss: 0.0014159465208649635, ACC:1.0\n",
      "Training iteration 333 loss: 0.0020568936597555876, ACC:1.0\n",
      "Training iteration 334 loss: 0.011395929381251335, ACC:1.0\n",
      "Training iteration 335 loss: 0.08178026229143143, ACC:0.96875\n",
      "Training iteration 336 loss: 0.051550742238759995, ACC:0.984375\n",
      "Training iteration 337 loss: 0.0066330949775874615, ACC:1.0\n",
      "Training iteration 338 loss: 0.07028111070394516, ACC:0.96875\n",
      "Training iteration 339 loss: 0.004477894399315119, ACC:1.0\n",
      "Training iteration 340 loss: 0.010921269655227661, ACC:1.0\n",
      "Training iteration 341 loss: 0.02472766675055027, ACC:0.984375\n",
      "Training iteration 342 loss: 0.010283504612743855, ACC:1.0\n",
      "Training iteration 343 loss: 0.006279482506215572, ACC:1.0\n",
      "Training iteration 344 loss: 0.006498808041214943, ACC:1.0\n",
      "Training iteration 345 loss: 0.06843575835227966, ACC:0.984375\n",
      "Training iteration 346 loss: 0.007284787483513355, ACC:1.0\n",
      "Training iteration 347 loss: 0.028344949707388878, ACC:0.984375\n",
      "Training iteration 348 loss: 0.004279218148440123, ACC:1.0\n",
      "Training iteration 349 loss: 0.0034344696905463934, ACC:1.0\n",
      "Training iteration 350 loss: 0.0023652815725654364, ACC:1.0\n",
      "Training iteration 351 loss: 0.007100300397723913, ACC:1.0\n",
      "Training iteration 352 loss: 0.010754410177469254, ACC:1.0\n",
      "Training iteration 353 loss: 0.07205881923437119, ACC:0.984375\n",
      "Training iteration 354 loss: 0.030095037072896957, ACC:0.984375\n",
      "Training iteration 355 loss: 0.005724044516682625, ACC:1.0\n",
      "Training iteration 356 loss: 0.019615938887000084, ACC:1.0\n",
      "Training iteration 357 loss: 0.007888391613960266, ACC:1.0\n",
      "Training iteration 358 loss: 0.005689145531505346, ACC:1.0\n",
      "Training iteration 359 loss: 0.02352270856499672, ACC:0.984375\n",
      "Training iteration 360 loss: 0.01877094991505146, ACC:0.984375\n",
      "Training iteration 361 loss: 0.029805997386574745, ACC:0.984375\n",
      "Training iteration 362 loss: 0.00566598866134882, ACC:1.0\n",
      "Training iteration 363 loss: 0.0009033030946739018, ACC:1.0\n",
      "Training iteration 364 loss: 0.013209354132413864, ACC:1.0\n",
      "Training iteration 365 loss: 0.002184359822422266, ACC:1.0\n",
      "Training iteration 366 loss: 0.0047301240265369415, ACC:1.0\n",
      "Training iteration 367 loss: 0.03691289946436882, ACC:0.984375\n",
      "Training iteration 368 loss: 0.042308371514081955, ACC:0.984375\n",
      "Training iteration 369 loss: 0.00935810524970293, ACC:1.0\n",
      "Training iteration 370 loss: 0.00477329408749938, ACC:1.0\n",
      "Training iteration 371 loss: 0.0024326296988874674, ACC:1.0\n",
      "Training iteration 372 loss: 0.003898581024259329, ACC:1.0\n",
      "Training iteration 373 loss: 0.07904114574193954, ACC:0.984375\n",
      "Training iteration 374 loss: 0.004571810830384493, ACC:1.0\n",
      "Training iteration 375 loss: 0.00015638027980457991, ACC:1.0\n",
      "Training iteration 376 loss: 0.012225192971527576, ACC:1.0\n",
      "Training iteration 377 loss: 0.0005967278848402202, ACC:1.0\n",
      "Training iteration 378 loss: 0.004214135464280844, ACC:1.0\n",
      "Training iteration 379 loss: 0.0374605730175972, ACC:0.984375\n",
      "Training iteration 380 loss: 0.017382727935910225, ACC:1.0\n",
      "Training iteration 381 loss: 0.001608337159268558, ACC:1.0\n",
      "Training iteration 382 loss: 0.00023267837241292, ACC:1.0\n",
      "Training iteration 383 loss: 0.00501939607784152, ACC:1.0\n",
      "Training iteration 384 loss: 0.0026004931423813105, ACC:1.0\n",
      "Training iteration 385 loss: 0.0025745683815330267, ACC:1.0\n",
      "Training iteration 386 loss: 0.0022418492008000612, ACC:1.0\n",
      "Training iteration 387 loss: 0.002936779288575053, ACC:1.0\n",
      "Training iteration 388 loss: 0.000730292871594429, ACC:1.0\n",
      "Training iteration 389 loss: 0.06342834234237671, ACC:0.984375\n",
      "Training iteration 390 loss: 0.0002662377373781055, ACC:1.0\n",
      "Training iteration 391 loss: 0.004002450034022331, ACC:1.0\n",
      "Training iteration 392 loss: 0.0015011652139946818, ACC:1.0\n",
      "Training iteration 393 loss: 0.13600224256515503, ACC:0.984375\n",
      "Training iteration 394 loss: 0.10183236002922058, ACC:0.96875\n",
      "Training iteration 395 loss: 0.0034659276716411114, ACC:1.0\n",
      "Training iteration 396 loss: 0.03743661567568779, ACC:0.984375\n",
      "Training iteration 397 loss: 0.04826393350958824, ACC:0.96875\n",
      "Training iteration 398 loss: 0.009664559736847878, ACC:1.0\n",
      "Training iteration 399 loss: 0.0036123229656368494, ACC:1.0\n",
      "Training iteration 400 loss: 0.11158987134695053, ACC:0.96875\n",
      "Training iteration 401 loss: 0.09192630648612976, ACC:0.953125\n",
      "Training iteration 402 loss: 0.05209197849035263, ACC:0.96875\n",
      "Training iteration 403 loss: 0.028334660455584526, ACC:0.984375\n",
      "Training iteration 404 loss: 0.005885764490813017, ACC:1.0\n",
      "Training iteration 405 loss: 0.002642118837684393, ACC:1.0\n",
      "Training iteration 406 loss: 0.02300056628882885, ACC:0.984375\n",
      "Training iteration 407 loss: 0.009006540291011333, ACC:1.0\n",
      "Training iteration 408 loss: 0.006688106805086136, ACC:1.0\n",
      "Training iteration 409 loss: 0.02813597209751606, ACC:0.984375\n",
      "Training iteration 410 loss: 0.01733141578733921, ACC:0.984375\n",
      "Training iteration 411 loss: 0.3490141034126282, ACC:0.96875\n",
      "Training iteration 412 loss: 0.009056382812559605, ACC:1.0\n",
      "Training iteration 413 loss: 0.0016028196550905704, ACC:1.0\n",
      "Training iteration 414 loss: 0.024177109822630882, ACC:0.984375\n",
      "Training iteration 415 loss: 0.0026725390926003456, ACC:1.0\n",
      "Training iteration 416 loss: 0.16174814105033875, ACC:0.953125\n",
      "Training iteration 417 loss: 0.012778757140040398, ACC:1.0\n",
      "Training iteration 418 loss: 0.036415111273527145, ACC:0.984375\n",
      "Training iteration 419 loss: 0.02283610962331295, ACC:1.0\n",
      "Training iteration 420 loss: 0.03831304982304573, ACC:0.984375\n",
      "Training iteration 421 loss: 0.03567979857325554, ACC:0.984375\n",
      "Training iteration 422 loss: 0.07369590550661087, ACC:0.953125\n",
      "Training iteration 423 loss: 0.1626785695552826, ACC:0.96875\n",
      "Training iteration 424 loss: 0.009488468058407307, ACC:1.0\n",
      "Training iteration 425 loss: 0.025162318721413612, ACC:0.984375\n",
      "Training iteration 426 loss: 0.012752274051308632, ACC:1.0\n",
      "Training iteration 427 loss: 0.04542713984847069, ACC:0.984375\n",
      "Training iteration 428 loss: 0.018985595554113388, ACC:0.984375\n",
      "Training iteration 429 loss: 0.03280636668205261, ACC:0.984375\n",
      "Training iteration 430 loss: 0.021977318450808525, ACC:1.0\n",
      "Training iteration 431 loss: 0.10665512084960938, ACC:0.953125\n",
      "Training iteration 432 loss: 0.07988353073596954, ACC:0.96875\n",
      "Training iteration 433 loss: 0.031023506075143814, ACC:0.984375\n",
      "Training iteration 434 loss: 0.060260623693466187, ACC:0.96875\n",
      "Training iteration 435 loss: 0.04346461221575737, ACC:0.96875\n",
      "Training iteration 436 loss: 0.01161050796508789, ACC:1.0\n",
      "Training iteration 437 loss: 0.009707489982247353, ACC:1.0\n",
      "Training iteration 438 loss: 0.008679811842739582, ACC:1.0\n",
      "Training iteration 439 loss: 0.047894589602947235, ACC:0.96875\n",
      "Training iteration 440 loss: 0.01263961661607027, ACC:1.0\n",
      "Training iteration 441 loss: 0.010767214000225067, ACC:1.0\n",
      "Training iteration 442 loss: 0.010108232498168945, ACC:1.0\n",
      "Training iteration 443 loss: 0.03134410083293915, ACC:1.0\n",
      "Training iteration 444 loss: 0.01307989377528429, ACC:1.0\n",
      "Training iteration 445 loss: 0.10483255982398987, ACC:0.984375\n",
      "Training iteration 446 loss: 0.15013760328292847, ACC:0.953125\n",
      "Training iteration 447 loss: 0.006056731566786766, ACC:1.0\n",
      "Training iteration 448 loss: 0.00627552205696702, ACC:1.0\n",
      "Training iteration 449 loss: 0.039326462894678116, ACC:0.984375\n",
      "Training iteration 450 loss: 0.08456096053123474, ACC:0.96875\n",
      "Validation iteration 451 loss: 0.09183306992053986, ACC: 0.96875\n",
      "Validation iteration 452 loss: 0.01647068001329899, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.021289801225066185, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.03226286917924881, ACC: 0.984375\n",
      "Validation iteration 455 loss: 0.06449560821056366, ACC: 0.96875\n",
      "Validation iteration 456 loss: 0.024885691702365875, ACC: 0.984375\n",
      "Validation iteration 457 loss: 0.00894688256084919, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.012119309045374393, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.018610721454024315, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.009851204231381416, ACC: 1.0\n",
      "Validation iteration 461 loss: 0.008963511325418949, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.009845348075032234, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.015318709425628185, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.0156807042658329, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.012307818047702312, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.05962737649679184, ACC: 0.96875\n",
      "Validation iteration 467 loss: 0.008503330871462822, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.023786377161741257, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.027864709496498108, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.005812822375446558, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.007945788092911243, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.02919393591582775, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.006077802740037441, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.009080423973500729, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.03792471066117287, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.026572823524475098, ACC: 0.984375\n",
      "Validation iteration 477 loss: 0.0249923225492239, ACC: 0.984375\n",
      "Validation iteration 478 loss: 0.01962871663272381, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.006852483376860619, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.04112815111875534, ACC: 0.984375\n",
      "Validation iteration 481 loss: 0.020005248486995697, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.008359470404684544, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.06350218504667282, ACC: 0.984375\n",
      "Validation iteration 484 loss: 0.02106265351176262, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.026227934285998344, ACC: 0.984375\n",
      "Validation iteration 486 loss: 0.008729030378162861, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.03547526150941849, ACC: 0.984375\n",
      "Validation iteration 488 loss: 0.012180742807686329, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.021066932007670403, ACC: 1.0\n",
      "Validation iteration 490 loss: 0.009274587035179138, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.0056869820691645145, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.00432423735037446, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.015269873663783073, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.026552272960543633, ACC: 0.984375\n",
      "Validation iteration 495 loss: 0.01397092454135418, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.03670347109436989, ACC: 0.984375\n",
      "Validation iteration 497 loss: 0.02599240466952324, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.021364593878388405, ACC: 0.984375\n",
      "Validation iteration 499 loss: 0.02323826774954796, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.005485301837325096, ACC: 1.0\n",
      "-- Epoch 6 done -- Train loss: 0.026952930392613376, train ACC: 0.9915277777777778, val loss: 0.022046921579167246, val ACC: 0.99375\n",
      "<--- 11603.373277425766 seconds --->\n",
      "Training iteration 1 loss: 0.0609903410077095, ACC:0.96875\n",
      "Training iteration 2 loss: 0.017674533650279045, ACC:1.0\n",
      "Training iteration 3 loss: 0.034303728491067886, ACC:0.984375\n",
      "Training iteration 4 loss: 0.01535077951848507, ACC:1.0\n",
      "Training iteration 5 loss: 0.03427096828818321, ACC:0.984375\n",
      "Training iteration 6 loss: 0.012501577846705914, ACC:1.0\n",
      "Training iteration 7 loss: 0.064192034304142, ACC:0.96875\n",
      "Training iteration 8 loss: 0.02176145277917385, ACC:0.984375\n",
      "Training iteration 9 loss: 0.040451761335134506, ACC:0.984375\n",
      "Training iteration 10 loss: 0.023946799337863922, ACC:1.0\n",
      "Training iteration 11 loss: 0.03492109850049019, ACC:1.0\n",
      "Training iteration 12 loss: 0.02457454800605774, ACC:0.984375\n",
      "Training iteration 13 loss: 0.054151300340890884, ACC:0.96875\n",
      "Training iteration 14 loss: 0.0070992386899888515, ACC:1.0\n",
      "Training iteration 15 loss: 0.031969014555215836, ACC:0.984375\n",
      "Training iteration 16 loss: 0.014917769469320774, ACC:1.0\n",
      "Training iteration 17 loss: 0.008389060385525227, ACC:1.0\n",
      "Training iteration 18 loss: 0.14849510788917542, ACC:0.96875\n",
      "Training iteration 19 loss: 0.003453802550211549, ACC:1.0\n",
      "Training iteration 20 loss: 0.03268231824040413, ACC:0.984375\n",
      "Training iteration 21 loss: 0.04246629774570465, ACC:0.984375\n",
      "Training iteration 22 loss: 0.03468158468604088, ACC:0.984375\n",
      "Training iteration 23 loss: 0.009199164807796478, ACC:1.0\n",
      "Training iteration 24 loss: 0.1323436051607132, ACC:0.984375\n",
      "Training iteration 25 loss: 0.048892151564359665, ACC:0.984375\n",
      "Training iteration 26 loss: 0.00429682619869709, ACC:1.0\n",
      "Training iteration 27 loss: 0.011285179294645786, ACC:1.0\n",
      "Training iteration 28 loss: 0.07440675795078278, ACC:0.984375\n",
      "Training iteration 29 loss: 0.005138085689395666, ACC:1.0\n",
      "Training iteration 30 loss: 0.011886803433299065, ACC:1.0\n",
      "Training iteration 31 loss: 0.011710567399859428, ACC:1.0\n",
      "Training iteration 32 loss: 0.06688934564590454, ACC:0.984375\n",
      "Training iteration 33 loss: 0.013271749019622803, ACC:0.984375\n",
      "Training iteration 34 loss: 0.03915337473154068, ACC:0.984375\n",
      "Training iteration 35 loss: 0.009881468489766121, ACC:1.0\n",
      "Training iteration 36 loss: 0.0037841261364519596, ACC:1.0\n",
      "Training iteration 37 loss: 0.0656425803899765, ACC:0.96875\n",
      "Training iteration 38 loss: 0.015615054406225681, ACC:1.0\n",
      "Training iteration 39 loss: 0.00498234573751688, ACC:1.0\n",
      "Training iteration 40 loss: 0.016908982768654823, ACC:1.0\n",
      "Training iteration 41 loss: 0.010685611516237259, ACC:1.0\n",
      "Training iteration 42 loss: 0.0014910766622051597, ACC:1.0\n",
      "Training iteration 43 loss: 0.0031615006737411022, ACC:1.0\n",
      "Training iteration 44 loss: 0.10061265528202057, ACC:0.96875\n",
      "Training iteration 45 loss: 0.0013851356925442815, ACC:1.0\n",
      "Training iteration 46 loss: 0.02376929484307766, ACC:0.984375\n",
      "Training iteration 47 loss: 0.05910932272672653, ACC:0.96875\n",
      "Training iteration 48 loss: 0.012384176254272461, ACC:1.0\n",
      "Training iteration 49 loss: 0.03234906122088432, ACC:0.984375\n",
      "Training iteration 50 loss: 0.006780574098229408, ACC:1.0\n",
      "Training iteration 51 loss: 0.009490498341619968, ACC:1.0\n",
      "Training iteration 52 loss: 0.03273938223719597, ACC:0.984375\n",
      "Training iteration 53 loss: 0.04955567792057991, ACC:0.984375\n",
      "Training iteration 54 loss: 0.008115020580589771, ACC:1.0\n",
      "Training iteration 55 loss: 0.0029108396265655756, ACC:1.0\n",
      "Training iteration 56 loss: 0.03830830007791519, ACC:0.984375\n",
      "Training iteration 57 loss: 0.007971996441483498, ACC:1.0\n",
      "Training iteration 58 loss: 0.02129472605884075, ACC:0.984375\n",
      "Training iteration 59 loss: 0.005064305849373341, ACC:1.0\n",
      "Training iteration 60 loss: 0.005370949860662222, ACC:1.0\n",
      "Training iteration 61 loss: 0.028152799233794212, ACC:0.984375\n",
      "Training iteration 62 loss: 0.011234819889068604, ACC:1.0\n",
      "Training iteration 63 loss: 0.04744498431682587, ACC:0.96875\n",
      "Training iteration 64 loss: 0.0020618033595383167, ACC:1.0\n",
      "Training iteration 65 loss: 0.00275530107319355, ACC:1.0\n",
      "Training iteration 66 loss: 0.0280387494713068, ACC:0.984375\n",
      "Training iteration 67 loss: 0.0009466767078265548, ACC:1.0\n",
      "Training iteration 68 loss: 0.0017115133814513683, ACC:1.0\n",
      "Training iteration 69 loss: 0.0033590856473892927, ACC:1.0\n",
      "Training iteration 70 loss: 0.008437924087047577, ACC:1.0\n",
      "Training iteration 71 loss: 0.1461676061153412, ACC:0.984375\n",
      "Training iteration 72 loss: 0.04446706920862198, ACC:1.0\n",
      "Training iteration 73 loss: 0.0980273112654686, ACC:0.984375\n",
      "Training iteration 74 loss: 0.009205818176269531, ACC:1.0\n",
      "Training iteration 75 loss: 0.03200562298297882, ACC:0.984375\n",
      "Training iteration 76 loss: 0.05111115798354149, ACC:0.96875\n",
      "Training iteration 77 loss: 0.002928638830780983, ACC:1.0\n",
      "Training iteration 78 loss: 0.06020912900567055, ACC:0.984375\n",
      "Training iteration 79 loss: 0.1644137054681778, ACC:0.953125\n",
      "Training iteration 80 loss: 0.021732298657298088, ACC:1.0\n",
      "Training iteration 81 loss: 0.005869985558092594, ACC:1.0\n",
      "Training iteration 82 loss: 0.030185595154762268, ACC:0.984375\n",
      "Training iteration 83 loss: 0.012478365562856197, ACC:1.0\n",
      "Training iteration 84 loss: 0.007282803300768137, ACC:1.0\n",
      "Training iteration 85 loss: 0.004112340044230223, ACC:1.0\n",
      "Training iteration 86 loss: 0.006208104081451893, ACC:1.0\n",
      "Training iteration 87 loss: 0.011173585429787636, ACC:1.0\n",
      "Training iteration 88 loss: 0.009791824035346508, ACC:1.0\n",
      "Training iteration 89 loss: 0.09600609540939331, ACC:0.96875\n",
      "Training iteration 90 loss: 0.11411359161138535, ACC:0.984375\n",
      "Training iteration 91 loss: 0.1006939560174942, ACC:0.96875\n",
      "Training iteration 92 loss: 0.04182305559515953, ACC:1.0\n",
      "Training iteration 93 loss: 0.04607251286506653, ACC:0.984375\n",
      "Training iteration 94 loss: 0.0380091592669487, ACC:0.984375\n",
      "Training iteration 95 loss: 0.0166426170617342, ACC:1.0\n",
      "Training iteration 96 loss: 0.016791503876447678, ACC:1.0\n",
      "Training iteration 97 loss: 0.032321054488420486, ACC:0.984375\n",
      "Training iteration 98 loss: 0.028291158378124237, ACC:0.984375\n",
      "Training iteration 99 loss: 0.0034489831887185574, ACC:1.0\n",
      "Training iteration 100 loss: 0.0018186074448749423, ACC:1.0\n",
      "Training iteration 101 loss: 0.0024726795963943005, ACC:1.0\n",
      "Training iteration 102 loss: 0.009472589008510113, ACC:1.0\n",
      "Training iteration 103 loss: 0.003653401741757989, ACC:1.0\n",
      "Training iteration 104 loss: 0.007599875796586275, ACC:1.0\n",
      "Training iteration 105 loss: 0.0286290030926466, ACC:0.984375\n",
      "Training iteration 106 loss: 0.03329862654209137, ACC:0.984375\n",
      "Training iteration 107 loss: 0.0007006945088505745, ACC:1.0\n",
      "Training iteration 108 loss: 0.010749093256890774, ACC:1.0\n",
      "Training iteration 109 loss: 0.014480866491794586, ACC:1.0\n",
      "Training iteration 110 loss: 0.008059121668338776, ACC:1.0\n",
      "Training iteration 111 loss: 0.09663969278335571, ACC:0.96875\n",
      "Training iteration 112 loss: 0.008760740049183369, ACC:1.0\n",
      "Training iteration 113 loss: 0.023176971822977066, ACC:0.984375\n",
      "Training iteration 114 loss: 0.01076256763190031, ACC:1.0\n",
      "Training iteration 115 loss: 0.11486072838306427, ACC:0.96875\n",
      "Training iteration 116 loss: 0.051233921200037, ACC:0.984375\n",
      "Training iteration 117 loss: 0.001181090367026627, ACC:1.0\n",
      "Training iteration 118 loss: 0.015621291473507881, ACC:0.984375\n",
      "Training iteration 119 loss: 0.03095463290810585, ACC:0.984375\n",
      "Training iteration 120 loss: 0.014362508431077003, ACC:0.984375\n",
      "Training iteration 121 loss: 0.015884367749094963, ACC:1.0\n",
      "Training iteration 122 loss: 0.0243721604347229, ACC:0.984375\n",
      "Training iteration 123 loss: 0.0038558568339794874, ACC:1.0\n",
      "Training iteration 124 loss: 0.027546890079975128, ACC:0.984375\n",
      "Training iteration 125 loss: 0.0026104883290827274, ACC:1.0\n",
      "Training iteration 126 loss: 0.0019780874717980623, ACC:1.0\n",
      "Training iteration 127 loss: 0.002563910558819771, ACC:1.0\n",
      "Training iteration 128 loss: 0.018081530928611755, ACC:1.0\n",
      "Training iteration 129 loss: 0.00318085472099483, ACC:1.0\n",
      "Training iteration 130 loss: 0.002906964858993888, ACC:1.0\n",
      "Training iteration 131 loss: 0.006467990577220917, ACC:1.0\n",
      "Training iteration 132 loss: 0.007265524473041296, ACC:1.0\n",
      "Training iteration 133 loss: 0.060271598398685455, ACC:0.984375\n",
      "Training iteration 134 loss: 0.00573305319994688, ACC:1.0\n",
      "Training iteration 135 loss: 0.0007044535013847053, ACC:1.0\n",
      "Training iteration 136 loss: 0.003757283091545105, ACC:1.0\n",
      "Training iteration 137 loss: 0.0022081963252276182, ACC:1.0\n",
      "Training iteration 138 loss: 0.0009447896154597402, ACC:1.0\n",
      "Training iteration 139 loss: 0.013345676474273205, ACC:1.0\n",
      "Training iteration 140 loss: 0.04673241078853607, ACC:0.984375\n",
      "Training iteration 141 loss: 0.008473658934235573, ACC:1.0\n",
      "Training iteration 142 loss: 0.03293570503592491, ACC:0.984375\n",
      "Training iteration 143 loss: 0.16128262877464294, ACC:0.984375\n",
      "Training iteration 144 loss: 0.0029943897388875484, ACC:1.0\n",
      "Training iteration 145 loss: 0.01212011557072401, ACC:1.0\n",
      "Training iteration 146 loss: 0.08467631787061691, ACC:0.96875\n",
      "Training iteration 147 loss: 0.015918070450425148, ACC:1.0\n",
      "Training iteration 148 loss: 0.009402657859027386, ACC:1.0\n",
      "Training iteration 149 loss: 0.008628599345684052, ACC:1.0\n",
      "Training iteration 150 loss: 0.03595475107431412, ACC:0.984375\n",
      "Training iteration 151 loss: 0.06309406459331512, ACC:0.96875\n",
      "Training iteration 152 loss: 0.024308830499649048, ACC:1.0\n",
      "Training iteration 153 loss: 0.01919511705636978, ACC:1.0\n",
      "Training iteration 154 loss: 0.00444957846775651, ACC:1.0\n",
      "Training iteration 155 loss: 0.006631114054471254, ACC:1.0\n",
      "Training iteration 156 loss: 0.0031666536815464497, ACC:1.0\n",
      "Training iteration 157 loss: 0.043043963611125946, ACC:0.984375\n",
      "Training iteration 158 loss: 0.04974714294075966, ACC:0.984375\n",
      "Training iteration 159 loss: 0.1456557661294937, ACC:0.984375\n",
      "Training iteration 160 loss: 0.004229228477925062, ACC:1.0\n",
      "Training iteration 161 loss: 0.006639882456511259, ACC:1.0\n",
      "Training iteration 162 loss: 0.030255760997533798, ACC:0.984375\n",
      "Training iteration 163 loss: 0.004716338124126196, ACC:1.0\n",
      "Training iteration 164 loss: 0.005012331530451775, ACC:1.0\n",
      "Training iteration 165 loss: 0.10433055460453033, ACC:0.96875\n",
      "Training iteration 166 loss: 0.11335442215204239, ACC:0.984375\n",
      "Training iteration 167 loss: 0.07520762830972672, ACC:0.953125\n",
      "Training iteration 168 loss: 0.051274195313453674, ACC:0.984375\n",
      "Training iteration 169 loss: 0.009799987077713013, ACC:1.0\n",
      "Training iteration 170 loss: 0.019533280283212662, ACC:1.0\n",
      "Training iteration 171 loss: 0.009842569939792156, ACC:1.0\n",
      "Training iteration 172 loss: 0.028245728462934494, ACC:0.984375\n",
      "Training iteration 173 loss: 0.017947610467672348, ACC:1.0\n",
      "Training iteration 174 loss: 0.0087399547919631, ACC:1.0\n",
      "Training iteration 175 loss: 0.06267376244068146, ACC:0.984375\n",
      "Training iteration 176 loss: 0.01651882566511631, ACC:0.984375\n",
      "Training iteration 177 loss: 0.1078336089849472, ACC:0.984375\n",
      "Training iteration 178 loss: 0.003405229886993766, ACC:1.0\n",
      "Training iteration 179 loss: 0.004068358335644007, ACC:1.0\n",
      "Training iteration 180 loss: 0.009706622920930386, ACC:1.0\n",
      "Training iteration 181 loss: 0.01123259961605072, ACC:1.0\n",
      "Training iteration 182 loss: 0.14023932814598083, ACC:0.96875\n",
      "Training iteration 183 loss: 0.008453979156911373, ACC:1.0\n",
      "Training iteration 184 loss: 0.013951179571449757, ACC:1.0\n",
      "Training iteration 185 loss: 0.003609024453908205, ACC:1.0\n",
      "Training iteration 186 loss: 0.03361496701836586, ACC:0.96875\n",
      "Training iteration 187 loss: 0.014887114986777306, ACC:1.0\n",
      "Training iteration 188 loss: 0.0141214057803154, ACC:1.0\n",
      "Training iteration 189 loss: 0.07773295044898987, ACC:0.984375\n",
      "Training iteration 190 loss: 0.017191627994179726, ACC:1.0\n",
      "Training iteration 191 loss: 0.02649177610874176, ACC:1.0\n",
      "Training iteration 192 loss: 0.007156162988394499, ACC:1.0\n",
      "Training iteration 193 loss: 0.12842892110347748, ACC:0.96875\n",
      "Training iteration 194 loss: 0.019123148173093796, ACC:1.0\n",
      "Training iteration 195 loss: 0.004719326738268137, ACC:1.0\n",
      "Training iteration 196 loss: 0.003294531488791108, ACC:1.0\n",
      "Training iteration 197 loss: 0.06753561645746231, ACC:0.984375\n",
      "Training iteration 198 loss: 0.00978704821318388, ACC:1.0\n",
      "Training iteration 199 loss: 0.020512938499450684, ACC:0.984375\n",
      "Training iteration 200 loss: 0.004878604784607887, ACC:1.0\n",
      "Training iteration 201 loss: 0.018991317600011826, ACC:0.984375\n",
      "Training iteration 202 loss: 0.011039121076464653, ACC:1.0\n",
      "Training iteration 203 loss: 0.0024398898240178823, ACC:1.0\n",
      "Training iteration 204 loss: 0.0033750245347619057, ACC:1.0\n",
      "Training iteration 205 loss: 0.05184980481863022, ACC:0.984375\n",
      "Training iteration 206 loss: 0.07959786057472229, ACC:0.984375\n",
      "Training iteration 207 loss: 0.02690042182803154, ACC:0.984375\n",
      "Training iteration 208 loss: 0.05954161658883095, ACC:0.984375\n",
      "Training iteration 209 loss: 0.03302691504359245, ACC:0.984375\n",
      "Training iteration 210 loss: 0.06546379625797272, ACC:0.984375\n",
      "Training iteration 211 loss: 0.021842364221811295, ACC:0.984375\n",
      "Training iteration 212 loss: 0.010731656104326248, ACC:1.0\n",
      "Training iteration 213 loss: 0.05625557899475098, ACC:0.96875\n",
      "Training iteration 214 loss: 0.007875388488173485, ACC:1.0\n",
      "Training iteration 215 loss: 0.012684216722846031, ACC:1.0\n",
      "Training iteration 216 loss: 0.005944688804447651, ACC:1.0\n",
      "Training iteration 217 loss: 0.03855624049901962, ACC:0.984375\n",
      "Training iteration 218 loss: 0.015284203924238682, ACC:1.0\n",
      "Training iteration 219 loss: 0.012659745290875435, ACC:1.0\n",
      "Training iteration 220 loss: 0.0025828173384070396, ACC:1.0\n",
      "Training iteration 221 loss: 0.01168944500386715, ACC:1.0\n",
      "Training iteration 222 loss: 0.005804018583148718, ACC:1.0\n",
      "Training iteration 223 loss: 0.010269001126289368, ACC:1.0\n",
      "Training iteration 224 loss: 0.012336734682321548, ACC:1.0\n",
      "Training iteration 225 loss: 0.01646977663040161, ACC:0.984375\n",
      "Training iteration 226 loss: 0.0016562920063734055, ACC:1.0\n",
      "Training iteration 227 loss: 0.07897485792636871, ACC:0.984375\n",
      "Training iteration 228 loss: 0.022906433790922165, ACC:1.0\n",
      "Training iteration 229 loss: 0.010148875415325165, ACC:1.0\n",
      "Training iteration 230 loss: 0.007366623263806105, ACC:1.0\n",
      "Training iteration 231 loss: 0.0011802285443991423, ACC:1.0\n",
      "Training iteration 232 loss: 0.025636443868279457, ACC:0.984375\n",
      "Training iteration 233 loss: 0.002187914913520217, ACC:1.0\n",
      "Training iteration 234 loss: 0.03565223515033722, ACC:0.984375\n",
      "Training iteration 235 loss: 0.019685795530676842, ACC:0.984375\n",
      "Training iteration 236 loss: 0.009934764355421066, ACC:1.0\n",
      "Training iteration 237 loss: 0.005784140899777412, ACC:1.0\n",
      "Training iteration 238 loss: 0.002420061267912388, ACC:1.0\n",
      "Training iteration 239 loss: 0.02143191732466221, ACC:1.0\n",
      "Training iteration 240 loss: 0.003281699726358056, ACC:1.0\n",
      "Training iteration 241 loss: 0.0025525232776999474, ACC:1.0\n",
      "Training iteration 242 loss: 0.00503580505028367, ACC:1.0\n",
      "Training iteration 243 loss: 0.0023078613448888063, ACC:1.0\n",
      "Training iteration 244 loss: 0.023234689608216286, ACC:0.984375\n",
      "Training iteration 245 loss: 0.007547234650701284, ACC:1.0\n",
      "Training iteration 246 loss: 0.010453091003000736, ACC:1.0\n",
      "Training iteration 247 loss: 0.006521185394376516, ACC:1.0\n",
      "Training iteration 248 loss: 0.02814200520515442, ACC:0.984375\n",
      "Training iteration 249 loss: 0.003105918876826763, ACC:1.0\n",
      "Training iteration 250 loss: 0.020232748240232468, ACC:0.984375\n",
      "Training iteration 251 loss: 0.001595586771145463, ACC:1.0\n",
      "Training iteration 252 loss: 0.0021503842435777187, ACC:1.0\n",
      "Training iteration 253 loss: 0.0022569068241864443, ACC:1.0\n",
      "Training iteration 254 loss: 0.0036061664577573538, ACC:1.0\n",
      "Training iteration 255 loss: 0.013601689599454403, ACC:1.0\n",
      "Training iteration 256 loss: 0.0017779851332306862, ACC:1.0\n",
      "Training iteration 257 loss: 0.023839280009269714, ACC:0.984375\n",
      "Training iteration 258 loss: 0.00992701854556799, ACC:1.0\n",
      "Training iteration 259 loss: 0.011434853076934814, ACC:1.0\n",
      "Training iteration 260 loss: 0.016749871894717216, ACC:0.984375\n",
      "Training iteration 261 loss: 0.004030168987810612, ACC:1.0\n",
      "Training iteration 262 loss: 0.0026841817889362574, ACC:1.0\n",
      "Training iteration 263 loss: 0.001993651734665036, ACC:1.0\n",
      "Training iteration 264 loss: 0.026984957978129387, ACC:0.984375\n",
      "Training iteration 265 loss: 0.044028084725141525, ACC:0.984375\n",
      "Training iteration 266 loss: 0.02235913835465908, ACC:0.984375\n",
      "Training iteration 267 loss: 0.0025197332724928856, ACC:1.0\n",
      "Training iteration 268 loss: 0.015653306618332863, ACC:0.984375\n",
      "Training iteration 269 loss: 0.0004811426915694028, ACC:1.0\n",
      "Training iteration 270 loss: 0.036189425736665726, ACC:0.984375\n",
      "Training iteration 271 loss: 0.0013031299458816648, ACC:1.0\n",
      "Training iteration 272 loss: 0.0005171333905309439, ACC:1.0\n",
      "Training iteration 273 loss: 0.010005575604736805, ACC:1.0\n",
      "Training iteration 274 loss: 0.05781440809369087, ACC:0.984375\n",
      "Training iteration 275 loss: 0.00019630181486718357, ACC:1.0\n",
      "Training iteration 276 loss: 0.002420971170067787, ACC:1.0\n",
      "Training iteration 277 loss: 0.0056031448766589165, ACC:1.0\n",
      "Training iteration 278 loss: 0.006167082116007805, ACC:1.0\n",
      "Training iteration 279 loss: 0.07960373908281326, ACC:0.96875\n",
      "Training iteration 280 loss: 0.044662702828645706, ACC:0.96875\n",
      "Training iteration 281 loss: 0.0004409951507113874, ACC:1.0\n",
      "Training iteration 282 loss: 0.013526134192943573, ACC:0.984375\n",
      "Training iteration 283 loss: 0.059086233377456665, ACC:0.984375\n",
      "Training iteration 284 loss: 0.027590764686465263, ACC:0.984375\n",
      "Training iteration 285 loss: 0.0056564961560070515, ACC:1.0\n",
      "Training iteration 286 loss: 0.034531544893980026, ACC:0.984375\n",
      "Training iteration 287 loss: 0.0021337971556931734, ACC:1.0\n",
      "Training iteration 288 loss: 0.009120352566242218, ACC:1.0\n",
      "Training iteration 289 loss: 0.002888701856136322, ACC:1.0\n",
      "Training iteration 290 loss: 0.001076789340004325, ACC:1.0\n",
      "Training iteration 291 loss: 0.050996169447898865, ACC:0.984375\n",
      "Training iteration 292 loss: 0.009221702814102173, ACC:1.0\n",
      "Training iteration 293 loss: 0.003157935105264187, ACC:1.0\n",
      "Training iteration 294 loss: 0.01406946498900652, ACC:1.0\n",
      "Training iteration 295 loss: 0.0007563346298411489, ACC:1.0\n",
      "Training iteration 296 loss: 0.01563020795583725, ACC:1.0\n",
      "Training iteration 297 loss: 0.0025982216466218233, ACC:1.0\n",
      "Training iteration 298 loss: 0.0014298843452706933, ACC:1.0\n",
      "Training iteration 299 loss: 0.0040017408318817616, ACC:1.0\n",
      "Training iteration 300 loss: 0.003364595351740718, ACC:1.0\n",
      "Training iteration 301 loss: 0.0005617050919681787, ACC:1.0\n",
      "Training iteration 302 loss: 0.009014441631734371, ACC:1.0\n",
      "Training iteration 303 loss: 0.032529205083847046, ACC:0.984375\n",
      "Training iteration 304 loss: 0.004021957516670227, ACC:1.0\n",
      "Training iteration 305 loss: 0.015614161267876625, ACC:0.984375\n",
      "Training iteration 306 loss: 0.0024532026145607233, ACC:1.0\n",
      "Training iteration 307 loss: 0.03193330392241478, ACC:0.984375\n",
      "Training iteration 308 loss: 0.006206247489899397, ACC:1.0\n",
      "Training iteration 309 loss: 0.005052751395851374, ACC:1.0\n",
      "Training iteration 310 loss: 0.003761493833735585, ACC:1.0\n",
      "Training iteration 311 loss: 0.0037182997912168503, ACC:1.0\n",
      "Training iteration 312 loss: 0.0004869233234785497, ACC:1.0\n",
      "Training iteration 313 loss: 0.0003213940653949976, ACC:1.0\n",
      "Training iteration 314 loss: 0.008905310183763504, ACC:1.0\n",
      "Training iteration 315 loss: 0.0030886789318174124, ACC:1.0\n",
      "Training iteration 316 loss: 0.031547911465168, ACC:0.984375\n",
      "Training iteration 317 loss: 0.002494954038411379, ACC:1.0\n",
      "Training iteration 318 loss: 0.003798006335273385, ACC:1.0\n",
      "Training iteration 319 loss: 0.0027477568946778774, ACC:1.0\n",
      "Training iteration 320 loss: 0.008522840216755867, ACC:1.0\n",
      "Training iteration 321 loss: 0.040671490132808685, ACC:0.984375\n",
      "Training iteration 322 loss: 0.0006955912103876472, ACC:1.0\n",
      "Training iteration 323 loss: 0.012801767326891422, ACC:0.984375\n",
      "Training iteration 324 loss: 0.011844917200505733, ACC:1.0\n",
      "Training iteration 325 loss: 0.0036810177844017744, ACC:1.0\n",
      "Training iteration 326 loss: 0.001383103197440505, ACC:1.0\n",
      "Training iteration 327 loss: 0.003428261261433363, ACC:1.0\n",
      "Training iteration 328 loss: 0.0005378081114031374, ACC:1.0\n",
      "Training iteration 329 loss: 0.0003374387160874903, ACC:1.0\n",
      "Training iteration 330 loss: 0.002306909067556262, ACC:1.0\n",
      "Training iteration 331 loss: 0.08630774170160294, ACC:0.984375\n",
      "Training iteration 332 loss: 0.004879582207649946, ACC:1.0\n",
      "Training iteration 333 loss: 0.0002712348650675267, ACC:1.0\n",
      "Training iteration 334 loss: 0.010690880008041859, ACC:1.0\n",
      "Training iteration 335 loss: 0.0146263986825943, ACC:1.0\n",
      "Training iteration 336 loss: 0.07459642738103867, ACC:0.96875\n",
      "Training iteration 337 loss: 0.011679580435156822, ACC:1.0\n",
      "Training iteration 338 loss: 0.0052746618166565895, ACC:1.0\n",
      "Training iteration 339 loss: 0.027375705540180206, ACC:0.984375\n",
      "Training iteration 340 loss: 0.0020425315015017986, ACC:1.0\n",
      "Training iteration 341 loss: 0.01537235826253891, ACC:0.984375\n",
      "Training iteration 342 loss: 0.004201160743832588, ACC:1.0\n",
      "Training iteration 343 loss: 0.00017495854990556836, ACC:1.0\n",
      "Training iteration 344 loss: 0.0003454683464951813, ACC:1.0\n",
      "Training iteration 345 loss: 0.01455567218363285, ACC:0.984375\n",
      "Training iteration 346 loss: 0.001098412787541747, ACC:1.0\n",
      "Training iteration 347 loss: 0.06292764842510223, ACC:0.96875\n",
      "Training iteration 348 loss: 0.0018486026674509048, ACC:1.0\n",
      "Training iteration 349 loss: 0.0016714025987312198, ACC:1.0\n",
      "Training iteration 350 loss: 0.0024643607903271914, ACC:1.0\n",
      "Training iteration 351 loss: 0.0075545841827988625, ACC:1.0\n",
      "Training iteration 352 loss: 0.08383912593126297, ACC:0.96875\n",
      "Training iteration 353 loss: 0.0005632467800751328, ACC:1.0\n",
      "Training iteration 354 loss: 0.005332718137651682, ACC:1.0\n",
      "Training iteration 355 loss: 0.05424242094159126, ACC:0.984375\n",
      "Training iteration 356 loss: 0.03704387694597244, ACC:0.96875\n",
      "Training iteration 357 loss: 0.011522033251821995, ACC:1.0\n",
      "Training iteration 358 loss: 0.030141405761241913, ACC:0.96875\n",
      "Training iteration 359 loss: 0.009577430784702301, ACC:1.0\n",
      "Training iteration 360 loss: 0.03313355892896652, ACC:0.984375\n",
      "Training iteration 361 loss: 0.06865965574979782, ACC:0.96875\n",
      "Training iteration 362 loss: 0.0018609741237014532, ACC:1.0\n",
      "Training iteration 363 loss: 0.006948459427803755, ACC:1.0\n",
      "Training iteration 364 loss: 0.0022992680314928293, ACC:1.0\n",
      "Training iteration 365 loss: 0.0013949337881058455, ACC:1.0\n",
      "Training iteration 366 loss: 0.001120530883781612, ACC:1.0\n",
      "Training iteration 367 loss: 0.006793491076678038, ACC:1.0\n",
      "Training iteration 368 loss: 0.12268797308206558, ACC:0.984375\n",
      "Training iteration 369 loss: 0.0025572264567017555, ACC:1.0\n",
      "Training iteration 370 loss: 0.032738298177719116, ACC:0.984375\n",
      "Training iteration 371 loss: 0.008084178902208805, ACC:1.0\n",
      "Training iteration 372 loss: 0.02823699451982975, ACC:0.984375\n",
      "Training iteration 373 loss: 0.08413510769605637, ACC:0.984375\n",
      "Training iteration 374 loss: 0.02803642861545086, ACC:0.984375\n",
      "Training iteration 375 loss: 0.0023784490767866373, ACC:1.0\n",
      "Training iteration 376 loss: 0.0012610985431820154, ACC:1.0\n",
      "Training iteration 377 loss: 0.0544632188975811, ACC:0.984375\n",
      "Training iteration 378 loss: 0.003190375631675124, ACC:1.0\n",
      "Training iteration 379 loss: 0.0018553712870925665, ACC:1.0\n",
      "Training iteration 380 loss: 0.0024547665379941463, ACC:1.0\n",
      "Training iteration 381 loss: 0.0013402282493188977, ACC:1.0\n",
      "Training iteration 382 loss: 0.028204260393977165, ACC:0.984375\n",
      "Training iteration 383 loss: 0.0015377233503386378, ACC:1.0\n",
      "Training iteration 384 loss: 0.016635490581393242, ACC:1.0\n",
      "Training iteration 385 loss: 0.009701073169708252, ACC:1.0\n",
      "Training iteration 386 loss: 0.002291919896379113, ACC:1.0\n",
      "Training iteration 387 loss: 0.0028159297071397305, ACC:1.0\n",
      "Training iteration 388 loss: 0.09300315380096436, ACC:0.984375\n",
      "Training iteration 389 loss: 0.01545972004532814, ACC:0.984375\n",
      "Training iteration 390 loss: 0.005562061443924904, ACC:1.0\n",
      "Training iteration 391 loss: 0.004565097391605377, ACC:1.0\n",
      "Training iteration 392 loss: 0.0460704080760479, ACC:0.984375\n",
      "Training iteration 393 loss: 0.034725870937108994, ACC:0.984375\n",
      "Training iteration 394 loss: 0.006489339750260115, ACC:1.0\n",
      "Training iteration 395 loss: 0.0008621831657364964, ACC:1.0\n",
      "Training iteration 396 loss: 0.00653260899707675, ACC:1.0\n",
      "Training iteration 397 loss: 0.006833534687757492, ACC:1.0\n",
      "Training iteration 398 loss: 0.02605476789176464, ACC:1.0\n",
      "Training iteration 399 loss: 0.0031317761167883873, ACC:1.0\n",
      "Training iteration 400 loss: 0.03965907543897629, ACC:0.984375\n",
      "Training iteration 401 loss: 0.03572481870651245, ACC:0.96875\n",
      "Training iteration 402 loss: 0.0018005577148869634, ACC:1.0\n",
      "Training iteration 403 loss: 0.0005758785991929471, ACC:1.0\n",
      "Training iteration 404 loss: 0.02663000486791134, ACC:0.984375\n",
      "Training iteration 405 loss: 0.0012289960868656635, ACC:1.0\n",
      "Training iteration 406 loss: 0.11724802106618881, ACC:0.984375\n",
      "Training iteration 407 loss: 0.014392204582691193, ACC:1.0\n",
      "Training iteration 408 loss: 0.09502828121185303, ACC:0.96875\n",
      "Training iteration 409 loss: 0.017869608476758003, ACC:0.984375\n",
      "Training iteration 410 loss: 0.008886066265404224, ACC:1.0\n",
      "Training iteration 411 loss: 0.008760984055697918, ACC:1.0\n",
      "Training iteration 412 loss: 0.0017317078309133649, ACC:1.0\n",
      "Training iteration 413 loss: 0.011347743682563305, ACC:1.0\n",
      "Training iteration 414 loss: 0.13046452403068542, ACC:0.984375\n",
      "Training iteration 415 loss: 0.00195333082228899, ACC:1.0\n",
      "Training iteration 416 loss: 0.00900144875049591, ACC:1.0\n",
      "Training iteration 417 loss: 0.0025760671123862267, ACC:1.0\n",
      "Training iteration 418 loss: 0.0037247445434331894, ACC:1.0\n",
      "Training iteration 419 loss: 0.0119345523416996, ACC:1.0\n",
      "Training iteration 420 loss: 0.009151301346719265, ACC:1.0\n",
      "Training iteration 421 loss: 0.010892125777900219, ACC:1.0\n",
      "Training iteration 422 loss: 0.002299895277246833, ACC:1.0\n",
      "Training iteration 423 loss: 0.009003477171063423, ACC:1.0\n",
      "Training iteration 424 loss: 0.011933403089642525, ACC:1.0\n",
      "Training iteration 425 loss: 0.02508413791656494, ACC:0.984375\n",
      "Training iteration 426 loss: 0.054694052785634995, ACC:0.984375\n",
      "Training iteration 427 loss: 0.004253542050719261, ACC:1.0\n",
      "Training iteration 428 loss: 0.004378162324428558, ACC:1.0\n",
      "Training iteration 429 loss: 0.006409951485693455, ACC:1.0\n",
      "Training iteration 430 loss: 0.027963712811470032, ACC:0.984375\n",
      "Training iteration 431 loss: 0.0016859382158145308, ACC:1.0\n",
      "Training iteration 432 loss: 0.0027004233561456203, ACC:1.0\n",
      "Training iteration 433 loss: 0.0009242002852261066, ACC:1.0\n",
      "Training iteration 434 loss: 0.0034070296678692102, ACC:1.0\n",
      "Training iteration 435 loss: 0.0006464588805101812, ACC:1.0\n",
      "Training iteration 436 loss: 0.007253165356814861, ACC:1.0\n",
      "Training iteration 437 loss: 0.001985848182812333, ACC:1.0\n",
      "Training iteration 438 loss: 0.0036627345252782106, ACC:1.0\n",
      "Training iteration 439 loss: 0.008623969741165638, ACC:1.0\n",
      "Training iteration 440 loss: 0.0011744769290089607, ACC:1.0\n",
      "Training iteration 441 loss: 0.0013816147111356258, ACC:1.0\n",
      "Training iteration 442 loss: 0.015356652438640594, ACC:0.984375\n",
      "Training iteration 443 loss: 0.03765908256173134, ACC:0.984375\n",
      "Training iteration 444 loss: 0.0014685927890241146, ACC:1.0\n",
      "Training iteration 445 loss: 0.0012917808489874005, ACC:1.0\n",
      "Training iteration 446 loss: 0.03833930194377899, ACC:0.984375\n",
      "Training iteration 447 loss: 0.005246730986982584, ACC:1.0\n",
      "Training iteration 448 loss: 0.0029998100362718105, ACC:1.0\n",
      "Training iteration 449 loss: 0.00048293895088136196, ACC:1.0\n",
      "Training iteration 450 loss: 0.0017876247875392437, ACC:1.0\n",
      "Validation iteration 451 loss: 0.0006225224351510406, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.007737969513982534, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.0017895135097205639, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.0043844012543559074, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.0007790158852003515, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.0001812353584682569, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.07565891742706299, ACC: 0.96875\n",
      "Validation iteration 458 loss: 0.004609527997672558, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.004228326957672834, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.004123581573367119, ACC: 1.0\n",
      "Validation iteration 461 loss: 0.019062520936131477, ACC: 0.984375\n",
      "Validation iteration 462 loss: 0.0037679129745811224, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.0019002112094312906, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.001603428041562438, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.0003189734125044197, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.0012937522260472178, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.08406790345907211, ACC: 0.984375\n",
      "Validation iteration 468 loss: 0.08992111682891846, ACC: 0.96875\n",
      "Validation iteration 469 loss: 0.013895335607230663, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.036940496414899826, ACC: 0.96875\n",
      "Validation iteration 471 loss: 0.014052662067115307, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.002680813428014517, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.005086613819003105, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.03710920363664627, ACC: 0.984375\n",
      "Validation iteration 475 loss: 0.03705920651555061, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.002197354333475232, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.03747677430510521, ACC: 0.96875\n",
      "Validation iteration 478 loss: 0.015556001104414463, ACC: 1.0\n",
      "Validation iteration 479 loss: 0.0006194243906065822, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.002607980975881219, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.0007582849939353764, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.0014921313850209117, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.0083574578166008, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.0037311550695449114, ACC: 1.0\n",
      "Validation iteration 485 loss: 0.010196458548307419, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.0006632368895225227, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.0050751641392707825, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.015449989587068558, ACC: 0.984375\n",
      "Validation iteration 489 loss: 0.03717496246099472, ACC: 0.984375\n",
      "Validation iteration 490 loss: 0.002219851827248931, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.025722265243530273, ACC: 0.984375\n",
      "Validation iteration 492 loss: 0.0003670283476822078, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.003722727531567216, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.001258618663996458, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.0043726954609155655, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.00021576115977950394, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.0007772293174639344, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.00021467790065798908, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.002220302587375045, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.017316987738013268, ACC: 0.984375\n",
      "-- Epoch 7 done -- Train loss: 0.02237599393850865, train ACC: 0.9934027777777777, val loss: 0.013052793685346842, val ACC: 0.995\n",
      "<--- 12237.852867364883 seconds --->\n",
      "Training iteration 1 loss: 0.0017111526103690267, ACC:1.0\n",
      "Training iteration 2 loss: 0.00877747219055891, ACC:1.0\n",
      "Training iteration 3 loss: 0.0009966669604182243, ACC:1.0\n",
      "Training iteration 4 loss: 0.002223764080554247, ACC:1.0\n",
      "Training iteration 5 loss: 0.0018935124389827251, ACC:1.0\n",
      "Training iteration 6 loss: 0.000527291267644614, ACC:1.0\n",
      "Training iteration 7 loss: 0.019254036247730255, ACC:0.984375\n",
      "Training iteration 8 loss: 0.025135915726423264, ACC:0.984375\n",
      "Training iteration 9 loss: 0.012622978538274765, ACC:1.0\n",
      "Training iteration 10 loss: 0.006299224216490984, ACC:1.0\n",
      "Training iteration 11 loss: 0.018317634239792824, ACC:0.984375\n",
      "Training iteration 12 loss: 0.0002248154196422547, ACC:1.0\n",
      "Training iteration 13 loss: 0.029111379757523537, ACC:0.984375\n",
      "Training iteration 14 loss: 0.006250564940273762, ACC:1.0\n",
      "Training iteration 15 loss: 0.0003305522259324789, ACC:1.0\n",
      "Training iteration 16 loss: 0.07532645016908646, ACC:0.984375\n",
      "Training iteration 17 loss: 0.0047134291380643845, ACC:1.0\n",
      "Training iteration 18 loss: 0.013741658069193363, ACC:0.984375\n",
      "Training iteration 19 loss: 0.0006639604107476771, ACC:1.0\n",
      "Training iteration 20 loss: 0.10107642412185669, ACC:0.984375\n",
      "Training iteration 21 loss: 0.012495824135839939, ACC:0.984375\n",
      "Training iteration 22 loss: 0.0009733628248795867, ACC:1.0\n",
      "Training iteration 23 loss: 0.021930823102593422, ACC:0.984375\n",
      "Training iteration 24 loss: 0.004316690377891064, ACC:1.0\n",
      "Training iteration 25 loss: 0.001809080713428557, ACC:1.0\n",
      "Training iteration 26 loss: 0.0029399299528449774, ACC:1.0\n",
      "Training iteration 27 loss: 0.005256941542029381, ACC:1.0\n",
      "Training iteration 28 loss: 0.10685113817453384, ACC:0.96875\n",
      "Training iteration 29 loss: 0.005185740999877453, ACC:1.0\n",
      "Training iteration 30 loss: 0.0040966118685901165, ACC:1.0\n",
      "Training iteration 31 loss: 0.004506371449679136, ACC:1.0\n",
      "Training iteration 32 loss: 0.0056229811161756516, ACC:1.0\n",
      "Training iteration 33 loss: 0.017284709960222244, ACC:0.984375\n",
      "Training iteration 34 loss: 0.006921766791492701, ACC:1.0\n",
      "Training iteration 35 loss: 0.09173254668712616, ACC:0.984375\n",
      "Training iteration 36 loss: 0.03033522516489029, ACC:1.0\n",
      "Training iteration 37 loss: 0.09329256415367126, ACC:0.984375\n",
      "Training iteration 38 loss: 0.10158838331699371, ACC:0.984375\n",
      "Training iteration 39 loss: 0.021863292902708054, ACC:1.0\n",
      "Training iteration 40 loss: 0.03629675880074501, ACC:0.984375\n",
      "Training iteration 41 loss: 0.008319880813360214, ACC:1.0\n",
      "Training iteration 42 loss: 0.008120655082166195, ACC:1.0\n",
      "Training iteration 43 loss: 0.022893119603395462, ACC:0.984375\n",
      "Training iteration 44 loss: 0.003517735283821821, ACC:1.0\n",
      "Training iteration 45 loss: 0.009360070340335369, ACC:1.0\n",
      "Training iteration 46 loss: 0.024550411850214005, ACC:0.984375\n",
      "Training iteration 47 loss: 0.0040019056759774685, ACC:1.0\n",
      "Training iteration 48 loss: 0.0013714691158384085, ACC:1.0\n",
      "Training iteration 49 loss: 0.006153330206871033, ACC:1.0\n",
      "Training iteration 50 loss: 0.005757369101047516, ACC:1.0\n",
      "Training iteration 51 loss: 0.013381115160882473, ACC:0.984375\n",
      "Training iteration 52 loss: 0.10773193091154099, ACC:0.984375\n",
      "Training iteration 53 loss: 0.0007247525500133634, ACC:1.0\n",
      "Training iteration 54 loss: 0.00158281612675637, ACC:1.0\n",
      "Training iteration 55 loss: 0.0006422554142773151, ACC:1.0\n",
      "Training iteration 56 loss: 0.001427664770744741, ACC:1.0\n",
      "Training iteration 57 loss: 0.007995414547622204, ACC:1.0\n",
      "Training iteration 58 loss: 0.000945607025641948, ACC:1.0\n",
      "Training iteration 59 loss: 0.03227784484624863, ACC:0.984375\n",
      "Training iteration 60 loss: 0.0012649723794311285, ACC:1.0\n",
      "Training iteration 61 loss: 0.0005201204330660403, ACC:1.0\n",
      "Training iteration 62 loss: 0.021258924156427383, ACC:1.0\n",
      "Training iteration 63 loss: 0.0003423551097512245, ACC:1.0\n",
      "Training iteration 64 loss: 0.0006458984571509063, ACC:1.0\n",
      "Training iteration 65 loss: 0.007535177282989025, ACC:1.0\n",
      "Training iteration 66 loss: 0.0016275419620797038, ACC:1.0\n",
      "Training iteration 67 loss: 0.0036424072459340096, ACC:1.0\n",
      "Training iteration 68 loss: 0.005547394976019859, ACC:1.0\n",
      "Training iteration 69 loss: 0.0019167851423844695, ACC:1.0\n",
      "Training iteration 70 loss: 0.06033270061016083, ACC:0.984375\n",
      "Training iteration 71 loss: 0.0015089013613760471, ACC:1.0\n",
      "Training iteration 72 loss: 0.0009266650304198265, ACC:1.0\n",
      "Training iteration 73 loss: 0.0007360866293311119, ACC:1.0\n",
      "Training iteration 74 loss: 0.010404261760413647, ACC:1.0\n",
      "Training iteration 75 loss: 0.018534036353230476, ACC:0.984375\n",
      "Training iteration 76 loss: 0.0026427432894706726, ACC:1.0\n",
      "Training iteration 77 loss: 0.003884507343173027, ACC:1.0\n",
      "Training iteration 78 loss: 0.006284215487539768, ACC:1.0\n",
      "Training iteration 79 loss: 0.020050494000315666, ACC:0.984375\n",
      "Training iteration 80 loss: 0.012716803699731827, ACC:1.0\n",
      "Training iteration 81 loss: 0.0040321629494428635, ACC:1.0\n",
      "Training iteration 82 loss: 0.0026062193792313337, ACC:1.0\n",
      "Training iteration 83 loss: 0.018762681633234024, ACC:0.984375\n",
      "Training iteration 84 loss: 0.003991068806499243, ACC:1.0\n",
      "Training iteration 85 loss: 0.0020407834090292454, ACC:1.0\n",
      "Training iteration 86 loss: 0.00546563696116209, ACC:1.0\n",
      "Training iteration 87 loss: 0.002319225575774908, ACC:1.0\n",
      "Training iteration 88 loss: 0.0259708222001791, ACC:0.984375\n",
      "Training iteration 89 loss: 0.02183631993830204, ACC:0.984375\n",
      "Training iteration 90 loss: 0.012525171972811222, ACC:1.0\n",
      "Training iteration 91 loss: 0.0013664698926731944, ACC:1.0\n",
      "Training iteration 92 loss: 0.10498765856027603, ACC:0.96875\n",
      "Training iteration 93 loss: 0.004945992957800627, ACC:1.0\n",
      "Training iteration 94 loss: 0.009098543785512447, ACC:1.0\n",
      "Training iteration 95 loss: 0.0010916274040937424, ACC:1.0\n",
      "Training iteration 96 loss: 0.039580684155225754, ACC:0.984375\n",
      "Training iteration 97 loss: 0.001930377446115017, ACC:1.0\n",
      "Training iteration 98 loss: 0.00029817825998179615, ACC:1.0\n",
      "Training iteration 99 loss: 0.005099545698612928, ACC:1.0\n",
      "Training iteration 100 loss: 0.005625613033771515, ACC:1.0\n",
      "Training iteration 101 loss: 0.0031408988870680332, ACC:1.0\n",
      "Training iteration 102 loss: 0.0016818278236314654, ACC:1.0\n",
      "Training iteration 103 loss: 0.0015480851288884878, ACC:1.0\n",
      "Training iteration 104 loss: 0.07867022603750229, ACC:0.984375\n",
      "Training iteration 105 loss: 0.009217030368745327, ACC:1.0\n",
      "Training iteration 106 loss: 0.03775864839553833, ACC:0.984375\n",
      "Training iteration 107 loss: 0.021633397787809372, ACC:0.984375\n",
      "Training iteration 108 loss: 0.007398019544780254, ACC:1.0\n",
      "Training iteration 109 loss: 0.006048415321856737, ACC:1.0\n",
      "Training iteration 110 loss: 0.0035486831329762936, ACC:1.0\n",
      "Training iteration 111 loss: 0.0028188973665237427, ACC:1.0\n",
      "Training iteration 112 loss: 0.001656601787544787, ACC:1.0\n",
      "Training iteration 113 loss: 0.000796732259914279, ACC:1.0\n",
      "Training iteration 114 loss: 0.0011875033378601074, ACC:1.0\n",
      "Training iteration 115 loss: 0.004155770409852266, ACC:1.0\n",
      "Training iteration 116 loss: 0.0015412654029205441, ACC:1.0\n",
      "Training iteration 117 loss: 0.0018656576285138726, ACC:1.0\n",
      "Training iteration 118 loss: 0.0005107627948746085, ACC:1.0\n",
      "Training iteration 119 loss: 0.004296459723263979, ACC:1.0\n",
      "Training iteration 120 loss: 0.0027967519126832485, ACC:1.0\n",
      "Training iteration 121 loss: 0.011995658278465271, ACC:1.0\n",
      "Training iteration 122 loss: 0.009182466194033623, ACC:1.0\n",
      "Training iteration 123 loss: 0.00038943690015003085, ACC:1.0\n",
      "Training iteration 124 loss: 0.0015481511363759637, ACC:1.0\n",
      "Training iteration 125 loss: 0.05517802760004997, ACC:0.984375\n",
      "Training iteration 126 loss: 0.0004991108435206115, ACC:1.0\n",
      "Training iteration 127 loss: 0.013283837586641312, ACC:0.984375\n",
      "Training iteration 128 loss: 0.02349056303501129, ACC:0.984375\n",
      "Training iteration 129 loss: 0.00015742387040518224, ACC:1.0\n",
      "Training iteration 130 loss: 0.0023037719074636698, ACC:1.0\n",
      "Training iteration 131 loss: 0.005037741269916296, ACC:1.0\n",
      "Training iteration 132 loss: 0.01090416219085455, ACC:1.0\n",
      "Training iteration 133 loss: 0.00030927080661058426, ACC:1.0\n",
      "Training iteration 134 loss: 0.1497117280960083, ACC:0.984375\n",
      "Training iteration 135 loss: 0.15777303278446198, ACC:0.953125\n",
      "Training iteration 136 loss: 0.014703621156513691, ACC:0.984375\n",
      "Training iteration 137 loss: 0.004945144988596439, ACC:1.0\n",
      "Training iteration 138 loss: 0.026475243270397186, ACC:0.984375\n",
      "Training iteration 139 loss: 0.012397076934576035, ACC:1.0\n",
      "Training iteration 140 loss: 0.004137987270951271, ACC:1.0\n",
      "Training iteration 141 loss: 0.010670645162463188, ACC:1.0\n",
      "Training iteration 142 loss: 0.0047233253717422485, ACC:1.0\n",
      "Training iteration 143 loss: 0.002330806339159608, ACC:1.0\n",
      "Training iteration 144 loss: 0.04635705426335335, ACC:0.96875\n",
      "Training iteration 145 loss: 0.08493179082870483, ACC:0.984375\n",
      "Training iteration 146 loss: 0.001585695892572403, ACC:1.0\n",
      "Training iteration 147 loss: 0.002963721053674817, ACC:1.0\n",
      "Training iteration 148 loss: 0.004221117589622736, ACC:1.0\n",
      "Training iteration 149 loss: 0.011976233683526516, ACC:0.984375\n",
      "Training iteration 150 loss: 0.0037563065998256207, ACC:1.0\n",
      "Training iteration 151 loss: 0.0742560625076294, ACC:0.984375\n",
      "Training iteration 152 loss: 0.02476928010582924, ACC:0.984375\n",
      "Training iteration 153 loss: 0.008049016818404198, ACC:1.0\n",
      "Training iteration 154 loss: 0.006932404823601246, ACC:1.0\n",
      "Training iteration 155 loss: 0.002114298287779093, ACC:1.0\n",
      "Training iteration 156 loss: 0.0024544443003833294, ACC:1.0\n",
      "Training iteration 157 loss: 0.08243526518344879, ACC:0.96875\n",
      "Training iteration 158 loss: 0.027972066774964333, ACC:0.984375\n",
      "Training iteration 159 loss: 0.0015538178849965334, ACC:1.0\n",
      "Training iteration 160 loss: 0.0020324743818491697, ACC:1.0\n",
      "Training iteration 161 loss: 0.025280889123678207, ACC:0.984375\n",
      "Training iteration 162 loss: 0.0026321825571358204, ACC:1.0\n",
      "Training iteration 163 loss: 0.003479686798527837, ACC:1.0\n",
      "Training iteration 164 loss: 0.013976444490253925, ACC:1.0\n",
      "Training iteration 165 loss: 0.001921329414471984, ACC:1.0\n",
      "Training iteration 166 loss: 0.0030949951615184546, ACC:1.0\n",
      "Training iteration 167 loss: 0.0027990704402327538, ACC:1.0\n",
      "Training iteration 168 loss: 0.0011188089847564697, ACC:1.0\n",
      "Training iteration 169 loss: 0.08858098089694977, ACC:0.984375\n",
      "Training iteration 170 loss: 0.0037767915055155754, ACC:1.0\n",
      "Training iteration 171 loss: 0.005079764872789383, ACC:1.0\n",
      "Training iteration 172 loss: 0.0016691549681127071, ACC:1.0\n",
      "Training iteration 173 loss: 0.0018237291369587183, ACC:1.0\n",
      "Training iteration 174 loss: 0.029614651575684547, ACC:0.984375\n",
      "Training iteration 175 loss: 0.0027443673461675644, ACC:1.0\n",
      "Training iteration 176 loss: 0.000986373284831643, ACC:1.0\n",
      "Training iteration 177 loss: 0.04219876974821091, ACC:0.984375\n",
      "Training iteration 178 loss: 0.0032993266358971596, ACC:1.0\n",
      "Training iteration 179 loss: 0.003161332570016384, ACC:1.0\n",
      "Training iteration 180 loss: 0.003505875589326024, ACC:1.0\n",
      "Training iteration 181 loss: 0.005486494395881891, ACC:1.0\n",
      "Training iteration 182 loss: 0.0021594329737126827, ACC:1.0\n",
      "Training iteration 183 loss: 0.00421474315226078, ACC:1.0\n",
      "Training iteration 184 loss: 0.030820602551102638, ACC:0.984375\n",
      "Training iteration 185 loss: 0.009006478823721409, ACC:1.0\n",
      "Training iteration 186 loss: 0.006854481529444456, ACC:1.0\n",
      "Training iteration 187 loss: 0.0019127856940031052, ACC:1.0\n",
      "Training iteration 188 loss: 0.030954021960496902, ACC:0.984375\n",
      "Training iteration 189 loss: 0.006472555920481682, ACC:1.0\n",
      "Training iteration 190 loss: 0.009072785265743732, ACC:1.0\n",
      "Training iteration 191 loss: 0.0008613488171249628, ACC:1.0\n",
      "Training iteration 192 loss: 0.004268859513103962, ACC:1.0\n",
      "Training iteration 193 loss: 0.0037841815501451492, ACC:1.0\n",
      "Training iteration 194 loss: 0.013926285319030285, ACC:1.0\n",
      "Training iteration 195 loss: 0.016326595097780228, ACC:1.0\n",
      "Training iteration 196 loss: 0.0009588569519110024, ACC:1.0\n",
      "Training iteration 197 loss: 0.007776523008942604, ACC:1.0\n",
      "Training iteration 198 loss: 0.0027282722294330597, ACC:1.0\n",
      "Training iteration 199 loss: 0.008554923348128796, ACC:1.0\n",
      "Training iteration 200 loss: 0.015153945423662663, ACC:0.984375\n",
      "Training iteration 201 loss: 0.0010643013520166278, ACC:1.0\n",
      "Training iteration 202 loss: 0.0004960634978488088, ACC:1.0\n",
      "Training iteration 203 loss: 0.036535486578941345, ACC:0.984375\n",
      "Training iteration 204 loss: 0.0011723084608092904, ACC:1.0\n",
      "Training iteration 205 loss: 0.0021709941793233156, ACC:1.0\n",
      "Training iteration 206 loss: 0.0011259842431172729, ACC:1.0\n",
      "Training iteration 207 loss: 0.00908068660646677, ACC:1.0\n",
      "Training iteration 208 loss: 0.013515206053853035, ACC:0.984375\n",
      "Training iteration 209 loss: 0.0006799558759666979, ACC:1.0\n",
      "Training iteration 210 loss: 0.0023929292801767588, ACC:1.0\n",
      "Training iteration 211 loss: 0.0235831830650568, ACC:0.984375\n",
      "Training iteration 212 loss: 0.000823222566395998, ACC:1.0\n",
      "Training iteration 213 loss: 0.06092403084039688, ACC:0.984375\n",
      "Training iteration 214 loss: 0.0008995410753414035, ACC:1.0\n",
      "Training iteration 215 loss: 0.0034387956839054823, ACC:1.0\n",
      "Training iteration 216 loss: 0.06617226451635361, ACC:0.96875\n",
      "Training iteration 217 loss: 0.0032867591362446547, ACC:1.0\n",
      "Training iteration 218 loss: 0.0052888840436935425, ACC:1.0\n",
      "Training iteration 219 loss: 0.001340916845947504, ACC:1.0\n",
      "Training iteration 220 loss: 0.0038432832807302475, ACC:1.0\n",
      "Training iteration 221 loss: 0.01616710238158703, ACC:0.984375\n",
      "Training iteration 222 loss: 0.006476730108261108, ACC:1.0\n",
      "Training iteration 223 loss: 0.00044833356514573097, ACC:1.0\n",
      "Training iteration 224 loss: 0.005444071255624294, ACC:1.0\n",
      "Training iteration 225 loss: 0.0011211810633540154, ACC:1.0\n",
      "Training iteration 226 loss: 0.008636116981506348, ACC:1.0\n",
      "Training iteration 227 loss: 0.10372240841388702, ACC:0.984375\n",
      "Training iteration 228 loss: 0.0051811980083584785, ACC:1.0\n",
      "Training iteration 229 loss: 0.00048821073141880333, ACC:1.0\n",
      "Training iteration 230 loss: 0.0023508435115218163, ACC:1.0\n",
      "Training iteration 231 loss: 0.001453795819543302, ACC:1.0\n",
      "Training iteration 232 loss: 0.005666287150233984, ACC:1.0\n",
      "Training iteration 233 loss: 0.05325305834412575, ACC:0.984375\n",
      "Training iteration 234 loss: 0.08918241411447525, ACC:0.96875\n",
      "Training iteration 235 loss: 0.0021280923392623663, ACC:1.0\n",
      "Training iteration 236 loss: 0.03341449424624443, ACC:0.984375\n",
      "Training iteration 237 loss: 0.011293754912912846, ACC:1.0\n",
      "Training iteration 238 loss: 0.001997655024752021, ACC:1.0\n",
      "Training iteration 239 loss: 0.00681502977386117, ACC:1.0\n",
      "Training iteration 240 loss: 0.05809003859758377, ACC:0.984375\n",
      "Training iteration 241 loss: 0.004376185592263937, ACC:1.0\n",
      "Training iteration 242 loss: 0.016605878248810768, ACC:0.984375\n",
      "Training iteration 243 loss: 0.046660661697387695, ACC:0.984375\n",
      "Training iteration 244 loss: 0.0036835307255387306, ACC:1.0\n",
      "Training iteration 245 loss: 0.015366234816610813, ACC:0.984375\n",
      "Training iteration 246 loss: 0.019134962931275368, ACC:0.984375\n",
      "Training iteration 247 loss: 0.005594369024038315, ACC:1.0\n",
      "Training iteration 248 loss: 0.002029933501034975, ACC:1.0\n",
      "Training iteration 249 loss: 0.003841572906821966, ACC:1.0\n",
      "Training iteration 250 loss: 0.026257287710905075, ACC:0.984375\n",
      "Training iteration 251 loss: 0.03264959901571274, ACC:0.984375\n",
      "Training iteration 252 loss: 0.007204151712357998, ACC:1.0\n",
      "Training iteration 253 loss: 0.002880621002987027, ACC:1.0\n",
      "Training iteration 254 loss: 0.011355438269674778, ACC:1.0\n",
      "Training iteration 255 loss: 0.015527501702308655, ACC:0.984375\n",
      "Training iteration 256 loss: 0.03100857138633728, ACC:0.984375\n",
      "Training iteration 257 loss: 0.0019629329908639193, ACC:1.0\n",
      "Training iteration 258 loss: 0.0015079567674547434, ACC:1.0\n",
      "Training iteration 259 loss: 0.0010179163655266166, ACC:1.0\n",
      "Training iteration 260 loss: 0.0015349326422438025, ACC:1.0\n",
      "Training iteration 261 loss: 0.04248113930225372, ACC:0.984375\n",
      "Training iteration 262 loss: 0.014885593205690384, ACC:0.984375\n",
      "Training iteration 263 loss: 0.0027579215820878744, ACC:1.0\n",
      "Training iteration 264 loss: 0.0009616807801648974, ACC:1.0\n",
      "Training iteration 265 loss: 0.001054595224559307, ACC:1.0\n",
      "Training iteration 266 loss: 0.0014497110387310386, ACC:1.0\n",
      "Training iteration 267 loss: 0.006683938670903444, ACC:1.0\n",
      "Training iteration 268 loss: 0.0006899366853758693, ACC:1.0\n",
      "Training iteration 269 loss: 0.014218681491911411, ACC:1.0\n",
      "Training iteration 270 loss: 0.00023087533190846443, ACC:1.0\n",
      "Training iteration 271 loss: 0.05074882507324219, ACC:0.96875\n",
      "Training iteration 272 loss: 0.0022910432890057564, ACC:1.0\n",
      "Training iteration 273 loss: 0.0014841633383184671, ACC:1.0\n",
      "Training iteration 274 loss: 0.07715921849012375, ACC:0.984375\n",
      "Training iteration 275 loss: 0.003657109336927533, ACC:1.0\n",
      "Training iteration 276 loss: 9.73558344412595e-05, ACC:1.0\n",
      "Training iteration 277 loss: 0.000255591090535745, ACC:1.0\n",
      "Training iteration 278 loss: 0.0019327971385791898, ACC:1.0\n",
      "Training iteration 279 loss: 0.0012197287287563086, ACC:1.0\n",
      "Training iteration 280 loss: 0.0016629586461931467, ACC:1.0\n",
      "Training iteration 281 loss: 0.12284373492002487, ACC:0.984375\n",
      "Training iteration 282 loss: 0.0638970211148262, ACC:0.984375\n",
      "Training iteration 283 loss: 0.013832681812345982, ACC:0.984375\n",
      "Training iteration 284 loss: 0.0017537344247102737, ACC:1.0\n",
      "Training iteration 285 loss: 0.02899772860109806, ACC:0.984375\n",
      "Training iteration 286 loss: 0.000768399506341666, ACC:1.0\n",
      "Training iteration 287 loss: 0.0011333167785778642, ACC:1.0\n",
      "Training iteration 288 loss: 0.08698670566082001, ACC:0.96875\n",
      "Training iteration 289 loss: 0.00032919482327997684, ACC:1.0\n",
      "Training iteration 290 loss: 0.0014028281439095736, ACC:1.0\n",
      "Training iteration 291 loss: 0.008078211918473244, ACC:1.0\n",
      "Training iteration 292 loss: 0.09033631533384323, ACC:0.984375\n",
      "Training iteration 293 loss: 0.003302057972177863, ACC:1.0\n",
      "Training iteration 294 loss: 0.0415797121822834, ACC:0.984375\n",
      "Training iteration 295 loss: 0.05367119982838631, ACC:0.96875\n",
      "Training iteration 296 loss: 0.02094762958586216, ACC:1.0\n",
      "Training iteration 297 loss: 0.014583014883100986, ACC:1.0\n",
      "Training iteration 298 loss: 0.03834335505962372, ACC:0.984375\n",
      "Training iteration 299 loss: 0.02940426766872406, ACC:0.984375\n",
      "Training iteration 300 loss: 0.010507360100746155, ACC:1.0\n",
      "Training iteration 301 loss: 0.007999282330274582, ACC:1.0\n",
      "Training iteration 302 loss: 0.04764245077967644, ACC:0.984375\n",
      "Training iteration 303 loss: 0.0016064622905105352, ACC:1.0\n",
      "Training iteration 304 loss: 0.007026991806924343, ACC:1.0\n",
      "Training iteration 305 loss: 0.005428335629403591, ACC:1.0\n",
      "Training iteration 306 loss: 0.037031255662441254, ACC:0.984375\n",
      "Training iteration 307 loss: 0.006568337790668011, ACC:1.0\n",
      "Training iteration 308 loss: 0.0009445350733585656, ACC:1.0\n",
      "Training iteration 309 loss: 0.06994586437940598, ACC:0.96875\n",
      "Training iteration 310 loss: 0.031953345984220505, ACC:0.984375\n",
      "Training iteration 311 loss: 0.016044696792960167, ACC:1.0\n",
      "Training iteration 312 loss: 0.0015643539372831583, ACC:1.0\n",
      "Training iteration 313 loss: 0.049567461013793945, ACC:0.96875\n",
      "Training iteration 314 loss: 0.10937748104333878, ACC:0.953125\n",
      "Training iteration 315 loss: 0.03324776142835617, ACC:0.984375\n",
      "Training iteration 316 loss: 0.011047053150832653, ACC:1.0\n",
      "Training iteration 317 loss: 0.0008408459834754467, ACC:1.0\n",
      "Training iteration 318 loss: 0.0035748870577663183, ACC:1.0\n",
      "Training iteration 319 loss: 0.01083880104124546, ACC:1.0\n",
      "Training iteration 320 loss: 0.02107374742627144, ACC:1.0\n",
      "Training iteration 321 loss: 0.017093978822231293, ACC:1.0\n",
      "Training iteration 322 loss: 0.016950294375419617, ACC:1.0\n",
      "Training iteration 323 loss: 0.09811100363731384, ACC:0.984375\n",
      "Training iteration 324 loss: 0.0024536491837352514, ACC:1.0\n",
      "Training iteration 325 loss: 0.0028622481040656567, ACC:1.0\n",
      "Training iteration 326 loss: 0.10783488303422928, ACC:0.96875\n",
      "Training iteration 327 loss: 0.01629764772951603, ACC:0.984375\n",
      "Training iteration 328 loss: 0.03307676687836647, ACC:0.984375\n",
      "Training iteration 329 loss: 0.02579035423696041, ACC:0.984375\n",
      "Training iteration 330 loss: 0.00518463458865881, ACC:1.0\n",
      "Training iteration 331 loss: 0.016067322343587875, ACC:1.0\n",
      "Training iteration 332 loss: 0.016111023724079132, ACC:0.984375\n",
      "Training iteration 333 loss: 0.002856203820556402, ACC:1.0\n",
      "Training iteration 334 loss: 0.005034899804741144, ACC:1.0\n",
      "Training iteration 335 loss: 0.015383988618850708, ACC:0.984375\n",
      "Training iteration 336 loss: 0.001456499914638698, ACC:1.0\n",
      "Training iteration 337 loss: 0.0187994372099638, ACC:0.984375\n",
      "Training iteration 338 loss: 0.020973823964595795, ACC:1.0\n",
      "Training iteration 339 loss: 0.03575268015265465, ACC:0.984375\n",
      "Training iteration 340 loss: 0.003475181758403778, ACC:1.0\n",
      "Training iteration 341 loss: 0.11589032411575317, ACC:0.984375\n",
      "Training iteration 342 loss: 0.003999329172074795, ACC:1.0\n",
      "Training iteration 343 loss: 0.0015424189623445272, ACC:1.0\n",
      "Training iteration 344 loss: 0.0032520550303161144, ACC:1.0\n",
      "Training iteration 345 loss: 0.0026659334544092417, ACC:1.0\n",
      "Training iteration 346 loss: 0.054948508739471436, ACC:0.984375\n",
      "Training iteration 347 loss: 0.014131979085505009, ACC:1.0\n",
      "Training iteration 348 loss: 0.012389042414724827, ACC:1.0\n",
      "Training iteration 349 loss: 0.07455233484506607, ACC:0.984375\n",
      "Training iteration 350 loss: 0.004272325895726681, ACC:1.0\n",
      "Training iteration 351 loss: 0.010343504138290882, ACC:1.0\n",
      "Training iteration 352 loss: 0.0043183546513319016, ACC:1.0\n",
      "Training iteration 353 loss: 0.005326866637915373, ACC:1.0\n",
      "Training iteration 354 loss: 0.00662749819457531, ACC:1.0\n",
      "Training iteration 355 loss: 0.0033909364137798548, ACC:1.0\n",
      "Training iteration 356 loss: 0.0027642587665468454, ACC:1.0\n",
      "Training iteration 357 loss: 0.002822349313646555, ACC:1.0\n",
      "Training iteration 358 loss: 0.005386197008192539, ACC:1.0\n",
      "Training iteration 359 loss: 0.002659856341779232, ACC:1.0\n",
      "Training iteration 360 loss: 0.000994970090687275, ACC:1.0\n",
      "Training iteration 361 loss: 0.008803186938166618, ACC:1.0\n",
      "Training iteration 362 loss: 0.002068644855171442, ACC:1.0\n",
      "Training iteration 363 loss: 0.15223678946495056, ACC:0.984375\n",
      "Training iteration 364 loss: 0.010646347887814045, ACC:1.0\n",
      "Training iteration 365 loss: 0.0009482800960540771, ACC:1.0\n",
      "Training iteration 366 loss: 0.0012746970169246197, ACC:1.0\n",
      "Training iteration 367 loss: 0.002183185424655676, ACC:1.0\n",
      "Training iteration 368 loss: 0.036132510751485825, ACC:0.96875\n",
      "Training iteration 369 loss: 0.0003215853648725897, ACC:1.0\n",
      "Training iteration 370 loss: 0.0010149930603802204, ACC:1.0\n",
      "Training iteration 371 loss: 0.013055594637989998, ACC:1.0\n",
      "Training iteration 372 loss: 0.0010275779059156775, ACC:1.0\n",
      "Training iteration 373 loss: 0.005546207539737225, ACC:1.0\n",
      "Training iteration 374 loss: 0.028854012489318848, ACC:0.984375\n",
      "Training iteration 375 loss: 0.021319644525647163, ACC:0.984375\n",
      "Training iteration 376 loss: 0.013066750019788742, ACC:0.984375\n",
      "Training iteration 377 loss: 0.012878550216555595, ACC:1.0\n",
      "Training iteration 378 loss: 0.10086993873119354, ACC:0.96875\n",
      "Training iteration 379 loss: 0.02559419348835945, ACC:0.984375\n",
      "Training iteration 380 loss: 0.03429378569126129, ACC:0.984375\n",
      "Training iteration 381 loss: 0.03071744367480278, ACC:0.984375\n",
      "Training iteration 382 loss: 0.0021102428436279297, ACC:1.0\n",
      "Training iteration 383 loss: 0.01316089741885662, ACC:1.0\n",
      "Training iteration 384 loss: 0.0046898797154426575, ACC:1.0\n",
      "Training iteration 385 loss: 0.03481669723987579, ACC:0.984375\n",
      "Training iteration 386 loss: 0.0030277182813733816, ACC:1.0\n",
      "Training iteration 387 loss: 0.006654269527643919, ACC:1.0\n",
      "Training iteration 388 loss: 0.007491491734981537, ACC:1.0\n",
      "Training iteration 389 loss: 0.005482781678438187, ACC:1.0\n",
      "Training iteration 390 loss: 0.030079437419772148, ACC:0.984375\n",
      "Training iteration 391 loss: 0.0018800024408847094, ACC:1.0\n",
      "Training iteration 392 loss: 0.013574943877756596, ACC:1.0\n",
      "Training iteration 393 loss: 0.006633584387600422, ACC:1.0\n",
      "Training iteration 394 loss: 0.004081868566572666, ACC:1.0\n",
      "Training iteration 395 loss: 0.0030676790047436953, ACC:1.0\n",
      "Training iteration 396 loss: 0.0019697649404406548, ACC:1.0\n",
      "Training iteration 397 loss: 0.00758594274520874, ACC:1.0\n",
      "Training iteration 398 loss: 0.001784187857992947, ACC:1.0\n",
      "Training iteration 399 loss: 0.0029041431844234467, ACC:1.0\n",
      "Training iteration 400 loss: 0.0017051525646820664, ACC:1.0\n",
      "Training iteration 401 loss: 0.0015590398106724024, ACC:1.0\n",
      "Training iteration 402 loss: 0.01826988346874714, ACC:0.984375\n",
      "Training iteration 403 loss: 0.004938469268381596, ACC:1.0\n",
      "Training iteration 404 loss: 0.005282512865960598, ACC:1.0\n",
      "Training iteration 405 loss: 0.037575721740722656, ACC:0.984375\n",
      "Training iteration 406 loss: 0.0003940299793612212, ACC:1.0\n",
      "Training iteration 407 loss: 0.0005256663425825536, ACC:1.0\n",
      "Training iteration 408 loss: 0.025704432278871536, ACC:0.984375\n",
      "Training iteration 409 loss: 0.0012571309925988317, ACC:1.0\n",
      "Training iteration 410 loss: 0.0014615128748118877, ACC:1.0\n",
      "Training iteration 411 loss: 0.025170758366584778, ACC:0.984375\n",
      "Training iteration 412 loss: 0.0020871348679065704, ACC:1.0\n",
      "Training iteration 413 loss: 0.002953634597361088, ACC:1.0\n",
      "Training iteration 414 loss: 0.13897527754306793, ACC:0.984375\n",
      "Training iteration 415 loss: 0.009762339293956757, ACC:1.0\n",
      "Training iteration 416 loss: 0.0029489381704479456, ACC:1.0\n",
      "Training iteration 417 loss: 0.0009015370160341263, ACC:1.0\n",
      "Training iteration 418 loss: 0.0006842468865215778, ACC:1.0\n",
      "Training iteration 419 loss: 0.003811180591583252, ACC:1.0\n",
      "Training iteration 420 loss: 0.04714759439229965, ACC:0.984375\n",
      "Training iteration 421 loss: 0.006339649204164743, ACC:1.0\n",
      "Training iteration 422 loss: 0.019359059631824493, ACC:0.984375\n",
      "Training iteration 423 loss: 0.0010572863975539804, ACC:1.0\n",
      "Training iteration 424 loss: 0.0001772436371538788, ACC:1.0\n",
      "Training iteration 425 loss: 0.06762891262769699, ACC:0.984375\n",
      "Training iteration 426 loss: 0.00032323680352419615, ACC:1.0\n",
      "Training iteration 427 loss: 0.0004199841932859272, ACC:1.0\n",
      "Training iteration 428 loss: 0.00124192179646343, ACC:1.0\n",
      "Training iteration 429 loss: 0.0048726266250014305, ACC:1.0\n",
      "Training iteration 430 loss: 0.0008000551024451852, ACC:1.0\n",
      "Training iteration 431 loss: 0.022728227078914642, ACC:1.0\n",
      "Training iteration 432 loss: 0.00048430304741486907, ACC:1.0\n",
      "Training iteration 433 loss: 0.002758480142802, ACC:1.0\n",
      "Training iteration 434 loss: 0.010778693482279778, ACC:1.0\n",
      "Training iteration 435 loss: 0.0005465091671794653, ACC:1.0\n",
      "Training iteration 436 loss: 0.0019676596857607365, ACC:1.0\n",
      "Training iteration 437 loss: 0.012650980614125729, ACC:0.984375\n",
      "Training iteration 438 loss: 0.00016332880477420986, ACC:1.0\n",
      "Training iteration 439 loss: 0.0011461344547569752, ACC:1.0\n",
      "Training iteration 440 loss: 0.0007908226689323783, ACC:1.0\n",
      "Training iteration 441 loss: 0.010622452944517136, ACC:1.0\n",
      "Training iteration 442 loss: 0.002495566150173545, ACC:1.0\n",
      "Training iteration 443 loss: 0.09560847282409668, ACC:0.984375\n",
      "Training iteration 444 loss: 0.020706254988908768, ACC:0.984375\n",
      "Training iteration 445 loss: 0.10870173573493958, ACC:0.984375\n",
      "Training iteration 446 loss: 0.003998769912868738, ACC:1.0\n",
      "Training iteration 447 loss: 0.004332561511546373, ACC:1.0\n",
      "Training iteration 448 loss: 0.006665362510830164, ACC:1.0\n",
      "Training iteration 449 loss: 0.0015206816606223583, ACC:1.0\n",
      "Training iteration 450 loss: 0.0012957360595464706, ACC:1.0\n",
      "Validation iteration 451 loss: 0.05661369860172272, ACC: 0.984375\n",
      "Validation iteration 452 loss: 0.00676372367888689, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.0015907245688140392, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.0032331107649952173, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.01095152460038662, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.014217686839401722, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.003496336517855525, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.0013837342849001288, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.004475816618651152, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.13069109618663788, ACC: 0.96875\n",
      "Validation iteration 461 loss: 0.033183835446834564, ACC: 0.984375\n",
      "Validation iteration 462 loss: 0.002023454988375306, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.0028714437503367662, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.02948594279587269, ACC: 0.984375\n",
      "Validation iteration 465 loss: 0.004429775755852461, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.0036156070418655872, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.00980990007519722, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.0021624653600156307, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.03498047590255737, ACC: 0.984375\n",
      "Validation iteration 470 loss: 0.0024780691601336002, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.03148346394300461, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.009303631260991096, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.0016372634563595057, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.005174539517611265, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.0013791345991194248, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.03101341612637043, ACC: 0.984375\n",
      "Validation iteration 477 loss: 0.0026950580067932606, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.013818301260471344, ACC: 1.0\n",
      "Validation iteration 479 loss: 0.0014142838772386312, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.0036047897301614285, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.10300689190626144, ACC: 0.96875\n",
      "Validation iteration 482 loss: 0.004348598420619965, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.017164533957839012, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.014264279045164585, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.0026542276609688997, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.002274130005389452, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.0021187684033066034, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.0022863249760121107, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.002869571093469858, ACC: 1.0\n",
      "Validation iteration 490 loss: 0.004589698743075132, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.031250376254320145, ACC: 0.984375\n",
      "Validation iteration 492 loss: 0.08639910817146301, ACC: 0.96875\n",
      "Validation iteration 493 loss: 0.0014916714280843735, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.01130632497370243, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.004652372561395168, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.0010329930810257792, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.005856469739228487, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.01852029375731945, ACC: 0.984375\n",
      "Validation iteration 499 loss: 0.008128083311021328, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.003653195220977068, ACC: 1.0\n",
      "-- Epoch 8 done -- Train loss: 0.016827283174562682, train ACC: 0.9949652777777778, val loss: 0.015757004348561168, val ACC: 0.9953125\n",
      "<--- 12877.443390607834 seconds --->\n",
      "Training iteration 1 loss: 0.004952938295900822, ACC:1.0\n",
      "Training iteration 2 loss: 0.00622759573161602, ACC:1.0\n",
      "Training iteration 3 loss: 0.0018796646036207676, ACC:1.0\n",
      "Training iteration 4 loss: 0.012567049823701382, ACC:1.0\n",
      "Training iteration 5 loss: 0.022876804694533348, ACC:0.984375\n",
      "Training iteration 6 loss: 0.01669309101998806, ACC:1.0\n",
      "Training iteration 7 loss: 0.006513284053653479, ACC:1.0\n",
      "Training iteration 8 loss: 0.0405154824256897, ACC:0.984375\n",
      "Training iteration 9 loss: 0.045182712376117706, ACC:0.984375\n",
      "Training iteration 10 loss: 0.0013656229712069035, ACC:1.0\n",
      "Training iteration 11 loss: 0.015955256298184395, ACC:1.0\n",
      "Training iteration 12 loss: 0.0016755190445110202, ACC:1.0\n",
      "Training iteration 13 loss: 0.0034735107328742743, ACC:1.0\n",
      "Training iteration 14 loss: 0.1250430792570114, ACC:0.984375\n",
      "Training iteration 15 loss: 0.0017062753904610872, ACC:1.0\n",
      "Training iteration 16 loss: 0.0024980029556900263, ACC:1.0\n",
      "Training iteration 17 loss: 0.019145047292113304, ACC:0.984375\n",
      "Training iteration 18 loss: 0.0016343813622370362, ACC:1.0\n",
      "Training iteration 19 loss: 0.0056470297276973724, ACC:1.0\n",
      "Training iteration 20 loss: 0.10134498029947281, ACC:0.96875\n",
      "Training iteration 21 loss: 0.03298253193497658, ACC:0.984375\n",
      "Training iteration 22 loss: 0.060115791857242584, ACC:0.984375\n",
      "Training iteration 23 loss: 0.05326853692531586, ACC:0.96875\n",
      "Training iteration 24 loss: 0.006306178402155638, ACC:1.0\n",
      "Training iteration 25 loss: 0.007807270623743534, ACC:1.0\n",
      "Training iteration 26 loss: 0.09146791696548462, ACC:0.984375\n",
      "Training iteration 27 loss: 0.022237645462155342, ACC:0.984375\n",
      "Training iteration 28 loss: 0.0037276174407452345, ACC:1.0\n",
      "Training iteration 29 loss: 0.005303177982568741, ACC:1.0\n",
      "Training iteration 30 loss: 0.015978170558810234, ACC:1.0\n",
      "Training iteration 31 loss: 0.038873668760061264, ACC:0.984375\n",
      "Training iteration 32 loss: 0.06654950231313705, ACC:0.96875\n",
      "Training iteration 33 loss: 0.0559798888862133, ACC:0.96875\n",
      "Training iteration 34 loss: 0.00856800377368927, ACC:1.0\n",
      "Training iteration 35 loss: 0.01461472362279892, ACC:1.0\n",
      "Training iteration 36 loss: 0.004583930131047964, ACC:1.0\n",
      "Training iteration 37 loss: 0.07266625761985779, ACC:0.953125\n",
      "Training iteration 38 loss: 0.16650612652301788, ACC:0.953125\n",
      "Training iteration 39 loss: 0.005489359609782696, ACC:1.0\n",
      "Training iteration 40 loss: 0.0023976704105734825, ACC:1.0\n",
      "Training iteration 41 loss: 0.0017637485871091485, ACC:1.0\n",
      "Training iteration 42 loss: 0.00356280361302197, ACC:1.0\n",
      "Training iteration 43 loss: 0.00407580379396677, ACC:1.0\n",
      "Training iteration 44 loss: 0.0020590403582900763, ACC:1.0\n",
      "Training iteration 45 loss: 0.0006698122597299516, ACC:1.0\n",
      "Training iteration 46 loss: 0.0008492351043969393, ACC:1.0\n",
      "Training iteration 47 loss: 0.002506461227312684, ACC:1.0\n",
      "Training iteration 48 loss: 0.00047860428458079696, ACC:1.0\n",
      "Training iteration 49 loss: 0.2062446027994156, ACC:0.96875\n",
      "Training iteration 50 loss: 0.00823377538472414, ACC:1.0\n",
      "Training iteration 51 loss: 0.00033127597998827696, ACC:1.0\n",
      "Training iteration 52 loss: 0.002393924631178379, ACC:1.0\n",
      "Training iteration 53 loss: 0.0007430688128806651, ACC:1.0\n",
      "Training iteration 54 loss: 0.028141247108578682, ACC:0.984375\n",
      "Training iteration 55 loss: 0.0011111198691651225, ACC:1.0\n",
      "Training iteration 56 loss: 0.005702930502593517, ACC:1.0\n",
      "Training iteration 57 loss: 0.01870230957865715, ACC:1.0\n",
      "Training iteration 58 loss: 0.0010664594592526555, ACC:1.0\n",
      "Training iteration 59 loss: 0.1191447526216507, ACC:0.984375\n",
      "Training iteration 60 loss: 0.002242409624159336, ACC:1.0\n",
      "Training iteration 61 loss: 0.03318984806537628, ACC:0.984375\n",
      "Training iteration 62 loss: 0.0012766433646902442, ACC:1.0\n",
      "Training iteration 63 loss: 0.01872764155268669, ACC:0.984375\n",
      "Training iteration 64 loss: 0.004783070646226406, ACC:1.0\n",
      "Training iteration 65 loss: 0.049458958208560944, ACC:0.984375\n",
      "Training iteration 66 loss: 0.0017282706685364246, ACC:1.0\n",
      "Training iteration 67 loss: 0.0025513970758765936, ACC:1.0\n",
      "Training iteration 68 loss: 0.00705219991505146, ACC:1.0\n",
      "Training iteration 69 loss: 0.0031334799714386463, ACC:1.0\n",
      "Training iteration 70 loss: 0.010150618851184845, ACC:1.0\n",
      "Training iteration 71 loss: 0.10458515584468842, ACC:0.984375\n",
      "Training iteration 72 loss: 0.0030399439856410027, ACC:1.0\n",
      "Training iteration 73 loss: 0.005159094464033842, ACC:1.0\n",
      "Training iteration 74 loss: 0.017858589068055153, ACC:0.984375\n",
      "Training iteration 75 loss: 0.004228317644447088, ACC:1.0\n",
      "Training iteration 76 loss: 0.004914033692330122, ACC:1.0\n",
      "Training iteration 77 loss: 0.003678911831229925, ACC:1.0\n",
      "Training iteration 78 loss: 0.013880880549550056, ACC:0.984375\n",
      "Training iteration 79 loss: 0.01895383931696415, ACC:0.984375\n",
      "Training iteration 80 loss: 0.005634550470858812, ACC:1.0\n",
      "Training iteration 81 loss: 0.05939916521310806, ACC:0.96875\n",
      "Training iteration 82 loss: 0.01666952855885029, ACC:0.984375\n",
      "Training iteration 83 loss: 0.019966503605246544, ACC:0.984375\n",
      "Training iteration 84 loss: 0.1396663635969162, ACC:0.984375\n",
      "Training iteration 85 loss: 0.0014247964136302471, ACC:1.0\n",
      "Training iteration 86 loss: 0.058530811220407486, ACC:0.984375\n",
      "Training iteration 87 loss: 0.0040089464746415615, ACC:1.0\n",
      "Training iteration 88 loss: 0.000817424850538373, ACC:1.0\n",
      "Training iteration 89 loss: 0.03873223438858986, ACC:0.984375\n",
      "Training iteration 90 loss: 0.0029278136789798737, ACC:1.0\n",
      "Training iteration 91 loss: 0.006503046955913305, ACC:1.0\n",
      "Training iteration 92 loss: 0.006371238734573126, ACC:1.0\n",
      "Training iteration 93 loss: 0.001154125202447176, ACC:1.0\n",
      "Training iteration 94 loss: 0.001364911557175219, ACC:1.0\n",
      "Training iteration 95 loss: 0.006489630788564682, ACC:1.0\n",
      "Training iteration 96 loss: 0.002779807895421982, ACC:1.0\n",
      "Training iteration 97 loss: 0.0027439380064606667, ACC:1.0\n",
      "Training iteration 98 loss: 0.003548608860000968, ACC:1.0\n",
      "Training iteration 99 loss: 0.032118670642375946, ACC:0.984375\n",
      "Training iteration 100 loss: 0.0038133510388433933, ACC:1.0\n",
      "Training iteration 101 loss: 0.05230027064681053, ACC:0.984375\n",
      "Training iteration 102 loss: 0.0006204419769346714, ACC:1.0\n",
      "Training iteration 103 loss: 0.03338567912578583, ACC:0.984375\n",
      "Training iteration 104 loss: 0.0007597352960146964, ACC:1.0\n",
      "Training iteration 105 loss: 0.10285469144582748, ACC:0.984375\n",
      "Training iteration 106 loss: 0.049173999577760696, ACC:0.984375\n",
      "Training iteration 107 loss: 0.0016713212244212627, ACC:1.0\n",
      "Training iteration 108 loss: 0.011924674734473228, ACC:1.0\n",
      "Training iteration 109 loss: 0.0007268295739777386, ACC:1.0\n",
      "Training iteration 110 loss: 0.0005084933363832533, ACC:1.0\n",
      "Training iteration 111 loss: 0.025538258254528046, ACC:0.984375\n",
      "Training iteration 112 loss: 0.003330797655507922, ACC:1.0\n",
      "Training iteration 113 loss: 0.0011187026975676417, ACC:1.0\n",
      "Training iteration 114 loss: 0.001281301723793149, ACC:1.0\n",
      "Training iteration 115 loss: 0.002957900520414114, ACC:1.0\n",
      "Training iteration 116 loss: 0.00015624533989466727, ACC:1.0\n",
      "Training iteration 117 loss: 0.10580174624919891, ACC:0.984375\n",
      "Training iteration 118 loss: 0.010610844008624554, ACC:1.0\n",
      "Training iteration 119 loss: 0.0008399261278100312, ACC:1.0\n",
      "Training iteration 120 loss: 0.006884546484798193, ACC:1.0\n",
      "Training iteration 121 loss: 0.0010234271176159382, ACC:1.0\n",
      "Training iteration 122 loss: 0.006563642993569374, ACC:1.0\n",
      "Training iteration 123 loss: 0.0016459900652989745, ACC:1.0\n",
      "Training iteration 124 loss: 0.0018555477727204561, ACC:1.0\n",
      "Training iteration 125 loss: 0.002054242417216301, ACC:1.0\n",
      "Training iteration 126 loss: 0.003594348905608058, ACC:1.0\n",
      "Training iteration 127 loss: 0.006340399384498596, ACC:1.0\n",
      "Training iteration 128 loss: 0.0033572474494576454, ACC:1.0\n",
      "Training iteration 129 loss: 0.0045940871350467205, ACC:1.0\n",
      "Training iteration 130 loss: 0.0157057773321867, ACC:1.0\n",
      "Training iteration 131 loss: 0.002293888945132494, ACC:1.0\n",
      "Training iteration 132 loss: 0.015005621127784252, ACC:0.984375\n",
      "Training iteration 133 loss: 0.006623914465308189, ACC:1.0\n",
      "Training iteration 134 loss: 0.03855385258793831, ACC:0.984375\n",
      "Training iteration 135 loss: 0.004034809768199921, ACC:1.0\n",
      "Training iteration 136 loss: 0.00100724957883358, ACC:1.0\n",
      "Training iteration 137 loss: 0.017541030421853065, ACC:0.984375\n",
      "Training iteration 138 loss: 0.004079120233654976, ACC:1.0\n",
      "Training iteration 139 loss: 0.00878705084323883, ACC:1.0\n",
      "Training iteration 140 loss: 0.00030178582528606057, ACC:1.0\n",
      "Training iteration 141 loss: 0.0031621090602129698, ACC:1.0\n",
      "Training iteration 142 loss: 0.0017931768670678139, ACC:1.0\n",
      "Training iteration 143 loss: 0.13213083148002625, ACC:0.984375\n",
      "Training iteration 144 loss: 0.0019046402303501964, ACC:1.0\n",
      "Training iteration 145 loss: 0.0008911026525311172, ACC:1.0\n",
      "Training iteration 146 loss: 0.00031651181052438915, ACC:1.0\n",
      "Training iteration 147 loss: 0.0025418614968657494, ACC:1.0\n",
      "Training iteration 148 loss: 0.0014701917534694076, ACC:1.0\n",
      "Training iteration 149 loss: 0.0015574111603200436, ACC:1.0\n",
      "Training iteration 150 loss: 0.0025514683220535517, ACC:1.0\n",
      "Training iteration 151 loss: 0.0005315787857398391, ACC:1.0\n",
      "Training iteration 152 loss: 0.001261817174963653, ACC:1.0\n",
      "Training iteration 153 loss: 0.030303195118904114, ACC:0.984375\n",
      "Training iteration 154 loss: 0.010546714998781681, ACC:1.0\n",
      "Training iteration 155 loss: 0.0006933551630936563, ACC:1.0\n",
      "Training iteration 156 loss: 0.0009542480111122131, ACC:1.0\n",
      "Training iteration 157 loss: 0.0011665760539472103, ACC:1.0\n",
      "Training iteration 158 loss: 0.008359063416719437, ACC:1.0\n",
      "Training iteration 159 loss: 0.01073472760617733, ACC:1.0\n",
      "Training iteration 160 loss: 0.039066068828105927, ACC:0.984375\n",
      "Training iteration 161 loss: 0.013414531946182251, ACC:1.0\n",
      "Training iteration 162 loss: 0.005817282944917679, ACC:1.0\n",
      "Training iteration 163 loss: 0.019560592249035835, ACC:0.984375\n",
      "Training iteration 164 loss: 0.0033044335432350636, ACC:1.0\n",
      "Training iteration 165 loss: 0.015895521268248558, ACC:1.0\n",
      "Training iteration 166 loss: 0.006985533516854048, ACC:1.0\n",
      "Training iteration 167 loss: 0.00926566869020462, ACC:1.0\n",
      "Training iteration 168 loss: 0.028138380497694016, ACC:0.984375\n",
      "Training iteration 169 loss: 0.016143886372447014, ACC:0.984375\n",
      "Training iteration 170 loss: 0.04226590320467949, ACC:0.984375\n",
      "Training iteration 171 loss: 0.0027596792206168175, ACC:1.0\n",
      "Training iteration 172 loss: 0.004336867947131395, ACC:1.0\n",
      "Training iteration 173 loss: 0.0006280376110225916, ACC:1.0\n",
      "Training iteration 174 loss: 0.003603803925216198, ACC:1.0\n",
      "Training iteration 175 loss: 0.0021971813403069973, ACC:1.0\n",
      "Training iteration 176 loss: 0.0009525147033855319, ACC:1.0\n",
      "Training iteration 177 loss: 0.0013189431047067046, ACC:1.0\n",
      "Training iteration 178 loss: 0.16411131620407104, ACC:0.953125\n",
      "Training iteration 179 loss: 0.06117719039320946, ACC:0.984375\n",
      "Training iteration 180 loss: 0.00471892673522234, ACC:1.0\n",
      "Training iteration 181 loss: 0.005107653792947531, ACC:1.0\n",
      "Training iteration 182 loss: 0.010966837406158447, ACC:1.0\n",
      "Training iteration 183 loss: 0.007014327682554722, ACC:1.0\n",
      "Training iteration 184 loss: 0.0020098425447940826, ACC:1.0\n",
      "Training iteration 185 loss: 0.011200363747775555, ACC:1.0\n",
      "Training iteration 186 loss: 0.0028447601944208145, ACC:1.0\n",
      "Training iteration 187 loss: 0.006848502904176712, ACC:1.0\n",
      "Training iteration 188 loss: 0.00128229521214962, ACC:1.0\n",
      "Training iteration 189 loss: 0.0035210922360420227, ACC:1.0\n",
      "Training iteration 190 loss: 0.03529791161417961, ACC:0.96875\n",
      "Training iteration 191 loss: 0.027590837329626083, ACC:0.984375\n",
      "Training iteration 192 loss: 0.0016437454614788294, ACC:1.0\n",
      "Training iteration 193 loss: 0.0010811551474034786, ACC:1.0\n",
      "Training iteration 194 loss: 0.020519781857728958, ACC:0.984375\n",
      "Training iteration 195 loss: 0.0014320064801722765, ACC:1.0\n",
      "Training iteration 196 loss: 0.02730284444987774, ACC:0.984375\n",
      "Training iteration 197 loss: 0.03665544092655182, ACC:0.984375\n",
      "Training iteration 198 loss: 0.020165279507637024, ACC:0.984375\n",
      "Training iteration 199 loss: 0.046410251408815384, ACC:0.984375\n",
      "Training iteration 200 loss: 0.000509514007717371, ACC:1.0\n",
      "Training iteration 201 loss: 0.03146374598145485, ACC:0.984375\n",
      "Training iteration 202 loss: 0.09812575578689575, ACC:0.96875\n",
      "Training iteration 203 loss: 0.014427341520786285, ACC:1.0\n",
      "Training iteration 204 loss: 0.0024592289701104164, ACC:1.0\n",
      "Training iteration 205 loss: 0.0028680614195764065, ACC:1.0\n",
      "Training iteration 206 loss: 0.005357817746698856, ACC:1.0\n",
      "Training iteration 207 loss: 0.09894376248121262, ACC:0.96875\n",
      "Training iteration 208 loss: 0.04983018338680267, ACC:0.984375\n",
      "Training iteration 209 loss: 0.0030521852895617485, ACC:1.0\n",
      "Training iteration 210 loss: 0.0013237479142844677, ACC:1.0\n",
      "Training iteration 211 loss: 0.0010503820376470685, ACC:1.0\n",
      "Training iteration 212 loss: 0.010184148326516151, ACC:1.0\n",
      "Training iteration 213 loss: 0.00960937887430191, ACC:1.0\n",
      "Training iteration 214 loss: 0.0025180764496326447, ACC:1.0\n",
      "Training iteration 215 loss: 0.0016316173132508993, ACC:1.0\n",
      "Training iteration 216 loss: 0.001840040786191821, ACC:1.0\n",
      "Training iteration 217 loss: 0.01572086103260517, ACC:0.984375\n",
      "Training iteration 218 loss: 0.0014247896615415812, ACC:1.0\n",
      "Training iteration 219 loss: 0.08333461731672287, ACC:0.984375\n",
      "Training iteration 220 loss: 0.010754150338470936, ACC:1.0\n",
      "Training iteration 221 loss: 0.002838159678503871, ACC:1.0\n",
      "Training iteration 222 loss: 0.0037582116201519966, ACC:1.0\n",
      "Training iteration 223 loss: 0.0014946514274924994, ACC:1.0\n",
      "Training iteration 224 loss: 0.030291536822915077, ACC:0.984375\n",
      "Training iteration 225 loss: 0.0020431741140782833, ACC:1.0\n",
      "Training iteration 226 loss: 0.00450607854872942, ACC:1.0\n",
      "Training iteration 227 loss: 0.058225199580192566, ACC:0.96875\n",
      "Training iteration 228 loss: 0.002388057764619589, ACC:1.0\n",
      "Training iteration 229 loss: 0.0026565862353891134, ACC:1.0\n",
      "Training iteration 230 loss: 0.0014238912845030427, ACC:1.0\n",
      "Training iteration 231 loss: 0.00636544032022357, ACC:1.0\n",
      "Training iteration 232 loss: 0.005130895879119635, ACC:1.0\n",
      "Training iteration 233 loss: 0.0018534319242462516, ACC:1.0\n",
      "Training iteration 234 loss: 0.0010754658142104745, ACC:1.0\n",
      "Training iteration 235 loss: 0.0011459108209237456, ACC:1.0\n",
      "Training iteration 236 loss: 0.006489668972790241, ACC:1.0\n",
      "Training iteration 237 loss: 0.007950122468173504, ACC:1.0\n",
      "Training iteration 238 loss: 0.0027674390003085136, ACC:1.0\n",
      "Training iteration 239 loss: 0.008318980224430561, ACC:1.0\n",
      "Training iteration 240 loss: 0.003965249750763178, ACC:1.0\n",
      "Training iteration 241 loss: 0.0014602832961827517, ACC:1.0\n",
      "Training iteration 242 loss: 0.0008229212835431099, ACC:1.0\n",
      "Training iteration 243 loss: 0.000503998133353889, ACC:1.0\n",
      "Training iteration 244 loss: 0.039055850356817245, ACC:0.96875\n",
      "Training iteration 245 loss: 0.016524454578757286, ACC:0.984375\n",
      "Training iteration 246 loss: 0.01718563586473465, ACC:0.984375\n",
      "Training iteration 247 loss: 0.0018838003743439913, ACC:1.0\n",
      "Training iteration 248 loss: 0.005579808261245489, ACC:1.0\n",
      "Training iteration 249 loss: 0.0048756725154817104, ACC:1.0\n",
      "Training iteration 250 loss: 0.002288178540766239, ACC:1.0\n",
      "Training iteration 251 loss: 0.0040526315569877625, ACC:1.0\n",
      "Training iteration 252 loss: 0.00026795215671882033, ACC:1.0\n",
      "Training iteration 253 loss: 0.040332961827516556, ACC:0.984375\n",
      "Training iteration 254 loss: 0.0002888824383262545, ACC:1.0\n",
      "Training iteration 255 loss: 0.10966809093952179, ACC:0.984375\n",
      "Training iteration 256 loss: 0.002506798133254051, ACC:1.0\n",
      "Training iteration 257 loss: 0.0007062599179334939, ACC:1.0\n",
      "Training iteration 258 loss: 0.00837075524032116, ACC:1.0\n",
      "Training iteration 259 loss: 0.021416811272501945, ACC:1.0\n",
      "Training iteration 260 loss: 0.009433425031602383, ACC:1.0\n",
      "Training iteration 261 loss: 0.00221188529394567, ACC:1.0\n",
      "Training iteration 262 loss: 0.14103470742702484, ACC:0.96875\n",
      "Training iteration 263 loss: 0.0017203909810632467, ACC:1.0\n",
      "Training iteration 264 loss: 0.0024039093405008316, ACC:1.0\n",
      "Training iteration 265 loss: 0.0574663020670414, ACC:0.984375\n",
      "Training iteration 266 loss: 0.015608196146786213, ACC:0.984375\n",
      "Training iteration 267 loss: 0.00044092824100516737, ACC:1.0\n",
      "Training iteration 268 loss: 0.0008222063770517707, ACC:1.0\n",
      "Training iteration 269 loss: 0.0018354709027335048, ACC:1.0\n",
      "Training iteration 270 loss: 0.0013410889077931643, ACC:1.0\n",
      "Training iteration 271 loss: 0.005882260389626026, ACC:1.0\n",
      "Training iteration 272 loss: 0.004029624164104462, ACC:1.0\n",
      "Training iteration 273 loss: 0.011270754970610142, ACC:1.0\n",
      "Training iteration 274 loss: 0.0006201667129062116, ACC:1.0\n",
      "Training iteration 275 loss: 0.013801838271319866, ACC:1.0\n",
      "Training iteration 276 loss: 0.0014119575498625636, ACC:1.0\n",
      "Training iteration 277 loss: 0.0003760386607609689, ACC:1.0\n",
      "Training iteration 278 loss: 0.024495333433151245, ACC:0.984375\n",
      "Training iteration 279 loss: 0.0007754400721751153, ACC:1.0\n",
      "Training iteration 280 loss: 0.0025101331993937492, ACC:1.0\n",
      "Training iteration 281 loss: 0.0003782691201195121, ACC:1.0\n",
      "Training iteration 282 loss: 0.007539968006312847, ACC:1.0\n",
      "Training iteration 283 loss: 0.00010467780521139503, ACC:1.0\n",
      "Training iteration 284 loss: 0.001734230201691389, ACC:1.0\n",
      "Training iteration 285 loss: 0.008102756924927235, ACC:1.0\n",
      "Training iteration 286 loss: 0.08353251218795776, ACC:0.984375\n",
      "Training iteration 287 loss: 0.00011715333675965667, ACC:1.0\n",
      "Training iteration 288 loss: 0.002221551723778248, ACC:1.0\n",
      "Training iteration 289 loss: 0.010139452293515205, ACC:1.0\n",
      "Training iteration 290 loss: 0.056876737624406815, ACC:0.984375\n",
      "Training iteration 291 loss: 0.007720249239355326, ACC:1.0\n",
      "Training iteration 292 loss: 0.005949396640062332, ACC:1.0\n",
      "Training iteration 293 loss: 0.028942827135324478, ACC:0.984375\n",
      "Training iteration 294 loss: 0.0006021047593094409, ACC:1.0\n",
      "Training iteration 295 loss: 0.0007279514102265239, ACC:1.0\n",
      "Training iteration 296 loss: 0.006726817227900028, ACC:1.0\n",
      "Training iteration 297 loss: 0.14795847237110138, ACC:0.96875\n",
      "Training iteration 298 loss: 0.008514495566487312, ACC:1.0\n",
      "Training iteration 299 loss: 0.005395714659243822, ACC:1.0\n",
      "Training iteration 300 loss: 0.005029498133808374, ACC:1.0\n",
      "Training iteration 301 loss: 0.008962010033428669, ACC:1.0\n",
      "Training iteration 302 loss: 0.006992669310420752, ACC:1.0\n",
      "Training iteration 303 loss: 0.0034200786612927914, ACC:1.0\n",
      "Training iteration 304 loss: 0.004920875187963247, ACC:1.0\n",
      "Training iteration 305 loss: 0.09120720624923706, ACC:0.984375\n",
      "Training iteration 306 loss: 0.00722228130325675, ACC:1.0\n",
      "Training iteration 307 loss: 0.0016363752074539661, ACC:1.0\n",
      "Training iteration 308 loss: 0.06203814968466759, ACC:0.984375\n",
      "Training iteration 309 loss: 0.005781474523246288, ACC:1.0\n",
      "Training iteration 310 loss: 0.0016182232648134232, ACC:1.0\n",
      "Training iteration 311 loss: 0.025166723877191544, ACC:0.984375\n",
      "Training iteration 312 loss: 0.0008112326613627374, ACC:1.0\n",
      "Training iteration 313 loss: 0.00042925868183374405, ACC:1.0\n",
      "Training iteration 314 loss: 0.007983002811670303, ACC:1.0\n",
      "Training iteration 315 loss: 0.0002513912913855165, ACC:1.0\n",
      "Training iteration 316 loss: 0.022662263363599777, ACC:0.984375\n",
      "Training iteration 317 loss: 0.012853167951107025, ACC:1.0\n",
      "Training iteration 318 loss: 0.000977769959717989, ACC:1.0\n",
      "Training iteration 319 loss: 0.005668880417943001, ACC:1.0\n",
      "Training iteration 320 loss: 0.005188794806599617, ACC:1.0\n",
      "Training iteration 321 loss: 0.0053412760607898235, ACC:1.0\n",
      "Training iteration 322 loss: 0.0027935183607041836, ACC:1.0\n",
      "Training iteration 323 loss: 0.0009927887003868818, ACC:1.0\n",
      "Training iteration 324 loss: 0.005880706012248993, ACC:1.0\n",
      "Training iteration 325 loss: 0.0015564323402941227, ACC:1.0\n",
      "Training iteration 326 loss: 0.0008249920210801065, ACC:1.0\n",
      "Training iteration 327 loss: 0.0013845391804352403, ACC:1.0\n",
      "Training iteration 328 loss: 0.006047963630408049, ACC:1.0\n",
      "Training iteration 329 loss: 0.0026451770681887865, ACC:1.0\n",
      "Training iteration 330 loss: 0.002573286183178425, ACC:1.0\n",
      "Training iteration 331 loss: 0.00039878999814391136, ACC:1.0\n",
      "Training iteration 332 loss: 0.00045206263894215226, ACC:1.0\n",
      "Training iteration 333 loss: 0.004728995263576508, ACC:1.0\n",
      "Training iteration 334 loss: 0.011015411466360092, ACC:1.0\n",
      "Training iteration 335 loss: 0.0028427201323211193, ACC:1.0\n",
      "Training iteration 336 loss: 0.0006443139282055199, ACC:1.0\n",
      "Training iteration 337 loss: 0.030736200511455536, ACC:0.984375\n",
      "Training iteration 338 loss: 0.0003875937545672059, ACC:1.0\n",
      "Training iteration 339 loss: 0.0002665216161403805, ACC:1.0\n",
      "Training iteration 340 loss: 0.0010356445563957095, ACC:1.0\n",
      "Training iteration 341 loss: 0.00397994089871645, ACC:1.0\n",
      "Training iteration 342 loss: 0.0034241359680891037, ACC:1.0\n",
      "Training iteration 343 loss: 0.007267882581800222, ACC:1.0\n",
      "Training iteration 344 loss: 0.0041876682080328465, ACC:1.0\n",
      "Training iteration 345 loss: 0.000738524307962507, ACC:1.0\n",
      "Training iteration 346 loss: 0.016574768349528313, ACC:1.0\n",
      "Training iteration 347 loss: 0.0010952462907880545, ACC:1.0\n",
      "Training iteration 348 loss: 0.0021152549888938665, ACC:1.0\n",
      "Training iteration 349 loss: 0.0011289187241345644, ACC:1.0\n",
      "Training iteration 350 loss: 0.001088045653887093, ACC:1.0\n",
      "Training iteration 351 loss: 0.000799692643340677, ACC:1.0\n",
      "Training iteration 352 loss: 0.0033854972571134567, ACC:1.0\n",
      "Training iteration 353 loss: 0.0015812062192708254, ACC:1.0\n",
      "Training iteration 354 loss: 0.0003371511120349169, ACC:1.0\n",
      "Training iteration 355 loss: 0.0016544985119253397, ACC:1.0\n",
      "Training iteration 356 loss: 0.0003136519808322191, ACC:1.0\n",
      "Training iteration 357 loss: 0.002087426371872425, ACC:1.0\n",
      "Training iteration 358 loss: 0.002173408167436719, ACC:1.0\n",
      "Training iteration 359 loss: 0.00015837933460716158, ACC:1.0\n",
      "Training iteration 360 loss: 0.0011709030950441957, ACC:1.0\n",
      "Training iteration 361 loss: 0.0038577611558139324, ACC:1.0\n",
      "Training iteration 362 loss: 0.02871464006602764, ACC:0.984375\n",
      "Training iteration 363 loss: 0.0004037487378809601, ACC:1.0\n",
      "Training iteration 364 loss: 1.3703555850952398e-05, ACC:1.0\n",
      "Training iteration 365 loss: 0.06105014309287071, ACC:0.984375\n",
      "Training iteration 366 loss: 0.09646310657262802, ACC:0.984375\n",
      "Training iteration 367 loss: 0.0003780859406106174, ACC:1.0\n",
      "Training iteration 368 loss: 0.01232386939227581, ACC:1.0\n",
      "Training iteration 369 loss: 0.038328610360622406, ACC:0.984375\n",
      "Training iteration 370 loss: 0.11611844599246979, ACC:0.96875\n",
      "Training iteration 371 loss: 0.009183022193610668, ACC:1.0\n",
      "Training iteration 372 loss: 0.006054308265447617, ACC:1.0\n",
      "Training iteration 373 loss: 0.007453012280166149, ACC:1.0\n",
      "Training iteration 374 loss: 0.035152073949575424, ACC:0.96875\n",
      "Training iteration 375 loss: 0.14130906760692596, ACC:0.953125\n",
      "Training iteration 376 loss: 0.006517547648400068, ACC:1.0\n",
      "Training iteration 377 loss: 0.01994253695011139, ACC:1.0\n",
      "Training iteration 378 loss: 0.023463385179638863, ACC:1.0\n",
      "Training iteration 379 loss: 0.012560350820422173, ACC:1.0\n",
      "Training iteration 380 loss: 0.025450969114899635, ACC:0.984375\n",
      "Training iteration 381 loss: 0.007430108264088631, ACC:1.0\n",
      "Training iteration 382 loss: 0.004573087207973003, ACC:1.0\n",
      "Training iteration 383 loss: 0.012031356804072857, ACC:1.0\n",
      "Training iteration 384 loss: 0.005558651406317949, ACC:1.0\n",
      "Training iteration 385 loss: 0.013653234578669071, ACC:1.0\n",
      "Training iteration 386 loss: 0.010361401364207268, ACC:1.0\n",
      "Training iteration 387 loss: 0.003331542946398258, ACC:1.0\n",
      "Training iteration 388 loss: 0.0033812117762863636, ACC:1.0\n",
      "Training iteration 389 loss: 0.003528529079630971, ACC:1.0\n",
      "Training iteration 390 loss: 0.001806063810363412, ACC:1.0\n",
      "Training iteration 391 loss: 0.011311899870634079, ACC:1.0\n",
      "Training iteration 392 loss: 0.009696455672383308, ACC:1.0\n",
      "Training iteration 393 loss: 0.0038218863774091005, ACC:1.0\n",
      "Training iteration 394 loss: 0.027810150757431984, ACC:0.984375\n",
      "Training iteration 395 loss: 0.07225088030099869, ACC:0.984375\n",
      "Training iteration 396 loss: 0.0036606357898563147, ACC:1.0\n",
      "Training iteration 397 loss: 0.008455846458673477, ACC:1.0\n",
      "Training iteration 398 loss: 0.04208492115139961, ACC:0.984375\n",
      "Training iteration 399 loss: 0.0021721154917031527, ACC:1.0\n",
      "Training iteration 400 loss: 0.01779934950172901, ACC:0.984375\n",
      "Training iteration 401 loss: 0.0185954961925745, ACC:0.984375\n",
      "Training iteration 402 loss: 0.0011662684846669436, ACC:1.0\n",
      "Training iteration 403 loss: 0.0005580642609857023, ACC:1.0\n",
      "Training iteration 404 loss: 0.010548249818384647, ACC:1.0\n",
      "Training iteration 405 loss: 0.0009501363965682685, ACC:1.0\n",
      "Training iteration 406 loss: 0.00038721822784282267, ACC:1.0\n",
      "Training iteration 407 loss: 0.01764041557908058, ACC:0.984375\n",
      "Training iteration 408 loss: 0.0008764020749367774, ACC:1.0\n",
      "Training iteration 409 loss: 0.012243866920471191, ACC:1.0\n",
      "Training iteration 410 loss: 0.21071338653564453, ACC:0.96875\n",
      "Training iteration 411 loss: 0.06890510022640228, ACC:0.984375\n",
      "Training iteration 412 loss: 0.0708385705947876, ACC:0.984375\n",
      "Training iteration 413 loss: 0.0018395641818642616, ACC:1.0\n",
      "Training iteration 414 loss: 0.016610046848654747, ACC:0.984375\n",
      "Training iteration 415 loss: 0.0023203042801469564, ACC:1.0\n",
      "Training iteration 416 loss: 0.0025995189789682627, ACC:1.0\n",
      "Training iteration 417 loss: 0.07683993875980377, ACC:0.984375\n",
      "Training iteration 418 loss: 0.0027281211223453283, ACC:1.0\n",
      "Training iteration 419 loss: 0.009320263750851154, ACC:1.0\n",
      "Training iteration 420 loss: 0.006326755508780479, ACC:1.0\n",
      "Training iteration 421 loss: 0.00739465095102787, ACC:1.0\n",
      "Training iteration 422 loss: 0.005789567716419697, ACC:1.0\n",
      "Training iteration 423 loss: 0.014740094542503357, ACC:1.0\n",
      "Training iteration 424 loss: 0.0038458104245364666, ACC:1.0\n",
      "Training iteration 425 loss: 0.003027106635272503, ACC:1.0\n",
      "Training iteration 426 loss: 0.013792179524898529, ACC:1.0\n",
      "Training iteration 427 loss: 0.026601312682032585, ACC:0.984375\n",
      "Training iteration 428 loss: 0.010595545172691345, ACC:1.0\n",
      "Training iteration 429 loss: 0.004824298899620771, ACC:1.0\n",
      "Training iteration 430 loss: 0.0006401068530976772, ACC:1.0\n",
      "Training iteration 431 loss: 0.007454481907188892, ACC:1.0\n",
      "Training iteration 432 loss: 0.018265072256326675, ACC:1.0\n",
      "Training iteration 433 loss: 0.012675980105996132, ACC:0.984375\n",
      "Training iteration 434 loss: 0.031664010137319565, ACC:0.984375\n",
      "Training iteration 435 loss: 0.005128353368490934, ACC:1.0\n",
      "Training iteration 436 loss: 0.0019234745996072888, ACC:1.0\n",
      "Training iteration 437 loss: 0.001541498932056129, ACC:1.0\n",
      "Training iteration 438 loss: 0.0024459022097289562, ACC:1.0\n",
      "Training iteration 439 loss: 0.006157127674669027, ACC:1.0\n",
      "Training iteration 440 loss: 0.0002680869656614959, ACC:1.0\n",
      "Training iteration 441 loss: 0.014093834906816483, ACC:1.0\n",
      "Training iteration 442 loss: 0.0033125916961580515, ACC:1.0\n",
      "Training iteration 443 loss: 0.004622405394911766, ACC:1.0\n",
      "Training iteration 444 loss: 0.00037258656811900437, ACC:1.0\n",
      "Training iteration 445 loss: 0.07624271512031555, ACC:0.953125\n",
      "Training iteration 446 loss: 0.00224252394400537, ACC:1.0\n",
      "Training iteration 447 loss: 0.001967295538634062, ACC:1.0\n",
      "Training iteration 448 loss: 0.0007046975078992546, ACC:1.0\n",
      "Training iteration 449 loss: 0.0023684287443757057, ACC:1.0\n",
      "Training iteration 450 loss: 0.004415607545524836, ACC:1.0\n",
      "Validation iteration 451 loss: 0.0016915940213948488, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.00033080490538850427, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.0009609906119294465, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.0070091127417981625, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.0069440025836229324, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.0038582845591008663, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.04185701906681061, ACC: 0.984375\n",
      "Validation iteration 458 loss: 0.0002507460303604603, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.09845204651355743, ACC: 0.984375\n",
      "Validation iteration 460 loss: 0.0007129866280592978, ACC: 1.0\n",
      "Validation iteration 461 loss: 0.0005030123866163194, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.0035625456366688013, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.11371178179979324, ACC: 0.984375\n",
      "Validation iteration 464 loss: 0.05200452730059624, ACC: 0.984375\n",
      "Validation iteration 465 loss: 0.004134021699428558, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.011845963075757027, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.007597331888973713, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.005175179801881313, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.003869557287544012, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.0035501751117408276, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.000703419849742204, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.0016879253089427948, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.00020394798775669187, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.0580550879240036, ACC: 0.984375\n",
      "Validation iteration 475 loss: 0.0003520592290442437, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.010693500749766827, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.001086990931071341, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.0011721707414835691, ACC: 1.0\n",
      "Validation iteration 479 loss: 0.004989363718777895, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.0021102838218212128, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.00033378321677446365, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.003336394904181361, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.007362015545368195, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.012409019283950329, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.10244040191173553, ACC: 0.984375\n",
      "Validation iteration 486 loss: 0.0015689351130276918, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.007824182510375977, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.0020001146476715803, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.03258773311972618, ACC: 0.984375\n",
      "Validation iteration 490 loss: 0.001321994117461145, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.0030089933425188065, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.0008680150494910777, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.018915152177214622, ACC: 0.984375\n",
      "Validation iteration 494 loss: 0.07788437604904175, ACC: 0.984375\n",
      "Validation iteration 495 loss: 0.07132835686206818, ACC: 0.96875\n",
      "Validation iteration 496 loss: 0.0076465727761387825, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.0015018715057522058, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.0024612918496131897, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.006052641198039055, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.006066930014640093, ACC: 1.0\n",
      "-- Epoch 9 done -- Train loss: 0.01666911151221939, train ACC: 0.9953819444444445, val loss: 0.016319904182164464, val ACC: 0.99625\n",
      "<--- 13527.763278245926 seconds --->\n",
      "Training iteration 1 loss: 0.005247802473604679, ACC:1.0\n",
      "Training iteration 2 loss: 0.000571760640013963, ACC:1.0\n",
      "Training iteration 3 loss: 0.0011077612871304154, ACC:1.0\n",
      "Training iteration 4 loss: 0.0005469581228680909, ACC:1.0\n",
      "Training iteration 5 loss: 0.0006539683090522885, ACC:1.0\n",
      "Training iteration 6 loss: 0.0008209572406485677, ACC:1.0\n",
      "Training iteration 7 loss: 0.023202024400234222, ACC:0.984375\n",
      "Training iteration 8 loss: 0.0002648213703650981, ACC:1.0\n",
      "Training iteration 9 loss: 0.016562458127737045, ACC:1.0\n",
      "Training iteration 10 loss: 0.00037377720582298934, ACC:1.0\n",
      "Training iteration 11 loss: 0.004555059131234884, ACC:1.0\n",
      "Training iteration 12 loss: 0.07742179185152054, ACC:0.984375\n",
      "Training iteration 13 loss: 0.01122788991779089, ACC:0.984375\n",
      "Training iteration 14 loss: 0.0015462141018360853, ACC:1.0\n",
      "Training iteration 15 loss: 0.0101705864071846, ACC:1.0\n",
      "Training iteration 16 loss: 0.0012625794624909759, ACC:1.0\n",
      "Training iteration 17 loss: 0.0009090205421671271, ACC:1.0\n",
      "Training iteration 18 loss: 0.0026672508101910353, ACC:1.0\n",
      "Training iteration 19 loss: 0.0019254510989412665, ACC:1.0\n",
      "Training iteration 20 loss: 0.0023548908066004515, ACC:1.0\n",
      "Training iteration 21 loss: 0.008787968195974827, ACC:1.0\n",
      "Training iteration 22 loss: 0.0009786442387849092, ACC:1.0\n",
      "Training iteration 23 loss: 0.010209698230028152, ACC:1.0\n",
      "Training iteration 24 loss: 0.0005645639030262828, ACC:1.0\n",
      "Training iteration 25 loss: 0.010614410042762756, ACC:1.0\n",
      "Training iteration 26 loss: 0.005725902505218983, ACC:1.0\n",
      "Training iteration 27 loss: 0.003283068770542741, ACC:1.0\n",
      "Training iteration 28 loss: 0.010975774377584457, ACC:1.0\n",
      "Training iteration 29 loss: 0.003532223869115114, ACC:1.0\n",
      "Training iteration 30 loss: 0.016127770766615868, ACC:1.0\n",
      "Training iteration 31 loss: 0.0016253968933597207, ACC:1.0\n",
      "Training iteration 32 loss: 0.001493177842348814, ACC:1.0\n",
      "Training iteration 33 loss: 0.0004249204939696938, ACC:1.0\n",
      "Training iteration 34 loss: 0.0024642215576022863, ACC:1.0\n",
      "Training iteration 35 loss: 0.025741811841726303, ACC:0.984375\n",
      "Training iteration 36 loss: 0.001302093151025474, ACC:1.0\n",
      "Training iteration 37 loss: 0.0026918358635157347, ACC:1.0\n",
      "Training iteration 38 loss: 0.0003666746197268367, ACC:1.0\n",
      "Training iteration 39 loss: 0.0018103093607351184, ACC:1.0\n",
      "Training iteration 40 loss: 0.011007789522409439, ACC:1.0\n",
      "Training iteration 41 loss: 0.0014440399827435613, ACC:1.0\n",
      "Training iteration 42 loss: 0.03444835543632507, ACC:0.984375\n",
      "Training iteration 43 loss: 0.048724595457315445, ACC:0.984375\n",
      "Training iteration 44 loss: 0.0002482124255038798, ACC:1.0\n",
      "Training iteration 45 loss: 0.005642520729452372, ACC:1.0\n",
      "Training iteration 46 loss: 6.257866334635764e-05, ACC:1.0\n",
      "Training iteration 47 loss: 0.04513340815901756, ACC:0.984375\n",
      "Training iteration 48 loss: 0.001168169197626412, ACC:1.0\n",
      "Training iteration 49 loss: 0.00256779370829463, ACC:1.0\n",
      "Training iteration 50 loss: 0.011702618561685085, ACC:0.984375\n",
      "Training iteration 51 loss: 0.013269812799990177, ACC:0.984375\n",
      "Training iteration 52 loss: 0.03613106533885002, ACC:0.984375\n",
      "Training iteration 53 loss: 0.01195908896625042, ACC:0.984375\n",
      "Training iteration 54 loss: 0.0003930293023586273, ACC:1.0\n",
      "Training iteration 55 loss: 0.009585432708263397, ACC:1.0\n",
      "Training iteration 56 loss: 0.16143108904361725, ACC:0.96875\n",
      "Training iteration 57 loss: 0.0029073702171444893, ACC:1.0\n",
      "Training iteration 58 loss: 0.0045119342394173145, ACC:1.0\n",
      "Training iteration 59 loss: 0.02751387096941471, ACC:0.984375\n",
      "Training iteration 60 loss: 0.002567876596003771, ACC:1.0\n",
      "Training iteration 61 loss: 0.009710784070193768, ACC:1.0\n",
      "Training iteration 62 loss: 0.000866872607730329, ACC:1.0\n",
      "Training iteration 63 loss: 0.0009314122726209462, ACC:1.0\n",
      "Training iteration 64 loss: 0.0010874866275116801, ACC:1.0\n",
      "Training iteration 65 loss: 0.003807133063673973, ACC:1.0\n",
      "Training iteration 66 loss: 0.10265909135341644, ACC:0.96875\n",
      "Training iteration 67 loss: 0.00954048614948988, ACC:1.0\n",
      "Training iteration 68 loss: 0.001093339524231851, ACC:1.0\n",
      "Training iteration 69 loss: 0.07204219698905945, ACC:0.96875\n",
      "Training iteration 70 loss: 0.060657232999801636, ACC:0.984375\n",
      "Training iteration 71 loss: 0.05782264843583107, ACC:0.984375\n",
      "Training iteration 72 loss: 0.035149697214365005, ACC:1.0\n",
      "Training iteration 73 loss: 0.07538002729415894, ACC:0.96875\n",
      "Training iteration 74 loss: 0.035629741847515106, ACC:1.0\n",
      "Training iteration 75 loss: 0.010948185808956623, ACC:1.0\n",
      "Training iteration 76 loss: 0.0607072152197361, ACC:0.96875\n",
      "Training iteration 77 loss: 0.0015739320078864694, ACC:1.0\n",
      "Training iteration 78 loss: 0.01141598541289568, ACC:1.0\n",
      "Training iteration 79 loss: 0.002560888882726431, ACC:1.0\n",
      "Training iteration 80 loss: 0.006673421710729599, ACC:1.0\n",
      "Training iteration 81 loss: 0.011337238363921642, ACC:1.0\n",
      "Training iteration 82 loss: 0.03300277888774872, ACC:0.984375\n",
      "Training iteration 83 loss: 0.007987373508512974, ACC:1.0\n",
      "Training iteration 84 loss: 0.08970942348241806, ACC:0.96875\n",
      "Training iteration 85 loss: 0.0027064706664532423, ACC:1.0\n",
      "Training iteration 86 loss: 0.008517900481820107, ACC:1.0\n",
      "Training iteration 87 loss: 0.015541529282927513, ACC:1.0\n",
      "Training iteration 88 loss: 0.00426914868876338, ACC:1.0\n",
      "Training iteration 89 loss: 0.003502535168081522, ACC:1.0\n",
      "Training iteration 90 loss: 0.012919491156935692, ACC:1.0\n",
      "Training iteration 91 loss: 0.006347843445837498, ACC:1.0\n",
      "Training iteration 92 loss: 0.02387116849422455, ACC:0.984375\n",
      "Training iteration 93 loss: 0.002877722028642893, ACC:1.0\n",
      "Training iteration 94 loss: 0.0007831047987565398, ACC:1.0\n",
      "Training iteration 95 loss: 0.009785219095647335, ACC:1.0\n",
      "Training iteration 96 loss: 0.0025018109008669853, ACC:1.0\n",
      "Training iteration 97 loss: 0.038594476878643036, ACC:0.96875\n",
      "Training iteration 98 loss: 0.0002806017582770437, ACC:1.0\n",
      "Training iteration 99 loss: 0.019278274849057198, ACC:0.984375\n",
      "Training iteration 100 loss: 0.011582071892917156, ACC:1.0\n",
      "Training iteration 101 loss: 0.00023646836052648723, ACC:1.0\n",
      "Training iteration 102 loss: 0.024263957515358925, ACC:0.984375\n",
      "Training iteration 103 loss: 0.011375809088349342, ACC:0.984375\n",
      "Training iteration 104 loss: 0.00010769558139145374, ACC:1.0\n",
      "Training iteration 105 loss: 0.0016265022568404675, ACC:1.0\n",
      "Training iteration 106 loss: 0.004986023996025324, ACC:1.0\n",
      "Training iteration 107 loss: 0.00032008608104661107, ACC:1.0\n",
      "Training iteration 108 loss: 3.178484985255636e-05, ACC:1.0\n",
      "Training iteration 109 loss: 0.00033891163184307516, ACC:1.0\n",
      "Training iteration 110 loss: 0.00013157198554836214, ACC:1.0\n",
      "Training iteration 111 loss: 4.421065386850387e-05, ACC:1.0\n",
      "Training iteration 112 loss: 0.0001998657244257629, ACC:1.0\n",
      "Training iteration 113 loss: 0.08387897163629532, ACC:0.984375\n",
      "Training iteration 114 loss: 0.0004525571712292731, ACC:1.0\n",
      "Training iteration 115 loss: 0.00015374856593552977, ACC:1.0\n",
      "Training iteration 116 loss: 0.0009812591597437859, ACC:1.0\n",
      "Training iteration 117 loss: 0.004695672541856766, ACC:1.0\n",
      "Training iteration 118 loss: 0.04012828692793846, ACC:0.984375\n",
      "Training iteration 119 loss: 0.00014268257655203342, ACC:1.0\n",
      "Training iteration 120 loss: 0.003616673406213522, ACC:1.0\n",
      "Training iteration 121 loss: 0.17301522195339203, ACC:0.984375\n",
      "Training iteration 122 loss: 0.005236247554421425, ACC:1.0\n",
      "Training iteration 123 loss: 0.0006981377955526114, ACC:1.0\n",
      "Training iteration 124 loss: 0.0005772219155915082, ACC:1.0\n",
      "Training iteration 125 loss: 0.008393878117203712, ACC:1.0\n",
      "Training iteration 126 loss: 0.012115152552723885, ACC:1.0\n",
      "Training iteration 127 loss: 0.0012441908475011587, ACC:1.0\n",
      "Training iteration 128 loss: 0.0005871827015653253, ACC:1.0\n",
      "Training iteration 129 loss: 0.05146893486380577, ACC:0.984375\n",
      "Training iteration 130 loss: 0.000654107250738889, ACC:1.0\n",
      "Training iteration 131 loss: 0.0010805086931213737, ACC:1.0\n",
      "Training iteration 132 loss: 0.0014712850097566843, ACC:1.0\n",
      "Training iteration 133 loss: 0.0007971799350343645, ACC:1.0\n",
      "Training iteration 134 loss: 0.002329764189198613, ACC:1.0\n",
      "Training iteration 135 loss: 0.000773013976868242, ACC:1.0\n",
      "Training iteration 136 loss: 0.04615440592169762, ACC:0.96875\n",
      "Training iteration 137 loss: 0.0038949288427829742, ACC:1.0\n",
      "Training iteration 138 loss: 0.0016925501404330134, ACC:1.0\n",
      "Training iteration 139 loss: 0.02060071937739849, ACC:0.984375\n",
      "Training iteration 140 loss: 0.02021041139960289, ACC:0.984375\n",
      "Training iteration 141 loss: 0.006716436706483364, ACC:1.0\n",
      "Training iteration 142 loss: 0.0041432809084653854, ACC:1.0\n",
      "Training iteration 143 loss: 0.0014085706789046526, ACC:1.0\n",
      "Training iteration 144 loss: 0.011403677985072136, ACC:1.0\n",
      "Training iteration 145 loss: 0.10737581551074982, ACC:0.984375\n",
      "Training iteration 146 loss: 0.0007971368613652885, ACC:1.0\n",
      "Training iteration 147 loss: 0.00032966953585855663, ACC:1.0\n",
      "Training iteration 148 loss: 0.00805253442376852, ACC:1.0\n",
      "Training iteration 149 loss: 0.018208296969532967, ACC:0.984375\n",
      "Training iteration 150 loss: 0.01745585724711418, ACC:0.984375\n",
      "Training iteration 151 loss: 0.0025953631848096848, ACC:1.0\n",
      "Training iteration 152 loss: 0.04071247577667236, ACC:0.984375\n",
      "Training iteration 153 loss: 0.001784390420652926, ACC:1.0\n",
      "Training iteration 154 loss: 0.0007860978948883712, ACC:1.0\n",
      "Training iteration 155 loss: 0.03247632831335068, ACC:0.984375\n",
      "Training iteration 156 loss: 0.047300297766923904, ACC:0.96875\n",
      "Training iteration 157 loss: 0.0003602020151447505, ACC:1.0\n",
      "Training iteration 158 loss: 0.061952460557222366, ACC:0.984375\n",
      "Training iteration 159 loss: 0.02002527192234993, ACC:1.0\n",
      "Training iteration 160 loss: 0.00035518722143024206, ACC:1.0\n",
      "Training iteration 161 loss: 0.000933022063691169, ACC:1.0\n",
      "Training iteration 162 loss: 0.003224906511604786, ACC:1.0\n",
      "Training iteration 163 loss: 0.004522207658737898, ACC:1.0\n",
      "Training iteration 164 loss: 0.005321310367435217, ACC:1.0\n",
      "Training iteration 165 loss: 0.00968936923891306, ACC:1.0\n",
      "Training iteration 166 loss: 0.0023296521976590157, ACC:1.0\n",
      "Training iteration 167 loss: 0.0021262923255562782, ACC:1.0\n",
      "Training iteration 168 loss: 0.0004105913103558123, ACC:1.0\n",
      "Training iteration 169 loss: 0.00981217436492443, ACC:1.0\n",
      "Training iteration 170 loss: 0.02494220808148384, ACC:0.984375\n",
      "Training iteration 171 loss: 0.006604576483368874, ACC:1.0\n",
      "Training iteration 172 loss: 0.0009218626073561609, ACC:1.0\n",
      "Training iteration 173 loss: 0.0035699910949915648, ACC:1.0\n",
      "Training iteration 174 loss: 0.0011446854332461953, ACC:1.0\n",
      "Training iteration 175 loss: 0.0005450896569527686, ACC:1.0\n",
      "Training iteration 176 loss: 0.012687545269727707, ACC:1.0\n",
      "Training iteration 177 loss: 0.000447313126642257, ACC:1.0\n",
      "Training iteration 178 loss: 0.009368589147925377, ACC:1.0\n",
      "Training iteration 179 loss: 0.019803259521722794, ACC:0.984375\n",
      "Training iteration 180 loss: 0.002900306135416031, ACC:1.0\n",
      "Training iteration 181 loss: 0.004227902740240097, ACC:1.0\n",
      "Training iteration 182 loss: 0.00047966180136427283, ACC:1.0\n",
      "Training iteration 183 loss: 0.1083897054195404, ACC:0.984375\n",
      "Training iteration 184 loss: 0.003991760313510895, ACC:1.0\n",
      "Training iteration 185 loss: 0.007242002058774233, ACC:1.0\n",
      "Training iteration 186 loss: 0.0018041480798274279, ACC:1.0\n",
      "Training iteration 187 loss: 0.000712831795681268, ACC:1.0\n",
      "Training iteration 188 loss: 0.0293281190097332, ACC:0.984375\n",
      "Training iteration 189 loss: 0.0028276590164750814, ACC:1.0\n",
      "Training iteration 190 loss: 0.09617994725704193, ACC:0.96875\n",
      "Training iteration 191 loss: 0.0022981141228228807, ACC:1.0\n",
      "Training iteration 192 loss: 0.0039074313826859, ACC:1.0\n",
      "Training iteration 193 loss: 0.0017724501667544246, ACC:1.0\n",
      "Training iteration 194 loss: 0.009008604101836681, ACC:1.0\n",
      "Training iteration 195 loss: 0.0038882254157215357, ACC:1.0\n",
      "Training iteration 196 loss: 0.005290112923830748, ACC:1.0\n",
      "Training iteration 197 loss: 0.0013507134281098843, ACC:1.0\n",
      "Training iteration 198 loss: 0.01621037721633911, ACC:0.984375\n",
      "Training iteration 199 loss: 0.003807230619713664, ACC:1.0\n",
      "Training iteration 200 loss: 0.0054117306135594845, ACC:1.0\n",
      "Training iteration 201 loss: 0.03040025569498539, ACC:0.984375\n",
      "Training iteration 202 loss: 0.0018932117382064462, ACC:1.0\n",
      "Training iteration 203 loss: 0.0038041523657739162, ACC:1.0\n",
      "Training iteration 204 loss: 0.003588705090805888, ACC:1.0\n",
      "Training iteration 205 loss: 0.0004998091608285904, ACC:1.0\n",
      "Training iteration 206 loss: 0.008933362551033497, ACC:1.0\n",
      "Training iteration 207 loss: 0.003824731567874551, ACC:1.0\n",
      "Training iteration 208 loss: 0.0011387228732928634, ACC:1.0\n",
      "Training iteration 209 loss: 0.015578676015138626, ACC:0.984375\n",
      "Training iteration 210 loss: 0.023245571181178093, ACC:0.984375\n",
      "Training iteration 211 loss: 0.0012122374027967453, ACC:1.0\n",
      "Training iteration 212 loss: 0.0115740355104208, ACC:1.0\n",
      "Training iteration 213 loss: 0.0021308669820427895, ACC:1.0\n",
      "Training iteration 214 loss: 0.00205106008797884, ACC:1.0\n",
      "Training iteration 215 loss: 0.024172378703951836, ACC:0.984375\n",
      "Training iteration 216 loss: 0.019945412874221802, ACC:0.984375\n",
      "Training iteration 217 loss: 0.0011498427484184504, ACC:1.0\n",
      "Training iteration 218 loss: 0.0021588492672890425, ACC:1.0\n",
      "Training iteration 219 loss: 0.006400327198207378, ACC:1.0\n",
      "Training iteration 220 loss: 9.035698167281225e-05, ACC:1.0\n",
      "Training iteration 221 loss: 0.01659695990383625, ACC:1.0\n",
      "Training iteration 222 loss: 0.01877845823764801, ACC:0.984375\n",
      "Training iteration 223 loss: 0.0012953397817909718, ACC:1.0\n",
      "Training iteration 224 loss: 0.055706195533275604, ACC:0.984375\n",
      "Training iteration 225 loss: 0.00024515687255188823, ACC:1.0\n",
      "Training iteration 226 loss: 0.22770166397094727, ACC:0.984375\n",
      "Training iteration 227 loss: 0.0014758061151951551, ACC:1.0\n",
      "Training iteration 228 loss: 0.04153211787343025, ACC:0.984375\n",
      "Training iteration 229 loss: 0.08175863325595856, ACC:0.984375\n",
      "Training iteration 230 loss: 0.016419261693954468, ACC:0.984375\n",
      "Training iteration 231 loss: 0.14407560229301453, ACC:0.96875\n",
      "Training iteration 232 loss: 0.022956864908337593, ACC:0.984375\n",
      "Training iteration 233 loss: 0.0011441095266491175, ACC:1.0\n",
      "Training iteration 234 loss: 0.006727662403136492, ACC:1.0\n",
      "Training iteration 235 loss: 0.002897523809224367, ACC:1.0\n",
      "Training iteration 236 loss: 0.0026358335744589567, ACC:1.0\n",
      "Training iteration 237 loss: 0.006057940423488617, ACC:1.0\n",
      "Training iteration 238 loss: 0.03485524281859398, ACC:0.984375\n",
      "Training iteration 239 loss: 0.005723399110138416, ACC:1.0\n",
      "Training iteration 240 loss: 0.010835136286914349, ACC:1.0\n",
      "Training iteration 241 loss: 0.025578543543815613, ACC:1.0\n",
      "Training iteration 242 loss: 0.022678323090076447, ACC:1.0\n",
      "Training iteration 243 loss: 0.058941587805747986, ACC:0.984375\n",
      "Training iteration 244 loss: 0.01274032797664404, ACC:1.0\n",
      "Training iteration 245 loss: 0.004533637780696154, ACC:1.0\n",
      "Training iteration 246 loss: 0.002173404907807708, ACC:1.0\n",
      "Training iteration 247 loss: 0.0015153259737417102, ACC:1.0\n",
      "Training iteration 248 loss: 0.0013391281245276332, ACC:1.0\n",
      "Training iteration 249 loss: 0.009343788959085941, ACC:1.0\n",
      "Training iteration 250 loss: 0.0014929405879229307, ACC:1.0\n",
      "Training iteration 251 loss: 0.01258361991494894, ACC:1.0\n",
      "Training iteration 252 loss: 0.004606437403708696, ACC:1.0\n",
      "Training iteration 253 loss: 0.003428123891353607, ACC:1.0\n",
      "Training iteration 254 loss: 0.012684027664363384, ACC:1.0\n",
      "Training iteration 255 loss: 0.007131626829504967, ACC:1.0\n",
      "Training iteration 256 loss: 0.002519083907827735, ACC:1.0\n",
      "Training iteration 257 loss: 0.013692561537027359, ACC:0.984375\n",
      "Training iteration 258 loss: 0.0025047743692994118, ACC:1.0\n",
      "Training iteration 259 loss: 0.006509122438728809, ACC:1.0\n",
      "Training iteration 260 loss: 0.03763778880238533, ACC:0.984375\n",
      "Training iteration 261 loss: 0.001384661183692515, ACC:1.0\n",
      "Training iteration 262 loss: 0.0021537193097174168, ACC:1.0\n",
      "Training iteration 263 loss: 0.004377198405563831, ACC:1.0\n",
      "Training iteration 264 loss: 0.0012174635194242, ACC:1.0\n",
      "Training iteration 265 loss: 0.0007632498163729906, ACC:1.0\n",
      "Training iteration 266 loss: 0.00076099339639768, ACC:1.0\n",
      "Training iteration 267 loss: 0.006983097176998854, ACC:1.0\n",
      "Training iteration 268 loss: 0.006510345730930567, ACC:1.0\n",
      "Training iteration 269 loss: 0.006343080196529627, ACC:1.0\n",
      "Training iteration 270 loss: 0.00046330600162036717, ACC:1.0\n",
      "Training iteration 271 loss: 0.06320195645093918, ACC:0.96875\n",
      "Training iteration 272 loss: 0.0011433177860453725, ACC:1.0\n",
      "Training iteration 273 loss: 0.030235618352890015, ACC:0.96875\n",
      "Training iteration 274 loss: 0.0037833063397556543, ACC:1.0\n",
      "Training iteration 275 loss: 0.0017182566225528717, ACC:1.0\n",
      "Training iteration 276 loss: 0.013317081145942211, ACC:0.984375\n",
      "Training iteration 277 loss: 0.005757279694080353, ACC:1.0\n",
      "Training iteration 278 loss: 0.0032352395355701447, ACC:1.0\n",
      "Training iteration 279 loss: 0.0013023762730881572, ACC:1.0\n",
      "Training iteration 280 loss: 0.00045113437226973474, ACC:1.0\n",
      "Training iteration 281 loss: 0.0006444435566663742, ACC:1.0\n",
      "Training iteration 282 loss: 0.018651455640792847, ACC:0.984375\n",
      "Training iteration 283 loss: 0.019021814689040184, ACC:0.984375\n",
      "Training iteration 284 loss: 0.00043470715172588825, ACC:1.0\n",
      "Training iteration 285 loss: 0.003093454986810684, ACC:1.0\n",
      "Training iteration 286 loss: 0.024040665477514267, ACC:0.984375\n",
      "Training iteration 287 loss: 0.0017242820467799902, ACC:1.0\n",
      "Training iteration 288 loss: 0.0008645259658806026, ACC:1.0\n",
      "Training iteration 289 loss: 0.01069621555507183, ACC:1.0\n",
      "Training iteration 290 loss: 0.0027663707733154297, ACC:1.0\n",
      "Training iteration 291 loss: 0.0037759379483759403, ACC:1.0\n",
      "Training iteration 292 loss: 0.03793526813387871, ACC:0.984375\n",
      "Training iteration 293 loss: 0.006655717734247446, ACC:1.0\n",
      "Training iteration 294 loss: 0.004333017393946648, ACC:1.0\n",
      "Training iteration 295 loss: 0.0025301838759332895, ACC:1.0\n",
      "Training iteration 296 loss: 0.0015025956090539694, ACC:1.0\n",
      "Training iteration 297 loss: 0.005048041231930256, ACC:1.0\n",
      "Training iteration 298 loss: 0.014620019122958183, ACC:1.0\n",
      "Training iteration 299 loss: 0.003359725931659341, ACC:1.0\n",
      "Training iteration 300 loss: 0.0009731602622196078, ACC:1.0\n",
      "Training iteration 301 loss: 0.0010733881499618292, ACC:1.0\n",
      "Training iteration 302 loss: 0.0014954814687371254, ACC:1.0\n",
      "Training iteration 303 loss: 0.0009736852371133864, ACC:1.0\n",
      "Training iteration 304 loss: 0.0058748312294483185, ACC:1.0\n",
      "Training iteration 305 loss: 0.004861414898186922, ACC:1.0\n",
      "Training iteration 306 loss: 0.00211069593206048, ACC:1.0\n",
      "Training iteration 307 loss: 0.0010455360170453787, ACC:1.0\n",
      "Training iteration 308 loss: 0.001900150440633297, ACC:1.0\n",
      "Training iteration 309 loss: 0.0006866815383546054, ACC:1.0\n",
      "Training iteration 310 loss: 0.017851030454039574, ACC:0.984375\n",
      "Training iteration 311 loss: 0.0017100682016462088, ACC:1.0\n",
      "Training iteration 312 loss: 0.0012594761792570353, ACC:1.0\n",
      "Training iteration 313 loss: 0.0010812593391165137, ACC:1.0\n",
      "Training iteration 314 loss: 0.04955940321087837, ACC:0.984375\n",
      "Training iteration 315 loss: 0.0006933003896847367, ACC:1.0\n",
      "Training iteration 316 loss: 0.01741732843220234, ACC:0.984375\n",
      "Training iteration 317 loss: 0.000623277563136071, ACC:1.0\n",
      "Training iteration 318 loss: 0.006643479224294424, ACC:1.0\n",
      "Training iteration 319 loss: 0.03151562809944153, ACC:0.984375\n",
      "Training iteration 320 loss: 0.0003367028257343918, ACC:1.0\n",
      "Training iteration 321 loss: 0.00070333102485165, ACC:1.0\n",
      "Training iteration 322 loss: 0.0007400594186037779, ACC:1.0\n",
      "Training iteration 323 loss: 0.00459275534376502, ACC:1.0\n",
      "Training iteration 324 loss: 0.0006120751495473087, ACC:1.0\n",
      "Training iteration 325 loss: 0.005495730321854353, ACC:1.0\n",
      "Training iteration 326 loss: 0.00045617471914738417, ACC:1.0\n",
      "Training iteration 327 loss: 9.136758308159187e-05, ACC:1.0\n",
      "Training iteration 328 loss: 0.006789775565266609, ACC:1.0\n",
      "Training iteration 329 loss: 0.0008091771160252392, ACC:1.0\n",
      "Training iteration 330 loss: 0.0007162279798649251, ACC:1.0\n",
      "Training iteration 331 loss: 0.0028407960198819637, ACC:1.0\n",
      "Training iteration 332 loss: 0.017885155975818634, ACC:0.984375\n",
      "Training iteration 333 loss: 0.0720539465546608, ACC:0.984375\n",
      "Training iteration 334 loss: 0.002158235991373658, ACC:1.0\n",
      "Training iteration 335 loss: 0.0192580409348011, ACC:1.0\n",
      "Training iteration 336 loss: 0.014905726537108421, ACC:0.984375\n",
      "Training iteration 337 loss: 0.0024153587874025106, ACC:1.0\n",
      "Training iteration 338 loss: 0.0003219831851311028, ACC:1.0\n",
      "Training iteration 339 loss: 0.006070579867810011, ACC:1.0\n",
      "Training iteration 340 loss: 0.0017400288488715887, ACC:1.0\n",
      "Training iteration 341 loss: 0.0024552433751523495, ACC:1.0\n",
      "Training iteration 342 loss: 0.004202407319098711, ACC:1.0\n",
      "Training iteration 343 loss: 0.0006852812948636711, ACC:1.0\n",
      "Training iteration 344 loss: 0.0821983739733696, ACC:0.984375\n",
      "Training iteration 345 loss: 0.0031543816439807415, ACC:1.0\n",
      "Training iteration 346 loss: 0.004044021014124155, ACC:1.0\n",
      "Training iteration 347 loss: 0.0007426179945468903, ACC:1.0\n",
      "Training iteration 348 loss: 0.14921380579471588, ACC:0.984375\n",
      "Training iteration 349 loss: 0.002172125969082117, ACC:1.0\n",
      "Training iteration 350 loss: 0.02330910414457321, ACC:0.984375\n",
      "Training iteration 351 loss: 0.0030258551705628633, ACC:1.0\n",
      "Training iteration 352 loss: 0.005509302951395512, ACC:1.0\n",
      "Training iteration 353 loss: 0.0053565106354653835, ACC:1.0\n",
      "Training iteration 354 loss: 0.004430192522704601, ACC:1.0\n",
      "Training iteration 355 loss: 0.007064859848469496, ACC:1.0\n",
      "Training iteration 356 loss: 0.015237039886415005, ACC:0.984375\n",
      "Training iteration 357 loss: 0.0015257276827469468, ACC:1.0\n",
      "Training iteration 358 loss: 0.006769295781850815, ACC:1.0\n",
      "Training iteration 359 loss: 0.0023187357001006603, ACC:1.0\n",
      "Training iteration 360 loss: 0.0065816957503557205, ACC:1.0\n",
      "Training iteration 361 loss: 0.003063998417928815, ACC:1.0\n",
      "Training iteration 362 loss: 0.014477086253464222, ACC:1.0\n",
      "Training iteration 363 loss: 0.003968344535678625, ACC:1.0\n",
      "Training iteration 364 loss: 0.01129963994026184, ACC:1.0\n",
      "Training iteration 365 loss: 0.005305679980665445, ACC:1.0\n",
      "Training iteration 366 loss: 0.0069873277097940445, ACC:1.0\n",
      "Training iteration 367 loss: 0.006933125201612711, ACC:1.0\n",
      "Training iteration 368 loss: 0.004571347031742334, ACC:1.0\n",
      "Training iteration 369 loss: 0.001058744266629219, ACC:1.0\n",
      "Training iteration 370 loss: 0.006149775814265013, ACC:1.0\n",
      "Training iteration 371 loss: 0.0010698928963392973, ACC:1.0\n",
      "Training iteration 372 loss: 0.00790666975080967, ACC:1.0\n",
      "Training iteration 373 loss: 0.0006895606056787074, ACC:1.0\n",
      "Training iteration 374 loss: 0.0005056277150288224, ACC:1.0\n",
      "Training iteration 375 loss: 0.0004943099338561296, ACC:1.0\n",
      "Training iteration 376 loss: 0.0002075246156891808, ACC:1.0\n",
      "Training iteration 377 loss: 0.009725060313940048, ACC:1.0\n",
      "Training iteration 378 loss: 0.00021211923740338534, ACC:1.0\n",
      "Training iteration 379 loss: 0.15547437965869904, ACC:0.953125\n",
      "Training iteration 380 loss: 0.0004142744583077729, ACC:1.0\n",
      "Training iteration 381 loss: 0.00028405067860148847, ACC:1.0\n",
      "Training iteration 382 loss: 0.07562549412250519, ACC:0.96875\n",
      "Training iteration 383 loss: 0.005214686039835215, ACC:1.0\n",
      "Training iteration 384 loss: 0.0007397808949463069, ACC:1.0\n",
      "Training iteration 385 loss: 0.0101632634177804, ACC:1.0\n",
      "Training iteration 386 loss: 0.0006397424731403589, ACC:1.0\n",
      "Training iteration 387 loss: 0.0007506738766096532, ACC:1.0\n",
      "Training iteration 388 loss: 0.00203399988822639, ACC:1.0\n",
      "Training iteration 389 loss: 0.0036914509255439043, ACC:1.0\n",
      "Training iteration 390 loss: 0.0018825901206582785, ACC:1.0\n",
      "Training iteration 391 loss: 0.010252152569591999, ACC:1.0\n",
      "Training iteration 392 loss: 0.0035330953542143106, ACC:1.0\n",
      "Training iteration 393 loss: 0.0012972543481737375, ACC:1.0\n",
      "Training iteration 394 loss: 0.006216064095497131, ACC:1.0\n",
      "Training iteration 395 loss: 0.0016839952440932393, ACC:1.0\n",
      "Training iteration 396 loss: 0.01345027144998312, ACC:0.984375\n",
      "Training iteration 397 loss: 0.0997454822063446, ACC:0.984375\n",
      "Training iteration 398 loss: 0.012778867036104202, ACC:1.0\n",
      "Training iteration 399 loss: 0.008750008419156075, ACC:1.0\n",
      "Training iteration 400 loss: 0.002720882184803486, ACC:1.0\n",
      "Training iteration 401 loss: 0.0006942304316908121, ACC:1.0\n",
      "Training iteration 402 loss: 0.0006219695205800235, ACC:1.0\n",
      "Training iteration 403 loss: 0.0014361576177179813, ACC:1.0\n",
      "Training iteration 404 loss: 0.06751209497451782, ACC:0.984375\n",
      "Training iteration 405 loss: 0.016521457582712173, ACC:1.0\n",
      "Training iteration 406 loss: 0.006959890481084585, ACC:1.0\n",
      "Training iteration 407 loss: 0.005993622820824385, ACC:1.0\n",
      "Training iteration 408 loss: 0.015605886466801167, ACC:1.0\n",
      "Training iteration 409 loss: 0.001253769383765757, ACC:1.0\n",
      "Training iteration 410 loss: 0.006275568623095751, ACC:1.0\n",
      "Training iteration 411 loss: 0.005889073479920626, ACC:1.0\n",
      "Training iteration 412 loss: 0.002739808987826109, ACC:1.0\n",
      "Training iteration 413 loss: 0.1250831037759781, ACC:0.96875\n",
      "Training iteration 414 loss: 0.0028766070026904345, ACC:1.0\n",
      "Training iteration 415 loss: 0.0010874556610360742, ACC:1.0\n",
      "Training iteration 416 loss: 0.0010835331631824374, ACC:1.0\n",
      "Training iteration 417 loss: 0.0017723477212712169, ACC:1.0\n",
      "Training iteration 418 loss: 0.0007102558156475425, ACC:1.0\n",
      "Training iteration 419 loss: 0.01502852514386177, ACC:0.984375\n",
      "Training iteration 420 loss: 0.0013733915984630585, ACC:1.0\n",
      "Training iteration 421 loss: 0.0007325989427044988, ACC:1.0\n",
      "Training iteration 422 loss: 0.00520689832046628, ACC:1.0\n",
      "Training iteration 423 loss: 0.009266555309295654, ACC:1.0\n",
      "Training iteration 424 loss: 0.007035982795059681, ACC:1.0\n",
      "Training iteration 425 loss: 0.0006848212797194719, ACC:1.0\n",
      "Training iteration 426 loss: 0.018582506105303764, ACC:0.984375\n",
      "Training iteration 427 loss: 0.0006519788294099271, ACC:1.0\n",
      "Training iteration 428 loss: 0.004796325229108334, ACC:1.0\n",
      "Training iteration 429 loss: 0.003361606039106846, ACC:1.0\n",
      "Training iteration 430 loss: 0.0006132572889328003, ACC:1.0\n",
      "Training iteration 431 loss: 0.01441542711108923, ACC:0.984375\n",
      "Training iteration 432 loss: 0.0013391966931521893, ACC:1.0\n",
      "Training iteration 433 loss: 0.0023573809303343296, ACC:1.0\n",
      "Training iteration 434 loss: 0.003020478179678321, ACC:1.0\n",
      "Training iteration 435 loss: 0.0026091416366398335, ACC:1.0\n",
      "Training iteration 436 loss: 0.0001862897479441017, ACC:1.0\n",
      "Training iteration 437 loss: 0.11589746177196503, ACC:0.984375\n",
      "Training iteration 438 loss: 0.0017873224569484591, ACC:1.0\n",
      "Training iteration 439 loss: 0.0023428681306540966, ACC:1.0\n",
      "Training iteration 440 loss: 0.007376175839453936, ACC:1.0\n",
      "Training iteration 441 loss: 0.00032169464975595474, ACC:1.0\n",
      "Training iteration 442 loss: 0.04684395715594292, ACC:0.984375\n",
      "Training iteration 443 loss: 0.07222007215023041, ACC:0.984375\n",
      "Training iteration 444 loss: 0.028920922428369522, ACC:0.984375\n",
      "Training iteration 445 loss: 0.0017004332039505243, ACC:1.0\n",
      "Training iteration 446 loss: 0.0013094153255224228, ACC:1.0\n",
      "Training iteration 447 loss: 0.03245027735829353, ACC:0.984375\n",
      "Training iteration 448 loss: 0.021953189745545387, ACC:0.984375\n",
      "Training iteration 449 loss: 0.005346911959350109, ACC:1.0\n",
      "Training iteration 450 loss: 0.0028530515264719725, ACC:1.0\n",
      "Validation iteration 451 loss: 0.0035393955186009407, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.00048313321894966066, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.1194230318069458, ACC: 0.984375\n",
      "Validation iteration 454 loss: 0.002752353437244892, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.04416632279753685, ACC: 0.984375\n",
      "Validation iteration 456 loss: 0.0372096411883831, ACC: 0.984375\n",
      "Validation iteration 457 loss: 0.05328536778688431, ACC: 0.984375\n",
      "Validation iteration 458 loss: 0.0009885469917207956, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.0007957314956001937, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.002349063754081726, ACC: 1.0\n",
      "Validation iteration 461 loss: 0.00386570836417377, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.001096219290047884, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.0005000783130526543, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.06262630969285965, ACC: 0.984375\n",
      "Validation iteration 465 loss: 0.0022760331630706787, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.0037426757626235485, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.0008581573492847383, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.001034610322676599, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.0019362897146493196, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.023635195568203926, ACC: 0.984375\n",
      "Validation iteration 471 loss: 0.0024274871684610844, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.0009393945802003145, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.0012018943671137094, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.0007567124557681382, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.001745737623423338, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.0013913363218307495, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.006520292721688747, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.008627011440694332, ACC: 1.0\n",
      "Validation iteration 479 loss: 0.007790558505803347, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.06771763414144516, ACC: 0.984375\n",
      "Validation iteration 481 loss: 0.0004076798213645816, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.010203705169260502, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.01369584258645773, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.0009400035487487912, ACC: 1.0\n",
      "Validation iteration 485 loss: 0.010735150426626205, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.0006149326800368726, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.05333045497536659, ACC: 0.984375\n",
      "Validation iteration 488 loss: 0.02191619947552681, ACC: 0.984375\n",
      "Validation iteration 489 loss: 0.11672262102365494, ACC: 0.96875\n",
      "Validation iteration 490 loss: 0.002669071778655052, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.004447978921234608, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.0028052229899913073, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.001339852111414075, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.020604342222213745, ACC: 0.984375\n",
      "Validation iteration 495 loss: 0.003031026339158416, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.0013865145156159997, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.2454516738653183, ACC: 0.96875\n",
      "Validation iteration 498 loss: 0.001270621782168746, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.001017986098304391, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.11403723806142807, ACC: 0.96875\n",
      "-- Epoch 10 done -- Train loss: 0.013727168934427835, train ACC: 0.9960763888888889, val loss: 0.021846200865111312, val ACC: 0.995\n",
      "<--- 14198.679396629333 seconds --->\n"
     ]
    }
   ],
   "source": [
    "# for timing model training purposes\n",
    "start_time = time.time()\n",
    "\n",
    "# input variables\n",
    "epoch_num = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# train model\n",
    "model = OLeNet()\n",
    "cost_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# to store loss for training & validation set\n",
    "jw_train_epoch = []  ## to store loss for training set by epoch (average loss of iterations)\n",
    "jw_val_epoch = []    ## to store loss for validation set by epoch (average loss of iterations)\n",
    "\n",
    "# to store accuracy of training & validation set\n",
    "acc_train_epoch = []\n",
    "acc_val_epoch = []\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "\n",
    "    # track train & validation loss & accuracy by iteration for each epoch\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    test_counter = 1\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ''' include below IF statement if want to limit number of batches '''\n",
    "        if i+1 == 501:  # should have total of 500 batches after train & val sets\n",
    "            break\n",
    "        \n",
    "        # use 80% for training, 20% for testing, and 10% for validation of the 40k training samples\n",
    "        ## batch size=64 so 625 batches total: 450 train batches, 50 val batches, and 125 test batches\n",
    "\n",
    "        if i+1 > 450:  # validate model with validation set (10% of total train data)\n",
    "            inputs, labels = data\n",
    "            \n",
    "            logits, outputs = model(inputs)\n",
    "            cost = cost_fn(logits, labels)\n",
    "            \n",
    "            jw_val = float(cost)\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "            correct = pred == labels\n",
    "            acc = sum(np.array(correct))/len(np.array(correct))\n",
    "\n",
    "            acc_val = acc\n",
    "\n",
    "            val_loss.append(jw_val)\n",
    "            val_acc.append(acc_val)\n",
    "            \n",
    "            print(f'Validation iteration {i+1} loss: {jw_val}, ACC: {acc_val}')\n",
    "            \n",
    "        else:  # train model with training set\n",
    "\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()            # zero the parameter gradients\n",
    "            logits, outputs = model(inputs)  # forward\n",
    "            cost = cost_fn(logits, labels)   # input logits prior to softmax activation into cost function\n",
    "            cost.backward()                  # backward\n",
    "            optimizer.step()                 # optimize\n",
    "\n",
    "            jw_train = float(cost)\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred = torch.Tensor.argmax(outputs, dim=1)            # get labels of prediction with highest probability\n",
    "            correct = pred == labels                              # compare to actual labels and see which was predicted correctly\n",
    "\n",
    "            acc = sum(np.array(correct))/len(np.array(correct))   # calculate accuracy\n",
    "\n",
    "            acc_train = acc\n",
    "\n",
    "            train_loss.append(jw_train)\n",
    "            train_acc.append(acc_train)\n",
    "            \n",
    "            print(f'Training iteration {i+1} loss: {jw_train}, ACC:{acc_train}')\n",
    "\n",
    "    # to save time, epoch loss = the lowest loss, epoch acc = highest acc in training\n",
    "    epoch_jw = np.mean(np.array(train_loss))\n",
    "    epoch_acc = np.mean(np.array(train_acc))\n",
    "\n",
    "    jw_train_epoch.append(epoch_jw)\n",
    "    jw_val_epoch.append(np.mean(val_loss))\n",
    "    acc_train_epoch.append(epoch_acc)\n",
    "    acc_val_epoch.append(np.mean(val_acc))\n",
    "    \n",
    "    print(f'-- Epoch {epoch+1} done -- Train loss: {epoch_jw}, train ACC: {epoch_acc}, val loss: {np.mean(val_loss)}, val ACC: {np.mean(val_acc)}')\n",
    "    \n",
    "    print(\"<--- %s seconds --->\" % (time.time() - start_time))\n",
    "\n",
    "    # save model at every epoch\n",
    "    path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/OLeNet_model_saves/00_relu_lr0.001/olenet_lr0.001_cpu_epoch{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwdS7oqJweUo"
   },
   "source": [
    "Plot loss and accuracy over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1636943798189,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "hZPUGYuZYBLH",
    "outputId": "1cb471e3-feda-4623-8054-1d4071c4c746"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss over epoch\n",
    "plt.plot([i+1 for i in range(len(jw_train_epoch))],jw_train_epoch)\n",
    "plt.plot([i+1 for i in range(len(jw_val_epoch))],jw_val_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Loss with learning rate {learning_rate}')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1636943798415,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "j8coUlN-YBLI",
    "outputId": "f3ed256f-1009-431f-dd88-4e72210310a2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracy over epoch\n",
    "plt.plot([i+1 for i in range(len(acc_train_epoch))],acc_train_epoch)\n",
    "plt.plot([i+1 for i in range(len(acc_val_epoch))],acc_val_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Accuracy with learning rate {learning_rate}')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJQvqXWnYBLI"
   },
   "source": [
    "**Calculate test accuracy for model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2578309,
     "status": "ok",
     "timestamp": 1637018560170,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "EoO1l_KoBvaf",
    "outputId": "b3083383-81d4-4c45-e956-f8ad466baeb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLeNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=576, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      " Batch 1 testing ACC = 100.0%\n",
      "\n",
      " Batch 2 testing ACC = 100.0%\n",
      "\n",
      " Batch 3 testing ACC = 100.0%\n",
      "\n",
      " Batch 4 testing ACC = 100.0%\n",
      "\n",
      " Batch 5 testing ACC = 100.0%\n",
      "\n",
      " Batch 6 testing ACC = 100.0%\n",
      "\n",
      " Batch 7 testing ACC = 100.0%\n",
      "\n",
      " Batch 8 testing ACC = 100.0%\n",
      "\n",
      " Batch 9 testing ACC = 100.0%\n",
      "\n",
      " Batch 10 testing ACC = 100.0%\n",
      "\n",
      " Batch 11 testing ACC = 100.0%\n",
      "\n",
      " Batch 12 testing ACC = 100.0%\n",
      "\n",
      " Batch 13 testing ACC = 100.0%\n",
      "\n",
      " Batch 14 testing ACC = 100.0%\n",
      "\n",
      " Batch 15 testing ACC = 100.0%\n",
      "\n",
      " Batch 16 testing ACC = 100.0%\n",
      "\n",
      " Batch 17 testing ACC = 100.0%\n",
      "\n",
      " Batch 18 testing ACC = 100.0%\n",
      "\n",
      " Batch 19 testing ACC = 100.0%\n",
      "\n",
      " Batch 20 testing ACC = 100.0%\n",
      "\n",
      " Batch 21 testing ACC = 99.9%\n",
      "\n",
      " Batch 22 testing ACC = 99.9%\n",
      "\n",
      " Batch 23 testing ACC = 99.9%\n",
      "\n",
      " Batch 24 testing ACC = 99.9%\n",
      "\n",
      " Batch 25 testing ACC = 99.9%\n",
      "\n",
      " Batch 26 testing ACC = 99.9%\n",
      "\n",
      " Batch 27 testing ACC = 99.9%\n",
      "\n",
      " Batch 28 testing ACC = 99.9%\n",
      "\n",
      " Batch 29 testing ACC = 99.9%\n",
      "\n",
      " Batch 30 testing ACC = 99.9%\n",
      "\n",
      " Batch 31 testing ACC = 99.9%\n",
      "\n",
      " Batch 32 testing ACC = 100.0%\n",
      "\n",
      " Batch 33 testing ACC = 99.9%\n",
      "\n",
      " Batch 34 testing ACC = 99.9%\n",
      "\n",
      " Batch 35 testing ACC = 99.9%\n",
      "\n",
      " Batch 36 testing ACC = 99.9%\n",
      "\n",
      " Batch 37 testing ACC = 99.9%\n",
      "\n",
      " Batch 38 testing ACC = 99.9%\n",
      "\n",
      " Batch 39 testing ACC = 99.9%\n",
      "\n",
      " Batch 40 testing ACC = 99.9%\n",
      "\n",
      " Batch 41 testing ACC = 99.9%\n",
      "\n",
      " Batch 42 testing ACC = 99.9%\n",
      "\n",
      " Batch 43 testing ACC = 99.9%\n",
      "\n",
      " Batch 44 testing ACC = 99.9%\n",
      "\n",
      " Batch 45 testing ACC = 99.9%\n",
      "\n",
      " Batch 46 testing ACC = 99.9%\n",
      "\n",
      " Batch 47 testing ACC = 99.8%\n",
      "\n",
      " Batch 48 testing ACC = 99.8%\n",
      "\n",
      " Batch 49 testing ACC = 99.8%\n",
      "\n",
      " Batch 50 testing ACC = 99.8%\n",
      "\n",
      " Batch 51 testing ACC = 99.8%\n",
      "\n",
      " Batch 52 testing ACC = 99.8%\n",
      "\n",
      " Batch 53 testing ACC = 99.8%\n",
      "\n",
      " Batch 54 testing ACC = 99.8%\n",
      "\n",
      " Batch 55 testing ACC = 99.8%\n",
      "\n",
      " Batch 56 testing ACC = 99.8%\n",
      "\n",
      " Batch 57 testing ACC = 99.8%\n",
      "\n",
      " Batch 58 testing ACC = 99.8%\n",
      "\n",
      " Batch 59 testing ACC = 99.8%\n",
      "\n",
      " Batch 60 testing ACC = 99.8%\n",
      "\n",
      " Batch 61 testing ACC = 99.8%\n",
      "\n",
      " Batch 62 testing ACC = 99.8%\n",
      "\n",
      " Batch 63 testing ACC = 99.8%\n",
      "\n",
      " Batch 64 testing ACC = 99.8%\n",
      "\n",
      " Batch 65 testing ACC = 99.8%\n",
      "\n",
      " Batch 66 testing ACC = 99.8%\n",
      "\n",
      " Batch 67 testing ACC = 99.8%\n",
      "\n",
      " Batch 68 testing ACC = 99.8%\n",
      "\n",
      " Batch 69 testing ACC = 99.8%\n",
      "\n",
      " Batch 70 testing ACC = 99.8%\n",
      "\n",
      " Batch 71 testing ACC = 99.8%\n",
      "\n",
      " Batch 72 testing ACC = 99.8%\n",
      "\n",
      " Batch 73 testing ACC = 99.8%\n",
      "\n",
      " Batch 74 testing ACC = 99.8%\n",
      "\n",
      " Batch 75 testing ACC = 99.8%\n",
      "\n",
      " Batch 76 testing ACC = 99.8%\n",
      "\n",
      " Batch 77 testing ACC = 99.8%\n",
      "\n",
      " Batch 78 testing ACC = 99.8%\n",
      "\n",
      " Batch 79 testing ACC = 99.8%\n",
      "\n",
      " Batch 80 testing ACC = 99.8%\n",
      "\n",
      " Batch 81 testing ACC = 99.8%\n",
      "\n",
      " Batch 82 testing ACC = 99.8%\n",
      "\n",
      " Batch 83 testing ACC = 99.8%\n",
      "\n",
      " Batch 84 testing ACC = 99.8%\n",
      "\n",
      " Batch 85 testing ACC = 99.8%\n",
      "\n",
      " Batch 86 testing ACC = 99.8%\n",
      "\n",
      " Batch 87 testing ACC = 99.8%\n",
      "\n",
      " Batch 88 testing ACC = 99.8%\n",
      "\n",
      " Batch 89 testing ACC = 99.8%\n",
      "\n",
      " Batch 90 testing ACC = 99.8%\n",
      "\n",
      " Batch 91 testing ACC = 99.8%\n",
      "\n",
      " Batch 92 testing ACC = 99.8%\n",
      "\n",
      " Batch 93 testing ACC = 99.8%\n",
      "\n",
      " Batch 94 testing ACC = 99.8%\n",
      "\n",
      " Batch 95 testing ACC = 99.8%\n",
      "\n",
      " Batch 96 testing ACC = 99.8%\n",
      "\n",
      " Batch 97 testing ACC = 99.8%\n",
      "\n",
      " Batch 98 testing ACC = 99.8%\n",
      "\n",
      " Batch 99 testing ACC = 99.8%\n",
      "\n",
      " Batch 100 testing ACC = 99.8%\n",
      "\n",
      " Batch 101 testing ACC = 99.8%\n",
      "\n",
      " Batch 102 testing ACC = 99.8%\n",
      "\n",
      " Batch 103 testing ACC = 99.8%\n",
      "\n",
      " Batch 104 testing ACC = 99.8%\n",
      "\n",
      " Batch 105 testing ACC = 99.8%\n",
      "\n",
      " Batch 106 testing ACC = 99.8%\n",
      "\n",
      " Batch 107 testing ACC = 99.7%\n",
      "\n",
      " Batch 108 testing ACC = 99.7%\n",
      "\n",
      " Batch 109 testing ACC = 99.7%\n",
      "\n",
      " Batch 110 testing ACC = 99.7%\n",
      "\n",
      " Batch 111 testing ACC = 99.7%\n",
      "\n",
      " Batch 112 testing ACC = 99.7%\n",
      "\n",
      " Batch 113 testing ACC = 99.8%\n",
      "\n",
      " Batch 114 testing ACC = 99.8%\n",
      "\n",
      " Batch 115 testing ACC = 99.7%\n",
      "\n",
      " Batch 116 testing ACC = 99.7%\n",
      "\n",
      " Batch 117 testing ACC = 99.7%\n",
      "\n",
      " Batch 118 testing ACC = 99.7%\n",
      "\n",
      " Batch 119 testing ACC = 99.7%\n",
      "\n",
      " Batch 120 testing ACC = 99.7%\n",
      "\n",
      " Batch 121 testing ACC = 99.7%\n",
      "\n",
      " Batch 122 testing ACC = 99.7%\n",
      "\n",
      " Batch 123 testing ACC = 99.7%\n",
      "\n",
      " Batch 124 testing ACC = 99.7%\n",
      "\n",
      " Batch 125 testing ACC = 99.7%\n"
     ]
    }
   ],
   "source": [
    "# load saved model \n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/OLeNet_model_saves/00_relu_lr0.001/olenet_lr0.001_cpu_epoch{7}.pth\"\n",
    "\n",
    "model = OLeNet()\n",
    "model.load_state_dict(torch.load(path))\n",
    "print(model.eval())\n",
    "\n",
    "# or use model in current session\n",
    "# model = model\n",
    "\n",
    "# calculate test accuracy of model at a given epoch\n",
    "\n",
    "correct_counter = 0\n",
    "data_counter = 0\n",
    "\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    if i+1 > 125:  # should have a total of 125 batches for test set\n",
    "        break\n",
    "    else:\n",
    "        inputs, labels = data\n",
    "        logits, outputs = model(inputs)\n",
    "\n",
    "        # calculate overall accuracy across all classes\n",
    "        pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "        correct = pred == labels\n",
    "\n",
    "        correct_cnt = sum(np.array(correct))\n",
    "        data_cnt = len(np.array(correct))\n",
    "\n",
    "        correct_counter += correct_cnt \n",
    "        data_counter += data_cnt\n",
    "\n",
    "        print(f'\\n Batch {i+1} testing ACC = {(correct_counter/data_counter)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17325,
     "status": "ok",
     "timestamp": 1636947499956,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "a62LgfqSQkEI",
    "outputId": "a2a5f365-29e6-4a20-c157-5da861b9d0ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLeNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=576, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      " Testing accuracy = 87.8%\n"
     ]
    }
   ],
   "source": [
    "# test accuracy with 1000 augmented images to see if model can handle \"new\" inputs\n",
    "\n",
    "# load saved model \n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/OLeNet_model_saves/00_relu_lr0.001/olenet_lr0.001_cpu_epoch{7}.pth\"\n",
    "\n",
    "model = OLeNet()\n",
    "model.load_state_dict(torch.load(path))\n",
    "print(model.eval())\n",
    "\n",
    "# or use model in current session\n",
    "# model = model\n",
    "\n",
    "# calculate test accuracy of model at a given epoch\n",
    "\n",
    "testset_path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/test_sets/small_1000_set.pth\"\n",
    "inputs, labels = torch.load(testset_path)\n",
    "\n",
    "logits, outputs = model(inputs)\n",
    "\n",
    "# calculate overall accuracy across all classes\n",
    "pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "correct = pred == labels\n",
    "\n",
    "correct_cnt = sum(np.array(correct))\n",
    "data_cnt = len(np.array(correct))\n",
    "\n",
    "print(f'\\n Testing accuracy = {(correct_cnt/data_cnt)*100:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OLeNet_model_relu_noAugmentation_lr0.001.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
