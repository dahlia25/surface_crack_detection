{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLEh-LthYBKv"
   },
   "source": [
    "# Surface Crack Images - LeNet5 model\n",
    "This notebook contains the code training the LeNet5 model (without augmented data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEb3XLxjYBK4"
   },
   "source": [
    "**Load packages/modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1636948315462,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "H-oXyKDyYEED",
    "outputId": "c36fdab4-fc0b-4348-cf1d-26a40ec5be8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27620,
     "status": "ok",
     "timestamp": 1636948345189,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "9AKkpfhtYBK5",
    "outputId": "62c9abfb-c56c-458e-9773-8fa47c00233d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.10.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print(f'Torch version: {torch .__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4159,
     "status": "ok",
     "timestamp": 1636948349328,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "i_OfDGEmYBK8",
    "outputId": "f9cf43e8-012a-4090-a407-28b79d49399c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "%pip install torchsummary\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysKmpc9cYBK8"
   },
   "source": [
    "**Load data into tensors first**<br>\n",
    "Then concatenate the tensors with original and augmented data into one for ease of model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6107,
     "status": "ok",
     "timestamp": 1636948355414,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "syPC_CMNYBK_"
   },
   "outputs": [],
   "source": [
    "# load data into tensors first\n",
    "data_dir = '/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/archive/original'\n",
    "# aug_data_dir = '/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/archive/augmented'\n",
    "batch_size = 64 \n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "data = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(data, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True, \n",
    "                                         pin_memory=True)\n",
    "\n",
    "# aug_dataloader = torch.utils.data.DataLoader(aug_data,\n",
    "#                                              batch_size=batch_size*9,  # multiply by 9 since augmented x9 images for each original\n",
    "#                                              shuffle=True,\n",
    "#                                              pin_memory=True)\n",
    "\n",
    "# images, labels = next(iter(dataloader))\n",
    "# aug_images, aug_labels = next(iter(aug_dataloader))\n",
    "\n",
    "# print(f'Original images shape: {images.shape}')\n",
    "# print(f'Augmented images shape: {aug_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9C4r55Yj-Pf"
   },
   "outputs": [],
   "source": [
    "# generate and save test set\n",
    "test_counter = 1\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    if i+1 > 500:\n",
    "        if test_counter == 1:\n",
    "            test_inputs, test_labels = data \n",
    "            test_counter += 1\n",
    "        else:\n",
    "            new_inputs, new_labels = data\n",
    "            test_inputs = torch.concat((test_inputs, new_inputs), 0)\n",
    "            test_labels = torch.concat((test_labels, new_labels), 0)\n",
    "            test_counter += 1\n",
    "\n",
    "        # print(f'Batch {i+1} for test set complete.')\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# print(f'Saving test inputs {test_inputs.shape}, test labels {test_labels.shape}')\n",
    "\n",
    "# save test set at the end of training\n",
    "testset = [test_inputs, test_labels]\n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_testset.pth\"\n",
    "torch.save(testset, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZaeGsiNYBLC"
   },
   "source": [
    "# Construct model\n",
    "**OLeNet model architecture:** <br>\n",
    " Conv2D - 50 x 50 x 32 <br>\n",
    " Conv2D - 50 x 50 x 32 <br>\n",
    " MaxPool2D - 25 x 25 x 32 <br>\n",
    " Drop - 25 x 25 x 32 <br>\n",
    " \n",
    " Conv2D - 50 x 50 x 64 <br>\n",
    " Conv2D - 50 x 50 x 64 <br>\n",
    " MaxPool2D - 12 x 12 x 64 <br>\n",
    " Drop - 12 x 12 x 64 <br>\n",
    "\n",
    " Flatten (9216) <br>\n",
    " Dense (256) <br>\n",
    " Drop (256) <br>\n",
    " Dense (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1636948395213,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "-0gG2OoGYBLD"
   },
   "outputs": [],
   "source": [
    "class OLeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, padding=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 5, padding=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5, padding=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 5, padding=2, stride=2)\n",
    "\n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(64*3*3, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)   # have two classes (has crack/no crack)\n",
    "        \n",
    "        # softmax layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)  # Max pool 2x2\n",
    "\n",
    "        # dropout layer\n",
    "        self.drop = nn.Dropout(p=0.5) # drop out with probability of 0.5\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = F.selu(self.conv1(x)) # layer 1\n",
    "        x = F.selu(self.conv2(x)) # layer 2\n",
    "        x = self.pool(x)          # pool layer\n",
    "        x = self.drop(x)          # dropout layer\n",
    "        \n",
    "        x = F.selu(self.conv3(x)) # layer 5\n",
    "        x = F.selu(self.conv4(x)) # layer 6\n",
    "        x = self.pool(x)          # pool layer\n",
    "        x = self.drop(x)          # dropout layer\n",
    "        \n",
    "        x = torch.flatten(x, 1)   # flatten layer\n",
    "        \n",
    "        x = F.selu(self.fc1(x))   # layer 7\n",
    "        x = self.drop(x)          # dropout layer\n",
    "        logit = self.fc2(x)       # layer \n",
    "        \n",
    "        output = self.softmax(logit) # output layer\n",
    "\n",
    "        return logit, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 666,
     "status": "ok",
     "timestamp": 1636929964615,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "0gbcU53BYBLE",
    "outputId": "e4138070-5a6f-4057-f030-180569a07cdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 114, 114]           2,432\n",
      "            Conv2d-2           [-1, 32, 57, 57]          25,632\n",
      "         MaxPool2d-3           [-1, 32, 28, 28]               0\n",
      "           Dropout-4           [-1, 32, 28, 28]               0\n",
      "            Conv2d-5           [-1, 64, 14, 14]          51,264\n",
      "            Conv2d-6             [-1, 64, 7, 7]         102,464\n",
      "         MaxPool2d-7             [-1, 64, 3, 3]               0\n",
      "           Dropout-8             [-1, 64, 3, 3]               0\n",
      "            Linear-9                  [-1, 256]         147,712\n",
      "          Dropout-10                  [-1, 256]               0\n",
      "           Linear-11                    [-1, 2]             514\n",
      "          Softmax-12                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 330,018\n",
      "Trainable params: 330,018\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 4.48\n",
      "Params size (MB): 1.26\n",
      "Estimated Total Size (MB): 6.33\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print model summary\n",
    "\n",
    "model = OLeNet()\n",
    "\n",
    "channels = 3\n",
    "H = 227\n",
    "W = 227\n",
    "\n",
    "summary(model, (channels, H, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp89_6KnjowJ"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17924289,
     "status": "ok",
     "timestamp": 1636947920580,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "0acPeyXXsDdk",
    "outputId": "66beefcb-53df-4946-852e-3c3581317bf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Training iteration 21 loss: 0.6568495035171509, ACC:0.65625\n",
      "Training iteration 22 loss: 0.7577661275863647, ACC:0.5\n",
      "Training iteration 23 loss: 0.6856878995895386, ACC:0.5625\n",
      "Training iteration 24 loss: 0.6502881050109863, ACC:0.625\n",
      "Training iteration 25 loss: 0.7039462924003601, ACC:0.46875\n",
      "Training iteration 26 loss: 0.5978245139122009, ACC:0.6875\n",
      "Training iteration 27 loss: 0.627285361289978, ACC:0.703125\n",
      "Training iteration 28 loss: 0.7126026153564453, ACC:0.484375\n",
      "Training iteration 29 loss: 0.6197784543037415, ACC:0.671875\n",
      "Training iteration 30 loss: 0.604407787322998, ACC:0.765625\n",
      "Training iteration 31 loss: 0.5338004231452942, ACC:0.78125\n",
      "Training iteration 32 loss: 0.5263457298278809, ACC:0.734375\n",
      "Training iteration 33 loss: 0.5258396863937378, ACC:0.75\n",
      "Training iteration 34 loss: 0.517855703830719, ACC:0.734375\n",
      "Training iteration 35 loss: 0.4777992069721222, ACC:0.78125\n",
      "Training iteration 36 loss: 0.4889291524887085, ACC:0.75\n",
      "Training iteration 37 loss: 0.6369882225990295, ACC:0.671875\n",
      "Training iteration 38 loss: 0.6980392932891846, ACC:0.65625\n",
      "Training iteration 39 loss: 0.4404972791671753, ACC:0.796875\n",
      "Training iteration 40 loss: 0.6614425182342529, ACC:0.515625\n",
      "Training iteration 41 loss: 0.48866137862205505, ACC:0.78125\n",
      "Training iteration 42 loss: 0.4416460394859314, ACC:0.84375\n",
      "Training iteration 43 loss: 0.4225877821445465, ACC:0.796875\n",
      "Training iteration 44 loss: 0.32673367857933044, ACC:0.90625\n",
      "Training iteration 45 loss: 0.5279943346977234, ACC:0.734375\n",
      "Training iteration 46 loss: 0.5227890610694885, ACC:0.734375\n",
      "Training iteration 47 loss: 0.3643028736114502, ACC:0.84375\n",
      "Training iteration 48 loss: 0.37720802426338196, ACC:0.890625\n",
      "Training iteration 49 loss: 0.4238768219947815, ACC:0.796875\n",
      "Training iteration 50 loss: 0.30172523856163025, ACC:0.921875\n",
      "Training iteration 51 loss: 0.2204977124929428, ACC:0.921875\n",
      "Training iteration 52 loss: 0.41714808344841003, ACC:0.765625\n",
      "Training iteration 53 loss: 0.3277345299720764, ACC:0.859375\n",
      "Training iteration 54 loss: 0.16586115956306458, ACC:0.9375\n",
      "Training iteration 55 loss: 0.3163750171661377, ACC:0.859375\n",
      "Training iteration 56 loss: 0.3122917413711548, ACC:0.875\n",
      "Training iteration 57 loss: 0.24936512112617493, ACC:0.921875\n",
      "Training iteration 58 loss: 0.3001691699028015, ACC:0.890625\n",
      "Training iteration 59 loss: 0.2496768832206726, ACC:0.921875\n",
      "Training iteration 60 loss: 0.32946905493736267, ACC:0.84375\n",
      "Training iteration 61 loss: 0.2766132056713104, ACC:0.921875\n",
      "Training iteration 62 loss: 0.15221671760082245, ACC:0.921875\n",
      "Training iteration 63 loss: 0.1592012643814087, ACC:0.9375\n",
      "Training iteration 64 loss: 0.17879566550254822, ACC:0.921875\n",
      "Training iteration 65 loss: 0.24262121319770813, ACC:0.875\n",
      "Training iteration 66 loss: 0.39749398827552795, ACC:0.859375\n",
      "Training iteration 67 loss: 0.26811373233795166, ACC:0.859375\n",
      "Training iteration 68 loss: 0.1791829913854599, ACC:0.953125\n",
      "Training iteration 69 loss: 0.15665173530578613, ACC:0.921875\n",
      "Training iteration 70 loss: 0.09910222887992859, ACC:0.953125\n",
      "Training iteration 71 loss: 0.10744299739599228, ACC:0.96875\n",
      "Training iteration 72 loss: 0.13019338250160217, ACC:0.953125\n",
      "Training iteration 73 loss: 0.16445131599903107, ACC:0.921875\n",
      "Training iteration 74 loss: 0.07849612832069397, ACC:0.984375\n",
      "Training iteration 75 loss: 0.29907816648483276, ACC:0.90625\n",
      "Training iteration 76 loss: 0.19767731428146362, ACC:0.90625\n",
      "Training iteration 77 loss: 0.16285976767539978, ACC:0.96875\n",
      "Training iteration 78 loss: 0.08388299494981766, ACC:0.984375\n",
      "Training iteration 79 loss: 0.06189989671111107, ACC:0.984375\n",
      "Training iteration 80 loss: 0.11932384222745895, ACC:0.984375\n",
      "Training iteration 81 loss: 0.2834034562110901, ACC:0.890625\n",
      "Training iteration 82 loss: 0.22140461206436157, ACC:0.921875\n",
      "Training iteration 83 loss: 0.14604038000106812, ACC:0.9375\n",
      "Training iteration 84 loss: 0.12345224618911743, ACC:0.953125\n",
      "Training iteration 85 loss: 0.21425829827785492, ACC:0.90625\n",
      "Training iteration 86 loss: 0.20158620178699493, ACC:0.9375\n",
      "Training iteration 87 loss: 0.11457640677690506, ACC:0.96875\n",
      "Training iteration 88 loss: 0.1161230280995369, ACC:0.953125\n",
      "Training iteration 89 loss: 0.11745982617139816, ACC:0.96875\n",
      "Training iteration 90 loss: 0.23126362264156342, ACC:0.953125\n",
      "Training iteration 91 loss: 0.26695290207862854, ACC:0.875\n",
      "Training iteration 92 loss: 0.05807632952928543, ACC:0.96875\n",
      "Training iteration 93 loss: 0.1135299801826477, ACC:0.953125\n",
      "Training iteration 94 loss: 0.283550888299942, ACC:0.921875\n",
      "Training iteration 95 loss: 0.3463130593299866, ACC:0.859375\n",
      "Training iteration 96 loss: 0.23083698749542236, ACC:0.9375\n",
      "Training iteration 97 loss: 0.22444511950016022, ACC:0.90625\n",
      "Training iteration 98 loss: 0.0452328696846962, ACC:0.96875\n",
      "Training iteration 99 loss: 0.12281326949596405, ACC:0.96875\n",
      "Training iteration 100 loss: 0.08436406403779984, ACC:0.984375\n",
      "Training iteration 101 loss: 0.17806889116764069, ACC:0.9375\n",
      "Training iteration 102 loss: 0.2137290984392166, ACC:0.90625\n",
      "Training iteration 103 loss: 0.1896389126777649, ACC:0.953125\n",
      "Training iteration 104 loss: 0.1610473245382309, ACC:0.96875\n",
      "Training iteration 105 loss: 0.11537706851959229, ACC:0.953125\n",
      "Training iteration 106 loss: 0.17246681451797485, ACC:0.953125\n",
      "Training iteration 107 loss: 0.06266054511070251, ACC:0.984375\n",
      "Training iteration 108 loss: 0.08941280096769333, ACC:0.96875\n",
      "Training iteration 109 loss: 0.08792761713266373, ACC:0.984375\n",
      "Training iteration 110 loss: 0.1289106011390686, ACC:0.953125\n",
      "Training iteration 111 loss: 0.14953799545764923, ACC:0.90625\n",
      "Training iteration 112 loss: 0.10265723615884781, ACC:0.984375\n",
      "Training iteration 113 loss: 0.07446211576461792, ACC:0.96875\n",
      "Training iteration 114 loss: 0.07612435519695282, ACC:0.984375\n",
      "Training iteration 115 loss: 0.08652558922767639, ACC:0.984375\n",
      "Training iteration 116 loss: 0.021865831688046455, ACC:1.0\n",
      "Training iteration 117 loss: 0.10618102550506592, ACC:0.953125\n",
      "Training iteration 118 loss: 0.11255808919668198, ACC:0.953125\n",
      "Training iteration 119 loss: 0.0556236170232296, ACC:0.953125\n",
      "Training iteration 120 loss: 0.03795517235994339, ACC:1.0\n",
      "Training iteration 121 loss: 0.11654549092054367, ACC:0.9375\n",
      "Training iteration 122 loss: 0.09523047506809235, ACC:0.96875\n",
      "Training iteration 123 loss: 0.07711029797792435, ACC:0.953125\n",
      "Training iteration 124 loss: 0.033896129578351974, ACC:1.0\n",
      "Training iteration 125 loss: 0.1499147266149521, ACC:0.9375\n",
      "Training iteration 126 loss: 0.05017724260687828, ACC:0.984375\n",
      "Training iteration 127 loss: 0.13794873654842377, ACC:0.953125\n",
      "Training iteration 128 loss: 0.06706579774618149, ACC:0.984375\n",
      "Training iteration 129 loss: 0.1271417737007141, ACC:0.96875\n",
      "Training iteration 130 loss: 0.02431495301425457, ACC:1.0\n",
      "Training iteration 131 loss: 0.054118577390909195, ACC:0.96875\n",
      "Training iteration 132 loss: 0.020867403596639633, ACC:0.984375\n",
      "Training iteration 133 loss: 0.07048264890909195, ACC:0.96875\n",
      "Training iteration 134 loss: 0.036247510462999344, ACC:0.984375\n",
      "Training iteration 135 loss: 0.09979541599750519, ACC:0.96875\n",
      "Training iteration 136 loss: 0.1881384402513504, ACC:0.953125\n",
      "Training iteration 137 loss: 0.09120264649391174, ACC:0.96875\n",
      "Training iteration 138 loss: 0.11110586673021317, ACC:0.953125\n",
      "Training iteration 139 loss: 0.05722542479634285, ACC:0.96875\n",
      "Training iteration 140 loss: 0.07968010008335114, ACC:0.984375\n",
      "Training iteration 141 loss: 0.016356367617845535, ACC:1.0\n",
      "Training iteration 142 loss: 0.22087503969669342, ACC:0.921875\n",
      "Training iteration 143 loss: 0.028690509498119354, ACC:0.984375\n",
      "Training iteration 144 loss: 0.03976184502243996, ACC:0.984375\n",
      "Training iteration 145 loss: 0.006415797863155603, ACC:1.0\n",
      "Training iteration 146 loss: 0.14327357709407806, ACC:0.96875\n",
      "Training iteration 147 loss: 0.08201494067907333, ACC:0.96875\n",
      "Training iteration 148 loss: 0.00716825108975172, ACC:1.0\n",
      "Training iteration 149 loss: 0.014822365716099739, ACC:1.0\n",
      "Training iteration 150 loss: 0.057284917682409286, ACC:0.96875\n",
      "Training iteration 151 loss: 0.09219742566347122, ACC:0.96875\n",
      "Training iteration 152 loss: 0.13307632505893707, ACC:0.953125\n",
      "Training iteration 153 loss: 0.04368187487125397, ACC:0.984375\n",
      "Training iteration 154 loss: 0.13289807736873627, ACC:0.984375\n",
      "Training iteration 155 loss: 0.043970976024866104, ACC:1.0\n",
      "Training iteration 156 loss: 0.08054087311029434, ACC:0.96875\n",
      "Training iteration 157 loss: 0.10331976413726807, ACC:0.9375\n",
      "Training iteration 158 loss: 0.015177829191088676, ACC:1.0\n",
      "Training iteration 159 loss: 0.10155361145734787, ACC:0.96875\n",
      "Training iteration 160 loss: 0.007939482107758522, ACC:1.0\n",
      "Training iteration 161 loss: 0.01612020470201969, ACC:1.0\n",
      "Training iteration 162 loss: 0.018520744517445564, ACC:1.0\n",
      "Training iteration 163 loss: 0.07206510007381439, ACC:0.984375\n",
      "Training iteration 164 loss: 0.14351442456245422, ACC:0.96875\n",
      "Training iteration 165 loss: 0.040260497480630875, ACC:0.984375\n",
      "Training iteration 166 loss: 0.07837306708097458, ACC:0.953125\n",
      "Training iteration 167 loss: 0.05287199467420578, ACC:0.984375\n",
      "Training iteration 168 loss: 0.12333973497152328, ACC:0.96875\n",
      "Training iteration 169 loss: 0.02967526577413082, ACC:0.984375\n",
      "Training iteration 170 loss: 0.03519422560930252, ACC:0.96875\n",
      "Training iteration 171 loss: 0.01166665367782116, ACC:1.0\n",
      "Training iteration 172 loss: 0.12999404966831207, ACC:0.953125\n",
      "Training iteration 173 loss: 0.0633249431848526, ACC:0.984375\n",
      "Training iteration 174 loss: 0.023390445858240128, ACC:1.0\n",
      "Training iteration 175 loss: 0.11819867044687271, ACC:0.953125\n",
      "Training iteration 176 loss: 0.040406811982393265, ACC:0.984375\n",
      "Training iteration 177 loss: 0.026466121897101402, ACC:0.984375\n",
      "Training iteration 178 loss: 0.016462381929159164, ACC:1.0\n",
      "Training iteration 179 loss: 0.10233817249536514, ACC:0.96875\n",
      "Training iteration 180 loss: 0.0230551790446043, ACC:0.984375\n",
      "Training iteration 181 loss: 0.0604676827788353, ACC:0.953125\n",
      "Training iteration 182 loss: 0.00523614976555109, ACC:1.0\n",
      "Training iteration 183 loss: 0.005588809959590435, ACC:1.0\n",
      "Training iteration 184 loss: 0.011327669024467468, ACC:1.0\n",
      "Training iteration 185 loss: 0.0013065692037343979, ACC:1.0\n",
      "Training iteration 186 loss: 0.007849203422665596, ACC:1.0\n",
      "Training iteration 187 loss: 0.03987520933151245, ACC:0.984375\n",
      "Training iteration 188 loss: 0.1484844982624054, ACC:0.9375\n",
      "Training iteration 189 loss: 0.008925974369049072, ACC:1.0\n",
      "Training iteration 190 loss: 0.0620364211499691, ACC:0.984375\n",
      "Training iteration 191 loss: 0.034757114946842194, ACC:0.984375\n",
      "Training iteration 192 loss: 0.03836863860487938, ACC:0.984375\n",
      "Training iteration 193 loss: 0.06970986723899841, ACC:0.984375\n",
      "Training iteration 194 loss: 0.06909765303134918, ACC:0.984375\n",
      "Training iteration 195 loss: 0.018367767333984375, ACC:1.0\n",
      "Training iteration 196 loss: 0.025659454986453056, ACC:0.984375\n",
      "Training iteration 197 loss: 0.05808543786406517, ACC:0.984375\n",
      "Training iteration 198 loss: 0.05601983517408371, ACC:0.984375\n",
      "Training iteration 199 loss: 0.05190463364124298, ACC:0.984375\n",
      "Training iteration 200 loss: 0.026693575084209442, ACC:1.0\n",
      "Training iteration 201 loss: 0.04349585995078087, ACC:0.96875\n",
      "Training iteration 202 loss: 0.051328472793102264, ACC:0.984375\n",
      "Training iteration 203 loss: 0.06344987452030182, ACC:0.96875\n",
      "Training iteration 204 loss: 0.02498811110854149, ACC:0.984375\n",
      "Training iteration 205 loss: 0.0381990522146225, ACC:0.984375\n",
      "Training iteration 206 loss: 0.13792279362678528, ACC:0.953125\n",
      "Training iteration 207 loss: 0.17633110284805298, ACC:0.96875\n",
      "Training iteration 208 loss: 0.00349161634221673, ACC:1.0\n",
      "Training iteration 209 loss: 0.014155928045511246, ACC:1.0\n",
      "Training iteration 210 loss: 0.05376861244440079, ACC:0.96875\n",
      "Training iteration 211 loss: 0.10207127779722214, ACC:0.984375\n",
      "Training iteration 212 loss: 0.027719832956790924, ACC:1.0\n",
      "Training iteration 213 loss: 0.09865501523017883, ACC:0.96875\n",
      "Training iteration 214 loss: 0.007880646735429764, ACC:1.0\n",
      "Training iteration 215 loss: 0.057159047573804855, ACC:0.984375\n",
      "Training iteration 216 loss: 0.028501901775598526, ACC:0.984375\n",
      "Training iteration 217 loss: 0.10531176626682281, ACC:0.984375\n",
      "Training iteration 218 loss: 0.015417986549437046, ACC:1.0\n",
      "Training iteration 219 loss: 0.035117726773023605, ACC:1.0\n",
      "Training iteration 220 loss: 0.0028127001132816076, ACC:1.0\n",
      "Training iteration 221 loss: 0.09510520100593567, ACC:0.96875\n",
      "Training iteration 222 loss: 0.21140865981578827, ACC:0.96875\n",
      "Training iteration 223 loss: 0.02823338285088539, ACC:0.984375\n",
      "Training iteration 224 loss: 0.015170862898230553, ACC:1.0\n",
      "Training iteration 225 loss: 0.09874602407217026, ACC:0.96875\n",
      "Training iteration 226 loss: 0.013812760822474957, ACC:1.0\n",
      "Training iteration 227 loss: 0.09247687458992004, ACC:0.953125\n",
      "Training iteration 228 loss: 0.03271814435720444, ACC:0.984375\n",
      "Training iteration 229 loss: 0.043810099363327026, ACC:0.984375\n",
      "Training iteration 230 loss: 0.0736706554889679, ACC:0.953125\n",
      "Training iteration 231 loss: 0.08330060541629791, ACC:0.96875\n",
      "Training iteration 232 loss: 0.05950288474559784, ACC:0.953125\n",
      "Training iteration 233 loss: 0.08126833289861679, ACC:0.984375\n",
      "Training iteration 234 loss: 0.23350684344768524, ACC:0.953125\n",
      "Training iteration 235 loss: 0.03482317179441452, ACC:0.984375\n",
      "Training iteration 236 loss: 0.04556814953684807, ACC:0.984375\n",
      "Training iteration 237 loss: 0.032282691448926926, ACC:0.984375\n",
      "Training iteration 238 loss: 0.028812410309910774, ACC:1.0\n",
      "Training iteration 239 loss: 0.09102755784988403, ACC:0.984375\n",
      "Training iteration 240 loss: 0.06598736345767975, ACC:0.953125\n",
      "Training iteration 241 loss: 0.0910377949476242, ACC:0.96875\n",
      "Training iteration 242 loss: 0.006801626645028591, ACC:1.0\n",
      "Training iteration 243 loss: 0.032722108066082, ACC:0.96875\n",
      "Training iteration 244 loss: 0.07219084352254868, ACC:0.96875\n",
      "Training iteration 245 loss: 0.004258130211383104, ACC:1.0\n",
      "Training iteration 246 loss: 0.085850290954113, ACC:0.96875\n",
      "Training iteration 247 loss: 0.04033840447664261, ACC:0.984375\n",
      "Training iteration 248 loss: 0.043123554438352585, ACC:0.984375\n",
      "Training iteration 249 loss: 0.01087958924472332, ACC:1.0\n",
      "Training iteration 250 loss: 0.0058276052586734295, ACC:1.0\n",
      "Training iteration 251 loss: 0.1072312742471695, ACC:0.96875\n",
      "Training iteration 252 loss: 0.07197622209787369, ACC:0.96875\n",
      "Training iteration 253 loss: 0.060296207666397095, ACC:0.953125\n",
      "Training iteration 254 loss: 0.013727888464927673, ACC:1.0\n",
      "Training iteration 255 loss: 0.015053139068186283, ACC:1.0\n",
      "Training iteration 256 loss: 0.03294098377227783, ACC:0.984375\n",
      "Training iteration 257 loss: 0.0206840168684721, ACC:1.0\n",
      "Training iteration 258 loss: 0.023211147636175156, ACC:1.0\n",
      "Training iteration 259 loss: 0.10903716832399368, ACC:0.984375\n",
      "Training iteration 260 loss: 0.0006995819276198745, ACC:1.0\n",
      "Training iteration 261 loss: 0.01830899715423584, ACC:1.0\n",
      "Training iteration 262 loss: 0.007768969517201185, ACC:1.0\n",
      "Training iteration 263 loss: 0.00766550051048398, ACC:1.0\n",
      "Training iteration 264 loss: 0.025835394859313965, ACC:0.984375\n",
      "Training iteration 265 loss: 0.0057738590985536575, ACC:1.0\n",
      "Training iteration 266 loss: 0.02631714567542076, ACC:0.984375\n",
      "Training iteration 267 loss: 0.08995263278484344, ACC:0.96875\n",
      "Training iteration 268 loss: 0.0032213979866355658, ACC:1.0\n",
      "Training iteration 269 loss: 0.003861617296934128, ACC:1.0\n",
      "Training iteration 270 loss: 0.02212672121822834, ACC:1.0\n",
      "Training iteration 271 loss: 0.052583541721105576, ACC:0.984375\n",
      "Training iteration 272 loss: 0.021150145679712296, ACC:1.0\n",
      "Training iteration 273 loss: 0.0351850651204586, ACC:1.0\n",
      "Training iteration 274 loss: 0.0524330697953701, ACC:0.984375\n",
      "Training iteration 275 loss: 0.11798948794603348, ACC:0.96875\n",
      "Training iteration 276 loss: 0.010449087247252464, ACC:1.0\n",
      "Training iteration 277 loss: 0.05101840943098068, ACC:0.984375\n",
      "Training iteration 278 loss: 0.05856449156999588, ACC:0.984375\n",
      "Training iteration 279 loss: 0.15452247858047485, ACC:0.9375\n",
      "Training iteration 280 loss: 0.03876420482993126, ACC:0.984375\n",
      "Training iteration 281 loss: 0.0006508975056931376, ACC:1.0\n",
      "Training iteration 282 loss: 0.03592966869473457, ACC:0.96875\n",
      "Training iteration 283 loss: 0.02625112608075142, ACC:0.984375\n",
      "Training iteration 284 loss: 0.03413091227412224, ACC:0.984375\n",
      "Training iteration 285 loss: 0.08898361772298813, ACC:0.953125\n",
      "Training iteration 286 loss: 0.24215063452720642, ACC:0.953125\n",
      "Training iteration 287 loss: 0.0580580048263073, ACC:0.96875\n",
      "Training iteration 288 loss: 0.029120992869138718, ACC:0.984375\n",
      "Training iteration 289 loss: 0.03246635943651199, ACC:0.984375\n",
      "Training iteration 290 loss: 0.020779838785529137, ACC:1.0\n",
      "Training iteration 291 loss: 0.14136004447937012, ACC:0.953125\n",
      "Training iteration 292 loss: 0.013195357285439968, ACC:1.0\n",
      "Training iteration 293 loss: 0.039867252111434937, ACC:0.984375\n",
      "Training iteration 294 loss: 0.060408953577280045, ACC:0.984375\n",
      "Training iteration 295 loss: 0.01728753000497818, ACC:1.0\n",
      "Training iteration 296 loss: 0.1777321696281433, ACC:0.921875\n",
      "Training iteration 297 loss: 0.03138747066259384, ACC:0.984375\n",
      "Training iteration 298 loss: 0.12730221450328827, ACC:0.953125\n",
      "Training iteration 299 loss: 0.06369373202323914, ACC:0.984375\n",
      "Training iteration 300 loss: 0.09299296885728836, ACC:0.96875\n",
      "Training iteration 301 loss: 0.08630439639091492, ACC:0.96875\n",
      "Training iteration 302 loss: 0.08224885910749435, ACC:0.96875\n",
      "Training iteration 303 loss: 0.0007654610089957714, ACC:1.0\n",
      "Training iteration 304 loss: 0.019524093717336655, ACC:1.0\n",
      "Training iteration 305 loss: 0.1200714260339737, ACC:0.953125\n",
      "Training iteration 306 loss: 0.23752669990062714, ACC:0.953125\n",
      "Training iteration 307 loss: 0.07473160326480865, ACC:0.984375\n",
      "Training iteration 308 loss: 0.019426338374614716, ACC:1.0\n",
      "Training iteration 309 loss: 0.11283278465270996, ACC:0.9375\n",
      "Training iteration 310 loss: 0.052112385630607605, ACC:0.984375\n",
      "Training iteration 311 loss: 0.0163015965372324, ACC:1.0\n",
      "Training iteration 312 loss: 0.011027979664504528, ACC:1.0\n",
      "Training iteration 313 loss: 0.05143994837999344, ACC:0.984375\n",
      "Training iteration 314 loss: 0.029646843671798706, ACC:0.984375\n",
      "Training iteration 315 loss: 0.029505770653486252, ACC:0.984375\n",
      "Training iteration 316 loss: 0.044026829302310944, ACC:0.96875\n",
      "Training iteration 317 loss: 0.005748934578150511, ACC:1.0\n",
      "Training iteration 318 loss: 0.08604124188423157, ACC:0.96875\n",
      "Training iteration 319 loss: 0.15000656247138977, ACC:0.96875\n",
      "Training iteration 320 loss: 0.024380715563893318, ACC:0.984375\n",
      "Training iteration 321 loss: 0.03724731504917145, ACC:0.984375\n",
      "Training iteration 322 loss: 0.07655523717403412, ACC:0.984375\n",
      "Training iteration 323 loss: 0.006345993373543024, ACC:1.0\n",
      "Training iteration 324 loss: 0.07927826046943665, ACC:0.984375\n",
      "Training iteration 325 loss: 0.0350872166454792, ACC:0.984375\n",
      "Training iteration 326 loss: 0.047688357532024384, ACC:0.984375\n",
      "Training iteration 327 loss: 0.005271494388580322, ACC:1.0\n",
      "Training iteration 328 loss: 0.25898849964141846, ACC:0.96875\n",
      "Training iteration 329 loss: 0.011540236882865429, ACC:1.0\n",
      "Training iteration 330 loss: 0.06485795229673386, ACC:0.984375\n",
      "Training iteration 331 loss: 0.319248229265213, ACC:0.953125\n",
      "Training iteration 332 loss: 0.015471694059669971, ACC:1.0\n",
      "Training iteration 333 loss: 0.027684610337018967, ACC:0.984375\n",
      "Training iteration 334 loss: 0.0059552183374762535, ACC:1.0\n",
      "Training iteration 335 loss: 0.20548301935195923, ACC:0.921875\n",
      "Training iteration 336 loss: 0.0744532123208046, ACC:0.96875\n",
      "Training iteration 337 loss: 0.012613348662853241, ACC:1.0\n",
      "Training iteration 338 loss: 0.07233411073684692, ACC:0.984375\n",
      "Training iteration 339 loss: 0.014577433466911316, ACC:1.0\n",
      "Training iteration 340 loss: 0.041215162724256516, ACC:0.984375\n",
      "Training iteration 341 loss: 0.08516530692577362, ACC:0.953125\n",
      "Training iteration 342 loss: 0.05486975982785225, ACC:0.96875\n",
      "Training iteration 343 loss: 0.16223570704460144, ACC:0.953125\n",
      "Training iteration 344 loss: 0.0027245988603681326, ACC:1.0\n",
      "Training iteration 345 loss: 0.034725818783044815, ACC:0.984375\n",
      "Training iteration 346 loss: 0.06386306136846542, ACC:0.984375\n",
      "Training iteration 347 loss: 0.03159448131918907, ACC:0.984375\n",
      "Training iteration 348 loss: 0.012095414102077484, ACC:1.0\n",
      "Training iteration 349 loss: 0.025678297504782677, ACC:0.984375\n",
      "Training iteration 350 loss: 0.06529717892408371, ACC:0.984375\n",
      "Training iteration 351 loss: 0.011044335551559925, ACC:1.0\n",
      "Training iteration 352 loss: 0.02468443661928177, ACC:1.0\n",
      "Training iteration 353 loss: 0.07811503112316132, ACC:0.953125\n",
      "Training iteration 354 loss: 0.05096084997057915, ACC:0.984375\n",
      "Training iteration 355 loss: 0.030069706961512566, ACC:1.0\n",
      "Training iteration 356 loss: 0.06400639563798904, ACC:0.953125\n",
      "Training iteration 357 loss: 0.04079980403184891, ACC:0.984375\n",
      "Training iteration 358 loss: 0.05963059514760971, ACC:0.953125\n",
      "Training iteration 359 loss: 0.009483189322054386, ACC:1.0\n",
      "Training iteration 360 loss: 0.1092805564403534, ACC:0.96875\n",
      "Training iteration 361 loss: 0.13108499348163605, ACC:0.96875\n",
      "Training iteration 362 loss: 0.02113453671336174, ACC:1.0\n",
      "Training iteration 363 loss: 0.021023252978920937, ACC:0.984375\n",
      "Training iteration 364 loss: 0.027469435706734657, ACC:0.984375\n",
      "Training iteration 365 loss: 0.029844943434000015, ACC:0.984375\n",
      "Training iteration 366 loss: 0.11419247835874557, ACC:0.984375\n",
      "Training iteration 367 loss: 0.006902647204697132, ACC:1.0\n",
      "Training iteration 368 loss: 0.01951782964169979, ACC:1.0\n",
      "Training iteration 369 loss: 0.023769648745656013, ACC:0.984375\n",
      "Training iteration 370 loss: 0.06109242886304855, ACC:0.984375\n",
      "Training iteration 371 loss: 0.13976900279521942, ACC:0.921875\n",
      "Training iteration 372 loss: 0.009115304797887802, ACC:1.0\n",
      "Training iteration 373 loss: 0.061832986772060394, ACC:0.96875\n",
      "Training iteration 374 loss: 0.07988864183425903, ACC:0.953125\n",
      "Training iteration 375 loss: 0.05458752438426018, ACC:0.984375\n",
      "Training iteration 376 loss: 0.029741302132606506, ACC:1.0\n",
      "Training iteration 377 loss: 0.12329661101102829, ACC:0.96875\n",
      "Training iteration 378 loss: 0.03206738829612732, ACC:0.984375\n",
      "Training iteration 379 loss: 0.001699418411590159, ACC:1.0\n",
      "Training iteration 380 loss: 0.05653078481554985, ACC:0.953125\n",
      "Training iteration 381 loss: 0.019480455666780472, ACC:1.0\n",
      "Training iteration 382 loss: 0.2116028517484665, ACC:0.953125\n",
      "Training iteration 383 loss: 0.005405886098742485, ACC:1.0\n",
      "Training iteration 384 loss: 0.10067788511514664, ACC:0.96875\n",
      "Training iteration 385 loss: 0.12385990470647812, ACC:0.96875\n",
      "Training iteration 386 loss: 0.02971670776605606, ACC:0.984375\n",
      "Training iteration 387 loss: 0.0019242127891629934, ACC:1.0\n",
      "Training iteration 388 loss: 0.1030985563993454, ACC:0.953125\n",
      "Training iteration 389 loss: 0.05649738013744354, ACC:0.984375\n",
      "Training iteration 390 loss: 0.08571216464042664, ACC:0.96875\n",
      "Training iteration 391 loss: 0.08420870453119278, ACC:0.96875\n",
      "Training iteration 392 loss: 0.004781889263540506, ACC:1.0\n",
      "Training iteration 393 loss: 0.03986329212784767, ACC:0.96875\n",
      "Training iteration 394 loss: 0.1351674199104309, ACC:0.96875\n",
      "Training iteration 395 loss: 0.1146470382809639, ACC:0.96875\n",
      "Training iteration 396 loss: 0.1001071035861969, ACC:0.96875\n",
      "Training iteration 397 loss: 0.010243412107229233, ACC:1.0\n",
      "Training iteration 398 loss: 0.04460551589727402, ACC:0.984375\n",
      "Training iteration 399 loss: 0.14182913303375244, ACC:0.9375\n",
      "Training iteration 400 loss: 0.016182973980903625, ACC:1.0\n",
      "Training iteration 401 loss: 0.008171801455318928, ACC:1.0\n",
      "Training iteration 402 loss: 0.15391585230827332, ACC:0.953125\n",
      "Training iteration 403 loss: 0.022840168327093124, ACC:1.0\n",
      "Training iteration 404 loss: 0.23239296674728394, ACC:0.9375\n",
      "Training iteration 405 loss: 0.060624439269304276, ACC:0.984375\n",
      "Training iteration 406 loss: 0.024991458281874657, ACC:0.984375\n",
      "Training iteration 407 loss: 0.07466425001621246, ACC:0.984375\n",
      "Training iteration 408 loss: 0.13492150604724884, ACC:0.953125\n",
      "Training iteration 409 loss: 0.05480913072824478, ACC:0.96875\n",
      "Training iteration 410 loss: 0.044162001460790634, ACC:0.984375\n",
      "Training iteration 411 loss: 0.062495071440935135, ACC:0.96875\n",
      "Training iteration 412 loss: 0.06336063146591187, ACC:0.984375\n",
      "Training iteration 413 loss: 0.03911426290869713, ACC:1.0\n",
      "Training iteration 414 loss: 0.018208492547273636, ACC:1.0\n",
      "Training iteration 415 loss: 0.1671251803636551, ACC:0.953125\n",
      "Training iteration 416 loss: 0.017817217856645584, ACC:0.984375\n",
      "Training iteration 417 loss: 0.09174059331417084, ACC:0.953125\n",
      "Training iteration 418 loss: 0.13735836744308472, ACC:0.984375\n",
      "Training iteration 419 loss: 0.10107406973838806, ACC:0.984375\n",
      "Training iteration 420 loss: 0.02523457445204258, ACC:0.984375\n",
      "Training iteration 421 loss: 0.02652529440820217, ACC:0.984375\n",
      "Training iteration 422 loss: 0.0041780779138207436, ACC:1.0\n",
      "Training iteration 423 loss: 0.01574341394007206, ACC:0.984375\n",
      "Training iteration 424 loss: 0.07436126470565796, ACC:0.984375\n",
      "Training iteration 425 loss: 0.0528697669506073, ACC:0.96875\n",
      "Training iteration 426 loss: 0.018287787213921547, ACC:1.0\n",
      "Training iteration 427 loss: 0.025427404791116714, ACC:1.0\n",
      "Training iteration 428 loss: 0.012354684993624687, ACC:1.0\n",
      "Training iteration 429 loss: 0.04177230969071388, ACC:0.984375\n",
      "Training iteration 430 loss: 0.041303858160972595, ACC:0.96875\n",
      "Training iteration 431 loss: 0.04609134420752525, ACC:0.96875\n",
      "Training iteration 432 loss: 0.013848712667822838, ACC:1.0\n",
      "Training iteration 433 loss: 0.0483587384223938, ACC:0.96875\n",
      "Training iteration 434 loss: 0.2199644148349762, ACC:0.9375\n",
      "Training iteration 435 loss: 0.024392886087298393, ACC:0.984375\n",
      "Training iteration 436 loss: 0.14716266095638275, ACC:0.921875\n",
      "Training iteration 437 loss: 0.039619650691747665, ACC:0.984375\n",
      "Training iteration 438 loss: 0.003606194630265236, ACC:1.0\n",
      "Training iteration 439 loss: 0.11340554803609848, ACC:0.953125\n",
      "Training iteration 440 loss: 0.003236407646909356, ACC:1.0\n",
      "Training iteration 441 loss: 0.07673385739326477, ACC:0.96875\n",
      "Training iteration 442 loss: 0.030357593670487404, ACC:0.984375\n",
      "Training iteration 443 loss: 0.021199021488428116, ACC:0.984375\n",
      "Training iteration 444 loss: 0.04048178717494011, ACC:0.984375\n",
      "Training iteration 445 loss: 0.006070225965231657, ACC:1.0\n",
      "Training iteration 446 loss: 0.013361159712076187, ACC:1.0\n",
      "Training iteration 447 loss: 0.005126940086483955, ACC:1.0\n",
      "Training iteration 448 loss: 0.0008211094536818564, ACC:1.0\n",
      "Training iteration 449 loss: 0.02803844027221203, ACC:0.984375\n",
      "Training iteration 450 loss: 0.02463974431157112, ACC:0.984375\n",
      "Validation iteration 451 loss: 0.0741049274802208, ACC: 0.984375\n",
      "Validation iteration 452 loss: 0.08352766931056976, ACC: 0.984375\n",
      "Validation iteration 453 loss: 0.022079331800341606, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.002220315858721733, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.012768425978720188, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.07603654265403748, ACC: 0.96875\n",
      "Validation iteration 457 loss: 0.05174509808421135, ACC: 0.984375\n",
      "Validation iteration 458 loss: 0.1420852392911911, ACC: 0.953125\n",
      "Validation iteration 459 loss: 0.1132407933473587, ACC: 0.984375\n",
      "Validation iteration 460 loss: 0.10154598206281662, ACC: 0.96875\n",
      "Validation iteration 461 loss: 0.023320091888308525, ACC: 0.984375\n",
      "Validation iteration 462 loss: 0.06895589828491211, ACC: 0.953125\n",
      "Validation iteration 463 loss: 0.06564744561910629, ACC: 0.96875\n",
      "Validation iteration 464 loss: 0.003652273677289486, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.04683605954051018, ACC: 0.984375\n",
      "Validation iteration 466 loss: 0.03223428130149841, ACC: 0.984375\n",
      "Validation iteration 467 loss: 0.12073151767253876, ACC: 0.984375\n",
      "Validation iteration 468 loss: 0.00796324759721756, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.013943800702691078, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.016555657610297203, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.08831354975700378, ACC: 0.953125\n",
      "Validation iteration 472 loss: 0.07568155974149704, ACC: 0.96875\n",
      "Validation iteration 473 loss: 0.006528862286359072, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.04848973825573921, ACC: 0.984375\n",
      "Validation iteration 475 loss: 0.05339872092008591, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.06117404252290726, ACC: 0.96875\n",
      "Validation iteration 477 loss: 0.06519217044115067, ACC: 0.953125\n",
      "Validation iteration 478 loss: 0.06460331380367279, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.059728097170591354, ACC: 0.96875\n",
      "Validation iteration 480 loss: 0.009244725108146667, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.19217373430728912, ACC: 0.921875\n",
      "Validation iteration 482 loss: 0.013444396667182446, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.03598851338028908, ACC: 0.96875\n",
      "Validation iteration 484 loss: 0.09864278137683868, ACC: 0.96875\n",
      "Validation iteration 485 loss: 0.06179950013756752, ACC: 0.96875\n",
      "Validation iteration 486 loss: 0.031685616821050644, ACC: 0.96875\n",
      "Validation iteration 487 loss: 0.0022930584382265806, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.0008818790665827692, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.060632508248090744, ACC: 0.953125\n",
      "Validation iteration 490 loss: 0.008018429391086102, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.016795935109257698, ACC: 0.984375\n",
      "Validation iteration 492 loss: 0.039406128227710724, ACC: 0.96875\n",
      "Validation iteration 493 loss: 0.0718865692615509, ACC: 0.96875\n",
      "Validation iteration 494 loss: 0.13527585566043854, ACC: 0.96875\n",
      "Validation iteration 495 loss: 0.02933366782963276, ACC: 0.96875\n",
      "Validation iteration 496 loss: 0.008283179253339767, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.08008667081594467, ACC: 0.96875\n",
      "Validation iteration 498 loss: 0.06113579124212265, ACC: 0.984375\n",
      "Validation iteration 499 loss: 0.00792434997856617, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.12937721610069275, ACC: 0.953125\n",
      "-- Epoch 1 done -- Train loss: 0.13975871748226282, train ACC: 0.9350347222222222, val loss: 0.05393230322166346, val ACC: 0.979375\n",
      "<--- 6967.63683795929 seconds --->\n",
      "Training iteration 1 loss: 0.019273364916443825, ACC:1.0\n",
      "Training iteration 2 loss: 0.003540279809385538, ACC:1.0\n",
      "Training iteration 3 loss: 0.009306663647294044, ACC:1.0\n",
      "Training iteration 4 loss: 0.011377688497304916, ACC:1.0\n",
      "Training iteration 5 loss: 0.02947135455906391, ACC:0.984375\n",
      "Training iteration 6 loss: 0.024577228352427483, ACC:0.984375\n",
      "Training iteration 7 loss: 0.03732734173536301, ACC:0.96875\n",
      "Training iteration 8 loss: 0.12090358883142471, ACC:0.96875\n",
      "Training iteration 9 loss: 0.07919616997241974, ACC:0.96875\n",
      "Training iteration 10 loss: 0.11415809392929077, ACC:0.953125\n",
      "Training iteration 11 loss: 0.012888944707810879, ACC:1.0\n",
      "Training iteration 12 loss: 0.0022692896891385317, ACC:1.0\n",
      "Training iteration 13 loss: 0.0621957927942276, ACC:0.96875\n",
      "Training iteration 14 loss: 0.016098756343126297, ACC:0.984375\n",
      "Training iteration 15 loss: 0.04354353994131088, ACC:0.984375\n",
      "Training iteration 16 loss: 0.01259221788495779, ACC:1.0\n",
      "Training iteration 17 loss: 0.05209120362997055, ACC:0.984375\n",
      "Training iteration 18 loss: 0.04831082373857498, ACC:0.984375\n",
      "Training iteration 19 loss: 0.06903247535228729, ACC:0.96875\n",
      "Training iteration 20 loss: 0.033094920217990875, ACC:0.96875\n",
      "Training iteration 21 loss: 0.011492419987916946, ACC:1.0\n",
      "Training iteration 22 loss: 0.00763422017917037, ACC:1.0\n",
      "Training iteration 23 loss: 0.0028444433119148016, ACC:1.0\n",
      "Training iteration 24 loss: 0.11586516350507736, ACC:0.96875\n",
      "Training iteration 25 loss: 0.009197634644806385, ACC:1.0\n",
      "Training iteration 26 loss: 0.0517442524433136, ACC:0.984375\n",
      "Training iteration 27 loss: 0.0441318042576313, ACC:0.984375\n",
      "Training iteration 28 loss: 0.020148998126387596, ACC:1.0\n",
      "Training iteration 29 loss: 0.02054131217300892, ACC:0.984375\n",
      "Training iteration 30 loss: 0.04536590725183487, ACC:0.984375\n",
      "Training iteration 31 loss: 0.00757605442777276, ACC:1.0\n",
      "Training iteration 32 loss: 0.008494109846651554, ACC:1.0\n",
      "Training iteration 33 loss: 0.014641373418271542, ACC:0.984375\n",
      "Training iteration 34 loss: 0.11253590881824493, ACC:0.96875\n",
      "Training iteration 35 loss: 0.008739957585930824, ACC:1.0\n",
      "Training iteration 36 loss: 0.016846610233187675, ACC:1.0\n",
      "Training iteration 37 loss: 0.004960167687386274, ACC:1.0\n",
      "Training iteration 38 loss: 0.013550104573369026, ACC:0.984375\n",
      "Training iteration 39 loss: 0.03172788396477699, ACC:0.984375\n",
      "Training iteration 40 loss: 0.14577554166316986, ACC:0.90625\n",
      "Training iteration 41 loss: 0.018800631165504456, ACC:0.984375\n",
      "Training iteration 42 loss: 0.07221513241529465, ACC:0.984375\n",
      "Training iteration 43 loss: 0.16359832882881165, ACC:0.953125\n",
      "Training iteration 44 loss: 0.061624690890312195, ACC:0.984375\n",
      "Training iteration 45 loss: 0.03251507878303528, ACC:0.984375\n",
      "Training iteration 46 loss: 0.05277744308114052, ACC:0.984375\n",
      "Training iteration 47 loss: 0.05814081057906151, ACC:0.96875\n",
      "Training iteration 48 loss: 0.0822148397564888, ACC:0.96875\n",
      "Training iteration 49 loss: 0.0011319019831717014, ACC:1.0\n",
      "Training iteration 50 loss: 0.03809303417801857, ACC:0.984375\n",
      "Training iteration 51 loss: 0.006205747835338116, ACC:1.0\n",
      "Training iteration 52 loss: 0.08817316591739655, ACC:0.96875\n",
      "Training iteration 53 loss: 0.008109253831207752, ACC:1.0\n",
      "Training iteration 54 loss: 0.18821901082992554, ACC:0.96875\n",
      "Training iteration 55 loss: 0.0008424020488746464, ACC:1.0\n",
      "Training iteration 56 loss: 0.02319648303091526, ACC:1.0\n",
      "Training iteration 57 loss: 0.0070095849223434925, ACC:1.0\n",
      "Training iteration 58 loss: 0.004475633148103952, ACC:1.0\n",
      "Training iteration 59 loss: 0.05308602750301361, ACC:0.984375\n",
      "Training iteration 60 loss: 0.015425104647874832, ACC:1.0\n",
      "Training iteration 61 loss: 0.07638261467218399, ACC:0.96875\n",
      "Training iteration 62 loss: 0.0807320699095726, ACC:0.96875\n",
      "Training iteration 63 loss: 0.06262306123971939, ACC:0.96875\n",
      "Training iteration 64 loss: 0.09032559394836426, ACC:0.96875\n",
      "Training iteration 65 loss: 0.01960843615233898, ACC:0.984375\n",
      "Training iteration 66 loss: 0.004870687145739794, ACC:1.0\n",
      "Training iteration 67 loss: 0.12514033913612366, ACC:0.96875\n",
      "Training iteration 68 loss: 0.05498005822300911, ACC:0.96875\n",
      "Training iteration 69 loss: 0.05250032991170883, ACC:0.96875\n",
      "Training iteration 70 loss: 0.17144574224948883, ACC:0.9375\n",
      "Training iteration 71 loss: 0.03982871398329735, ACC:0.96875\n",
      "Training iteration 72 loss: 0.010667193681001663, ACC:1.0\n",
      "Training iteration 73 loss: 0.06879311800003052, ACC:0.96875\n",
      "Training iteration 74 loss: 0.06041039898991585, ACC:0.984375\n",
      "Training iteration 75 loss: 0.015261633321642876, ACC:0.984375\n",
      "Training iteration 76 loss: 0.01382691040635109, ACC:1.0\n",
      "Training iteration 77 loss: 0.02511577680706978, ACC:0.984375\n",
      "Training iteration 78 loss: 0.03708469495177269, ACC:0.96875\n",
      "Training iteration 79 loss: 0.0018687705742195249, ACC:1.0\n",
      "Training iteration 80 loss: 0.013599247671663761, ACC:1.0\n",
      "Training iteration 81 loss: 0.008218642324209213, ACC:1.0\n",
      "Training iteration 82 loss: 0.012316860258579254, ACC:1.0\n",
      "Training iteration 83 loss: 0.04592703655362129, ACC:0.96875\n",
      "Training iteration 84 loss: 0.00969807431101799, ACC:1.0\n",
      "Training iteration 85 loss: 0.06631916016340256, ACC:0.984375\n",
      "Training iteration 86 loss: 0.032112106680870056, ACC:0.984375\n",
      "Training iteration 87 loss: 0.026953238993883133, ACC:0.984375\n",
      "Training iteration 88 loss: 0.028119530528783798, ACC:0.96875\n",
      "Training iteration 89 loss: 0.009962901473045349, ACC:1.0\n",
      "Training iteration 90 loss: 0.280244380235672, ACC:0.9375\n",
      "Training iteration 91 loss: 0.014243277721107006, ACC:0.984375\n",
      "Training iteration 92 loss: 0.03396068140864372, ACC:0.984375\n",
      "Training iteration 93 loss: 0.023323755711317062, ACC:0.984375\n",
      "Training iteration 94 loss: 0.11705703288316727, ACC:0.953125\n",
      "Training iteration 95 loss: 0.09985002875328064, ACC:0.953125\n",
      "Training iteration 96 loss: 0.005793355405330658, ACC:1.0\n",
      "Training iteration 97 loss: 0.013234022073447704, ACC:0.984375\n",
      "Training iteration 98 loss: 0.19693255424499512, ACC:0.90625\n",
      "Training iteration 99 loss: 0.1525561660528183, ACC:0.984375\n",
      "Training iteration 100 loss: 0.07366499304771423, ACC:0.984375\n",
      "Training iteration 101 loss: 0.11162984371185303, ACC:0.953125\n",
      "Training iteration 102 loss: 0.058433521538972855, ACC:0.984375\n",
      "Training iteration 103 loss: 0.07864226400852203, ACC:0.984375\n",
      "Training iteration 104 loss: 0.0885431319475174, ACC:0.96875\n",
      "Training iteration 105 loss: 0.03040466271340847, ACC:0.96875\n",
      "Training iteration 106 loss: 0.062423497438430786, ACC:0.984375\n",
      "Training iteration 107 loss: 0.03683677688241005, ACC:0.984375\n",
      "Training iteration 108 loss: 0.059003204107284546, ACC:0.96875\n",
      "Training iteration 109 loss: 0.018986357375979424, ACC:0.984375\n",
      "Training iteration 110 loss: 0.011521699838340282, ACC:1.0\n",
      "Training iteration 111 loss: 0.13471922278404236, ACC:0.96875\n",
      "Training iteration 112 loss: 0.1618441343307495, ACC:0.921875\n",
      "Training iteration 113 loss: 0.027190078049898148, ACC:1.0\n",
      "Training iteration 114 loss: 0.26492366194725037, ACC:0.875\n",
      "Training iteration 115 loss: 0.006211097352206707, ACC:1.0\n",
      "Training iteration 116 loss: 0.038151588290929794, ACC:0.984375\n",
      "Training iteration 117 loss: 0.09794490039348602, ACC:0.984375\n",
      "Training iteration 118 loss: 0.07126788049936295, ACC:0.96875\n",
      "Training iteration 119 loss: 0.5252428650856018, ACC:0.84375\n",
      "Training iteration 120 loss: 0.49139636754989624, ACC:0.890625\n",
      "Training iteration 121 loss: 0.30069980025291443, ACC:0.9375\n",
      "Training iteration 122 loss: 0.07603529095649719, ACC:0.984375\n",
      "Training iteration 123 loss: 0.9627599120140076, ACC:0.625\n",
      "Training iteration 124 loss: 0.2234668880701065, ACC:0.921875\n",
      "Training iteration 125 loss: 0.43936842679977417, ACC:0.796875\n",
      "Training iteration 126 loss: 0.5976150035858154, ACC:0.8125\n",
      "Training iteration 127 loss: 0.5501235127449036, ACC:0.78125\n",
      "Training iteration 128 loss: 0.712611734867096, ACC:0.703125\n",
      "Training iteration 129 loss: 1.3485984802246094, ACC:0.734375\n",
      "Training iteration 130 loss: 0.31946831941604614, ACC:0.921875\n",
      "Training iteration 131 loss: 0.5571012496948242, ACC:0.71875\n",
      "Training iteration 132 loss: 0.4494158625602722, ACC:0.828125\n",
      "Training iteration 133 loss: 0.4959900379180908, ACC:0.890625\n",
      "Training iteration 134 loss: 0.10861750692129135, ACC:0.953125\n",
      "Training iteration 135 loss: 0.07225532084703445, ACC:0.96875\n",
      "Training iteration 136 loss: 0.5648823976516724, ACC:0.75\n",
      "Training iteration 137 loss: 0.17155446112155914, ACC:0.9375\n",
      "Training iteration 138 loss: 0.13596291840076447, ACC:0.953125\n",
      "Training iteration 139 loss: 0.17326784133911133, ACC:0.921875\n",
      "Training iteration 140 loss: 0.14150479435920715, ACC:0.953125\n",
      "Training iteration 141 loss: 0.4066765308380127, ACC:0.796875\n",
      "Training iteration 142 loss: 0.3444006145000458, ACC:0.828125\n",
      "Training iteration 143 loss: 0.15273477137088776, ACC:0.921875\n",
      "Training iteration 144 loss: 0.1292923390865326, ACC:0.984375\n",
      "Training iteration 145 loss: 0.14773914217948914, ACC:0.921875\n",
      "Training iteration 146 loss: 0.2340945452451706, ACC:0.921875\n",
      "Training iteration 147 loss: 0.1308293491601944, ACC:0.96875\n",
      "Training iteration 148 loss: 0.3564685583114624, ACC:0.859375\n",
      "Training iteration 149 loss: 0.13347122073173523, ACC:0.953125\n",
      "Training iteration 150 loss: 0.29951053857803345, ACC:0.9375\n",
      "Training iteration 151 loss: 0.10627993941307068, ACC:0.953125\n",
      "Training iteration 152 loss: 0.09513987600803375, ACC:0.984375\n",
      "Training iteration 153 loss: 0.1206400990486145, ACC:0.9375\n",
      "Training iteration 154 loss: 0.07893895357847214, ACC:0.96875\n",
      "Training iteration 155 loss: 0.12851840257644653, ACC:0.921875\n",
      "Training iteration 156 loss: 0.009143747389316559, ACC:1.0\n",
      "Training iteration 157 loss: 0.0748145803809166, ACC:0.984375\n",
      "Training iteration 158 loss: 0.3269745111465454, ACC:0.9375\n",
      "Training iteration 159 loss: 0.17325973510742188, ACC:0.9375\n",
      "Training iteration 160 loss: 0.14030976593494415, ACC:0.96875\n",
      "Training iteration 161 loss: 0.07554086297750473, ACC:0.984375\n",
      "Training iteration 162 loss: 0.16363796591758728, ACC:0.953125\n",
      "Training iteration 163 loss: 0.018615033477544785, ACC:1.0\n",
      "Training iteration 164 loss: 0.028888043016195297, ACC:0.984375\n",
      "Training iteration 165 loss: 0.06250227242708206, ACC:0.96875\n",
      "Training iteration 166 loss: 0.012478606775403023, ACC:1.0\n",
      "Training iteration 167 loss: 0.09239734709262848, ACC:0.984375\n",
      "Training iteration 168 loss: 0.06904570758342743, ACC:0.984375\n",
      "Training iteration 169 loss: 0.07322109490633011, ACC:0.984375\n",
      "Training iteration 170 loss: 0.08442379534244537, ACC:0.9375\n",
      "Training iteration 171 loss: 0.006724152248352766, ACC:1.0\n",
      "Training iteration 172 loss: 0.031970661133527756, ACC:0.984375\n",
      "Training iteration 173 loss: 0.13635565340518951, ACC:0.984375\n",
      "Training iteration 174 loss: 0.02194751426577568, ACC:1.0\n",
      "Training iteration 175 loss: 0.016653046011924744, ACC:1.0\n",
      "Training iteration 176 loss: 0.08219180256128311, ACC:0.984375\n",
      "Training iteration 177 loss: 0.0817076787352562, ACC:0.984375\n",
      "Training iteration 178 loss: 0.0409281887114048, ACC:0.984375\n",
      "Training iteration 179 loss: 0.04956056922674179, ACC:0.984375\n",
      "Training iteration 180 loss: 0.037558313459157944, ACC:1.0\n",
      "Training iteration 181 loss: 0.01695968210697174, ACC:1.0\n",
      "Training iteration 182 loss: 0.03567551076412201, ACC:0.96875\n",
      "Training iteration 183 loss: 0.0893152505159378, ACC:0.953125\n",
      "Training iteration 184 loss: 0.07738548517227173, ACC:0.984375\n",
      "Training iteration 185 loss: 0.06304554641246796, ACC:0.984375\n",
      "Training iteration 186 loss: 0.08078907430171967, ACC:0.984375\n",
      "Training iteration 187 loss: 0.08596478402614594, ACC:0.96875\n",
      "Training iteration 188 loss: 0.005232431460171938, ACC:1.0\n",
      "Training iteration 189 loss: 0.09904834628105164, ACC:0.96875\n",
      "Training iteration 190 loss: 0.008066038601100445, ACC:1.0\n",
      "Training iteration 191 loss: 0.1061849594116211, ACC:0.96875\n",
      "Training iteration 192 loss: 0.05299725383520126, ACC:0.96875\n",
      "Training iteration 193 loss: 0.07255712151527405, ACC:0.984375\n",
      "Training iteration 194 loss: 0.06865580379962921, ACC:0.984375\n",
      "Training iteration 195 loss: 0.0235899668186903, ACC:1.0\n",
      "Training iteration 196 loss: 0.014110814779996872, ACC:1.0\n",
      "Training iteration 197 loss: 0.040623001754283905, ACC:0.984375\n",
      "Training iteration 198 loss: 0.03948568180203438, ACC:1.0\n",
      "Training iteration 199 loss: 0.037635575979948044, ACC:0.984375\n",
      "Training iteration 200 loss: 0.057971253991127014, ACC:0.96875\n",
      "Training iteration 201 loss: 0.02663438394665718, ACC:0.984375\n",
      "Training iteration 202 loss: 0.01625823974609375, ACC:1.0\n",
      "Training iteration 203 loss: 0.05283191427588463, ACC:0.984375\n",
      "Training iteration 204 loss: 0.016727861016988754, ACC:0.984375\n",
      "Training iteration 205 loss: 0.10579360276460648, ACC:0.953125\n",
      "Training iteration 206 loss: 0.004714851267635822, ACC:1.0\n",
      "Training iteration 207 loss: 0.1750943660736084, ACC:0.953125\n",
      "Training iteration 208 loss: 0.0018860336858779192, ACC:1.0\n",
      "Training iteration 209 loss: 0.08516328781843185, ACC:0.984375\n",
      "Training iteration 210 loss: 0.0405685193836689, ACC:0.984375\n",
      "Training iteration 211 loss: 0.0074232700280845165, ACC:1.0\n",
      "Training iteration 212 loss: 0.04449085891246796, ACC:0.984375\n",
      "Training iteration 213 loss: 0.07350902259349823, ACC:0.953125\n",
      "Training iteration 214 loss: 0.04950736463069916, ACC:0.96875\n",
      "Training iteration 215 loss: 0.0027604480274021626, ACC:1.0\n",
      "Training iteration 216 loss: 0.13809910416603088, ACC:0.9375\n",
      "Training iteration 217 loss: 0.06850112974643707, ACC:0.984375\n",
      "Training iteration 218 loss: 0.001705183181911707, ACC:1.0\n",
      "Training iteration 219 loss: 0.058461349457502365, ACC:0.984375\n",
      "Training iteration 220 loss: 0.011693430133163929, ACC:0.984375\n",
      "Training iteration 221 loss: 0.018036045134067535, ACC:0.984375\n",
      "Training iteration 222 loss: 0.0033609564416110516, ACC:1.0\n",
      "Training iteration 223 loss: 0.0005024090060032904, ACC:1.0\n",
      "Training iteration 224 loss: 0.02375602535903454, ACC:0.984375\n",
      "Training iteration 225 loss: 0.01899033598601818, ACC:0.984375\n",
      "Training iteration 226 loss: 0.010307800956070423, ACC:1.0\n",
      "Training iteration 227 loss: 0.005742323584854603, ACC:1.0\n",
      "Training iteration 228 loss: 0.005424970295280218, ACC:1.0\n",
      "Training iteration 229 loss: 0.01226763240993023, ACC:1.0\n",
      "Training iteration 230 loss: 0.039571695029735565, ACC:0.984375\n",
      "Training iteration 231 loss: 0.0440196730196476, ACC:0.984375\n",
      "Training iteration 232 loss: 0.01812184415757656, ACC:1.0\n",
      "Training iteration 233 loss: 0.04565093666315079, ACC:0.984375\n",
      "Training iteration 234 loss: 0.01361309364438057, ACC:1.0\n",
      "Training iteration 235 loss: 0.03440231829881668, ACC:0.984375\n",
      "Training iteration 236 loss: 0.02458157204091549, ACC:0.984375\n",
      "Training iteration 237 loss: 0.002952448558062315, ACC:1.0\n",
      "Training iteration 238 loss: 0.014793244190514088, ACC:0.984375\n",
      "Training iteration 239 loss: 0.05150015652179718, ACC:0.984375\n",
      "Training iteration 240 loss: 0.052768558263778687, ACC:0.984375\n",
      "Training iteration 241 loss: 0.032460566610097885, ACC:0.984375\n",
      "Training iteration 242 loss: 0.11285530775785446, ACC:0.96875\n",
      "Training iteration 243 loss: 0.07461872696876526, ACC:0.984375\n",
      "Training iteration 244 loss: 0.006750899367034435, ACC:1.0\n",
      "Training iteration 245 loss: 0.029048293828964233, ACC:0.984375\n",
      "Training iteration 246 loss: 0.05229628458619118, ACC:0.984375\n",
      "Training iteration 247 loss: 0.02081075683236122, ACC:1.0\n",
      "Training iteration 248 loss: 0.13664156198501587, ACC:0.953125\n",
      "Training iteration 249 loss: 0.11460273712873459, ACC:0.984375\n",
      "Training iteration 250 loss: 0.053457148373126984, ACC:0.984375\n",
      "Training iteration 251 loss: 0.04725079983472824, ACC:0.984375\n",
      "Training iteration 252 loss: 0.05829830467700958, ACC:0.984375\n",
      "Training iteration 253 loss: 0.09135598689317703, ACC:0.984375\n",
      "Training iteration 254 loss: 0.012469420209527016, ACC:1.0\n",
      "Training iteration 255 loss: 0.01897537335753441, ACC:0.984375\n",
      "Training iteration 256 loss: 0.005954778753221035, ACC:1.0\n",
      "Training iteration 257 loss: 0.0018833784852176905, ACC:1.0\n",
      "Training iteration 258 loss: 0.1012900173664093, ACC:0.96875\n",
      "Training iteration 259 loss: 0.02122829109430313, ACC:0.984375\n",
      "Training iteration 260 loss: 0.05773777887225151, ACC:0.984375\n",
      "Training iteration 261 loss: 0.0010475936578586698, ACC:1.0\n",
      "Training iteration 262 loss: 0.013466316275298595, ACC:0.984375\n",
      "Training iteration 263 loss: 0.05845404788851738, ACC:0.984375\n",
      "Training iteration 264 loss: 0.16472473740577698, ACC:0.9375\n",
      "Training iteration 265 loss: 0.0021811951883137226, ACC:1.0\n",
      "Training iteration 266 loss: 0.029526114463806152, ACC:0.96875\n",
      "Training iteration 267 loss: 0.37026774883270264, ACC:0.953125\n",
      "Training iteration 268 loss: 0.04611337184906006, ACC:0.984375\n",
      "Training iteration 269 loss: 0.043587710708379745, ACC:0.96875\n",
      "Training iteration 270 loss: 0.05446679890155792, ACC:0.984375\n",
      "Training iteration 271 loss: 0.008232596330344677, ACC:1.0\n",
      "Training iteration 272 loss: 0.2437121570110321, ACC:0.96875\n",
      "Training iteration 273 loss: 0.006083465646952391, ACC:1.0\n",
      "Training iteration 274 loss: 0.07491777092218399, ACC:0.96875\n",
      "Training iteration 275 loss: 0.0030498832929879427, ACC:1.0\n",
      "Training iteration 276 loss: 0.11881672590970993, ACC:0.96875\n",
      "Training iteration 277 loss: 0.08152838051319122, ACC:0.96875\n",
      "Training iteration 278 loss: 0.011144686490297318, ACC:1.0\n",
      "Training iteration 279 loss: 0.012347490526735783, ACC:1.0\n",
      "Training iteration 280 loss: 0.026085546240210533, ACC:0.984375\n",
      "Training iteration 281 loss: 0.024242384359240532, ACC:1.0\n",
      "Training iteration 282 loss: 0.020966922864317894, ACC:1.0\n",
      "Training iteration 283 loss: 0.0520392507314682, ACC:0.984375\n",
      "Training iteration 284 loss: 0.002454975387081504, ACC:1.0\n",
      "Training iteration 285 loss: 0.008497340604662895, ACC:1.0\n",
      "Training iteration 286 loss: 0.022904768586158752, ACC:0.984375\n",
      "Training iteration 287 loss: 0.09909158945083618, ACC:0.984375\n",
      "Training iteration 288 loss: 0.13437095284461975, ACC:0.96875\n",
      "Training iteration 289 loss: 0.11461035907268524, ACC:0.96875\n",
      "Training iteration 290 loss: 0.013897525146603584, ACC:1.0\n",
      "Training iteration 291 loss: 0.00810222327709198, ACC:1.0\n",
      "Training iteration 292 loss: 0.11583313345909119, ACC:0.984375\n",
      "Training iteration 293 loss: 0.021085066720843315, ACC:0.984375\n",
      "Training iteration 294 loss: 0.027990106493234634, ACC:0.984375\n",
      "Training iteration 295 loss: 0.05945836752653122, ACC:0.96875\n",
      "Training iteration 296 loss: 0.008740175515413284, ACC:1.0\n",
      "Training iteration 297 loss: 0.013357286341488361, ACC:1.0\n",
      "Training iteration 298 loss: 0.035282671451568604, ACC:0.984375\n",
      "Training iteration 299 loss: 0.003211531788110733, ACC:1.0\n",
      "Training iteration 300 loss: 0.03379891440272331, ACC:0.96875\n",
      "Training iteration 301 loss: 0.008058917708694935, ACC:1.0\n",
      "Training iteration 302 loss: 0.01557750254869461, ACC:1.0\n",
      "Training iteration 303 loss: 0.001917374669574201, ACC:1.0\n",
      "Training iteration 304 loss: 0.005545035004615784, ACC:1.0\n",
      "Training iteration 305 loss: 0.01601429469883442, ACC:1.0\n",
      "Training iteration 306 loss: 0.022063318639993668, ACC:0.984375\n",
      "Training iteration 307 loss: 0.0058991564437747, ACC:1.0\n",
      "Training iteration 308 loss: 0.025968020781874657, ACC:0.984375\n",
      "Training iteration 309 loss: 0.0015068858629092574, ACC:1.0\n",
      "Training iteration 310 loss: 0.02224321849644184, ACC:1.0\n",
      "Training iteration 311 loss: 0.011283325962722301, ACC:1.0\n",
      "Training iteration 312 loss: 0.05973656475543976, ACC:0.984375\n",
      "Training iteration 313 loss: 0.005623433273285627, ACC:1.0\n",
      "Training iteration 314 loss: 0.003390992758795619, ACC:1.0\n",
      "Training iteration 315 loss: 0.09517789632081985, ACC:0.984375\n",
      "Training iteration 316 loss: 0.01979352906346321, ACC:0.984375\n",
      "Training iteration 317 loss: 0.03776523843407631, ACC:0.984375\n",
      "Training iteration 318 loss: 0.00541367381811142, ACC:1.0\n",
      "Training iteration 319 loss: 0.03064168617129326, ACC:0.96875\n",
      "Training iteration 320 loss: 0.1463315784931183, ACC:0.984375\n",
      "Training iteration 321 loss: 0.06650971621274948, ACC:0.984375\n",
      "Training iteration 322 loss: 0.009279180318117142, ACC:1.0\n",
      "Training iteration 323 loss: 0.0065962933003902435, ACC:1.0\n",
      "Training iteration 324 loss: 0.002411633962765336, ACC:1.0\n",
      "Training iteration 325 loss: 0.0007009411929175258, ACC:1.0\n",
      "Training iteration 326 loss: 0.003445846727117896, ACC:1.0\n",
      "Training iteration 327 loss: 0.0033791372552514076, ACC:1.0\n",
      "Training iteration 328 loss: 0.0006522806943394244, ACC:1.0\n",
      "Training iteration 329 loss: 0.33768823742866516, ACC:0.96875\n",
      "Training iteration 330 loss: 0.03719088062644005, ACC:0.984375\n",
      "Training iteration 331 loss: 0.07993976026773453, ACC:0.96875\n",
      "Training iteration 332 loss: 0.015710635110735893, ACC:0.984375\n",
      "Training iteration 333 loss: 0.07559036463499069, ACC:0.96875\n",
      "Training iteration 334 loss: 0.0300760380923748, ACC:0.96875\n",
      "Training iteration 335 loss: 0.024661844596266747, ACC:1.0\n",
      "Training iteration 336 loss: 0.019302673637866974, ACC:1.0\n",
      "Training iteration 337 loss: 0.029042812064290047, ACC:0.984375\n",
      "Training iteration 338 loss: 0.015192419290542603, ACC:1.0\n",
      "Training iteration 339 loss: 0.014421174302697182, ACC:1.0\n",
      "Training iteration 340 loss: 0.026229605078697205, ACC:0.984375\n",
      "Training iteration 341 loss: 0.004656168166548014, ACC:1.0\n",
      "Training iteration 342 loss: 0.0022229189053177834, ACC:1.0\n",
      "Training iteration 343 loss: 0.00770075898617506, ACC:1.0\n",
      "Training iteration 344 loss: 0.006959470920264721, ACC:1.0\n",
      "Training iteration 345 loss: 0.032603371888399124, ACC:1.0\n",
      "Training iteration 346 loss: 0.010726098902523518, ACC:1.0\n",
      "Training iteration 347 loss: 0.11901142448186874, ACC:0.984375\n",
      "Training iteration 348 loss: 0.0069873337633907795, ACC:1.0\n",
      "Training iteration 349 loss: 0.007812295109033585, ACC:1.0\n",
      "Training iteration 350 loss: 0.005054878070950508, ACC:1.0\n",
      "Training iteration 351 loss: 0.06999513506889343, ACC:0.984375\n",
      "Training iteration 352 loss: 0.0013937539188191295, ACC:1.0\n",
      "Training iteration 353 loss: 0.01096033863723278, ACC:1.0\n",
      "Training iteration 354 loss: 0.0031777205877006054, ACC:1.0\n",
      "Training iteration 355 loss: 0.00989347230643034, ACC:1.0\n",
      "Training iteration 356 loss: 0.0035413948353379965, ACC:1.0\n",
      "Training iteration 357 loss: 0.1096029058098793, ACC:0.9375\n",
      "Training iteration 358 loss: 0.06974029541015625, ACC:0.984375\n",
      "Training iteration 359 loss: 0.02007627673447132, ACC:1.0\n",
      "Training iteration 360 loss: 0.1024976596236229, ACC:0.984375\n",
      "Training iteration 361 loss: 0.0019183626864105463, ACC:1.0\n",
      "Training iteration 362 loss: 0.027552686631679535, ACC:0.984375\n",
      "Training iteration 363 loss: 0.029114656150341034, ACC:0.984375\n",
      "Training iteration 364 loss: 0.005935171153396368, ACC:1.0\n",
      "Training iteration 365 loss: 0.0546843521296978, ACC:0.984375\n",
      "Training iteration 366 loss: 0.002023156965151429, ACC:1.0\n",
      "Training iteration 367 loss: 0.18714140355587006, ACC:0.953125\n",
      "Training iteration 368 loss: 0.04844503849744797, ACC:0.984375\n",
      "Training iteration 369 loss: 0.033695317804813385, ACC:0.984375\n",
      "Training iteration 370 loss: 0.02851685881614685, ACC:0.984375\n",
      "Training iteration 371 loss: 0.008064608089625835, ACC:1.0\n",
      "Training iteration 372 loss: 0.0033293357118964195, ACC:1.0\n",
      "Training iteration 373 loss: 0.004590773023664951, ACC:1.0\n",
      "Training iteration 374 loss: 0.00512002594769001, ACC:1.0\n",
      "Training iteration 375 loss: 0.013516170904040337, ACC:1.0\n",
      "Training iteration 376 loss: 0.00376618979498744, ACC:1.0\n",
      "Training iteration 377 loss: 0.01622169092297554, ACC:1.0\n",
      "Training iteration 378 loss: 0.026685047894716263, ACC:0.984375\n",
      "Training iteration 379 loss: 0.008676964789628983, ACC:1.0\n",
      "Training iteration 380 loss: 0.03615167364478111, ACC:0.984375\n",
      "Training iteration 381 loss: 0.072870634496212, ACC:0.984375\n",
      "Training iteration 382 loss: 0.003311420790851116, ACC:1.0\n",
      "Training iteration 383 loss: 0.006767970975488424, ACC:1.0\n",
      "Training iteration 384 loss: 0.0035429582931101322, ACC:1.0\n",
      "Training iteration 385 loss: 0.05029616132378578, ACC:0.984375\n",
      "Training iteration 386 loss: 0.10006342828273773, ACC:0.953125\n",
      "Training iteration 387 loss: 0.006672258023172617, ACC:1.0\n",
      "Training iteration 388 loss: 0.0835736021399498, ACC:0.96875\n",
      "Training iteration 389 loss: 0.009142260998487473, ACC:1.0\n",
      "Training iteration 390 loss: 0.025047006085515022, ACC:0.984375\n",
      "Training iteration 391 loss: 0.0005754783633165061, ACC:1.0\n",
      "Training iteration 392 loss: 0.0009747120784595609, ACC:1.0\n",
      "Training iteration 393 loss: 0.0006473808898590505, ACC:1.0\n",
      "Training iteration 394 loss: 0.0036218101158738136, ACC:1.0\n",
      "Training iteration 395 loss: 0.0031272522173821926, ACC:1.0\n",
      "Training iteration 396 loss: 0.017270658165216446, ACC:0.984375\n",
      "Training iteration 397 loss: 0.00536731630563736, ACC:1.0\n",
      "Training iteration 398 loss: 0.07414648681879044, ACC:0.984375\n",
      "Training iteration 399 loss: 0.002539563924074173, ACC:1.0\n",
      "Training iteration 400 loss: 0.003643936477601528, ACC:1.0\n",
      "Training iteration 401 loss: 0.0004997533978894353, ACC:1.0\n",
      "Training iteration 402 loss: 0.0016135007608681917, ACC:1.0\n",
      "Training iteration 403 loss: 0.0963822677731514, ACC:0.9375\n",
      "Training iteration 404 loss: 0.012463224120438099, ACC:1.0\n",
      "Training iteration 405 loss: 0.0023045772686600685, ACC:1.0\n",
      "Training iteration 406 loss: 0.07622070610523224, ACC:0.953125\n",
      "Training iteration 407 loss: 0.014987731352448463, ACC:0.984375\n",
      "Training iteration 408 loss: 0.004819878377020359, ACC:1.0\n",
      "Training iteration 409 loss: 0.003584181657060981, ACC:1.0\n",
      "Training iteration 410 loss: 0.004743569064885378, ACC:1.0\n",
      "Training iteration 411 loss: 0.006242851261049509, ACC:1.0\n",
      "Training iteration 412 loss: 0.003131174948066473, ACC:1.0\n",
      "Training iteration 413 loss: 0.0022339685820043087, ACC:1.0\n",
      "Training iteration 414 loss: 0.13956841826438904, ACC:0.984375\n",
      "Training iteration 415 loss: 0.02257046476006508, ACC:0.984375\n",
      "Training iteration 416 loss: 0.000797145301476121, ACC:1.0\n",
      "Training iteration 417 loss: 0.02738342434167862, ACC:0.984375\n",
      "Training iteration 418 loss: 0.11028806120157242, ACC:0.96875\n",
      "Training iteration 419 loss: 0.08580919355154037, ACC:0.984375\n",
      "Training iteration 420 loss: 0.07068146020174026, ACC:0.96875\n",
      "Training iteration 421 loss: 0.007066326681524515, ACC:1.0\n",
      "Training iteration 422 loss: 0.0008413477917201817, ACC:1.0\n",
      "Training iteration 423 loss: 0.05084896832704544, ACC:0.984375\n",
      "Training iteration 424 loss: 0.00390033982694149, ACC:1.0\n",
      "Training iteration 425 loss: 0.0018997492734342813, ACC:1.0\n",
      "Training iteration 426 loss: 0.02750188112258911, ACC:0.984375\n",
      "Training iteration 427 loss: 0.06706447899341583, ACC:0.984375\n",
      "Training iteration 428 loss: 0.011092753149569035, ACC:1.0\n",
      "Training iteration 429 loss: 0.05965784192085266, ACC:0.96875\n",
      "Training iteration 430 loss: 0.04769283905625343, ACC:0.96875\n",
      "Training iteration 431 loss: 0.21531009674072266, ACC:0.96875\n",
      "Training iteration 432 loss: 0.010322720743715763, ACC:1.0\n",
      "Training iteration 433 loss: 0.0025788217317312956, ACC:1.0\n",
      "Training iteration 434 loss: 0.03530556708574295, ACC:0.984375\n",
      "Training iteration 435 loss: 0.005598106887191534, ACC:1.0\n",
      "Training iteration 436 loss: 0.010939912870526314, ACC:1.0\n",
      "Training iteration 437 loss: 0.008890721015632153, ACC:1.0\n",
      "Training iteration 438 loss: 0.02384636551141739, ACC:1.0\n",
      "Training iteration 439 loss: 0.011500325053930283, ACC:1.0\n",
      "Training iteration 440 loss: 0.05105653777718544, ACC:0.984375\n",
      "Training iteration 441 loss: 0.0020240626763552427, ACC:1.0\n",
      "Training iteration 442 loss: 0.0021606951486319304, ACC:1.0\n",
      "Training iteration 443 loss: 0.09205567091703415, ACC:0.96875\n",
      "Training iteration 444 loss: 0.020841574296355247, ACC:0.984375\n",
      "Training iteration 445 loss: 0.002059654798358679, ACC:1.0\n",
      "Training iteration 446 loss: 0.0009627625695429742, ACC:1.0\n",
      "Training iteration 447 loss: 0.0016877510352060199, ACC:1.0\n",
      "Training iteration 448 loss: 0.1285221427679062, ACC:0.96875\n",
      "Training iteration 449 loss: 0.0003177515754941851, ACC:1.0\n",
      "Training iteration 450 loss: 0.03241419047117233, ACC:0.984375\n",
      "Validation iteration 451 loss: 0.0006990895490162075, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.029869478195905685, ACC: 0.984375\n",
      "Validation iteration 453 loss: 0.01857995241880417, ACC: 0.984375\n",
      "Validation iteration 454 loss: 0.03710798919200897, ACC: 0.984375\n",
      "Validation iteration 455 loss: 0.15645469725131989, ACC: 0.96875\n",
      "Validation iteration 456 loss: 0.000314222153974697, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.0022647862788289785, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.0008859778754413128, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.09759184718132019, ACC: 0.96875\n",
      "Validation iteration 460 loss: 0.10748917609453201, ACC: 0.984375\n",
      "Validation iteration 461 loss: 0.013194049708545208, ACC: 0.984375\n",
      "Validation iteration 462 loss: 0.07671543210744858, ACC: 0.984375\n",
      "Validation iteration 463 loss: 0.03957310691475868, ACC: 0.96875\n",
      "Validation iteration 464 loss: 0.0004098159843124449, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.10423482209444046, ACC: 0.96875\n",
      "Validation iteration 466 loss: 0.1865517646074295, ACC: 0.96875\n",
      "Validation iteration 467 loss: 0.12514226138591766, ACC: 0.96875\n",
      "Validation iteration 468 loss: 0.0002379215438850224, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.09759805351495743, ACC: 0.96875\n",
      "Validation iteration 470 loss: 0.00031672121258452535, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.045239128172397614, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.014677776955068111, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.02802302874624729, ACC: 0.984375\n",
      "Validation iteration 474 loss: 0.013582336716353893, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.0803547203540802, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.0007130578742362559, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.005836849100887775, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.04015353322029114, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.003480762941762805, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.0004989624139852822, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.005318599287420511, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.0027073852252215147, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.004104336723685265, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.008106868714094162, ACC: 1.0\n",
      "Validation iteration 485 loss: 0.0006394957890734076, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.10563664138317108, ACC: 0.96875\n",
      "Validation iteration 487 loss: 0.033417463302612305, ACC: 0.984375\n",
      "Validation iteration 488 loss: 0.003874438349157572, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.05708436295390129, ACC: 0.96875\n",
      "Validation iteration 490 loss: 0.01521755289286375, ACC: 0.984375\n",
      "Validation iteration 491 loss: 0.005581920966506004, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.12355796247720718, ACC: 0.96875\n",
      "Validation iteration 493 loss: 0.005904916673898697, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.01064895000308752, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.009332209825515747, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.038526784628629684, ACC: 0.984375\n",
      "Validation iteration 497 loss: 0.12202809005975723, ACC: 0.984375\n",
      "Validation iteration 498 loss: 0.00042710822890512645, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.03303464502096176, ACC: 0.984375\n",
      "Validation iteration 500 loss: 0.0654517412185669, ACC: 0.96875\n",
      "-- Epoch 2 done -- Train loss: 0.06804234971311719, train ACC: 0.9773611111111111, val loss: 0.03956785590969957, val ACC: 0.988125\n",
      "<--- 8858.025413036346 seconds --->\n",
      "Training iteration 1 loss: 0.020298151299357414, ACC:0.984375\n",
      "Training iteration 2 loss: 0.0018998893210664392, ACC:1.0\n",
      "Training iteration 3 loss: 0.01855725236237049, ACC:0.984375\n",
      "Training iteration 4 loss: 0.03919030353426933, ACC:0.984375\n",
      "Training iteration 5 loss: 0.016322845593094826, ACC:0.984375\n",
      "Training iteration 6 loss: 0.0034950110130012035, ACC:1.0\n",
      "Training iteration 7 loss: 0.04044097289443016, ACC:0.96875\n",
      "Training iteration 8 loss: 0.014017505571246147, ACC:1.0\n",
      "Training iteration 9 loss: 0.10061053931713104, ACC:0.984375\n",
      "Training iteration 10 loss: 0.015176193788647652, ACC:1.0\n",
      "Training iteration 11 loss: 0.10379001498222351, ACC:0.984375\n",
      "Training iteration 12 loss: 0.0019787477795034647, ACC:1.0\n",
      "Training iteration 13 loss: 0.11333553493022919, ACC:0.96875\n",
      "Training iteration 14 loss: 0.0361032597720623, ACC:0.984375\n",
      "Training iteration 15 loss: 0.0019380118465051055, ACC:1.0\n",
      "Training iteration 16 loss: 0.013550330884754658, ACC:1.0\n",
      "Training iteration 17 loss: 0.08509671688079834, ACC:0.953125\n",
      "Training iteration 18 loss: 0.021290248259902, ACC:1.0\n",
      "Training iteration 19 loss: 0.005676353815943003, ACC:1.0\n",
      "Training iteration 20 loss: 0.0038276868872344494, ACC:1.0\n",
      "Training iteration 21 loss: 0.06026553735136986, ACC:0.984375\n",
      "Training iteration 22 loss: 0.020742978900671005, ACC:1.0\n",
      "Training iteration 23 loss: 0.04705250635743141, ACC:0.984375\n",
      "Training iteration 24 loss: 0.014421015046536922, ACC:1.0\n",
      "Training iteration 25 loss: 0.0071041882038116455, ACC:1.0\n",
      "Training iteration 26 loss: 0.028721274808049202, ACC:1.0\n",
      "Training iteration 27 loss: 0.06433025002479553, ACC:0.96875\n",
      "Training iteration 28 loss: 0.0034472073893994093, ACC:1.0\n",
      "Training iteration 29 loss: 0.004987998865544796, ACC:1.0\n",
      "Training iteration 30 loss: 0.013215331360697746, ACC:1.0\n",
      "Training iteration 31 loss: 0.004904240369796753, ACC:1.0\n",
      "Training iteration 32 loss: 0.11963558197021484, ACC:0.953125\n",
      "Training iteration 33 loss: 0.023562107235193253, ACC:0.984375\n",
      "Training iteration 34 loss: 0.07730603218078613, ACC:0.984375\n",
      "Training iteration 35 loss: 0.03629247099161148, ACC:0.984375\n",
      "Training iteration 36 loss: 0.06878230720758438, ACC:0.96875\n",
      "Training iteration 37 loss: 0.011224084533751011, ACC:1.0\n",
      "Training iteration 38 loss: 0.007992067374289036, ACC:1.0\n",
      "Training iteration 39 loss: 0.007386329583823681, ACC:1.0\n",
      "Training iteration 40 loss: 0.00572294183075428, ACC:1.0\n",
      "Training iteration 41 loss: 0.023550203070044518, ACC:0.984375\n",
      "Training iteration 42 loss: 0.05883968621492386, ACC:0.96875\n",
      "Training iteration 43 loss: 0.047318533062934875, ACC:0.96875\n",
      "Training iteration 44 loss: 0.10966453701257706, ACC:0.96875\n",
      "Training iteration 45 loss: 0.020734455436468124, ACC:0.984375\n",
      "Training iteration 46 loss: 0.009104329161345959, ACC:1.0\n",
      "Training iteration 47 loss: 0.07237633317708969, ACC:0.984375\n",
      "Training iteration 48 loss: 0.011935039423406124, ACC:1.0\n",
      "Training iteration 49 loss: 0.010409416630864143, ACC:1.0\n",
      "Training iteration 50 loss: 0.003358677262440324, ACC:1.0\n",
      "Training iteration 51 loss: 0.04885758459568024, ACC:0.984375\n",
      "Training iteration 52 loss: 0.025639379397034645, ACC:0.984375\n",
      "Training iteration 53 loss: 0.02959301881492138, ACC:0.984375\n",
      "Training iteration 54 loss: 0.0031317579559981823, ACC:1.0\n",
      "Training iteration 55 loss: 0.04204397276043892, ACC:0.984375\n",
      "Training iteration 56 loss: 0.004870792385190725, ACC:1.0\n",
      "Training iteration 57 loss: 0.011369992978870869, ACC:1.0\n",
      "Training iteration 58 loss: 0.005777668207883835, ACC:1.0\n",
      "Training iteration 59 loss: 0.014529317617416382, ACC:1.0\n",
      "Training iteration 60 loss: 0.003919512033462524, ACC:1.0\n",
      "Training iteration 61 loss: 0.0021605875808745623, ACC:1.0\n",
      "Training iteration 62 loss: 0.06571362167596817, ACC:0.96875\n",
      "Training iteration 63 loss: 0.044906873255968094, ACC:0.96875\n",
      "Training iteration 64 loss: 0.10044524818658829, ACC:0.96875\n",
      "Training iteration 65 loss: 0.03509305790066719, ACC:0.984375\n",
      "Training iteration 66 loss: 0.08186449110507965, ACC:0.984375\n",
      "Training iteration 67 loss: 0.009217709302902222, ACC:1.0\n",
      "Training iteration 68 loss: 0.07575985789299011, ACC:0.984375\n",
      "Training iteration 69 loss: 0.04948225989937782, ACC:0.96875\n",
      "Training iteration 70 loss: 0.042432136833667755, ACC:0.984375\n",
      "Training iteration 71 loss: 0.015051417052745819, ACC:1.0\n",
      "Training iteration 72 loss: 0.1252700686454773, ACC:0.9375\n",
      "Training iteration 73 loss: 0.017325492575764656, ACC:0.984375\n",
      "Training iteration 74 loss: 0.01940043456852436, ACC:0.984375\n",
      "Training iteration 75 loss: 0.09918443858623505, ACC:0.9375\n",
      "Training iteration 76 loss: 0.019644418731331825, ACC:0.984375\n",
      "Training iteration 77 loss: 0.014444871805608273, ACC:0.984375\n",
      "Training iteration 78 loss: 0.010525367222726345, ACC:1.0\n",
      "Training iteration 79 loss: 0.10016365349292755, ACC:0.96875\n",
      "Training iteration 80 loss: 0.005253950599581003, ACC:1.0\n",
      "Training iteration 81 loss: 0.016232609748840332, ACC:0.984375\n",
      "Training iteration 82 loss: 0.08077208697795868, ACC:0.96875\n",
      "Training iteration 83 loss: 0.08666981011629105, ACC:0.96875\n",
      "Training iteration 84 loss: 0.010602434165775776, ACC:1.0\n",
      "Training iteration 85 loss: 0.004592175129801035, ACC:1.0\n",
      "Training iteration 86 loss: 0.022945210337638855, ACC:0.984375\n",
      "Training iteration 87 loss: 0.01600928232073784, ACC:1.0\n",
      "Training iteration 88 loss: 0.004013961646705866, ACC:1.0\n",
      "Training iteration 89 loss: 0.00875546783208847, ACC:1.0\n",
      "Training iteration 90 loss: 0.006834015250205994, ACC:1.0\n",
      "Training iteration 91 loss: 0.0403963066637516, ACC:0.984375\n",
      "Training iteration 92 loss: 0.09852336347103119, ACC:0.96875\n",
      "Training iteration 93 loss: 0.027973080053925514, ACC:0.96875\n",
      "Training iteration 94 loss: 0.008294991217553616, ACC:1.0\n",
      "Training iteration 95 loss: 0.035355955362319946, ACC:0.984375\n",
      "Training iteration 96 loss: 0.04268304258584976, ACC:0.96875\n",
      "Training iteration 97 loss: 0.007917915470898151, ACC:1.0\n",
      "Training iteration 98 loss: 0.0017619075952097774, ACC:1.0\n",
      "Training iteration 99 loss: 0.002269033109769225, ACC:1.0\n",
      "Training iteration 100 loss: 0.01308389101177454, ACC:1.0\n",
      "Training iteration 101 loss: 0.0038505620323121548, ACC:1.0\n",
      "Training iteration 102 loss: 0.004738708958029747, ACC:1.0\n",
      "Training iteration 103 loss: 0.009112732484936714, ACC:1.0\n",
      "Training iteration 104 loss: 0.0008082808926701546, ACC:1.0\n",
      "Training iteration 105 loss: 0.010679875500500202, ACC:1.0\n",
      "Training iteration 106 loss: 0.008592714555561543, ACC:1.0\n",
      "Training iteration 107 loss: 0.004150687251240015, ACC:1.0\n",
      "Training iteration 108 loss: 0.0009180711349472404, ACC:1.0\n",
      "Training iteration 109 loss: 0.014930817298591137, ACC:0.984375\n",
      "Training iteration 110 loss: 0.08652802556753159, ACC:0.96875\n",
      "Training iteration 111 loss: 0.010958020575344563, ACC:1.0\n",
      "Training iteration 112 loss: 0.03834502026438713, ACC:0.984375\n",
      "Training iteration 113 loss: 0.07117137312889099, ACC:0.984375\n",
      "Training iteration 114 loss: 0.0005391588783822954, ACC:1.0\n",
      "Training iteration 115 loss: 0.003222063882276416, ACC:1.0\n",
      "Training iteration 116 loss: 0.0011753904400393367, ACC:1.0\n",
      "Training iteration 117 loss: 0.0013593046460300684, ACC:1.0\n",
      "Training iteration 118 loss: 0.0024308674037456512, ACC:1.0\n",
      "Training iteration 119 loss: 0.0006389265181496739, ACC:1.0\n",
      "Training iteration 120 loss: 0.004275140352547169, ACC:1.0\n",
      "Training iteration 121 loss: 0.01718827895820141, ACC:0.984375\n",
      "Training iteration 122 loss: 0.004025555681437254, ACC:1.0\n",
      "Training iteration 123 loss: 0.00025691220071166754, ACC:1.0\n",
      "Training iteration 124 loss: 0.05477563664317131, ACC:0.96875\n",
      "Training iteration 125 loss: 0.0076764849945902824, ACC:1.0\n",
      "Training iteration 126 loss: 0.07437482476234436, ACC:0.984375\n",
      "Training iteration 127 loss: 0.022580500692129135, ACC:0.984375\n",
      "Training iteration 128 loss: 0.053407516330480576, ACC:0.984375\n",
      "Training iteration 129 loss: 0.07172045856714249, ACC:0.96875\n",
      "Training iteration 130 loss: 0.04024674370884895, ACC:0.984375\n",
      "Training iteration 131 loss: 0.016514386981725693, ACC:1.0\n",
      "Training iteration 132 loss: 0.09222495555877686, ACC:0.96875\n",
      "Training iteration 133 loss: 0.03916376084089279, ACC:0.984375\n",
      "Training iteration 134 loss: 0.021218886598944664, ACC:0.984375\n",
      "Training iteration 135 loss: 0.04989489167928696, ACC:0.96875\n",
      "Training iteration 136 loss: 0.005074875429272652, ACC:1.0\n",
      "Training iteration 137 loss: 0.0029763998463749886, ACC:1.0\n",
      "Training iteration 138 loss: 0.04248064011335373, ACC:0.984375\n",
      "Training iteration 139 loss: 0.03618238866329193, ACC:0.984375\n",
      "Training iteration 140 loss: 0.04005073010921478, ACC:0.984375\n",
      "Training iteration 141 loss: 0.003560868324711919, ACC:1.0\n",
      "Training iteration 142 loss: 0.0057256026193499565, ACC:1.0\n",
      "Training iteration 143 loss: 0.0034302189014852047, ACC:1.0\n",
      "Training iteration 144 loss: 0.0008191467495635152, ACC:1.0\n",
      "Training iteration 145 loss: 0.04459889605641365, ACC:0.96875\n",
      "Training iteration 146 loss: 0.003689208533614874, ACC:1.0\n",
      "Training iteration 147 loss: 0.005738662555813789, ACC:1.0\n",
      "Training iteration 148 loss: 0.07700078934431076, ACC:0.984375\n",
      "Training iteration 149 loss: 0.017429575324058533, ACC:1.0\n",
      "Training iteration 150 loss: 0.03838840872049332, ACC:0.984375\n",
      "Training iteration 151 loss: 0.003337569534778595, ACC:1.0\n",
      "Training iteration 152 loss: 0.039430346339941025, ACC:0.984375\n",
      "Training iteration 153 loss: 0.05845559015870094, ACC:0.96875\n",
      "Training iteration 154 loss: 0.032061707228422165, ACC:0.984375\n",
      "Training iteration 155 loss: 0.010672371834516525, ACC:1.0\n",
      "Training iteration 156 loss: 0.0012901434674859047, ACC:1.0\n",
      "Training iteration 157 loss: 0.04535374045372009, ACC:0.984375\n",
      "Training iteration 158 loss: 0.005638226866722107, ACC:1.0\n",
      "Training iteration 159 loss: 0.007590064313262701, ACC:1.0\n",
      "Training iteration 160 loss: 0.0005779998027719557, ACC:1.0\n",
      "Training iteration 161 loss: 0.08852968364953995, ACC:0.96875\n",
      "Training iteration 162 loss: 0.0018903542077168822, ACC:1.0\n",
      "Training iteration 163 loss: 0.2081240564584732, ACC:0.953125\n",
      "Training iteration 164 loss: 0.012953775003552437, ACC:1.0\n",
      "Training iteration 165 loss: 0.027116868644952774, ACC:0.984375\n",
      "Training iteration 166 loss: 0.013643048703670502, ACC:1.0\n",
      "Training iteration 167 loss: 0.001745270099490881, ACC:1.0\n",
      "Training iteration 168 loss: 0.0009818216785788536, ACC:1.0\n",
      "Training iteration 169 loss: 0.009828230366110802, ACC:1.0\n",
      "Training iteration 170 loss: 0.015110030770301819, ACC:0.984375\n",
      "Training iteration 171 loss: 0.0766114741563797, ACC:0.96875\n",
      "Training iteration 172 loss: 0.004412086680531502, ACC:1.0\n",
      "Training iteration 173 loss: 0.004828487057238817, ACC:1.0\n",
      "Training iteration 174 loss: 0.002977523021399975, ACC:1.0\n",
      "Training iteration 175 loss: 0.008654332719743252, ACC:1.0\n",
      "Training iteration 176 loss: 0.014772456139326096, ACC:1.0\n",
      "Training iteration 177 loss: 0.0367492139339447, ACC:0.984375\n",
      "Training iteration 178 loss: 0.004023594316095114, ACC:1.0\n",
      "Training iteration 179 loss: 0.010228656232357025, ACC:1.0\n",
      "Training iteration 180 loss: 0.011118327267467976, ACC:1.0\n",
      "Training iteration 181 loss: 0.0049195666797459126, ACC:1.0\n",
      "Training iteration 182 loss: 0.0054852282628417015, ACC:1.0\n",
      "Training iteration 183 loss: 0.003377833403646946, ACC:1.0\n",
      "Training iteration 184 loss: 0.0008210279047489166, ACC:1.0\n",
      "Training iteration 185 loss: 0.00029246104531921446, ACC:1.0\n",
      "Training iteration 186 loss: 0.007408058736473322, ACC:1.0\n",
      "Training iteration 187 loss: 0.008929646573960781, ACC:1.0\n",
      "Training iteration 188 loss: 0.047985661774873734, ACC:0.984375\n",
      "Training iteration 189 loss: 0.02756456471979618, ACC:0.984375\n",
      "Training iteration 190 loss: 0.006991129368543625, ACC:1.0\n",
      "Training iteration 191 loss: 0.0004905213718302548, ACC:1.0\n",
      "Training iteration 192 loss: 0.16644179821014404, ACC:0.953125\n",
      "Training iteration 193 loss: 0.039718035608530045, ACC:0.984375\n",
      "Training iteration 194 loss: 0.01332814246416092, ACC:0.984375\n",
      "Training iteration 195 loss: 0.002050737151876092, ACC:1.0\n",
      "Training iteration 196 loss: 0.0027416956145316362, ACC:1.0\n",
      "Training iteration 197 loss: 0.01127225998789072, ACC:1.0\n",
      "Training iteration 198 loss: 0.007373308762907982, ACC:1.0\n",
      "Training iteration 199 loss: 0.007277831435203552, ACC:1.0\n",
      "Training iteration 200 loss: 0.0030165836215019226, ACC:1.0\n",
      "Training iteration 201 loss: 0.007599379867315292, ACC:1.0\n",
      "Training iteration 202 loss: 0.06519347429275513, ACC:0.96875\n",
      "Training iteration 203 loss: 0.003866663435474038, ACC:1.0\n",
      "Training iteration 204 loss: 0.013225860893726349, ACC:1.0\n",
      "Training iteration 205 loss: 0.052895378321409225, ACC:0.984375\n",
      "Training iteration 206 loss: 0.008720672689378262, ACC:1.0\n",
      "Training iteration 207 loss: 0.012174210511147976, ACC:1.0\n",
      "Training iteration 208 loss: 0.010457352735102177, ACC:1.0\n",
      "Training iteration 209 loss: 0.00016208489250857383, ACC:1.0\n",
      "Training iteration 210 loss: 0.032900042831897736, ACC:0.984375\n",
      "Training iteration 211 loss: 0.01927575282752514, ACC:1.0\n",
      "Training iteration 212 loss: 0.06748218089342117, ACC:0.984375\n",
      "Training iteration 213 loss: 0.0006735161296091974, ACC:1.0\n",
      "Training iteration 214 loss: 0.042664844542741776, ACC:0.984375\n",
      "Training iteration 215 loss: 0.05073421820998192, ACC:0.984375\n",
      "Training iteration 216 loss: 0.0007635937654413283, ACC:1.0\n",
      "Training iteration 217 loss: 0.033335667103528976, ACC:0.984375\n",
      "Training iteration 218 loss: 0.0887954905629158, ACC:0.984375\n",
      "Training iteration 219 loss: 0.0024781429674476385, ACC:1.0\n",
      "Training iteration 220 loss: 0.06730294972658157, ACC:0.984375\n",
      "Training iteration 221 loss: 0.017561616376042366, ACC:0.984375\n",
      "Training iteration 222 loss: 0.006564128678292036, ACC:1.0\n",
      "Training iteration 223 loss: 0.00697671715170145, ACC:1.0\n",
      "Training iteration 224 loss: 0.0016541748773306608, ACC:1.0\n",
      "Training iteration 225 loss: 0.0030081025324761868, ACC:1.0\n",
      "Training iteration 226 loss: 0.0011579415295273066, ACC:1.0\n",
      "Training iteration 227 loss: 0.002764512784779072, ACC:1.0\n",
      "Training iteration 228 loss: 0.11671925336122513, ACC:0.984375\n",
      "Training iteration 229 loss: 0.01799059845507145, ACC:0.984375\n",
      "Training iteration 230 loss: 0.006149718537926674, ACC:1.0\n",
      "Training iteration 231 loss: 0.0024467252660542727, ACC:1.0\n",
      "Training iteration 232 loss: 0.0059062279760837555, ACC:1.0\n",
      "Training iteration 233 loss: 0.001100655528716743, ACC:1.0\n",
      "Training iteration 234 loss: 0.04853552579879761, ACC:0.984375\n",
      "Training iteration 235 loss: 0.01125816535204649, ACC:1.0\n",
      "Training iteration 236 loss: 0.0035478139761835337, ACC:1.0\n",
      "Training iteration 237 loss: 0.011453780345618725, ACC:1.0\n",
      "Training iteration 238 loss: 0.0026330507826060057, ACC:1.0\n",
      "Training iteration 239 loss: 0.0006916537531651556, ACC:1.0\n",
      "Training iteration 240 loss: 0.0057166279293596745, ACC:1.0\n",
      "Training iteration 241 loss: 0.009530853480100632, ACC:1.0\n",
      "Training iteration 242 loss: 0.0031359451822936535, ACC:1.0\n",
      "Training iteration 243 loss: 0.0010435597505420446, ACC:1.0\n",
      "Training iteration 244 loss: 0.06208035722374916, ACC:0.984375\n",
      "Training iteration 245 loss: 0.003716682083904743, ACC:1.0\n",
      "Training iteration 246 loss: 0.013006879016757011, ACC:1.0\n",
      "Training iteration 247 loss: 0.11398066580295563, ACC:0.96875\n",
      "Training iteration 248 loss: 0.03726213425397873, ACC:0.984375\n",
      "Training iteration 249 loss: 0.06141744554042816, ACC:0.96875\n",
      "Training iteration 250 loss: 0.02618490345776081, ACC:0.984375\n",
      "Training iteration 251 loss: 0.007514967583119869, ACC:1.0\n",
      "Training iteration 252 loss: 0.057497814297676086, ACC:0.984375\n",
      "Training iteration 253 loss: 0.012339008040726185, ACC:1.0\n",
      "Training iteration 254 loss: 0.03272026777267456, ACC:0.984375\n",
      "Training iteration 255 loss: 0.004318317864090204, ACC:1.0\n",
      "Training iteration 256 loss: 0.00851911585777998, ACC:1.0\n",
      "Training iteration 257 loss: 0.04633823782205582, ACC:0.984375\n",
      "Training iteration 258 loss: 0.0022833680268377066, ACC:1.0\n",
      "Training iteration 259 loss: 0.007171869743615389, ACC:1.0\n",
      "Training iteration 260 loss: 0.0837908536195755, ACC:0.984375\n",
      "Training iteration 261 loss: 0.0031346215400844812, ACC:1.0\n",
      "Training iteration 262 loss: 0.03177006170153618, ACC:0.984375\n",
      "Training iteration 263 loss: 0.0027901874855160713, ACC:1.0\n",
      "Training iteration 264 loss: 0.002687067724764347, ACC:1.0\n",
      "Training iteration 265 loss: 0.007229323498904705, ACC:1.0\n",
      "Training iteration 266 loss: 0.015407860279083252, ACC:0.984375\n",
      "Training iteration 267 loss: 0.025872906669974327, ACC:0.984375\n",
      "Training iteration 268 loss: 0.005684445146471262, ACC:1.0\n",
      "Training iteration 269 loss: 0.031106067821383476, ACC:0.984375\n",
      "Training iteration 270 loss: 0.011684894561767578, ACC:1.0\n",
      "Training iteration 271 loss: 0.006296544335782528, ACC:1.0\n",
      "Training iteration 272 loss: 0.033784542232751846, ACC:0.984375\n",
      "Training iteration 273 loss: 0.010523511096835136, ACC:1.0\n",
      "Training iteration 274 loss: 0.00538394832983613, ACC:1.0\n",
      "Training iteration 275 loss: 0.04542833939194679, ACC:0.984375\n",
      "Training iteration 276 loss: 0.002634804928675294, ACC:1.0\n",
      "Training iteration 277 loss: 0.0013981674565002322, ACC:1.0\n",
      "Training iteration 278 loss: 0.0029718151781708, ACC:1.0\n",
      "Training iteration 279 loss: 0.0044897329062223434, ACC:1.0\n",
      "Training iteration 280 loss: 0.01961805671453476, ACC:0.984375\n",
      "Training iteration 281 loss: 0.01693332940340042, ACC:0.984375\n",
      "Training iteration 282 loss: 0.019596971571445465, ACC:0.984375\n",
      "Training iteration 283 loss: 0.033760979771614075, ACC:0.96875\n",
      "Training iteration 284 loss: 0.027872085571289062, ACC:0.984375\n",
      "Training iteration 285 loss: 0.0016892080893740058, ACC:1.0\n",
      "Training iteration 286 loss: 0.03628060594201088, ACC:0.984375\n",
      "Training iteration 287 loss: 0.0013510545250028372, ACC:1.0\n",
      "Training iteration 288 loss: 0.0018818364478647709, ACC:1.0\n",
      "Training iteration 289 loss: 0.001934658968821168, ACC:1.0\n",
      "Training iteration 290 loss: 0.01838953234255314, ACC:0.984375\n",
      "Training iteration 291 loss: 0.061500433832407, ACC:0.984375\n",
      "Training iteration 292 loss: 0.001366487005725503, ACC:1.0\n",
      "Training iteration 293 loss: 0.0010249739279970527, ACC:1.0\n",
      "Training iteration 294 loss: 0.04035591706633568, ACC:0.984375\n",
      "Training iteration 295 loss: 0.014037542976439, ACC:1.0\n",
      "Training iteration 296 loss: 0.050118543207645416, ACC:0.984375\n",
      "Training iteration 297 loss: 0.027913333848118782, ACC:0.984375\n",
      "Training iteration 298 loss: 0.01397786010056734, ACC:1.0\n",
      "Training iteration 299 loss: 0.025130745023489, ACC:0.984375\n",
      "Training iteration 300 loss: 0.11145009100437164, ACC:0.96875\n",
      "Training iteration 301 loss: 0.0062269363552331924, ACC:1.0\n",
      "Training iteration 302 loss: 0.0075063942931592464, ACC:1.0\n",
      "Training iteration 303 loss: 0.028982331976294518, ACC:0.984375\n",
      "Training iteration 304 loss: 0.006533103529363871, ACC:1.0\n",
      "Training iteration 305 loss: 0.01814046874642372, ACC:0.984375\n",
      "Training iteration 306 loss: 0.008606970310211182, ACC:1.0\n",
      "Training iteration 307 loss: 0.002913923002779484, ACC:1.0\n",
      "Training iteration 308 loss: 0.0964621901512146, ACC:0.96875\n",
      "Training iteration 309 loss: 0.05554822459816933, ACC:0.984375\n",
      "Training iteration 310 loss: 0.012330135330557823, ACC:1.0\n",
      "Training iteration 311 loss: 0.01909523457288742, ACC:1.0\n",
      "Training iteration 312 loss: 0.023262489587068558, ACC:1.0\n",
      "Training iteration 313 loss: 0.00996880792081356, ACC:1.0\n",
      "Training iteration 314 loss: 0.07683326303958893, ACC:0.96875\n",
      "Training iteration 315 loss: 0.0480712354183197, ACC:0.984375\n",
      "Training iteration 316 loss: 0.038442306220531464, ACC:0.984375\n",
      "Training iteration 317 loss: 0.03853857144713402, ACC:0.984375\n",
      "Training iteration 318 loss: 0.0004581682151183486, ACC:1.0\n",
      "Training iteration 319 loss: 0.0332694910466671, ACC:0.96875\n",
      "Training iteration 320 loss: 0.005580127239227295, ACC:1.0\n",
      "Training iteration 321 loss: 0.004152686335146427, ACC:1.0\n",
      "Training iteration 322 loss: 7.35422145226039e-05, ACC:1.0\n",
      "Training iteration 323 loss: 0.02213871292769909, ACC:0.984375\n",
      "Training iteration 324 loss: 0.00022669596364721656, ACC:1.0\n",
      "Training iteration 325 loss: 0.07189386337995529, ACC:0.96875\n",
      "Training iteration 326 loss: 0.0001812735863495618, ACC:1.0\n",
      "Training iteration 327 loss: 0.0003438715939410031, ACC:1.0\n",
      "Training iteration 328 loss: 0.0009954178240150213, ACC:1.0\n",
      "Training iteration 329 loss: 0.0030370291788131, ACC:1.0\n",
      "Training iteration 330 loss: 0.0015236245235428214, ACC:1.0\n",
      "Training iteration 331 loss: 0.1384318321943283, ACC:0.921875\n",
      "Training iteration 332 loss: 0.0090658999979496, ACC:1.0\n",
      "Training iteration 333 loss: 0.007553723640739918, ACC:1.0\n",
      "Training iteration 334 loss: 0.03649164363741875, ACC:0.96875\n",
      "Training iteration 335 loss: 0.0012532520340755582, ACC:1.0\n",
      "Training iteration 336 loss: 0.11463510990142822, ACC:0.96875\n",
      "Training iteration 337 loss: 0.180730402469635, ACC:0.96875\n",
      "Training iteration 338 loss: 0.11956219375133514, ACC:0.9375\n",
      "Training iteration 339 loss: 0.0043717967346310616, ACC:1.0\n",
      "Training iteration 340 loss: 0.017930487170815468, ACC:1.0\n",
      "Training iteration 341 loss: 0.044553112238645554, ACC:0.984375\n",
      "Training iteration 342 loss: 0.1270456314086914, ACC:0.96875\n",
      "Training iteration 343 loss: 0.052261222153902054, ACC:0.984375\n",
      "Training iteration 344 loss: 0.07978787273168564, ACC:0.953125\n",
      "Training iteration 345 loss: 0.011978104710578918, ACC:1.0\n",
      "Training iteration 346 loss: 0.014369608834385872, ACC:1.0\n",
      "Training iteration 347 loss: 0.003932521678507328, ACC:1.0\n",
      "Training iteration 348 loss: 0.00796470046043396, ACC:1.0\n",
      "Training iteration 349 loss: 0.0010017891181632876, ACC:1.0\n",
      "Training iteration 350 loss: 0.006464905571192503, ACC:1.0\n",
      "Training iteration 351 loss: 0.002607078291475773, ACC:1.0\n",
      "Training iteration 352 loss: 0.0009433169034309685, ACC:1.0\n",
      "Training iteration 353 loss: 0.07088781893253326, ACC:0.984375\n",
      "Training iteration 354 loss: 0.0013183305272832513, ACC:1.0\n",
      "Training iteration 355 loss: 0.007728117518126965, ACC:1.0\n",
      "Training iteration 356 loss: 0.0011389835271984339, ACC:1.0\n",
      "Training iteration 357 loss: 0.004693023394793272, ACC:1.0\n",
      "Training iteration 358 loss: 0.05089699476957321, ACC:0.984375\n",
      "Training iteration 359 loss: 0.0029049059376120567, ACC:1.0\n",
      "Training iteration 360 loss: 0.0005892073968425393, ACC:1.0\n",
      "Training iteration 361 loss: 0.0032943030819296837, ACC:1.0\n",
      "Training iteration 362 loss: 0.15853843092918396, ACC:0.96875\n",
      "Training iteration 363 loss: 0.00630194740369916, ACC:1.0\n",
      "Training iteration 364 loss: 0.027278171852231026, ACC:0.984375\n",
      "Training iteration 365 loss: 0.0024122470058500767, ACC:1.0\n",
      "Training iteration 366 loss: 0.22636599838733673, ACC:0.953125\n",
      "Training iteration 367 loss: 0.0007780560408718884, ACC:1.0\n",
      "Training iteration 368 loss: 0.0016761516453698277, ACC:1.0\n",
      "Training iteration 369 loss: 0.004076594021171331, ACC:1.0\n",
      "Training iteration 370 loss: 0.071839839220047, ACC:0.984375\n",
      "Training iteration 371 loss: 0.09294766932725906, ACC:0.984375\n",
      "Training iteration 372 loss: 0.009886152110993862, ACC:1.0\n",
      "Training iteration 373 loss: 0.1250104457139969, ACC:0.96875\n",
      "Training iteration 374 loss: 0.01300201565027237, ACC:1.0\n",
      "Training iteration 375 loss: 0.06875058263540268, ACC:0.984375\n",
      "Training iteration 376 loss: 0.006513926200568676, ACC:1.0\n",
      "Training iteration 377 loss: 0.05123588442802429, ACC:0.984375\n",
      "Training iteration 378 loss: 0.011192545294761658, ACC:1.0\n",
      "Training iteration 379 loss: 0.017748447135090828, ACC:1.0\n",
      "Training iteration 380 loss: 0.02978065051138401, ACC:0.984375\n",
      "Training iteration 381 loss: 0.00621897354722023, ACC:1.0\n",
      "Training iteration 382 loss: 0.04581755772233009, ACC:0.984375\n",
      "Training iteration 383 loss: 0.005169917829334736, ACC:1.0\n",
      "Training iteration 384 loss: 0.013086062856018543, ACC:1.0\n",
      "Training iteration 385 loss: 0.0038278650026768446, ACC:1.0\n",
      "Training iteration 386 loss: 0.03394085168838501, ACC:0.984375\n",
      "Training iteration 387 loss: 0.0035449780989438295, ACC:1.0\n",
      "Training iteration 388 loss: 0.026673411950469017, ACC:0.984375\n",
      "Training iteration 389 loss: 0.04430706426501274, ACC:0.984375\n",
      "Training iteration 390 loss: 0.0027807774022221565, ACC:1.0\n",
      "Training iteration 391 loss: 0.003721165005117655, ACC:1.0\n",
      "Training iteration 392 loss: 0.009472712874412537, ACC:1.0\n",
      "Training iteration 393 loss: 0.013497471809387207, ACC:1.0\n",
      "Training iteration 394 loss: 0.008864952251315117, ACC:1.0\n",
      "Training iteration 395 loss: 0.05333863943815231, ACC:0.96875\n",
      "Training iteration 396 loss: 0.008846547454595566, ACC:1.0\n",
      "Training iteration 397 loss: 0.006752674002200365, ACC:1.0\n",
      "Training iteration 398 loss: 0.007970373146235943, ACC:1.0\n",
      "Training iteration 399 loss: 0.045600853860378265, ACC:0.984375\n",
      "Training iteration 400 loss: 0.024447960779070854, ACC:0.984375\n",
      "Training iteration 401 loss: 0.0018703597597777843, ACC:1.0\n",
      "Training iteration 402 loss: 0.0017942518461495638, ACC:1.0\n",
      "Training iteration 403 loss: 0.0003899402800016105, ACC:1.0\n",
      "Training iteration 404 loss: 0.00031275631044991314, ACC:1.0\n",
      "Training iteration 405 loss: 0.0781676396727562, ACC:0.96875\n",
      "Training iteration 406 loss: 0.0012824368895962834, ACC:1.0\n",
      "Training iteration 407 loss: 0.04416457936167717, ACC:0.984375\n",
      "Training iteration 408 loss: 0.09328116476535797, ACC:0.96875\n",
      "Training iteration 409 loss: 0.0010632715420797467, ACC:1.0\n",
      "Training iteration 410 loss: 0.09792196750640869, ACC:0.96875\n",
      "Training iteration 411 loss: 0.0015636219177395105, ACC:1.0\n",
      "Training iteration 412 loss: 0.004910788498818874, ACC:1.0\n",
      "Training iteration 413 loss: 0.0010828576050698757, ACC:1.0\n",
      "Training iteration 414 loss: 0.0110525107011199, ACC:1.0\n",
      "Training iteration 415 loss: 0.004045263864099979, ACC:1.0\n",
      "Training iteration 416 loss: 0.03882122039794922, ACC:0.984375\n",
      "Training iteration 417 loss: 0.001128053991124034, ACC:1.0\n",
      "Training iteration 418 loss: 0.011227477341890335, ACC:1.0\n",
      "Training iteration 419 loss: 0.004603471606969833, ACC:1.0\n",
      "Training iteration 420 loss: 0.03550436720252037, ACC:0.984375\n",
      "Training iteration 421 loss: 0.2068127989768982, ACC:0.984375\n",
      "Training iteration 422 loss: 0.11769673228263855, ACC:0.96875\n",
      "Training iteration 423 loss: 0.005558188538998365, ACC:1.0\n",
      "Training iteration 424 loss: 0.06383637338876724, ACC:0.984375\n",
      "Training iteration 425 loss: 0.0014336403692141175, ACC:1.0\n",
      "Training iteration 426 loss: 0.0005120894056744874, ACC:1.0\n",
      "Training iteration 427 loss: 0.09569908678531647, ACC:0.96875\n",
      "Training iteration 428 loss: 0.03555425629019737, ACC:0.984375\n",
      "Training iteration 429 loss: 0.03387904912233353, ACC:0.984375\n",
      "Training iteration 430 loss: 0.03180432319641113, ACC:0.984375\n",
      "Training iteration 431 loss: 0.04795636236667633, ACC:0.984375\n",
      "Training iteration 432 loss: 0.014892732724547386, ACC:0.984375\n",
      "Training iteration 433 loss: 0.011675642803311348, ACC:1.0\n",
      "Training iteration 434 loss: 0.009083803743124008, ACC:1.0\n",
      "Training iteration 435 loss: 0.015505109913647175, ACC:1.0\n",
      "Training iteration 436 loss: 0.01552623137831688, ACC:1.0\n",
      "Training iteration 437 loss: 0.04725290834903717, ACC:0.96875\n",
      "Training iteration 438 loss: 0.0042862193658947945, ACC:1.0\n",
      "Training iteration 439 loss: 0.0007940363720990717, ACC:1.0\n",
      "Training iteration 440 loss: 0.009623738937079906, ACC:1.0\n",
      "Training iteration 441 loss: 0.14295601844787598, ACC:0.96875\n",
      "Training iteration 442 loss: 0.021896807476878166, ACC:0.984375\n",
      "Training iteration 443 loss: 0.005406291224062443, ACC:1.0\n",
      "Training iteration 444 loss: 0.034392934292554855, ACC:0.96875\n",
      "Training iteration 445 loss: 0.023961948230862617, ACC:0.984375\n",
      "Training iteration 446 loss: 0.060187116265296936, ACC:0.96875\n",
      "Training iteration 447 loss: 0.002811043756082654, ACC:1.0\n",
      "Training iteration 448 loss: 0.011875193566083908, ACC:1.0\n",
      "Training iteration 449 loss: 0.0032388323452323675, ACC:1.0\n",
      "Training iteration 450 loss: 0.002605025889351964, ACC:1.0\n",
      "Validation iteration 451 loss: 0.0027707689441740513, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.014274943619966507, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.029718635603785515, ACC: 0.984375\n",
      "Validation iteration 454 loss: 0.029137635603547096, ACC: 0.984375\n",
      "Validation iteration 455 loss: 0.0010496677132323384, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.02882646583020687, ACC: 0.984375\n",
      "Validation iteration 457 loss: 0.007497875485569239, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.03189685940742493, ACC: 0.984375\n",
      "Validation iteration 459 loss: 0.0312727726995945, ACC: 0.984375\n",
      "Validation iteration 460 loss: 0.007760241162031889, ACC: 1.0\n",
      "Validation iteration 461 loss: 0.016716735437512398, ACC: 0.984375\n",
      "Validation iteration 462 loss: 0.00693476852029562, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.042496200650930405, ACC: 0.984375\n",
      "Validation iteration 464 loss: 0.0017826546682044864, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.00652915146201849, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.004376597702503204, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.04544637352228165, ACC: 0.984375\n",
      "Validation iteration 468 loss: 0.01904410682618618, ACC: 0.984375\n",
      "Validation iteration 469 loss: 0.03314957022666931, ACC: 0.96875\n",
      "Validation iteration 470 loss: 0.008138778619468212, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.021926362067461014, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.02388962358236313, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.029776375740766525, ACC: 0.96875\n",
      "Validation iteration 474 loss: 0.012268461287021637, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.006149421911686659, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.02655274234712124, ACC: 0.984375\n",
      "Validation iteration 477 loss: 0.014283871278166771, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.020534604787826538, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.06491389870643616, ACC: 0.96875\n",
      "Validation iteration 480 loss: 0.028047999367117882, ACC: 0.984375\n",
      "Validation iteration 481 loss: 0.04439213499426842, ACC: 0.984375\n",
      "Validation iteration 482 loss: 0.030114542692899704, ACC: 0.984375\n",
      "Validation iteration 483 loss: 0.007165857590734959, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.03684702888131142, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.008154400624334812, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.02043880522251129, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.006765637546777725, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.1320464313030243, ACC: 0.953125\n",
      "Validation iteration 489 loss: 0.004054378718137741, ACC: 1.0\n",
      "Validation iteration 490 loss: 0.004187700338661671, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.008437576703727245, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.0020300729665905237, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.03522694483399391, ACC: 0.984375\n",
      "Validation iteration 494 loss: 0.02215174399316311, ACC: 0.984375\n",
      "Validation iteration 495 loss: 0.0028263370040804148, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.0032512813340872526, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.004283412825316191, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.046753738075494766, ACC: 0.984375\n",
      "Validation iteration 499 loss: 0.003131482983008027, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.01959221251308918, ACC: 0.984375\n",
      "-- Epoch 3 done -- Train loss: 0.027567726014256347, train ACC: 0.9908333333333333, val loss: 0.021180317718535662, val ACC: 0.990625\n",
      "<--- 9788.260395288467 seconds --->\n",
      "Training iteration 1 loss: 0.009814008139073849, ACC:1.0\n",
      "Training iteration 2 loss: 0.010581986978650093, ACC:1.0\n",
      "Training iteration 3 loss: 0.018081041052937508, ACC:0.984375\n",
      "Training iteration 4 loss: 0.005114879459142685, ACC:1.0\n",
      "Training iteration 5 loss: 0.049917809665203094, ACC:0.96875\n",
      "Training iteration 6 loss: 0.06419163942337036, ACC:0.96875\n",
      "Training iteration 7 loss: 0.039112113416194916, ACC:0.984375\n",
      "Training iteration 8 loss: 0.00924456026405096, ACC:1.0\n",
      "Training iteration 9 loss: 0.004896722733974457, ACC:1.0\n",
      "Training iteration 10 loss: 0.01187550462782383, ACC:1.0\n",
      "Training iteration 11 loss: 0.005907789338380098, ACC:1.0\n",
      "Training iteration 12 loss: 0.009019918739795685, ACC:1.0\n",
      "Training iteration 13 loss: 0.0036908502224832773, ACC:1.0\n",
      "Training iteration 14 loss: 0.00017985195154324174, ACC:1.0\n",
      "Training iteration 15 loss: 0.047070275992155075, ACC:0.984375\n",
      "Training iteration 16 loss: 0.0010629119351506233, ACC:1.0\n",
      "Training iteration 17 loss: 0.0016341129085049033, ACC:1.0\n",
      "Training iteration 18 loss: 0.05559910088777542, ACC:0.984375\n",
      "Training iteration 19 loss: 0.006273706443607807, ACC:1.0\n",
      "Training iteration 20 loss: 0.12065622955560684, ACC:0.984375\n",
      "Training iteration 21 loss: 0.051381900906562805, ACC:0.984375\n",
      "Training iteration 22 loss: 0.07190468162298203, ACC:0.984375\n",
      "Training iteration 23 loss: 0.0025436854921281338, ACC:1.0\n",
      "Training iteration 24 loss: 0.0037279892712831497, ACC:1.0\n",
      "Training iteration 25 loss: 0.0014583209995180368, ACC:1.0\n",
      "Training iteration 26 loss: 0.003165946342051029, ACC:1.0\n",
      "Training iteration 27 loss: 0.004294986370950937, ACC:1.0\n",
      "Training iteration 28 loss: 0.03708162531256676, ACC:0.984375\n",
      "Training iteration 29 loss: 0.002485275734215975, ACC:1.0\n",
      "Training iteration 30 loss: 0.022882981225848198, ACC:0.984375\n",
      "Training iteration 31 loss: 0.019498493522405624, ACC:1.0\n",
      "Training iteration 32 loss: 0.054121870547533035, ACC:0.984375\n",
      "Training iteration 33 loss: 0.005759777035564184, ACC:1.0\n",
      "Training iteration 34 loss: 0.04708457738161087, ACC:0.984375\n",
      "Training iteration 35 loss: 0.0005417432403191924, ACC:1.0\n",
      "Training iteration 36 loss: 0.0008427304564975202, ACC:1.0\n",
      "Training iteration 37 loss: 0.026118921115994453, ACC:0.984375\n",
      "Training iteration 38 loss: 0.08969485759735107, ACC:0.984375\n",
      "Training iteration 39 loss: 0.07308884710073471, ACC:0.984375\n",
      "Training iteration 40 loss: 0.04364549741148949, ACC:0.984375\n",
      "Training iteration 41 loss: 0.008449606597423553, ACC:1.0\n",
      "Training iteration 42 loss: 0.0008197627030313015, ACC:1.0\n",
      "Training iteration 43 loss: 0.0004400454636197537, ACC:1.0\n",
      "Training iteration 44 loss: 0.030279938131570816, ACC:0.984375\n",
      "Training iteration 45 loss: 0.005323323421180248, ACC:1.0\n",
      "Training iteration 46 loss: 0.0024004620499908924, ACC:1.0\n",
      "Training iteration 47 loss: 0.0030604470521211624, ACC:1.0\n",
      "Training iteration 48 loss: 0.011896423064172268, ACC:1.0\n",
      "Training iteration 49 loss: 0.1448989361524582, ACC:0.984375\n",
      "Training iteration 50 loss: 0.0032291903626173735, ACC:1.0\n",
      "Training iteration 51 loss: 0.005911378189921379, ACC:1.0\n",
      "Training iteration 52 loss: 0.0012585397344082594, ACC:1.0\n",
      "Training iteration 53 loss: 0.0016427150694653392, ACC:1.0\n",
      "Training iteration 54 loss: 0.015224107541143894, ACC:0.984375\n",
      "Training iteration 55 loss: 0.019269617274403572, ACC:0.984375\n",
      "Training iteration 56 loss: 0.00032811681739985943, ACC:1.0\n",
      "Training iteration 57 loss: 0.07808050513267517, ACC:0.96875\n",
      "Training iteration 58 loss: 0.0011181868612766266, ACC:1.0\n",
      "Training iteration 59 loss: 0.0016048140823841095, ACC:1.0\n",
      "Training iteration 60 loss: 0.005269776098430157, ACC:1.0\n",
      "Training iteration 61 loss: 0.03827905282378197, ACC:0.984375\n",
      "Training iteration 62 loss: 0.00039353116881102324, ACC:1.0\n",
      "Training iteration 63 loss: 0.006480803247541189, ACC:1.0\n",
      "Training iteration 64 loss: 0.08834389597177505, ACC:0.984375\n",
      "Training iteration 65 loss: 0.001370732206851244, ACC:1.0\n",
      "Training iteration 66 loss: 0.0007051998400129378, ACC:1.0\n",
      "Training iteration 67 loss: 0.0004738176066894084, ACC:1.0\n",
      "Training iteration 68 loss: 0.030604703351855278, ACC:0.984375\n",
      "Training iteration 69 loss: 0.17494690418243408, ACC:0.953125\n",
      "Training iteration 70 loss: 0.001792967552319169, ACC:1.0\n",
      "Training iteration 71 loss: 0.005175831727683544, ACC:1.0\n",
      "Training iteration 72 loss: 0.06600832939147949, ACC:0.984375\n",
      "Training iteration 73 loss: 0.008797988295555115, ACC:1.0\n",
      "Training iteration 74 loss: 0.011543245986104012, ACC:1.0\n",
      "Training iteration 75 loss: 0.02171698957681656, ACC:1.0\n",
      "Training iteration 76 loss: 0.028682423755526543, ACC:0.984375\n",
      "Training iteration 77 loss: 0.0064946310594677925, ACC:1.0\n",
      "Training iteration 78 loss: 0.002049081027507782, ACC:1.0\n",
      "Training iteration 79 loss: 0.024905936792492867, ACC:0.984375\n",
      "Training iteration 80 loss: 0.0010831911349669099, ACC:1.0\n",
      "Training iteration 81 loss: 0.0021197793539613485, ACC:1.0\n",
      "Training iteration 82 loss: 0.03217027708888054, ACC:0.984375\n",
      "Training iteration 83 loss: 0.0010220211697742343, ACC:1.0\n",
      "Training iteration 84 loss: 0.006295889150351286, ACC:1.0\n",
      "Training iteration 85 loss: 0.002349515212699771, ACC:1.0\n",
      "Training iteration 86 loss: 0.1261339634656906, ACC:0.984375\n",
      "Training iteration 87 loss: 0.0004840936162509024, ACC:1.0\n",
      "Training iteration 88 loss: 0.026955246925354004, ACC:0.984375\n",
      "Training iteration 89 loss: 0.000946756626944989, ACC:1.0\n",
      "Training iteration 90 loss: 0.02127859927713871, ACC:0.984375\n",
      "Training iteration 91 loss: 0.12945136427879333, ACC:0.984375\n",
      "Training iteration 92 loss: 0.08140307664871216, ACC:0.96875\n",
      "Training iteration 93 loss: 0.027154909446835518, ACC:0.984375\n",
      "Training iteration 94 loss: 0.03977913036942482, ACC:0.984375\n",
      "Training iteration 95 loss: 0.008112041279673576, ACC:1.0\n",
      "Training iteration 96 loss: 0.007252446375787258, ACC:1.0\n",
      "Training iteration 97 loss: 0.006559685803949833, ACC:1.0\n",
      "Training iteration 98 loss: 0.03510098159313202, ACC:1.0\n",
      "Training iteration 99 loss: 0.024421125650405884, ACC:1.0\n",
      "Training iteration 100 loss: 0.03686773031949997, ACC:0.984375\n",
      "Training iteration 101 loss: 0.006917754653841257, ACC:1.0\n",
      "Training iteration 102 loss: 0.00237776106223464, ACC:1.0\n",
      "Training iteration 103 loss: 0.008873957209289074, ACC:1.0\n",
      "Training iteration 104 loss: 0.0004981823731213808, ACC:1.0\n",
      "Training iteration 105 loss: 0.00040769277256913483, ACC:1.0\n",
      "Training iteration 106 loss: 0.08320414274930954, ACC:0.984375\n",
      "Training iteration 107 loss: 0.0018093534745275974, ACC:1.0\n",
      "Training iteration 108 loss: 0.002526654629036784, ACC:1.0\n",
      "Training iteration 109 loss: 0.0005164280300959945, ACC:1.0\n",
      "Training iteration 110 loss: 0.0022475472651422024, ACC:1.0\n",
      "Training iteration 111 loss: 0.00755118066444993, ACC:1.0\n",
      "Training iteration 112 loss: 0.001587436068803072, ACC:1.0\n",
      "Training iteration 113 loss: 0.00015406897000502795, ACC:1.0\n",
      "Training iteration 114 loss: 0.0011329075787216425, ACC:1.0\n",
      "Training iteration 115 loss: 0.04561346769332886, ACC:0.96875\n",
      "Training iteration 116 loss: 0.03463129699230194, ACC:0.984375\n",
      "Training iteration 117 loss: 0.11437678337097168, ACC:0.984375\n",
      "Training iteration 118 loss: 0.003943261224776506, ACC:1.0\n",
      "Training iteration 119 loss: 0.024435756728053093, ACC:0.96875\n",
      "Training iteration 120 loss: 0.02085876278579235, ACC:0.984375\n",
      "Training iteration 121 loss: 0.0015174990985542536, ACC:1.0\n",
      "Training iteration 122 loss: 0.0006916029960848391, ACC:1.0\n",
      "Training iteration 123 loss: 0.02143794298171997, ACC:0.984375\n",
      "Training iteration 124 loss: 0.004997070878744125, ACC:1.0\n",
      "Training iteration 125 loss: 0.005459904205054045, ACC:1.0\n",
      "Training iteration 126 loss: 0.00019782359595410526, ACC:1.0\n",
      "Training iteration 127 loss: 0.014107069931924343, ACC:0.984375\n",
      "Training iteration 128 loss: 0.006205330602824688, ACC:1.0\n",
      "Training iteration 129 loss: 0.1285771131515503, ACC:0.984375\n",
      "Training iteration 130 loss: 0.002291516400873661, ACC:1.0\n",
      "Training iteration 131 loss: 0.0016744510503485799, ACC:1.0\n",
      "Training iteration 132 loss: 0.00023981732374522835, ACC:1.0\n",
      "Training iteration 133 loss: 0.00022848417575005442, ACC:1.0\n",
      "Training iteration 134 loss: 0.00036817355430684984, ACC:1.0\n",
      "Training iteration 135 loss: 0.006044910289347172, ACC:1.0\n",
      "Training iteration 136 loss: 0.0002436370268696919, ACC:1.0\n",
      "Training iteration 137 loss: 0.009347274899482727, ACC:1.0\n",
      "Training iteration 138 loss: 0.00013233778008725494, ACC:1.0\n",
      "Training iteration 139 loss: 0.04385899752378464, ACC:0.96875\n",
      "Training iteration 140 loss: 0.0010622137924656272, ACC:1.0\n",
      "Training iteration 141 loss: 0.0012874483363702893, ACC:1.0\n",
      "Training iteration 142 loss: 0.0007025005179457366, ACC:1.0\n",
      "Training iteration 143 loss: 0.004708041436970234, ACC:1.0\n",
      "Training iteration 144 loss: 0.004664620850235224, ACC:1.0\n",
      "Training iteration 145 loss: 0.008093168027698994, ACC:1.0\n",
      "Training iteration 146 loss: 0.003587127663195133, ACC:1.0\n",
      "Training iteration 147 loss: 0.0005750650307163596, ACC:1.0\n",
      "Training iteration 148 loss: 0.21255846321582794, ACC:0.96875\n",
      "Training iteration 149 loss: 0.00042088382178917527, ACC:1.0\n",
      "Training iteration 150 loss: 0.011005246080458164, ACC:1.0\n",
      "Training iteration 151 loss: 0.01863344945013523, ACC:0.984375\n",
      "Training iteration 152 loss: 0.11612757295370102, ACC:0.96875\n",
      "Training iteration 153 loss: 0.02058175764977932, ACC:0.984375\n",
      "Training iteration 154 loss: 0.0017478747759014368, ACC:1.0\n",
      "Training iteration 155 loss: 0.01172423642128706, ACC:0.984375\n",
      "Training iteration 156 loss: 0.0042766788974404335, ACC:1.0\n",
      "Training iteration 157 loss: 0.0007004474755376577, ACC:1.0\n",
      "Training iteration 158 loss: 0.015913071110844612, ACC:0.984375\n",
      "Training iteration 159 loss: 0.036760713905096054, ACC:0.984375\n",
      "Training iteration 160 loss: 0.027827655896544456, ACC:1.0\n",
      "Training iteration 161 loss: 0.01923036016523838, ACC:0.984375\n",
      "Training iteration 162 loss: 0.008380801416933537, ACC:1.0\n",
      "Training iteration 163 loss: 0.05441595986485481, ACC:0.96875\n",
      "Training iteration 164 loss: 0.0064605409279465675, ACC:1.0\n",
      "Training iteration 165 loss: 0.00648459792137146, ACC:1.0\n",
      "Training iteration 166 loss: 0.011481677182018757, ACC:1.0\n",
      "Training iteration 167 loss: 0.00362319964915514, ACC:1.0\n",
      "Training iteration 168 loss: 0.09513717889785767, ACC:0.984375\n",
      "Training iteration 169 loss: 0.0456940196454525, ACC:0.984375\n",
      "Training iteration 170 loss: 0.17862167954444885, ACC:0.953125\n",
      "Training iteration 171 loss: 0.010159600526094437, ACC:1.0\n",
      "Training iteration 172 loss: 0.0003835391835309565, ACC:1.0\n",
      "Training iteration 173 loss: 0.008956250734627247, ACC:1.0\n",
      "Training iteration 174 loss: 0.000350039335899055, ACC:1.0\n",
      "Training iteration 175 loss: 0.07586506754159927, ACC:0.984375\n",
      "Training iteration 176 loss: 0.002848341828212142, ACC:1.0\n",
      "Training iteration 177 loss: 0.004875825718045235, ACC:1.0\n",
      "Training iteration 178 loss: 0.03058350272476673, ACC:0.984375\n",
      "Training iteration 179 loss: 0.00214677513577044, ACC:1.0\n",
      "Training iteration 180 loss: 0.0040723951533436775, ACC:1.0\n",
      "Training iteration 181 loss: 0.01652558334171772, ACC:1.0\n",
      "Training iteration 182 loss: 0.032790832221508026, ACC:0.984375\n",
      "Training iteration 183 loss: 0.010522818192839622, ACC:1.0\n",
      "Training iteration 184 loss: 0.0468331016600132, ACC:0.984375\n",
      "Training iteration 185 loss: 0.05301196873188019, ACC:0.984375\n",
      "Training iteration 186 loss: 0.027173953130841255, ACC:1.0\n",
      "Training iteration 187 loss: 0.06403885036706924, ACC:0.96875\n",
      "Training iteration 188 loss: 0.020689919590950012, ACC:1.0\n",
      "Training iteration 189 loss: 0.0025063343346118927, ACC:1.0\n",
      "Training iteration 190 loss: 0.004362085368484259, ACC:1.0\n",
      "Training iteration 191 loss: 0.016362877562642097, ACC:1.0\n",
      "Training iteration 192 loss: 0.0013609020970761776, ACC:1.0\n",
      "Training iteration 193 loss: 0.028830336406826973, ACC:0.96875\n",
      "Training iteration 194 loss: 0.13747820258140564, ACC:0.96875\n",
      "Training iteration 195 loss: 0.001486484194174409, ACC:1.0\n",
      "Training iteration 196 loss: 0.06538527458906174, ACC:0.984375\n",
      "Training iteration 197 loss: 0.10547719895839691, ACC:0.96875\n",
      "Training iteration 198 loss: 0.05705968290567398, ACC:0.96875\n",
      "Training iteration 199 loss: 0.0005689033423550427, ACC:1.0\n",
      "Training iteration 200 loss: 0.0036521030124276876, ACC:1.0\n",
      "Training iteration 201 loss: 0.00226422818377614, ACC:1.0\n",
      "Training iteration 202 loss: 0.08639296889305115, ACC:0.96875\n",
      "Training iteration 203 loss: 0.00536789046600461, ACC:1.0\n",
      "Training iteration 204 loss: 0.004659968428313732, ACC:1.0\n",
      "Training iteration 205 loss: 0.002188080456107855, ACC:1.0\n",
      "Training iteration 206 loss: 0.0017381017096340656, ACC:1.0\n",
      "Training iteration 207 loss: 0.008812608197331429, ACC:1.0\n",
      "Training iteration 208 loss: 0.01699703373014927, ACC:0.984375\n",
      "Training iteration 209 loss: 0.004740157164633274, ACC:1.0\n",
      "Training iteration 210 loss: 0.008547303266823292, ACC:1.0\n",
      "Training iteration 211 loss: 0.022298134863376617, ACC:0.984375\n",
      "Training iteration 212 loss: 0.01575646735727787, ACC:0.984375\n",
      "Training iteration 213 loss: 0.0017745839431881905, ACC:1.0\n",
      "Training iteration 214 loss: 0.047266289591789246, ACC:0.984375\n",
      "Training iteration 215 loss: 0.002120286226272583, ACC:1.0\n",
      "Training iteration 216 loss: 0.06814928352832794, ACC:0.96875\n",
      "Training iteration 217 loss: 0.05669654905796051, ACC:0.984375\n",
      "Training iteration 218 loss: 0.03661777079105377, ACC:0.96875\n",
      "Training iteration 219 loss: 0.08525145053863525, ACC:0.984375\n",
      "Training iteration 220 loss: 0.01402914710342884, ACC:1.0\n",
      "Training iteration 221 loss: 0.01596863754093647, ACC:1.0\n",
      "Training iteration 222 loss: 0.007619422860443592, ACC:1.0\n",
      "Training iteration 223 loss: 0.001708167139440775, ACC:1.0\n",
      "Training iteration 224 loss: 0.06014014035463333, ACC:0.984375\n",
      "Training iteration 225 loss: 0.003006561892107129, ACC:1.0\n",
      "Training iteration 226 loss: 0.009463511407375336, ACC:1.0\n",
      "Training iteration 227 loss: 0.000635349890217185, ACC:1.0\n",
      "Training iteration 228 loss: 0.006477657705545425, ACC:1.0\n",
      "Training iteration 229 loss: 0.03244893625378609, ACC:0.984375\n",
      "Training iteration 230 loss: 0.0006511115934699774, ACC:1.0\n",
      "Training iteration 231 loss: 0.1073434054851532, ACC:0.984375\n",
      "Training iteration 232 loss: 0.005429237615317106, ACC:1.0\n",
      "Training iteration 233 loss: 0.036211155354976654, ACC:0.984375\n",
      "Training iteration 234 loss: 0.017714381217956543, ACC:1.0\n",
      "Training iteration 235 loss: 0.008264590054750443, ACC:1.0\n",
      "Training iteration 236 loss: 0.0010625200811773539, ACC:1.0\n",
      "Training iteration 237 loss: 0.000504814088344574, ACC:1.0\n",
      "Training iteration 238 loss: 0.12717580795288086, ACC:0.984375\n",
      "Training iteration 239 loss: 0.06598630547523499, ACC:0.984375\n",
      "Training iteration 240 loss: 0.001308640232309699, ACC:1.0\n",
      "Training iteration 241 loss: 0.001910307095386088, ACC:1.0\n",
      "Training iteration 242 loss: 0.03605959564447403, ACC:0.984375\n",
      "Training iteration 243 loss: 0.003837845753878355, ACC:1.0\n",
      "Training iteration 244 loss: 0.006073831580579281, ACC:1.0\n",
      "Training iteration 245 loss: 0.00231929961591959, ACC:1.0\n",
      "Training iteration 246 loss: 0.0011456115171313286, ACC:1.0\n",
      "Training iteration 247 loss: 0.0009544772910885513, ACC:1.0\n",
      "Training iteration 248 loss: 0.0007133987965062261, ACC:1.0\n",
      "Training iteration 249 loss: 0.003713703015819192, ACC:1.0\n",
      "Training iteration 250 loss: 0.07777178287506104, ACC:0.953125\n",
      "Training iteration 251 loss: 0.0013738989364355803, ACC:1.0\n",
      "Training iteration 252 loss: 0.017463847994804382, ACC:1.0\n",
      "Training iteration 253 loss: 0.0012852068757638335, ACC:1.0\n",
      "Training iteration 254 loss: 0.006460210774093866, ACC:1.0\n",
      "Training iteration 255 loss: 0.010415593162178993, ACC:1.0\n",
      "Training iteration 256 loss: 0.02649160660803318, ACC:1.0\n",
      "Training iteration 257 loss: 0.0021134885028004646, ACC:1.0\n",
      "Training iteration 258 loss: 0.008910217322409153, ACC:1.0\n",
      "Training iteration 259 loss: 0.044109541922807693, ACC:0.984375\n",
      "Training iteration 260 loss: 0.0007334131514653563, ACC:1.0\n",
      "Training iteration 261 loss: 0.015216214582324028, ACC:0.984375\n",
      "Training iteration 262 loss: 0.03713136538863182, ACC:0.984375\n",
      "Training iteration 263 loss: 0.0020280368626117706, ACC:1.0\n",
      "Training iteration 264 loss: 0.0035893672611564398, ACC:1.0\n",
      "Training iteration 265 loss: 0.05115374177694321, ACC:0.984375\n",
      "Training iteration 266 loss: 0.10049354285001755, ACC:0.984375\n",
      "Training iteration 267 loss: 0.0006205703248269856, ACC:1.0\n",
      "Training iteration 268 loss: 0.0075317854061722755, ACC:1.0\n",
      "Training iteration 269 loss: 0.009174681268632412, ACC:1.0\n",
      "Training iteration 270 loss: 0.01556373666971922, ACC:1.0\n",
      "Training iteration 271 loss: 0.043371979147195816, ACC:0.96875\n",
      "Training iteration 272 loss: 0.03816112130880356, ACC:0.984375\n",
      "Training iteration 273 loss: 0.00233972305431962, ACC:1.0\n",
      "Training iteration 274 loss: 0.015159527771174908, ACC:0.984375\n",
      "Training iteration 275 loss: 0.001573464716784656, ACC:1.0\n",
      "Training iteration 276 loss: 0.002409758511930704, ACC:1.0\n",
      "Training iteration 277 loss: 0.08328501135110855, ACC:0.96875\n",
      "Training iteration 278 loss: 0.003420954803004861, ACC:1.0\n",
      "Training iteration 279 loss: 0.008495227433741093, ACC:1.0\n",
      "Training iteration 280 loss: 0.016346991062164307, ACC:1.0\n",
      "Training iteration 281 loss: 0.0025521505158394575, ACC:1.0\n",
      "Training iteration 282 loss: 0.0005231384420767426, ACC:1.0\n",
      "Training iteration 283 loss: 0.0015126917278394103, ACC:1.0\n",
      "Training iteration 284 loss: 0.000888610549736768, ACC:1.0\n",
      "Training iteration 285 loss: 0.00025991967413574457, ACC:1.0\n",
      "Training iteration 286 loss: 0.001484861713834107, ACC:1.0\n",
      "Training iteration 287 loss: 0.00016845889331307262, ACC:1.0\n",
      "Training iteration 288 loss: 0.0003257518692407757, ACC:1.0\n",
      "Training iteration 289 loss: 0.0004944229149259627, ACC:1.0\n",
      "Training iteration 290 loss: 0.000251994002610445, ACC:1.0\n",
      "Training iteration 291 loss: 0.03641822561621666, ACC:0.984375\n",
      "Training iteration 292 loss: 0.005931583233177662, ACC:1.0\n",
      "Training iteration 293 loss: 0.0003129389078821987, ACC:1.0\n",
      "Training iteration 294 loss: 0.0018059075810015202, ACC:1.0\n",
      "Training iteration 295 loss: 0.00012734792835544795, ACC:1.0\n",
      "Training iteration 296 loss: 0.0016133820172399282, ACC:1.0\n",
      "Training iteration 297 loss: 0.0005122623988427222, ACC:1.0\n",
      "Training iteration 298 loss: 0.00037919677561149, ACC:1.0\n",
      "Training iteration 299 loss: 0.0002852916077245027, ACC:1.0\n",
      "Training iteration 300 loss: 0.0021391157060861588, ACC:1.0\n",
      "Training iteration 301 loss: 0.009389149025082588, ACC:1.0\n",
      "Training iteration 302 loss: 0.0017330949194729328, ACC:1.0\n",
      "Training iteration 303 loss: 0.008669371716678143, ACC:1.0\n",
      "Training iteration 304 loss: 0.0005336790345609188, ACC:1.0\n",
      "Training iteration 305 loss: 0.0003862511948682368, ACC:1.0\n",
      "Training iteration 306 loss: 0.06240358576178551, ACC:0.984375\n",
      "Training iteration 307 loss: 0.0007050202693790197, ACC:1.0\n",
      "Training iteration 308 loss: 0.0023002149537205696, ACC:1.0\n",
      "Training iteration 309 loss: 0.08301292359828949, ACC:0.96875\n",
      "Training iteration 310 loss: 0.0021135779097676277, ACC:1.0\n",
      "Training iteration 311 loss: 0.01989457942545414, ACC:0.984375\n",
      "Training iteration 312 loss: 0.06506384909152985, ACC:0.984375\n",
      "Training iteration 313 loss: 0.030062392354011536, ACC:0.984375\n",
      "Training iteration 314 loss: 0.002003537956625223, ACC:1.0\n",
      "Training iteration 315 loss: 0.00984994787722826, ACC:1.0\n",
      "Training iteration 316 loss: 0.015220127999782562, ACC:1.0\n",
      "Training iteration 317 loss: 0.018628668040037155, ACC:0.984375\n",
      "Training iteration 318 loss: 0.013319415971636772, ACC:1.0\n",
      "Training iteration 319 loss: 0.050539080053567886, ACC:0.984375\n",
      "Training iteration 320 loss: 0.035613588988780975, ACC:0.984375\n",
      "Training iteration 321 loss: 0.17080512642860413, ACC:0.984375\n",
      "Training iteration 322 loss: 0.005672254599630833, ACC:1.0\n",
      "Training iteration 323 loss: 0.0038091368041932583, ACC:1.0\n",
      "Training iteration 324 loss: 0.022983312606811523, ACC:1.0\n",
      "Training iteration 325 loss: 0.0020522719714790583, ACC:1.0\n",
      "Training iteration 326 loss: 0.004999998025596142, ACC:1.0\n",
      "Training iteration 327 loss: 0.001826046616770327, ACC:1.0\n",
      "Training iteration 328 loss: 0.037723325192928314, ACC:0.984375\n",
      "Training iteration 329 loss: 0.011646971106529236, ACC:1.0\n",
      "Training iteration 330 loss: 0.10965505987405777, ACC:0.984375\n",
      "Training iteration 331 loss: 0.020952217280864716, ACC:1.0\n",
      "Training iteration 332 loss: 0.05467962101101875, ACC:0.984375\n",
      "Training iteration 333 loss: 0.030660320073366165, ACC:0.984375\n",
      "Training iteration 334 loss: 0.011972776614129543, ACC:1.0\n",
      "Training iteration 335 loss: 0.03991832956671715, ACC:0.984375\n",
      "Training iteration 336 loss: 0.005386969540268183, ACC:1.0\n",
      "Training iteration 337 loss: 0.0011560135753825307, ACC:1.0\n",
      "Training iteration 338 loss: 0.0020132085774093866, ACC:1.0\n",
      "Training iteration 339 loss: 0.0050171189941465855, ACC:1.0\n",
      "Training iteration 340 loss: 0.0003956050204578787, ACC:1.0\n",
      "Training iteration 341 loss: 0.0006139216711744666, ACC:1.0\n",
      "Training iteration 342 loss: 0.03821517527103424, ACC:0.984375\n",
      "Training iteration 343 loss: 0.0024416069500148296, ACC:1.0\n",
      "Training iteration 344 loss: 0.017120275646448135, ACC:0.984375\n",
      "Training iteration 345 loss: 0.01860959827899933, ACC:1.0\n",
      "Training iteration 346 loss: 0.015272199176251888, ACC:1.0\n",
      "Training iteration 347 loss: 0.03999903053045273, ACC:0.96875\n",
      "Training iteration 348 loss: 0.0003515204880386591, ACC:1.0\n",
      "Training iteration 349 loss: 0.0010264504235237837, ACC:1.0\n",
      "Training iteration 350 loss: 0.03996416926383972, ACC:0.984375\n",
      "Training iteration 351 loss: 0.005437320098280907, ACC:1.0\n",
      "Training iteration 352 loss: 0.08897996693849564, ACC:0.953125\n",
      "Training iteration 353 loss: 0.00044638587860390544, ACC:1.0\n",
      "Training iteration 354 loss: 0.0003575207374524325, ACC:1.0\n",
      "Training iteration 355 loss: 0.017460523173213005, ACC:0.984375\n",
      "Training iteration 356 loss: 0.00067509850487113, ACC:1.0\n",
      "Training iteration 357 loss: 0.008209208026528358, ACC:1.0\n",
      "Training iteration 358 loss: 0.04129434749484062, ACC:0.984375\n",
      "Training iteration 359 loss: 0.0028326883912086487, ACC:1.0\n",
      "Training iteration 360 loss: 0.0024721308145672083, ACC:1.0\n",
      "Training iteration 361 loss: 0.0037327210884541273, ACC:1.0\n",
      "Training iteration 362 loss: 0.005526751279830933, ACC:1.0\n",
      "Training iteration 363 loss: 0.03156282380223274, ACC:0.984375\n",
      "Training iteration 364 loss: 0.00027103046886622906, ACC:1.0\n",
      "Training iteration 365 loss: 0.012052209116518497, ACC:0.984375\n",
      "Training iteration 366 loss: 0.014811481349170208, ACC:1.0\n",
      "Training iteration 367 loss: 0.16072635352611542, ACC:0.984375\n",
      "Training iteration 368 loss: 0.0072347973473370075, ACC:1.0\n",
      "Training iteration 369 loss: 0.0037096787709742785, ACC:1.0\n",
      "Training iteration 370 loss: 0.07515756785869598, ACC:0.984375\n",
      "Training iteration 371 loss: 0.0003123167552985251, ACC:1.0\n",
      "Training iteration 372 loss: 0.09762731194496155, ACC:0.984375\n",
      "Training iteration 373 loss: 0.004071200266480446, ACC:1.0\n",
      "Training iteration 374 loss: 0.008016404695808887, ACC:1.0\n",
      "Training iteration 375 loss: 0.005952024832367897, ACC:1.0\n",
      "Training iteration 376 loss: 0.007385557051748037, ACC:1.0\n",
      "Training iteration 377 loss: 0.048378556966781616, ACC:0.96875\n",
      "Training iteration 378 loss: 0.00415252847597003, ACC:1.0\n",
      "Training iteration 379 loss: 0.010076646693050861, ACC:1.0\n",
      "Training iteration 380 loss: 0.0017796277534216642, ACC:1.0\n",
      "Training iteration 381 loss: 0.005250362679362297, ACC:1.0\n",
      "Training iteration 382 loss: 0.078280508518219, ACC:0.984375\n",
      "Training iteration 383 loss: 0.013578412123024464, ACC:0.984375\n",
      "Training iteration 384 loss: 0.08490557968616486, ACC:0.984375\n",
      "Training iteration 385 loss: 0.0010128538124263287, ACC:1.0\n",
      "Training iteration 386 loss: 0.041610896587371826, ACC:0.984375\n",
      "Training iteration 387 loss: 0.15051041543483734, ACC:0.96875\n",
      "Training iteration 388 loss: 0.11307095736265182, ACC:0.984375\n",
      "Training iteration 389 loss: 0.025895219296216965, ACC:0.984375\n",
      "Training iteration 390 loss: 0.027651110664010048, ACC:0.96875\n",
      "Training iteration 391 loss: 0.025043310597538948, ACC:0.984375\n",
      "Training iteration 392 loss: 0.019269999116659164, ACC:0.984375\n",
      "Training iteration 393 loss: 0.008139526471495628, ACC:1.0\n",
      "Training iteration 394 loss: 0.011648569256067276, ACC:1.0\n",
      "Training iteration 395 loss: 0.02023119106888771, ACC:0.984375\n",
      "Training iteration 396 loss: 0.004163986537605524, ACC:1.0\n",
      "Training iteration 397 loss: 0.009778209030628204, ACC:1.0\n",
      "Training iteration 398 loss: 0.0027953078970313072, ACC:1.0\n",
      "Training iteration 399 loss: 0.003208997193723917, ACC:1.0\n",
      "Training iteration 400 loss: 0.09063037484884262, ACC:0.984375\n",
      "Training iteration 401 loss: 0.0026250872761011124, ACC:1.0\n",
      "Training iteration 402 loss: 0.05403272062540054, ACC:0.984375\n",
      "Training iteration 403 loss: 0.009659606032073498, ACC:1.0\n",
      "Training iteration 404 loss: 0.0012591518461704254, ACC:1.0\n",
      "Training iteration 405 loss: 0.0011475891806185246, ACC:1.0\n",
      "Training iteration 406 loss: 0.001229339512065053, ACC:1.0\n",
      "Training iteration 407 loss: 0.008018312975764275, ACC:1.0\n",
      "Training iteration 408 loss: 0.07069960981607437, ACC:0.984375\n",
      "Training iteration 409 loss: 0.0037771740462630987, ACC:1.0\n",
      "Training iteration 410 loss: 0.006298237945884466, ACC:1.0\n",
      "Training iteration 411 loss: 0.012847361154854298, ACC:1.0\n",
      "Training iteration 412 loss: 0.008740566670894623, ACC:1.0\n",
      "Training iteration 413 loss: 0.007119910791516304, ACC:1.0\n",
      "Training iteration 414 loss: 0.002126060426235199, ACC:1.0\n",
      "Training iteration 415 loss: 0.006882227957248688, ACC:1.0\n",
      "Training iteration 416 loss: 0.0026837855111807585, ACC:1.0\n",
      "Training iteration 417 loss: 0.0029359254986047745, ACC:1.0\n",
      "Training iteration 418 loss: 0.004322720691561699, ACC:1.0\n",
      "Training iteration 419 loss: 0.021320361644029617, ACC:0.984375\n",
      "Training iteration 420 loss: 0.0025175740011036396, ACC:1.0\n",
      "Training iteration 421 loss: 0.0004939472419209778, ACC:1.0\n",
      "Training iteration 422 loss: 0.022619975730776787, ACC:0.984375\n",
      "Training iteration 423 loss: 0.0004424622457008809, ACC:1.0\n",
      "Training iteration 424 loss: 0.0009491012897342443, ACC:1.0\n",
      "Training iteration 425 loss: 0.043711382895708084, ACC:0.96875\n",
      "Training iteration 426 loss: 0.01963692158460617, ACC:1.0\n",
      "Training iteration 427 loss: 0.002156226895749569, ACC:1.0\n",
      "Training iteration 428 loss: 0.007791919633746147, ACC:1.0\n",
      "Training iteration 429 loss: 0.003984218463301659, ACC:1.0\n",
      "Training iteration 430 loss: 0.011851056478917599, ACC:1.0\n",
      "Training iteration 431 loss: 0.0593709871172905, ACC:0.96875\n",
      "Training iteration 432 loss: 0.026826161891222, ACC:0.984375\n",
      "Training iteration 433 loss: 0.0027042559813708067, ACC:1.0\n",
      "Training iteration 434 loss: 0.00011335150338709354, ACC:1.0\n",
      "Training iteration 435 loss: 0.00020285016216803342, ACC:1.0\n",
      "Training iteration 436 loss: 0.00021325811394490302, ACC:1.0\n",
      "Training iteration 437 loss: 0.010006366297602654, ACC:1.0\n",
      "Training iteration 438 loss: 0.008197356946766376, ACC:1.0\n",
      "Training iteration 439 loss: 0.0006716552888974547, ACC:1.0\n",
      "Training iteration 440 loss: 0.053803347051143646, ACC:0.984375\n",
      "Training iteration 441 loss: 0.0014013100881129503, ACC:1.0\n",
      "Training iteration 442 loss: 0.002165342215448618, ACC:1.0\n",
      "Training iteration 443 loss: 0.002933247946202755, ACC:1.0\n",
      "Training iteration 444 loss: 0.006176518741995096, ACC:1.0\n",
      "Training iteration 445 loss: 0.00329349422827363, ACC:1.0\n",
      "Training iteration 446 loss: 0.0022732552606612444, ACC:1.0\n",
      "Training iteration 447 loss: 0.0019765817560255527, ACC:1.0\n",
      "Training iteration 448 loss: 0.0026731635443866253, ACC:1.0\n",
      "Training iteration 449 loss: 0.06537944823503494, ACC:0.984375\n",
      "Training iteration 450 loss: 0.009211580269038677, ACC:1.0\n",
      "Validation iteration 451 loss: 0.036743149161338806, ACC: 0.984375\n",
      "Validation iteration 452 loss: 0.07795576751232147, ACC: 0.984375\n",
      "Validation iteration 453 loss: 0.0033109383657574654, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.0016041636699810624, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.00021175615256652236, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.0825607106089592, ACC: 0.984375\n",
      "Validation iteration 457 loss: 0.08634325116872787, ACC: 0.96875\n",
      "Validation iteration 458 loss: 0.0007640282856300473, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.0054174489341676235, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.0038758371956646442, ACC: 1.0\n",
      "Validation iteration 461 loss: 0.0006751475157216191, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.021681278944015503, ACC: 0.984375\n",
      "Validation iteration 463 loss: 0.00568007305264473, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.017924923449754715, ACC: 0.984375\n",
      "Validation iteration 465 loss: 0.00029497870127670467, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.0006303252885118127, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.0005819926736876369, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.002028641989454627, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.08331920206546783, ACC: 0.96875\n",
      "Validation iteration 470 loss: 0.0005220163147896528, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.1776183843612671, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.1286284625530243, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.030046310275793076, ACC: 0.984375\n",
      "Validation iteration 474 loss: 0.0020361810456961393, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.0009341405238956213, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.051942966878414154, ACC: 0.984375\n",
      "Validation iteration 477 loss: 0.0008987435721792281, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.014225526712834835, ACC: 1.0\n",
      "Validation iteration 479 loss: 0.06793201714754105, ACC: 0.96875\n",
      "Validation iteration 480 loss: 0.003803621046245098, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.0029032486490905285, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.03531961143016815, ACC: 0.984375\n",
      "Validation iteration 483 loss: 0.0012162638595327735, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.006526158191263676, ACC: 1.0\n",
      "Validation iteration 485 loss: 0.0007555062184110284, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.0003071112441830337, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.01518737431615591, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.00037699780659750104, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.2561212480068207, ACC: 0.953125\n",
      "Validation iteration 490 loss: 0.013985511846840382, ACC: 0.984375\n",
      "Validation iteration 491 loss: 0.006968678906559944, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.0013011794071644545, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.008278265595436096, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.007992081344127655, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.012152411043643951, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.0036898902617394924, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.000270647753495723, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.0009482273599132895, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.0034089377149939537, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.0023466802667826414, ACC: 1.0\n",
      "-- Epoch 4 done -- Train loss: 0.0218233995008955, train ACC: 0.9937152777777778, val loss: 0.02580496032780502, val ACC: 0.99375\n",
      "<--- 10516.534317731857 seconds --->\n",
      "Training iteration 1 loss: 0.003594153095036745, ACC:1.0\n",
      "Training iteration 2 loss: 0.009620700031518936, ACC:1.0\n",
      "Training iteration 3 loss: 0.02163248509168625, ACC:1.0\n",
      "Training iteration 4 loss: 0.004907524678856134, ACC:1.0\n",
      "Training iteration 5 loss: 0.00047604867722839117, ACC:1.0\n",
      "Training iteration 6 loss: 0.008298108354210854, ACC:1.0\n",
      "Training iteration 7 loss: 0.08779233694076538, ACC:0.984375\n",
      "Training iteration 8 loss: 9.24857595236972e-05, ACC:1.0\n",
      "Training iteration 9 loss: 0.0004753261455334723, ACC:1.0\n",
      "Training iteration 10 loss: 0.01464857254177332, ACC:0.984375\n",
      "Training iteration 11 loss: 0.006344189867377281, ACC:1.0\n",
      "Training iteration 12 loss: 0.00017826062685344368, ACC:1.0\n",
      "Training iteration 13 loss: 0.05405212193727493, ACC:0.984375\n",
      "Training iteration 14 loss: 0.019082143902778625, ACC:0.984375\n",
      "Training iteration 15 loss: 0.010278566740453243, ACC:1.0\n",
      "Training iteration 16 loss: 0.11594116687774658, ACC:0.984375\n",
      "Training iteration 17 loss: 0.15913167595863342, ACC:0.984375\n",
      "Training iteration 18 loss: 0.06522361189126968, ACC:0.984375\n",
      "Training iteration 19 loss: 0.01871567778289318, ACC:0.984375\n",
      "Training iteration 20 loss: 0.0021544289775192738, ACC:1.0\n",
      "Training iteration 21 loss: 0.012571366503834724, ACC:0.984375\n",
      "Training iteration 22 loss: 0.012392105534672737, ACC:1.0\n",
      "Training iteration 23 loss: 0.001642552437260747, ACC:1.0\n",
      "Training iteration 24 loss: 0.009785959497094154, ACC:1.0\n",
      "Training iteration 25 loss: 0.023097094148397446, ACC:0.984375\n",
      "Training iteration 26 loss: 0.002524408744648099, ACC:1.0\n",
      "Training iteration 27 loss: 0.014745382592082024, ACC:0.984375\n",
      "Training iteration 28 loss: 0.035246532410383224, ACC:0.984375\n",
      "Training iteration 29 loss: 0.04111102595925331, ACC:0.96875\n",
      "Training iteration 30 loss: 0.00046731464681215584, ACC:1.0\n",
      "Training iteration 31 loss: 0.0014883639523759484, ACC:1.0\n",
      "Training iteration 32 loss: 0.0005583668244071305, ACC:1.0\n",
      "Training iteration 33 loss: 0.004529969766736031, ACC:1.0\n",
      "Training iteration 34 loss: 0.002751051215454936, ACC:1.0\n",
      "Training iteration 35 loss: 0.003497994737699628, ACC:1.0\n",
      "Training iteration 36 loss: 0.0006352904019877315, ACC:1.0\n",
      "Training iteration 37 loss: 0.0030444972217082977, ACC:1.0\n",
      "Training iteration 38 loss: 0.0034851329401135445, ACC:1.0\n",
      "Training iteration 39 loss: 0.0018729806179180741, ACC:1.0\n",
      "Training iteration 40 loss: 0.0028909328393638134, ACC:1.0\n",
      "Training iteration 41 loss: 0.00012767764565069228, ACC:1.0\n",
      "Training iteration 42 loss: 0.0003557958407327533, ACC:1.0\n",
      "Training iteration 43 loss: 0.00010443331120768562, ACC:1.0\n",
      "Training iteration 44 loss: 0.0028923465870320797, ACC:1.0\n",
      "Training iteration 45 loss: 0.0002322625950910151, ACC:1.0\n",
      "Training iteration 46 loss: 0.0007856047595851123, ACC:1.0\n",
      "Training iteration 47 loss: 0.0004570314777083695, ACC:1.0\n",
      "Training iteration 48 loss: 0.00010132091847481206, ACC:1.0\n",
      "Training iteration 49 loss: 0.00012174258881714195, ACC:1.0\n",
      "Training iteration 50 loss: 0.00010351676610298455, ACC:1.0\n",
      "Training iteration 51 loss: 0.0006627161055803299, ACC:1.0\n",
      "Training iteration 52 loss: 0.08052317798137665, ACC:0.984375\n",
      "Training iteration 53 loss: 0.0009494444821029902, ACC:1.0\n",
      "Training iteration 54 loss: 0.0008738485630601645, ACC:1.0\n",
      "Training iteration 55 loss: 0.020665502175688744, ACC:0.984375\n",
      "Training iteration 56 loss: 0.10931439697742462, ACC:0.984375\n",
      "Training iteration 57 loss: 0.12792715430259705, ACC:0.96875\n",
      "Training iteration 58 loss: 0.007454254664480686, ACC:1.0\n",
      "Training iteration 59 loss: 0.022526536136865616, ACC:0.984375\n",
      "Training iteration 60 loss: 0.01659846492111683, ACC:0.984375\n",
      "Training iteration 61 loss: 0.03166499361395836, ACC:0.984375\n",
      "Training iteration 62 loss: 0.0005627485807053745, ACC:1.0\n",
      "Training iteration 63 loss: 0.0023704892955720425, ACC:1.0\n",
      "Training iteration 64 loss: 0.00503862090408802, ACC:1.0\n",
      "Training iteration 65 loss: 0.005823444575071335, ACC:1.0\n",
      "Training iteration 66 loss: 0.0004970939480699599, ACC:1.0\n",
      "Training iteration 67 loss: 0.03316020220518112, ACC:0.984375\n",
      "Training iteration 68 loss: 0.003215337870642543, ACC:1.0\n",
      "Training iteration 69 loss: 0.00227659335359931, ACC:1.0\n",
      "Training iteration 70 loss: 0.013922348618507385, ACC:0.984375\n",
      "Training iteration 71 loss: 0.0045878831297159195, ACC:1.0\n",
      "Training iteration 72 loss: 0.0024934581015259027, ACC:1.0\n",
      "Training iteration 73 loss: 0.010807635262608528, ACC:1.0\n",
      "Training iteration 74 loss: 0.00017498599481768906, ACC:1.0\n",
      "Training iteration 75 loss: 0.0005747442482970655, ACC:1.0\n",
      "Training iteration 76 loss: 0.022766171023249626, ACC:0.984375\n",
      "Training iteration 77 loss: 0.003679286688566208, ACC:1.0\n",
      "Training iteration 78 loss: 0.002060377039015293, ACC:1.0\n",
      "Training iteration 79 loss: 0.0007057702750898898, ACC:1.0\n",
      "Training iteration 80 loss: 0.01011643186211586, ACC:1.0\n",
      "Training iteration 81 loss: 0.0010310333454981446, ACC:1.0\n",
      "Training iteration 82 loss: 0.08458210527896881, ACC:0.984375\n",
      "Training iteration 83 loss: 0.00018529949011281133, ACC:1.0\n",
      "Training iteration 84 loss: 3.9895989175420254e-05, ACC:1.0\n",
      "Training iteration 85 loss: 0.0007168282754719257, ACC:1.0\n",
      "Training iteration 86 loss: 0.008420057594776154, ACC:1.0\n",
      "Training iteration 87 loss: 0.00018601071496959776, ACC:1.0\n",
      "Training iteration 88 loss: 0.025645408779382706, ACC:0.984375\n",
      "Training iteration 89 loss: 0.0848408043384552, ACC:0.984375\n",
      "Training iteration 90 loss: 0.00024240145285148174, ACC:1.0\n",
      "Training iteration 91 loss: 0.005136186257004738, ACC:1.0\n",
      "Training iteration 92 loss: 0.11878019571304321, ACC:0.96875\n",
      "Training iteration 93 loss: 0.05314505472779274, ACC:0.984375\n",
      "Training iteration 94 loss: 0.0007096120389178395, ACC:1.0\n",
      "Training iteration 95 loss: 0.003211687318980694, ACC:1.0\n",
      "Training iteration 96 loss: 0.026685746386647224, ACC:0.984375\n",
      "Training iteration 97 loss: 0.000801880843937397, ACC:1.0\n",
      "Training iteration 98 loss: 0.00023516893270425498, ACC:1.0\n",
      "Training iteration 99 loss: 0.01826569437980652, ACC:1.0\n",
      "Training iteration 100 loss: 0.01295686885714531, ACC:1.0\n",
      "Training iteration 101 loss: 0.0003289650194346905, ACC:1.0\n",
      "Training iteration 102 loss: 0.010732854716479778, ACC:1.0\n",
      "Training iteration 103 loss: 0.10095719993114471, ACC:0.96875\n",
      "Training iteration 104 loss: 0.1154523566365242, ACC:0.96875\n",
      "Training iteration 105 loss: 0.0015649100532755256, ACC:1.0\n",
      "Training iteration 106 loss: 0.0005800769431516528, ACC:1.0\n",
      "Training iteration 107 loss: 0.007045372389256954, ACC:1.0\n",
      "Training iteration 108 loss: 0.08308719098567963, ACC:0.96875\n",
      "Training iteration 109 loss: 0.011617240495979786, ACC:1.0\n",
      "Training iteration 110 loss: 0.07666987925767899, ACC:0.953125\n",
      "Training iteration 111 loss: 0.003236565040424466, ACC:1.0\n",
      "Training iteration 112 loss: 0.0008291152189485729, ACC:1.0\n",
      "Training iteration 113 loss: 0.0005656840512529016, ACC:1.0\n",
      "Training iteration 114 loss: 0.031624481081962585, ACC:0.984375\n",
      "Training iteration 115 loss: 0.0006313889753073454, ACC:1.0\n",
      "Training iteration 116 loss: 0.016039172187447548, ACC:0.984375\n",
      "Training iteration 117 loss: 0.003267434425652027, ACC:1.0\n",
      "Training iteration 118 loss: 0.0019932943396270275, ACC:1.0\n",
      "Training iteration 119 loss: 0.021453527733683586, ACC:0.984375\n",
      "Training iteration 120 loss: 0.0002717848401516676, ACC:1.0\n",
      "Training iteration 121 loss: 0.01987331174314022, ACC:0.984375\n",
      "Training iteration 122 loss: 0.10327684879302979, ACC:0.96875\n",
      "Training iteration 123 loss: 0.002635862911120057, ACC:1.0\n",
      "Training iteration 124 loss: 0.0005200862651690841, ACC:1.0\n",
      "Training iteration 125 loss: 0.017444290220737457, ACC:0.984375\n",
      "Training iteration 126 loss: 0.044967375695705414, ACC:0.984375\n",
      "Training iteration 127 loss: 0.04002942889928818, ACC:0.96875\n",
      "Training iteration 128 loss: 0.034745633602142334, ACC:0.984375\n",
      "Training iteration 129 loss: 0.002401019912213087, ACC:1.0\n",
      "Training iteration 130 loss: 0.0070952605456113815, ACC:1.0\n",
      "Training iteration 131 loss: 0.001234244555234909, ACC:1.0\n",
      "Training iteration 132 loss: 0.006425024475902319, ACC:1.0\n",
      "Training iteration 133 loss: 0.009835300035774708, ACC:1.0\n",
      "Training iteration 134 loss: 0.001241300138644874, ACC:1.0\n",
      "Training iteration 135 loss: 0.0011874539777636528, ACC:1.0\n",
      "Training iteration 136 loss: 0.0014240938471630216, ACC:1.0\n",
      "Training iteration 137 loss: 0.0029776606243103743, ACC:1.0\n",
      "Training iteration 138 loss: 0.0031130060087889433, ACC:1.0\n",
      "Training iteration 139 loss: 0.0008941599517129362, ACC:1.0\n",
      "Training iteration 140 loss: 0.07044734060764313, ACC:0.984375\n",
      "Training iteration 141 loss: 0.00041723772301338613, ACC:1.0\n",
      "Training iteration 142 loss: 0.06942886859178543, ACC:0.984375\n",
      "Training iteration 143 loss: 0.004940441809594631, ACC:1.0\n",
      "Training iteration 144 loss: 0.0055121309123933315, ACC:1.0\n",
      "Training iteration 145 loss: 0.01792094297707081, ACC:0.984375\n",
      "Training iteration 146 loss: 0.017090262845158577, ACC:0.984375\n",
      "Training iteration 147 loss: 0.001748978509567678, ACC:1.0\n",
      "Training iteration 148 loss: 0.04931363835930824, ACC:0.984375\n",
      "Training iteration 149 loss: 0.012664882466197014, ACC:1.0\n",
      "Training iteration 150 loss: 0.010697796940803528, ACC:1.0\n",
      "Training iteration 151 loss: 0.008244701661169529, ACC:1.0\n",
      "Training iteration 152 loss: 0.007371973711997271, ACC:1.0\n",
      "Training iteration 153 loss: 0.018333742395043373, ACC:1.0\n",
      "Training iteration 154 loss: 0.1040530651807785, ACC:0.953125\n",
      "Training iteration 155 loss: 0.005107397213578224, ACC:1.0\n",
      "Training iteration 156 loss: 0.004847896751016378, ACC:1.0\n",
      "Training iteration 157 loss: 0.2408324033021927, ACC:0.96875\n",
      "Training iteration 158 loss: 0.0009967079386115074, ACC:1.0\n",
      "Training iteration 159 loss: 0.05395645648241043, ACC:0.984375\n",
      "Training iteration 160 loss: 0.030545687302947044, ACC:0.984375\n",
      "Training iteration 161 loss: 0.002467490965500474, ACC:1.0\n",
      "Training iteration 162 loss: 0.0006559044122695923, ACC:1.0\n",
      "Training iteration 163 loss: 0.06363450735807419, ACC:0.984375\n",
      "Training iteration 164 loss: 7.906343671493232e-05, ACC:1.0\n",
      "Training iteration 165 loss: 0.288107305765152, ACC:0.96875\n",
      "Training iteration 166 loss: 0.002452613553032279, ACC:1.0\n",
      "Training iteration 167 loss: 0.004938251338899136, ACC:1.0\n",
      "Training iteration 168 loss: 0.0023805235978215933, ACC:1.0\n",
      "Training iteration 169 loss: 0.014482304453849792, ACC:0.984375\n",
      "Training iteration 170 loss: 0.006927496753633022, ACC:1.0\n",
      "Training iteration 171 loss: 0.0033775055781006813, ACC:1.0\n",
      "Training iteration 172 loss: 0.002666991436854005, ACC:1.0\n",
      "Training iteration 173 loss: 0.03760155662894249, ACC:0.984375\n",
      "Training iteration 174 loss: 0.01155080646276474, ACC:1.0\n",
      "Training iteration 175 loss: 0.008584229275584221, ACC:1.0\n",
      "Training iteration 176 loss: 0.0036510122008621693, ACC:1.0\n",
      "Training iteration 177 loss: 0.018233468756079674, ACC:0.984375\n",
      "Training iteration 178 loss: 0.053658876568078995, ACC:0.984375\n",
      "Training iteration 179 loss: 0.000939853664021939, ACC:1.0\n",
      "Training iteration 180 loss: 0.020269375294446945, ACC:0.984375\n",
      "Training iteration 181 loss: 0.001035013934597373, ACC:1.0\n",
      "Training iteration 182 loss: 0.013174801133573055, ACC:1.0\n",
      "Training iteration 183 loss: 0.01855051890015602, ACC:0.984375\n",
      "Training iteration 184 loss: 0.09203953295946121, ACC:0.984375\n",
      "Training iteration 185 loss: 0.0023993670474737883, ACC:1.0\n",
      "Training iteration 186 loss: 0.0005337227485142648, ACC:1.0\n",
      "Training iteration 187 loss: 0.03784511610865593, ACC:0.984375\n",
      "Training iteration 188 loss: 0.00754085136577487, ACC:1.0\n",
      "Training iteration 189 loss: 0.030231939628720284, ACC:0.984375\n",
      "Training iteration 190 loss: 0.07820490002632141, ACC:0.984375\n",
      "Training iteration 191 loss: 0.010161732323467731, ACC:1.0\n",
      "Training iteration 192 loss: 0.0037869184743613005, ACC:1.0\n",
      "Training iteration 193 loss: 0.0029362961649894714, ACC:1.0\n",
      "Training iteration 194 loss: 0.060396961867809296, ACC:0.984375\n",
      "Training iteration 195 loss: 0.0075542135164141655, ACC:1.0\n",
      "Training iteration 196 loss: 0.0011501730186864734, ACC:1.0\n",
      "Training iteration 197 loss: 0.03349662944674492, ACC:0.984375\n",
      "Training iteration 198 loss: 0.17487820982933044, ACC:0.953125\n",
      "Training iteration 199 loss: 0.09438730031251907, ACC:0.984375\n",
      "Training iteration 200 loss: 0.033145055174827576, ACC:0.984375\n",
      "Training iteration 201 loss: 0.04399440810084343, ACC:0.984375\n",
      "Training iteration 202 loss: 0.025412175804376602, ACC:0.984375\n",
      "Training iteration 203 loss: 0.0045035118237137794, ACC:1.0\n",
      "Training iteration 204 loss: 0.030070243403315544, ACC:1.0\n",
      "Training iteration 205 loss: 0.029908331111073494, ACC:1.0\n",
      "Training iteration 206 loss: 0.28589677810668945, ACC:0.953125\n",
      "Training iteration 207 loss: 0.00821761880069971, ACC:1.0\n",
      "Training iteration 208 loss: 0.005598512943834066, ACC:1.0\n",
      "Training iteration 209 loss: 0.004434566479176283, ACC:1.0\n",
      "Training iteration 210 loss: 0.012986299581825733, ACC:1.0\n",
      "Training iteration 211 loss: 0.06733709573745728, ACC:0.96875\n",
      "Training iteration 212 loss: 0.0018058683490380645, ACC:1.0\n",
      "Training iteration 213 loss: 0.023776082322001457, ACC:0.984375\n",
      "Training iteration 214 loss: 0.03047625720500946, ACC:0.984375\n",
      "Training iteration 215 loss: 0.060517556965351105, ACC:0.984375\n",
      "Training iteration 216 loss: 0.023722907528281212, ACC:0.984375\n",
      "Training iteration 217 loss: 0.04947002977132797, ACC:0.984375\n",
      "Training iteration 218 loss: 0.054528411477804184, ACC:0.984375\n",
      "Training iteration 219 loss: 0.00782286562025547, ACC:1.0\n",
      "Training iteration 220 loss: 0.03086869977414608, ACC:0.984375\n",
      "Training iteration 221 loss: 0.001927236095070839, ACC:1.0\n",
      "Training iteration 222 loss: 0.006380285602062941, ACC:1.0\n",
      "Training iteration 223 loss: 0.003187625901773572, ACC:1.0\n",
      "Training iteration 224 loss: 0.036259785294532776, ACC:0.96875\n",
      "Training iteration 225 loss: 0.0022069313563406467, ACC:1.0\n",
      "Training iteration 226 loss: 0.028926454484462738, ACC:0.984375\n",
      "Training iteration 227 loss: 0.0018586519872769713, ACC:1.0\n",
      "Training iteration 228 loss: 0.0015309792943298817, ACC:1.0\n",
      "Training iteration 229 loss: 0.011876147240400314, ACC:1.0\n",
      "Training iteration 230 loss: 0.03114801086485386, ACC:0.984375\n",
      "Training iteration 231 loss: 0.004691353999078274, ACC:1.0\n",
      "Training iteration 232 loss: 0.007254807744175196, ACC:1.0\n",
      "Training iteration 233 loss: 0.021706225350499153, ACC:0.984375\n",
      "Training iteration 234 loss: 0.006589047145098448, ACC:1.0\n",
      "Training iteration 235 loss: 0.006135217845439911, ACC:1.0\n",
      "Training iteration 236 loss: 0.0041195182129740715, ACC:1.0\n",
      "Training iteration 237 loss: 0.018431123346090317, ACC:0.984375\n",
      "Training iteration 238 loss: 0.0002768505655694753, ACC:1.0\n",
      "Training iteration 239 loss: 0.0002043558342847973, ACC:1.0\n",
      "Training iteration 240 loss: 0.048226989805698395, ACC:0.984375\n",
      "Training iteration 241 loss: 0.00039792791358195245, ACC:1.0\n",
      "Training iteration 242 loss: 0.03139350563287735, ACC:0.984375\n",
      "Training iteration 243 loss: 0.037768956273794174, ACC:0.984375\n",
      "Training iteration 244 loss: 0.004996288102120161, ACC:1.0\n",
      "Training iteration 245 loss: 0.03707224875688553, ACC:0.984375\n",
      "Training iteration 246 loss: 0.01215882133692503, ACC:1.0\n",
      "Training iteration 247 loss: 0.013941633515059948, ACC:1.0\n",
      "Training iteration 248 loss: 0.015425567515194416, ACC:1.0\n",
      "Training iteration 249 loss: 0.0019435640424489975, ACC:1.0\n",
      "Training iteration 250 loss: 0.003998459316790104, ACC:1.0\n",
      "Training iteration 251 loss: 0.002211201936006546, ACC:1.0\n",
      "Training iteration 252 loss: 0.00030216106097213924, ACC:1.0\n",
      "Training iteration 253 loss: 0.03535204380750656, ACC:0.984375\n",
      "Training iteration 254 loss: 0.014762499369680882, ACC:0.984375\n",
      "Training iteration 255 loss: 0.0014941541012376547, ACC:1.0\n",
      "Training iteration 256 loss: 0.00041328262886963785, ACC:1.0\n",
      "Training iteration 257 loss: 0.0006080222083255649, ACC:1.0\n",
      "Training iteration 258 loss: 0.0018908195197582245, ACC:1.0\n",
      "Training iteration 259 loss: 0.09503549337387085, ACC:0.984375\n",
      "Training iteration 260 loss: 0.002943390514701605, ACC:1.0\n",
      "Training iteration 261 loss: 0.0005879630916751921, ACC:1.0\n",
      "Training iteration 262 loss: 0.0007359879673458636, ACC:1.0\n",
      "Training iteration 263 loss: 0.008481951430439949, ACC:1.0\n",
      "Training iteration 264 loss: 0.0016763430321589112, ACC:1.0\n",
      "Training iteration 265 loss: 0.0007825977518223226, ACC:1.0\n",
      "Training iteration 266 loss: 0.10532791912555695, ACC:0.984375\n",
      "Training iteration 267 loss: 0.00026882460224442184, ACC:1.0\n",
      "Training iteration 268 loss: 0.02364957332611084, ACC:1.0\n",
      "Training iteration 269 loss: 0.0004448418621905148, ACC:1.0\n",
      "Training iteration 270 loss: 0.004862241446971893, ACC:1.0\n",
      "Training iteration 271 loss: 0.000714037858415395, ACC:1.0\n",
      "Training iteration 272 loss: 0.004215402994304895, ACC:1.0\n",
      "Training iteration 273 loss: 0.019910641014575958, ACC:0.984375\n",
      "Training iteration 274 loss: 0.0017913670744746923, ACC:1.0\n",
      "Training iteration 275 loss: 0.112489253282547, ACC:0.984375\n",
      "Training iteration 276 loss: 0.04275970533490181, ACC:0.984375\n",
      "Training iteration 277 loss: 0.001124309841543436, ACC:1.0\n",
      "Training iteration 278 loss: 0.07056742906570435, ACC:0.984375\n",
      "Training iteration 279 loss: 0.0797630175948143, ACC:0.984375\n",
      "Training iteration 280 loss: 0.007709939032793045, ACC:1.0\n",
      "Training iteration 281 loss: 0.12803177535533905, ACC:0.96875\n",
      "Training iteration 282 loss: 0.0573873370885849, ACC:0.984375\n",
      "Training iteration 283 loss: 0.0013769909273833036, ACC:1.0\n",
      "Training iteration 284 loss: 0.051236920058727264, ACC:0.96875\n",
      "Training iteration 285 loss: 0.004293477162718773, ACC:1.0\n",
      "Training iteration 286 loss: 0.07584826648235321, ACC:0.984375\n",
      "Training iteration 287 loss: 0.010706077329814434, ACC:1.0\n",
      "Training iteration 288 loss: 0.0018565101781859994, ACC:1.0\n",
      "Training iteration 289 loss: 0.001964306691661477, ACC:1.0\n",
      "Training iteration 290 loss: 0.01659528911113739, ACC:1.0\n",
      "Training iteration 291 loss: 0.008228654973208904, ACC:1.0\n",
      "Training iteration 292 loss: 0.02415408194065094, ACC:0.984375\n",
      "Training iteration 293 loss: 0.0024517765268683434, ACC:1.0\n",
      "Training iteration 294 loss: 0.002885543741285801, ACC:1.0\n",
      "Training iteration 295 loss: 0.013870512135326862, ACC:1.0\n",
      "Training iteration 296 loss: 0.0017340892227366567, ACC:1.0\n",
      "Training iteration 297 loss: 0.00901891104876995, ACC:1.0\n",
      "Training iteration 298 loss: 0.0043031079694628716, ACC:1.0\n",
      "Training iteration 299 loss: 0.13539806008338928, ACC:0.96875\n",
      "Training iteration 300 loss: 0.001508260378614068, ACC:1.0\n",
      "Training iteration 301 loss: 0.032077521085739136, ACC:0.984375\n",
      "Training iteration 302 loss: 0.019976792857050896, ACC:0.984375\n",
      "Training iteration 303 loss: 0.0016699415864422917, ACC:1.0\n",
      "Training iteration 304 loss: 0.02854769304394722, ACC:0.984375\n",
      "Training iteration 305 loss: 0.0027136337012052536, ACC:1.0\n",
      "Training iteration 306 loss: 0.0006799135589972138, ACC:1.0\n",
      "Training iteration 307 loss: 0.0014823921956121922, ACC:1.0\n",
      "Training iteration 308 loss: 0.08310423046350479, ACC:0.984375\n",
      "Training iteration 309 loss: 0.068778857588768, ACC:0.984375\n",
      "Training iteration 310 loss: 0.030940599739551544, ACC:0.984375\n",
      "Training iteration 311 loss: 0.04630975425243378, ACC:0.984375\n",
      "Training iteration 312 loss: 0.0022656882647424936, ACC:1.0\n",
      "Training iteration 313 loss: 0.002483594696968794, ACC:1.0\n",
      "Training iteration 314 loss: 0.006834440864622593, ACC:1.0\n",
      "Training iteration 315 loss: 0.06954091787338257, ACC:0.984375\n",
      "Training iteration 316 loss: 0.1225801557302475, ACC:0.96875\n",
      "Training iteration 317 loss: 0.04311133921146393, ACC:0.984375\n",
      "Training iteration 318 loss: 0.0911254808306694, ACC:0.984375\n",
      "Training iteration 319 loss: 0.01578276976943016, ACC:1.0\n",
      "Training iteration 320 loss: 0.00032764513161964715, ACC:1.0\n",
      "Training iteration 321 loss: 0.12116812914609909, ACC:0.984375\n",
      "Training iteration 322 loss: 0.020407631993293762, ACC:1.0\n",
      "Training iteration 323 loss: 0.09773003309965134, ACC:0.984375\n",
      "Training iteration 324 loss: 0.03955830633640289, ACC:0.984375\n",
      "Training iteration 325 loss: 0.05643223226070404, ACC:0.984375\n",
      "Training iteration 326 loss: 0.0367613211274147, ACC:0.984375\n",
      "Training iteration 327 loss: 0.018965965136885643, ACC:1.0\n",
      "Training iteration 328 loss: 0.020843375474214554, ACC:1.0\n",
      "Training iteration 329 loss: 0.004092093091458082, ACC:1.0\n",
      "Training iteration 330 loss: 0.0337795726954937, ACC:0.984375\n",
      "Training iteration 331 loss: 0.010568900033831596, ACC:1.0\n",
      "Training iteration 332 loss: 0.04929409176111221, ACC:0.984375\n",
      "Training iteration 333 loss: 0.004020797088742256, ACC:1.0\n",
      "Training iteration 334 loss: 0.017996590584516525, ACC:1.0\n",
      "Training iteration 335 loss: 0.001194156357087195, ACC:1.0\n",
      "Training iteration 336 loss: 0.014271914958953857, ACC:1.0\n",
      "Training iteration 337 loss: 0.003396892687305808, ACC:1.0\n",
      "Training iteration 338 loss: 0.0322420597076416, ACC:0.984375\n",
      "Training iteration 339 loss: 0.00954372901469469, ACC:1.0\n",
      "Training iteration 340 loss: 0.023177461698651314, ACC:0.984375\n",
      "Training iteration 341 loss: 0.0028631528839468956, ACC:1.0\n",
      "Training iteration 342 loss: 0.0028041354380548, ACC:1.0\n",
      "Training iteration 343 loss: 0.00036643544444814324, ACC:1.0\n",
      "Training iteration 344 loss: 0.27738672494888306, ACC:0.96875\n",
      "Training iteration 345 loss: 0.006327378563582897, ACC:1.0\n",
      "Training iteration 346 loss: 0.03384191542863846, ACC:0.984375\n",
      "Training iteration 347 loss: 0.06306108832359314, ACC:0.984375\n",
      "Training iteration 348 loss: 0.00028244894929230213, ACC:1.0\n",
      "Training iteration 349 loss: 0.15960392355918884, ACC:0.984375\n",
      "Training iteration 350 loss: 0.3728425204753876, ACC:0.984375\n",
      "Training iteration 351 loss: 0.02452743798494339, ACC:0.984375\n",
      "Training iteration 352 loss: 0.008457135409116745, ACC:1.0\n",
      "Training iteration 353 loss: 0.0025471183471381664, ACC:1.0\n",
      "Training iteration 354 loss: 0.16772866249084473, ACC:0.953125\n",
      "Training iteration 355 loss: 0.009050068445503712, ACC:1.0\n",
      "Training iteration 356 loss: 0.07087944447994232, ACC:0.96875\n",
      "Training iteration 357 loss: 0.12586762011051178, ACC:0.953125\n",
      "Training iteration 358 loss: 0.07544486969709396, ACC:0.96875\n",
      "Training iteration 359 loss: 0.022140204906463623, ACC:0.984375\n",
      "Training iteration 360 loss: 0.017619449645280838, ACC:1.0\n",
      "Training iteration 361 loss: 0.053301554173231125, ACC:0.984375\n",
      "Training iteration 362 loss: 0.06747107952833176, ACC:0.96875\n",
      "Training iteration 363 loss: 0.023024000227451324, ACC:0.984375\n",
      "Training iteration 364 loss: 0.010435707867145538, ACC:1.0\n",
      "Training iteration 365 loss: 0.006050365511327982, ACC:1.0\n",
      "Training iteration 366 loss: 0.008637158200144768, ACC:1.0\n",
      "Training iteration 367 loss: 0.020817534998059273, ACC:0.984375\n",
      "Training iteration 368 loss: 0.008734511211514473, ACC:1.0\n",
      "Training iteration 369 loss: 0.0007449518889188766, ACC:1.0\n",
      "Training iteration 370 loss: 0.00036763958632946014, ACC:1.0\n",
      "Training iteration 371 loss: 0.0019718469120562077, ACC:1.0\n",
      "Training iteration 372 loss: 0.0012554266722872853, ACC:1.0\n",
      "Training iteration 373 loss: 0.010028212331235409, ACC:1.0\n",
      "Training iteration 374 loss: 0.00013478912296704948, ACC:1.0\n",
      "Training iteration 375 loss: 0.005339427385479212, ACC:1.0\n",
      "Training iteration 376 loss: 0.00026599556440487504, ACC:1.0\n",
      "Training iteration 377 loss: 0.008191953413188457, ACC:1.0\n",
      "Training iteration 378 loss: 0.007173897698521614, ACC:1.0\n",
      "Training iteration 379 loss: 0.22594308853149414, ACC:0.96875\n",
      "Training iteration 380 loss: 0.0002120543795172125, ACC:1.0\n",
      "Training iteration 381 loss: 0.15510810911655426, ACC:0.96875\n",
      "Training iteration 382 loss: 0.0002704366634134203, ACC:1.0\n",
      "Training iteration 383 loss: 0.09767389297485352, ACC:0.984375\n",
      "Training iteration 384 loss: 0.0007535190088674426, ACC:1.0\n",
      "Training iteration 385 loss: 0.0009716666536405683, ACC:1.0\n",
      "Training iteration 386 loss: 0.006283799652010202, ACC:1.0\n",
      "Training iteration 387 loss: 8.377581252716482e-05, ACC:1.0\n",
      "Training iteration 388 loss: 0.1268778145313263, ACC:0.96875\n",
      "Training iteration 389 loss: 0.0012745859567075968, ACC:1.0\n",
      "Training iteration 390 loss: 0.0012215360766276717, ACC:1.0\n",
      "Training iteration 391 loss: 0.005209145601838827, ACC:1.0\n",
      "Training iteration 392 loss: 0.039693597704172134, ACC:0.96875\n",
      "Training iteration 393 loss: 0.04382650554180145, ACC:0.984375\n",
      "Training iteration 394 loss: 0.020644787698984146, ACC:1.0\n",
      "Training iteration 395 loss: 0.01697380840778351, ACC:1.0\n",
      "Training iteration 396 loss: 0.023947644978761673, ACC:1.0\n",
      "Training iteration 397 loss: 0.031173519790172577, ACC:0.984375\n",
      "Training iteration 398 loss: 0.007164481561630964, ACC:1.0\n",
      "Training iteration 399 loss: 0.04401681572198868, ACC:0.984375\n",
      "Training iteration 400 loss: 0.005576552823185921, ACC:1.0\n",
      "Training iteration 401 loss: 0.0058403355069458485, ACC:1.0\n",
      "Training iteration 402 loss: 0.004743744153529406, ACC:1.0\n",
      "Training iteration 403 loss: 0.041281070560216904, ACC:0.984375\n",
      "Training iteration 404 loss: 0.0021989853121340275, ACC:1.0\n",
      "Training iteration 405 loss: 0.002002276014536619, ACC:1.0\n",
      "Training iteration 406 loss: 0.0028412833344191313, ACC:1.0\n",
      "Training iteration 407 loss: 0.019639816135168076, ACC:0.984375\n",
      "Training iteration 408 loss: 0.0022020002361387014, ACC:1.0\n",
      "Training iteration 409 loss: 0.027286440134048462, ACC:0.984375\n",
      "Training iteration 410 loss: 0.0019477778114378452, ACC:1.0\n",
      "Training iteration 411 loss: 0.0012387142051011324, ACC:1.0\n",
      "Training iteration 412 loss: 0.002042709616944194, ACC:1.0\n",
      "Training iteration 413 loss: 0.042629092931747437, ACC:0.96875\n",
      "Training iteration 414 loss: 0.0018005752936005592, ACC:1.0\n",
      "Training iteration 415 loss: 0.12039844691753387, ACC:0.984375\n",
      "Training iteration 416 loss: 0.00011238642036914825, ACC:1.0\n",
      "Training iteration 417 loss: 0.009725848212838173, ACC:1.0\n",
      "Training iteration 418 loss: 0.019730959087610245, ACC:1.0\n",
      "Training iteration 419 loss: 0.011274339631199837, ACC:1.0\n",
      "Training iteration 420 loss: 0.0016268714098259807, ACC:1.0\n",
      "Training iteration 421 loss: 0.13421599566936493, ACC:0.96875\n",
      "Training iteration 422 loss: 0.04292843118309975, ACC:0.984375\n",
      "Training iteration 423 loss: 0.03435039147734642, ACC:0.984375\n",
      "Training iteration 424 loss: 0.06108178570866585, ACC:0.984375\n",
      "Training iteration 425 loss: 0.013421013019979, ACC:1.0\n",
      "Training iteration 426 loss: 0.07925688475370407, ACC:0.984375\n",
      "Training iteration 427 loss: 0.013495287857949734, ACC:1.0\n",
      "Training iteration 428 loss: 0.02352629229426384, ACC:1.0\n",
      "Training iteration 429 loss: 0.009198003448545933, ACC:1.0\n",
      "Training iteration 430 loss: 0.03937292844057083, ACC:0.984375\n",
      "Training iteration 431 loss: 0.04954233393073082, ACC:0.984375\n",
      "Training iteration 432 loss: 0.027393141761422157, ACC:0.984375\n",
      "Training iteration 433 loss: 0.036568526178598404, ACC:0.984375\n",
      "Training iteration 434 loss: 0.00833645835518837, ACC:1.0\n",
      "Training iteration 435 loss: 0.0006945215282030404, ACC:1.0\n",
      "Training iteration 436 loss: 0.008190296590328217, ACC:1.0\n",
      "Training iteration 437 loss: 0.0058731380850076675, ACC:1.0\n",
      "Training iteration 438 loss: 0.10128919780254364, ACC:0.953125\n",
      "Training iteration 439 loss: 0.00027832057094201446, ACC:1.0\n",
      "Training iteration 440 loss: 0.014987323433160782, ACC:1.0\n",
      "Training iteration 441 loss: 0.002793231513351202, ACC:1.0\n",
      "Training iteration 442 loss: 0.03042563609778881, ACC:0.984375\n",
      "Training iteration 443 loss: 0.09350796043872833, ACC:0.984375\n",
      "Training iteration 444 loss: 0.0009583127684891224, ACC:1.0\n",
      "Training iteration 445 loss: 0.02044481784105301, ACC:0.984375\n",
      "Training iteration 446 loss: 0.006904644425958395, ACC:1.0\n",
      "Training iteration 447 loss: 0.0034953535068780184, ACC:1.0\n",
      "Training iteration 448 loss: 0.004774859175086021, ACC:1.0\n",
      "Training iteration 449 loss: 0.012804858386516571, ACC:0.984375\n",
      "Training iteration 450 loss: 0.050197381526231766, ACC:0.96875\n",
      "Validation iteration 451 loss: 0.037193384021520615, ACC: 0.984375\n",
      "Validation iteration 452 loss: 0.01394687406718731, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.06673486530780792, ACC: 0.984375\n",
      "Validation iteration 454 loss: 0.014256736263632774, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.04308933764696121, ACC: 0.96875\n",
      "Validation iteration 456 loss: 0.09742745757102966, ACC: 0.984375\n",
      "Validation iteration 457 loss: 0.0020228007342666388, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.004120355471968651, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.006661588326096535, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.051314424723386765, ACC: 0.984375\n",
      "Validation iteration 461 loss: 0.019884638488292694, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.01182241179049015, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.021052980795502663, ACC: 0.984375\n",
      "Validation iteration 464 loss: 0.08856990933418274, ACC: 0.984375\n",
      "Validation iteration 465 loss: 0.05300822854042053, ACC: 0.96875\n",
      "Validation iteration 466 loss: 0.04011344164609909, ACC: 0.984375\n",
      "Validation iteration 467 loss: 0.040949128568172455, ACC: 0.984375\n",
      "Validation iteration 468 loss: 0.0038520663511008024, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.08309727162122726, ACC: 0.984375\n",
      "Validation iteration 470 loss: 0.009663066826760769, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.04218820855021477, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.0317794568836689, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.04906564578413963, ACC: 0.96875\n",
      "Validation iteration 474 loss: 0.20340274274349213, ACC: 0.96875\n",
      "Validation iteration 475 loss: 0.013506005518138409, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.010477048344910145, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.003791066352277994, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.016285490244627, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.01302016619592905, ACC: 0.984375\n",
      "Validation iteration 480 loss: 0.12773190438747406, ACC: 0.953125\n",
      "Validation iteration 481 loss: 0.015494614839553833, ACC: 0.984375\n",
      "Validation iteration 482 loss: 0.00262067629955709, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.0035098898224532604, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.08204745501279831, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.0006168137770146132, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.109481580555439, ACC: 0.96875\n",
      "Validation iteration 487 loss: 0.11737652868032455, ACC: 0.984375\n",
      "Validation iteration 488 loss: 0.04282528907060623, ACC: 0.984375\n",
      "Validation iteration 489 loss: 0.07438825815916061, ACC: 0.984375\n",
      "Validation iteration 490 loss: 0.10952066630125046, ACC: 0.96875\n",
      "Validation iteration 491 loss: 0.06444963067770004, ACC: 0.953125\n",
      "Validation iteration 492 loss: 0.011701318435370922, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.0319993793964386, ACC: 0.984375\n",
      "Validation iteration 494 loss: 0.03703576698899269, ACC: 0.984375\n",
      "Validation iteration 495 loss: 0.2761389911174774, ACC: 0.96875\n",
      "Validation iteration 496 loss: 0.008717495948076248, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.022971495985984802, ACC: 0.984375\n",
      "Validation iteration 498 loss: 0.020342718809843063, ACC: 0.984375\n",
      "Validation iteration 499 loss: 0.04221680015325546, ACC: 0.984375\n",
      "Validation iteration 500 loss: 0.003914416301995516, ACC: 1.0\n",
      "-- Epoch 5 done -- Train loss: 0.026707044880038465, train ACC: 0.9928125, val loss: 0.04594796978868544, val ACC: 0.9865625\n",
      "<--- 11257.960134029388 seconds --->\n",
      "Training iteration 1 loss: 0.003238885896280408, ACC:1.0\n",
      "Training iteration 2 loss: 0.05716593563556671, ACC:0.96875\n",
      "Training iteration 3 loss: 0.004314783029258251, ACC:1.0\n",
      "Training iteration 4 loss: 0.029428161680698395, ACC:0.984375\n",
      "Training iteration 5 loss: 0.03180449455976486, ACC:0.984375\n",
      "Training iteration 6 loss: 0.003466141875833273, ACC:1.0\n",
      "Training iteration 7 loss: 0.0011156114051118493, ACC:1.0\n",
      "Training iteration 8 loss: 0.0077042970806360245, ACC:1.0\n",
      "Training iteration 9 loss: 0.0010190753964707255, ACC:1.0\n",
      "Training iteration 10 loss: 0.04395786300301552, ACC:0.96875\n",
      "Training iteration 11 loss: 0.0006169476546347141, ACC:1.0\n",
      "Training iteration 12 loss: 0.016024386510252953, ACC:0.984375\n",
      "Training iteration 13 loss: 0.0013174355262890458, ACC:1.0\n",
      "Training iteration 14 loss: 0.00039556415867991745, ACC:1.0\n",
      "Training iteration 15 loss: 0.008607417345046997, ACC:1.0\n",
      "Training iteration 16 loss: 0.007758856285363436, ACC:1.0\n",
      "Training iteration 17 loss: 0.1252923607826233, ACC:0.984375\n",
      "Training iteration 18 loss: 0.01322344783693552, ACC:1.0\n",
      "Training iteration 19 loss: 0.0017377317417412996, ACC:1.0\n",
      "Training iteration 20 loss: 0.03440102934837341, ACC:0.984375\n",
      "Training iteration 21 loss: 0.0003712163306772709, ACC:1.0\n",
      "Training iteration 22 loss: 0.003688166383653879, ACC:1.0\n",
      "Training iteration 23 loss: 0.0034245490096509457, ACC:1.0\n",
      "Training iteration 24 loss: 0.0008586151525378227, ACC:1.0\n",
      "Training iteration 25 loss: 0.06106393411755562, ACC:0.984375\n",
      "Training iteration 26 loss: 0.004013712052255869, ACC:1.0\n",
      "Training iteration 27 loss: 0.0062867687083780766, ACC:1.0\n",
      "Training iteration 28 loss: 0.011307844892144203, ACC:1.0\n",
      "Training iteration 29 loss: 0.010139900259673595, ACC:1.0\n",
      "Training iteration 30 loss: 0.0058966209180653095, ACC:1.0\n",
      "Training iteration 31 loss: 0.016983501613140106, ACC:0.984375\n",
      "Training iteration 32 loss: 0.008979953825473785, ACC:1.0\n",
      "Training iteration 33 loss: 0.004721050150692463, ACC:1.0\n",
      "Training iteration 34 loss: 0.000522852991707623, ACC:1.0\n",
      "Training iteration 35 loss: 0.0010513145243749022, ACC:1.0\n",
      "Training iteration 36 loss: 0.017928078770637512, ACC:0.984375\n",
      "Training iteration 37 loss: 0.0019147770944982767, ACC:1.0\n",
      "Training iteration 38 loss: 0.00677027553319931, ACC:1.0\n",
      "Training iteration 39 loss: 0.024841245263814926, ACC:0.984375\n",
      "Training iteration 40 loss: 0.00012911199883092195, ACC:1.0\n",
      "Training iteration 41 loss: 0.0002851860481314361, ACC:1.0\n",
      "Training iteration 42 loss: 0.0005067163147032261, ACC:1.0\n",
      "Training iteration 43 loss: 0.00015108304796740413, ACC:1.0\n",
      "Training iteration 44 loss: 0.0001728483912302181, ACC:1.0\n",
      "Training iteration 45 loss: 0.08886191248893738, ACC:0.953125\n",
      "Training iteration 46 loss: 0.0004801239992957562, ACC:1.0\n",
      "Training iteration 47 loss: 0.0002655140997376293, ACC:1.0\n",
      "Training iteration 48 loss: 0.01370831299573183, ACC:0.984375\n",
      "Training iteration 49 loss: 0.0006581483758054674, ACC:1.0\n",
      "Training iteration 50 loss: 0.00647742347791791, ACC:1.0\n",
      "Training iteration 51 loss: 0.00011019216617569327, ACC:1.0\n",
      "Training iteration 52 loss: 0.0019694981165230274, ACC:1.0\n",
      "Training iteration 53 loss: 0.0777353048324585, ACC:0.984375\n",
      "Training iteration 54 loss: 0.0002157268754672259, ACC:1.0\n",
      "Training iteration 55 loss: 0.0005205682828091085, ACC:1.0\n",
      "Training iteration 56 loss: 0.00015117133443709463, ACC:1.0\n",
      "Training iteration 57 loss: 0.00313641713000834, ACC:1.0\n",
      "Training iteration 58 loss: 0.0002992964582517743, ACC:1.0\n",
      "Training iteration 59 loss: 0.00023501251416746527, ACC:1.0\n",
      "Training iteration 60 loss: 0.0003623341035563499, ACC:1.0\n",
      "Training iteration 61 loss: 0.0017159746494144201, ACC:1.0\n",
      "Training iteration 62 loss: 0.0008682947373017669, ACC:1.0\n",
      "Training iteration 63 loss: 0.0018468593480065465, ACC:1.0\n",
      "Training iteration 64 loss: 0.016373246908187866, ACC:1.0\n",
      "Training iteration 65 loss: 0.00036151186213828623, ACC:1.0\n",
      "Training iteration 66 loss: 0.00017217399727087468, ACC:1.0\n",
      "Training iteration 67 loss: 0.008115447126328945, ACC:1.0\n",
      "Training iteration 68 loss: 0.12973739206790924, ACC:0.96875\n",
      "Training iteration 69 loss: 0.002415269846096635, ACC:1.0\n",
      "Training iteration 70 loss: 0.005953086540102959, ACC:1.0\n",
      "Training iteration 71 loss: 0.0738004744052887, ACC:0.984375\n",
      "Training iteration 72 loss: 0.0038970173336565495, ACC:1.0\n",
      "Training iteration 73 loss: 0.0019193536136299372, ACC:1.0\n",
      "Training iteration 74 loss: 0.006338567938655615, ACC:1.0\n",
      "Training iteration 75 loss: 0.004884430207312107, ACC:1.0\n",
      "Training iteration 76 loss: 0.0006322389817796648, ACC:1.0\n",
      "Training iteration 77 loss: 0.00043867435306310654, ACC:1.0\n",
      "Training iteration 78 loss: 0.12654438614845276, ACC:0.984375\n",
      "Training iteration 79 loss: 0.0005773404845967889, ACC:1.0\n",
      "Training iteration 80 loss: 0.010317285545170307, ACC:1.0\n",
      "Training iteration 81 loss: 0.003686535870656371, ACC:1.0\n",
      "Training iteration 82 loss: 0.0021202254574745893, ACC:1.0\n",
      "Training iteration 83 loss: 0.0058328621089458466, ACC:1.0\n",
      "Training iteration 84 loss: 0.06964289397001266, ACC:0.96875\n",
      "Training iteration 85 loss: 0.0032434361055493355, ACC:1.0\n",
      "Training iteration 86 loss: 0.03638163581490517, ACC:0.984375\n",
      "Training iteration 87 loss: 0.0005771810538135469, ACC:1.0\n",
      "Training iteration 88 loss: 0.0017176733817905188, ACC:1.0\n",
      "Training iteration 89 loss: 0.0005625160411000252, ACC:1.0\n",
      "Training iteration 90 loss: 0.0004932865267619491, ACC:1.0\n",
      "Training iteration 91 loss: 0.0002486183657310903, ACC:1.0\n",
      "Training iteration 92 loss: 0.1066313087940216, ACC:0.96875\n",
      "Training iteration 93 loss: 0.0014784446684643626, ACC:1.0\n",
      "Training iteration 94 loss: 0.022240402176976204, ACC:0.984375\n",
      "Training iteration 95 loss: 0.0008538122638128698, ACC:1.0\n",
      "Training iteration 96 loss: 0.010965337045490742, ACC:1.0\n",
      "Training iteration 97 loss: 0.0033059404231607914, ACC:1.0\n",
      "Training iteration 98 loss: 0.057848211377859116, ACC:0.984375\n",
      "Training iteration 99 loss: 0.0009500847663730383, ACC:1.0\n",
      "Training iteration 100 loss: 0.03364599123597145, ACC:0.984375\n",
      "Training iteration 101 loss: 0.02220657840371132, ACC:0.984375\n",
      "Training iteration 102 loss: 0.023085955530405045, ACC:0.984375\n",
      "Training iteration 103 loss: 0.001885223900899291, ACC:1.0\n",
      "Training iteration 104 loss: 0.0005901789991185069, ACC:1.0\n",
      "Training iteration 105 loss: 0.0007801339379511774, ACC:1.0\n",
      "Training iteration 106 loss: 0.0075354911386966705, ACC:1.0\n",
      "Training iteration 107 loss: 0.058515869081020355, ACC:0.96875\n",
      "Training iteration 108 loss: 0.000505356932990253, ACC:1.0\n",
      "Training iteration 109 loss: 0.014836207032203674, ACC:0.984375\n",
      "Training iteration 110 loss: 0.0042033870704472065, ACC:1.0\n",
      "Training iteration 111 loss: 0.0002632511605042964, ACC:1.0\n",
      "Training iteration 112 loss: 0.0001631478371564299, ACC:1.0\n",
      "Training iteration 113 loss: 0.0006021849112585187, ACC:1.0\n",
      "Training iteration 114 loss: 0.0012765057617798448, ACC:1.0\n",
      "Training iteration 115 loss: 0.03705258294939995, ACC:0.984375\n",
      "Training iteration 116 loss: 0.0002672173432074487, ACC:1.0\n",
      "Training iteration 117 loss: 0.00251837563700974, ACC:1.0\n",
      "Training iteration 118 loss: 0.0007675617816857994, ACC:1.0\n",
      "Training iteration 119 loss: 0.003999322187155485, ACC:1.0\n",
      "Training iteration 120 loss: 0.0013837168226018548, ACC:1.0\n",
      "Training iteration 121 loss: 0.010133616626262665, ACC:1.0\n",
      "Training iteration 122 loss: 0.10483643412590027, ACC:0.96875\n",
      "Training iteration 123 loss: 0.017031284049153328, ACC:1.0\n",
      "Training iteration 124 loss: 0.025582846254110336, ACC:0.984375\n",
      "Training iteration 125 loss: 0.00130270526278764, ACC:1.0\n",
      "Training iteration 126 loss: 0.009980402886867523, ACC:1.0\n",
      "Training iteration 127 loss: 0.005009141750633717, ACC:1.0\n",
      "Training iteration 128 loss: 0.0009239990031346679, ACC:1.0\n",
      "Training iteration 129 loss: 0.000615510216448456, ACC:1.0\n",
      "Training iteration 130 loss: 0.0018994716228917241, ACC:1.0\n",
      "Training iteration 131 loss: 0.006308138836175203, ACC:1.0\n",
      "Training iteration 132 loss: 0.004814100451767445, ACC:1.0\n",
      "Training iteration 133 loss: 0.01952422596514225, ACC:0.984375\n",
      "Training iteration 134 loss: 0.06981701403856277, ACC:0.96875\n",
      "Training iteration 135 loss: 0.004810051526874304, ACC:1.0\n",
      "Training iteration 136 loss: 0.00018550714594312012, ACC:1.0\n",
      "Training iteration 137 loss: 0.0010025204392150044, ACC:1.0\n",
      "Training iteration 138 loss: 0.034879572689533234, ACC:0.984375\n",
      "Training iteration 139 loss: 0.032006699591875076, ACC:0.984375\n",
      "Training iteration 140 loss: 0.0028591686859726906, ACC:1.0\n",
      "Training iteration 141 loss: 0.004091634415090084, ACC:1.0\n",
      "Training iteration 142 loss: 0.007217475213110447, ACC:1.0\n",
      "Training iteration 143 loss: 0.0009633778827264905, ACC:1.0\n",
      "Training iteration 144 loss: 0.0345446839928627, ACC:0.96875\n",
      "Training iteration 145 loss: 0.004498444963246584, ACC:1.0\n",
      "Training iteration 146 loss: 0.015514904633164406, ACC:0.984375\n",
      "Training iteration 147 loss: 0.0662691742181778, ACC:0.953125\n",
      "Training iteration 148 loss: 0.0006590879056602716, ACC:1.0\n",
      "Training iteration 149 loss: 0.0020268207881599665, ACC:1.0\n",
      "Training iteration 150 loss: 0.028259756043553352, ACC:0.984375\n",
      "Training iteration 151 loss: 0.0020355740562081337, ACC:1.0\n",
      "Training iteration 152 loss: 0.016297779977321625, ACC:0.984375\n",
      "Training iteration 153 loss: 0.00027815764769911766, ACC:1.0\n",
      "Training iteration 154 loss: 0.0007822299376130104, ACC:1.0\n",
      "Training iteration 155 loss: 0.01249460969120264, ACC:0.984375\n",
      "Training iteration 156 loss: 0.00037923891795799136, ACC:1.0\n",
      "Training iteration 157 loss: 0.1311749517917633, ACC:0.984375\n",
      "Training iteration 158 loss: 0.10534478724002838, ACC:0.984375\n",
      "Training iteration 159 loss: 0.00010941666550934315, ACC:1.0\n",
      "Training iteration 160 loss: 0.0006447694031521678, ACC:1.0\n",
      "Training iteration 161 loss: 0.004729175008833408, ACC:1.0\n",
      "Training iteration 162 loss: 0.003968341741710901, ACC:1.0\n",
      "Training iteration 163 loss: 0.0027018114924430847, ACC:1.0\n",
      "Training iteration 164 loss: 0.0009127286612056196, ACC:1.0\n",
      "Training iteration 165 loss: 0.025242064148187637, ACC:0.984375\n",
      "Training iteration 166 loss: 0.0005259664612822235, ACC:1.0\n",
      "Training iteration 167 loss: 0.018792543560266495, ACC:0.984375\n",
      "Training iteration 168 loss: 0.00279217935167253, ACC:1.0\n",
      "Training iteration 169 loss: 0.0018162325723096728, ACC:1.0\n",
      "Training iteration 170 loss: 0.02176571451127529, ACC:0.984375\n",
      "Training iteration 171 loss: 0.007653573527932167, ACC:1.0\n",
      "Training iteration 172 loss: 0.030563386157155037, ACC:0.984375\n",
      "Training iteration 173 loss: 0.019191499799489975, ACC:0.984375\n",
      "Training iteration 174 loss: 0.00013745629985351115, ACC:1.0\n",
      "Training iteration 175 loss: 0.0003486814093776047, ACC:1.0\n",
      "Training iteration 176 loss: 0.003105336334556341, ACC:1.0\n",
      "Training iteration 177 loss: 0.010303940623998642, ACC:1.0\n",
      "Training iteration 178 loss: 0.02155255898833275, ACC:0.984375\n",
      "Training iteration 179 loss: 0.03660297766327858, ACC:0.984375\n",
      "Training iteration 180 loss: 0.0006549553945660591, ACC:1.0\n",
      "Training iteration 181 loss: 0.0008295484003610909, ACC:1.0\n",
      "Training iteration 182 loss: 4.2483763536438346e-05, ACC:1.0\n",
      "Training iteration 183 loss: 0.011471839621663094, ACC:1.0\n",
      "Training iteration 184 loss: 0.05098273232579231, ACC:0.984375\n",
      "Training iteration 185 loss: 0.00016628771845716983, ACC:1.0\n",
      "Training iteration 186 loss: 0.0002871942415367812, ACC:1.0\n",
      "Training iteration 187 loss: 0.056505236774683, ACC:0.984375\n",
      "Training iteration 188 loss: 0.0014017692301422358, ACC:1.0\n",
      "Training iteration 189 loss: 0.0002463301061652601, ACC:1.0\n",
      "Training iteration 190 loss: 0.0005771468277089298, ACC:1.0\n",
      "Training iteration 191 loss: 0.006555227562785149, ACC:1.0\n",
      "Training iteration 192 loss: 0.0013719070702791214, ACC:1.0\n",
      "Training iteration 193 loss: 0.0006400875863619149, ACC:1.0\n",
      "Training iteration 194 loss: 0.01605570688843727, ACC:1.0\n",
      "Training iteration 195 loss: 0.09610594063997269, ACC:0.984375\n",
      "Training iteration 196 loss: 0.0008144504390656948, ACC:1.0\n",
      "Training iteration 197 loss: 0.00030592430266551673, ACC:1.0\n",
      "Training iteration 198 loss: 0.008239743299782276, ACC:1.0\n",
      "Training iteration 199 loss: 0.003465148853138089, ACC:1.0\n",
      "Training iteration 200 loss: 0.03393947705626488, ACC:0.984375\n",
      "Training iteration 201 loss: 0.007265920285135508, ACC:1.0\n",
      "Training iteration 202 loss: 0.0017806453397497535, ACC:1.0\n",
      "Training iteration 203 loss: 0.04876988008618355, ACC:0.984375\n",
      "Training iteration 204 loss: 0.09402984380722046, ACC:0.953125\n",
      "Training iteration 205 loss: 0.001322844997048378, ACC:1.0\n",
      "Training iteration 206 loss: 0.00940888375043869, ACC:1.0\n",
      "Training iteration 207 loss: 0.17039558291435242, ACC:0.9375\n",
      "Training iteration 208 loss: 0.015237242914736271, ACC:0.984375\n",
      "Training iteration 209 loss: 0.0005392680177465081, ACC:1.0\n",
      "Training iteration 210 loss: 0.051101841032505035, ACC:0.96875\n",
      "Training iteration 211 loss: 0.006969456095248461, ACC:1.0\n",
      "Training iteration 212 loss: 0.0016049706609919667, ACC:1.0\n",
      "Training iteration 213 loss: 0.04036363959312439, ACC:0.984375\n",
      "Training iteration 214 loss: 0.005936746951192617, ACC:1.0\n",
      "Training iteration 215 loss: 0.0008428688743151724, ACC:1.0\n",
      "Training iteration 216 loss: 0.06866157054901123, ACC:0.984375\n",
      "Training iteration 217 loss: 0.0012937458232045174, ACC:1.0\n",
      "Training iteration 218 loss: 0.0030754334293305874, ACC:1.0\n",
      "Training iteration 219 loss: 0.04045836627483368, ACC:0.984375\n",
      "Training iteration 220 loss: 0.01606876589357853, ACC:0.984375\n",
      "Training iteration 221 loss: 0.006973348557949066, ACC:1.0\n",
      "Training iteration 222 loss: 0.17108315229415894, ACC:0.96875\n",
      "Training iteration 223 loss: 0.07509754598140717, ACC:0.984375\n",
      "Training iteration 224 loss: 0.05074073746800423, ACC:0.984375\n",
      "Training iteration 225 loss: 0.07677996158599854, ACC:0.96875\n",
      "Training iteration 226 loss: 0.08620330691337585, ACC:0.984375\n",
      "Training iteration 227 loss: 0.018827414140105247, ACC:0.984375\n",
      "Training iteration 228 loss: 0.009931683540344238, ACC:1.0\n",
      "Training iteration 229 loss: 0.017133032903075218, ACC:0.984375\n",
      "Training iteration 230 loss: 0.004935285542160273, ACC:1.0\n",
      "Training iteration 231 loss: 0.1291821300983429, ACC:0.984375\n",
      "Training iteration 232 loss: 0.08198466151952744, ACC:0.96875\n",
      "Training iteration 233 loss: 0.10030905157327652, ACC:0.953125\n",
      "Training iteration 234 loss: 0.012385175563395023, ACC:1.0\n",
      "Training iteration 235 loss: 0.14457236230373383, ACC:0.984375\n",
      "Training iteration 236 loss: 0.0027845457661896944, ACC:1.0\n",
      "Training iteration 237 loss: 0.01199902594089508, ACC:1.0\n",
      "Training iteration 238 loss: 0.018138090148568153, ACC:0.984375\n",
      "Training iteration 239 loss: 0.020020300522446632, ACC:0.984375\n",
      "Training iteration 240 loss: 0.020389903336763382, ACC:0.984375\n",
      "Training iteration 241 loss: 0.03578529134392738, ACC:0.984375\n",
      "Training iteration 242 loss: 0.0037799638230353594, ACC:1.0\n",
      "Training iteration 243 loss: 0.029086630791425705, ACC:0.984375\n",
      "Training iteration 244 loss: 0.004766352474689484, ACC:1.0\n",
      "Training iteration 245 loss: 0.0009139173198491335, ACC:1.0\n",
      "Training iteration 246 loss: 0.002033197320997715, ACC:1.0\n",
      "Training iteration 247 loss: 6.286503776209429e-05, ACC:1.0\n",
      "Training iteration 248 loss: 0.13814660906791687, ACC:0.953125\n",
      "Training iteration 249 loss: 0.004358671605587006, ACC:1.0\n",
      "Training iteration 250 loss: 0.054684340953826904, ACC:0.96875\n",
      "Training iteration 251 loss: 0.00748702185228467, ACC:1.0\n",
      "Training iteration 252 loss: 0.01205763965845108, ACC:1.0\n",
      "Training iteration 253 loss: 0.011909919790923595, ACC:1.0\n",
      "Training iteration 254 loss: 0.002546369330957532, ACC:1.0\n",
      "Training iteration 255 loss: 0.015845824033021927, ACC:0.984375\n",
      "Training iteration 256 loss: 0.001337192370556295, ACC:1.0\n",
      "Training iteration 257 loss: 0.003318923758342862, ACC:1.0\n",
      "Training iteration 258 loss: 0.035294074565172195, ACC:0.984375\n",
      "Training iteration 259 loss: 0.0007252416107803583, ACC:1.0\n",
      "Training iteration 260 loss: 0.000525919022038579, ACC:1.0\n",
      "Training iteration 261 loss: 0.0005434912745840847, ACC:1.0\n",
      "Training iteration 262 loss: 0.00047122861724346876, ACC:1.0\n",
      "Training iteration 263 loss: 0.002801222261041403, ACC:1.0\n",
      "Training iteration 264 loss: 0.04573538526892662, ACC:0.984375\n",
      "Training iteration 265 loss: 0.007058416027575731, ACC:1.0\n",
      "Training iteration 266 loss: 0.00266426638700068, ACC:1.0\n",
      "Training iteration 267 loss: 0.00031447713263332844, ACC:1.0\n",
      "Training iteration 268 loss: 0.00031184175168164074, ACC:1.0\n",
      "Training iteration 269 loss: 0.002116063144057989, ACC:1.0\n",
      "Training iteration 270 loss: 0.25517675280570984, ACC:0.984375\n",
      "Training iteration 271 loss: 0.0014939969405531883, ACC:1.0\n",
      "Training iteration 272 loss: 0.03887009620666504, ACC:0.984375\n",
      "Training iteration 273 loss: 0.0049917856231331825, ACC:1.0\n",
      "Training iteration 274 loss: 0.023303600028157234, ACC:0.984375\n",
      "Training iteration 275 loss: 0.0014763660728931427, ACC:1.0\n",
      "Training iteration 276 loss: 0.04500710591673851, ACC:0.984375\n",
      "Training iteration 277 loss: 0.04592873901128769, ACC:0.984375\n",
      "Training iteration 278 loss: 0.02382279746234417, ACC:0.984375\n",
      "Training iteration 279 loss: 0.01042251754552126, ACC:1.0\n",
      "Training iteration 280 loss: 0.007822675630450249, ACC:1.0\n",
      "Training iteration 281 loss: 0.0033027376048266888, ACC:1.0\n",
      "Training iteration 282 loss: 0.004062329884618521, ACC:1.0\n",
      "Training iteration 283 loss: 0.026198161765933037, ACC:0.984375\n",
      "Training iteration 284 loss: 0.002580345841124654, ACC:1.0\n",
      "Training iteration 285 loss: 0.015071547590196133, ACC:1.0\n",
      "Training iteration 286 loss: 0.0045298002660274506, ACC:1.0\n",
      "Training iteration 287 loss: 0.015578413382172585, ACC:0.984375\n",
      "Training iteration 288 loss: 0.002036327961832285, ACC:1.0\n",
      "Training iteration 289 loss: 0.001961039612069726, ACC:1.0\n",
      "Training iteration 290 loss: 0.020298194140195847, ACC:0.984375\n",
      "Training iteration 291 loss: 0.00037644035182893276, ACC:1.0\n",
      "Training iteration 292 loss: 0.0010583268012851477, ACC:1.0\n",
      "Training iteration 293 loss: 0.011582179926335812, ACC:0.984375\n",
      "Training iteration 294 loss: 0.000533744168933481, ACC:1.0\n",
      "Training iteration 295 loss: 0.0046464367769658566, ACC:1.0\n",
      "Training iteration 296 loss: 0.0641167163848877, ACC:0.984375\n",
      "Training iteration 297 loss: 0.0003097352455370128, ACC:1.0\n",
      "Training iteration 298 loss: 0.13806550204753876, ACC:0.984375\n",
      "Training iteration 299 loss: 2.6375189918326214e-05, ACC:1.0\n",
      "Training iteration 300 loss: 0.003216950921341777, ACC:1.0\n",
      "Training iteration 301 loss: 0.0012406631140038371, ACC:1.0\n",
      "Training iteration 302 loss: 0.01567947119474411, ACC:0.984375\n",
      "Training iteration 303 loss: 0.06326114386320114, ACC:0.984375\n",
      "Training iteration 304 loss: 0.4390861392021179, ACC:0.953125\n",
      "Training iteration 305 loss: 0.010455421172082424, ACC:1.0\n",
      "Training iteration 306 loss: 0.001379932975396514, ACC:1.0\n",
      "Training iteration 307 loss: 0.05374383181333542, ACC:0.984375\n",
      "Training iteration 308 loss: 0.0056627243757247925, ACC:1.0\n",
      "Training iteration 309 loss: 0.121613509953022, ACC:0.96875\n",
      "Training iteration 310 loss: 0.02011289820075035, ACC:1.0\n",
      "Training iteration 311 loss: 0.09328038990497589, ACC:0.96875\n",
      "Training iteration 312 loss: 0.009918718598783016, ACC:1.0\n",
      "Training iteration 313 loss: 0.022911792621016502, ACC:0.984375\n",
      "Training iteration 314 loss: 0.006025838665664196, ACC:1.0\n",
      "Training iteration 315 loss: 0.050071246922016144, ACC:0.96875\n",
      "Training iteration 316 loss: 0.002342997817322612, ACC:1.0\n",
      "Training iteration 317 loss: 0.0037331674247980118, ACC:1.0\n",
      "Training iteration 318 loss: 0.04666793346405029, ACC:0.984375\n",
      "Training iteration 319 loss: 0.006893666926771402, ACC:1.0\n",
      "Training iteration 320 loss: 0.0022590532898902893, ACC:1.0\n",
      "Training iteration 321 loss: 0.003427802585065365, ACC:1.0\n",
      "Training iteration 322 loss: 0.003667638171464205, ACC:1.0\n",
      "Training iteration 323 loss: 0.05683114379644394, ACC:0.984375\n",
      "Training iteration 324 loss: 0.000383282225811854, ACC:1.0\n",
      "Training iteration 325 loss: 0.00040289529715664685, ACC:1.0\n",
      "Training iteration 326 loss: 0.0016849001403898, ACC:1.0\n",
      "Training iteration 327 loss: 8.549674384994432e-05, ACC:1.0\n",
      "Training iteration 328 loss: 0.09595581144094467, ACC:0.96875\n",
      "Training iteration 329 loss: 0.0474240817129612, ACC:0.984375\n",
      "Training iteration 330 loss: 0.21310801804065704, ACC:0.984375\n",
      "Training iteration 331 loss: 0.23506079614162445, ACC:0.96875\n",
      "Training iteration 332 loss: 0.0010270506609231234, ACC:1.0\n",
      "Training iteration 333 loss: 0.07787570357322693, ACC:0.96875\n",
      "Training iteration 334 loss: 0.002283095847815275, ACC:1.0\n",
      "Training iteration 335 loss: 0.04041433706879616, ACC:0.984375\n",
      "Training iteration 336 loss: 0.0049257222563028336, ACC:1.0\n",
      "Training iteration 337 loss: 0.04854990541934967, ACC:0.96875\n",
      "Training iteration 338 loss: 0.018722331151366234, ACC:1.0\n",
      "Training iteration 339 loss: 0.008633419871330261, ACC:1.0\n",
      "Training iteration 340 loss: 0.021937832236289978, ACC:1.0\n",
      "Training iteration 341 loss: 0.029519977048039436, ACC:1.0\n",
      "Training iteration 342 loss: 0.02886977419257164, ACC:0.984375\n",
      "Training iteration 343 loss: 0.005620536860078573, ACC:1.0\n",
      "Training iteration 344 loss: 0.009625982493162155, ACC:1.0\n",
      "Training iteration 345 loss: 0.12374826520681381, ACC:0.96875\n",
      "Training iteration 346 loss: 0.005657892674207687, ACC:1.0\n",
      "Training iteration 347 loss: 0.00382522726431489, ACC:1.0\n",
      "Training iteration 348 loss: 0.0020682616159319878, ACC:1.0\n",
      "Training iteration 349 loss: 0.06789479404687881, ACC:0.984375\n",
      "Training iteration 350 loss: 0.15731213986873627, ACC:0.96875\n",
      "Training iteration 351 loss: 0.04685711860656738, ACC:0.984375\n",
      "Training iteration 352 loss: 0.0005281462799757719, ACC:1.0\n",
      "Training iteration 353 loss: 0.04106040298938751, ACC:0.984375\n",
      "Training iteration 354 loss: 0.002941802842542529, ACC:1.0\n",
      "Training iteration 355 loss: 0.006450230721384287, ACC:1.0\n",
      "Training iteration 356 loss: 0.000810058496426791, ACC:1.0\n",
      "Training iteration 357 loss: 0.004673948511481285, ACC:1.0\n",
      "Training iteration 358 loss: 0.04529115557670593, ACC:0.96875\n",
      "Training iteration 359 loss: 0.012953758239746094, ACC:1.0\n",
      "Training iteration 360 loss: 0.007645641919225454, ACC:1.0\n",
      "Training iteration 361 loss: 0.002136806957423687, ACC:1.0\n",
      "Training iteration 362 loss: 0.03904293105006218, ACC:0.984375\n",
      "Training iteration 363 loss: 0.007129767909646034, ACC:1.0\n",
      "Training iteration 364 loss: 0.011135930195450783, ACC:1.0\n",
      "Training iteration 365 loss: 0.010616395622491837, ACC:1.0\n",
      "Training iteration 366 loss: 0.0019469109829515219, ACC:1.0\n",
      "Training iteration 367 loss: 0.0022308959160000086, ACC:1.0\n",
      "Training iteration 368 loss: 0.0054382761009037495, ACC:1.0\n",
      "Training iteration 369 loss: 0.009727663360536098, ACC:1.0\n",
      "Training iteration 370 loss: 0.0016133759636431932, ACC:1.0\n",
      "Training iteration 371 loss: 0.00234200875274837, ACC:1.0\n",
      "Training iteration 372 loss: 0.0027138337027281523, ACC:1.0\n",
      "Training iteration 373 loss: 0.20858028531074524, ACC:0.96875\n",
      "Training iteration 374 loss: 0.0007123196264728904, ACC:1.0\n",
      "Training iteration 375 loss: 0.012600812129676342, ACC:1.0\n",
      "Training iteration 376 loss: 0.025727732107043266, ACC:0.984375\n",
      "Training iteration 377 loss: 0.0067653474397957325, ACC:1.0\n",
      "Training iteration 378 loss: 0.0013364899205043912, ACC:1.0\n",
      "Training iteration 379 loss: 0.007852704264223576, ACC:1.0\n",
      "Training iteration 380 loss: 0.002432046225294471, ACC:1.0\n",
      "Training iteration 381 loss: 0.0003121506597381085, ACC:1.0\n",
      "Training iteration 382 loss: 0.08080059289932251, ACC:0.984375\n",
      "Training iteration 383 loss: 0.00040383535088039935, ACC:1.0\n",
      "Training iteration 384 loss: 0.005202421918511391, ACC:1.0\n",
      "Training iteration 385 loss: 0.0267674308270216, ACC:0.984375\n",
      "Training iteration 386 loss: 0.010054169222712517, ACC:1.0\n",
      "Training iteration 387 loss: 0.000455612811492756, ACC:1.0\n",
      "Training iteration 388 loss: 0.0007792350370436907, ACC:1.0\n",
      "Training iteration 389 loss: 0.029077857732772827, ACC:0.984375\n",
      "Training iteration 390 loss: 0.00014204913168214262, ACC:1.0\n",
      "Training iteration 391 loss: 0.0009569964604452252, ACC:1.0\n",
      "Training iteration 392 loss: 0.13053426146507263, ACC:0.953125\n",
      "Training iteration 393 loss: 2.7737245545722544e-05, ACC:1.0\n",
      "Training iteration 394 loss: 0.0007991322781890631, ACC:1.0\n",
      "Training iteration 395 loss: 0.04027814418077469, ACC:0.984375\n",
      "Training iteration 396 loss: 0.03456003963947296, ACC:0.984375\n",
      "Training iteration 397 loss: 0.12543080747127533, ACC:0.953125\n",
      "Training iteration 398 loss: 0.051229078322649, ACC:0.984375\n",
      "Training iteration 399 loss: 0.03736890107393265, ACC:0.984375\n",
      "Training iteration 400 loss: 0.0015848154434934258, ACC:1.0\n",
      "Training iteration 401 loss: 0.042310576885938644, ACC:0.984375\n",
      "Training iteration 402 loss: 0.0013384222984313965, ACC:1.0\n",
      "Training iteration 403 loss: 0.031941480934619904, ACC:0.984375\n",
      "Training iteration 404 loss: 0.01097794994711876, ACC:1.0\n",
      "Training iteration 405 loss: 0.01594892144203186, ACC:1.0\n",
      "Training iteration 406 loss: 0.01764705218374729, ACC:0.984375\n",
      "Training iteration 407 loss: 0.004286634270101786, ACC:1.0\n",
      "Training iteration 408 loss: 0.002064040396362543, ACC:1.0\n",
      "Training iteration 409 loss: 0.09194028377532959, ACC:0.984375\n",
      "Training iteration 410 loss: 0.020335236564278603, ACC:0.984375\n",
      "Training iteration 411 loss: 0.003115271683782339, ACC:1.0\n",
      "Training iteration 412 loss: 0.002203227486461401, ACC:1.0\n",
      "Training iteration 413 loss: 0.011777332983911037, ACC:1.0\n",
      "Training iteration 414 loss: 0.0015329498564824462, ACC:1.0\n",
      "Training iteration 415 loss: 0.009995201602578163, ACC:1.0\n",
      "Training iteration 416 loss: 0.009547516703605652, ACC:1.0\n",
      "Training iteration 417 loss: 0.003993440419435501, ACC:1.0\n",
      "Training iteration 418 loss: 0.0071174041368067265, ACC:1.0\n",
      "Training iteration 419 loss: 0.0005388485733419657, ACC:1.0\n",
      "Training iteration 420 loss: 0.0007784203044138849, ACC:1.0\n",
      "Training iteration 421 loss: 0.04293360561132431, ACC:0.984375\n",
      "Training iteration 422 loss: 0.0010773218236863613, ACC:1.0\n",
      "Training iteration 423 loss: 0.002256065374240279, ACC:1.0\n",
      "Training iteration 424 loss: 0.036464545875787735, ACC:0.96875\n",
      "Training iteration 425 loss: 0.02276739664375782, ACC:0.984375\n",
      "Training iteration 426 loss: 0.004874503705650568, ACC:1.0\n",
      "Training iteration 427 loss: 0.08185507357120514, ACC:0.96875\n",
      "Training iteration 428 loss: 0.001838062540628016, ACC:1.0\n",
      "Training iteration 429 loss: 0.001390264485962689, ACC:1.0\n",
      "Training iteration 430 loss: 0.014724932610988617, ACC:1.0\n",
      "Training iteration 431 loss: 0.0034296512603759766, ACC:1.0\n",
      "Training iteration 432 loss: 0.0016713215736672282, ACC:1.0\n",
      "Training iteration 433 loss: 0.00850742682814598, ACC:1.0\n",
      "Training iteration 434 loss: 0.0676150694489479, ACC:0.96875\n",
      "Training iteration 435 loss: 0.003544341307133436, ACC:1.0\n",
      "Training iteration 436 loss: 0.0013185171410441399, ACC:1.0\n",
      "Training iteration 437 loss: 0.00468045100569725, ACC:1.0\n",
      "Training iteration 438 loss: 0.0015357376541942358, ACC:1.0\n",
      "Training iteration 439 loss: 0.0017221657326444983, ACC:1.0\n",
      "Training iteration 440 loss: 0.06775543093681335, ACC:0.984375\n",
      "Training iteration 441 loss: 0.0016546666156500578, ACC:1.0\n",
      "Training iteration 442 loss: 0.018147379159927368, ACC:0.984375\n",
      "Training iteration 443 loss: 0.00036661914782598615, ACC:1.0\n",
      "Training iteration 444 loss: 0.00034750724444165826, ACC:1.0\n",
      "Training iteration 445 loss: 0.0031088090036064386, ACC:1.0\n",
      "Training iteration 446 loss: 0.00044765209895558655, ACC:1.0\n",
      "Training iteration 447 loss: 0.08850087225437164, ACC:0.984375\n",
      "Training iteration 448 loss: 0.032928336411714554, ACC:0.984375\n",
      "Training iteration 449 loss: 0.007063235156238079, ACC:1.0\n",
      "Training iteration 450 loss: 0.017793351784348488, ACC:1.0\n",
      "Validation iteration 451 loss: 0.0023193422239273787, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.0003120425681117922, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.12160045653581619, ACC: 0.984375\n",
      "Validation iteration 454 loss: 0.013713126070797443, ACC: 0.984375\n",
      "Validation iteration 455 loss: 0.0007548684952780604, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.001665598712861538, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.021124236285686493, ACC: 0.984375\n",
      "Validation iteration 458 loss: 0.0030893986113369465, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.057122692465782166, ACC: 0.984375\n",
      "Validation iteration 460 loss: 0.0005978830740787089, ACC: 1.0\n",
      "Validation iteration 461 loss: 0.0012506884522736073, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.0005910907639190555, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.008073536679148674, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.0024374714121222496, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.001145100686699152, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.009225236251950264, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.03050001710653305, ACC: 0.984375\n",
      "Validation iteration 468 loss: 0.0007377390866167843, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.0026461384259164333, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.0007881412748247385, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.042497847229242325, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.0008113532094284892, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.019568201154470444, ACC: 0.984375\n",
      "Validation iteration 474 loss: 0.0003996735031250864, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.00287548522464931, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.07877664268016815, ACC: 0.984375\n",
      "Validation iteration 477 loss: 0.015168891288340092, ACC: 0.984375\n",
      "Validation iteration 478 loss: 0.0007683666190132499, ACC: 1.0\n",
      "Validation iteration 479 loss: 0.06970434635877609, ACC: 0.984375\n",
      "Validation iteration 480 loss: 0.041339173913002014, ACC: 0.984375\n",
      "Validation iteration 481 loss: 0.1758498102426529, ACC: 0.984375\n",
      "Validation iteration 482 loss: 0.007908523082733154, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.028198877349495888, ACC: 0.984375\n",
      "Validation iteration 484 loss: 0.029876355081796646, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.0005199959850870073, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.0010578209767118096, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.001277774223126471, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.000566378585062921, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.00043728086166083813, ACC: 1.0\n",
      "Validation iteration 490 loss: 0.0022127851843833923, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.006957896985113621, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.06419423967599869, ACC: 0.984375\n",
      "Validation iteration 493 loss: 0.02612561173737049, ACC: 0.984375\n",
      "Validation iteration 494 loss: 0.0014672406250610948, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.0018745826091617346, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.005499404389411211, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.0009250232833437622, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.0013630012981593609, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.009866363368928432, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.045787014067173004, ACC: 0.984375\n",
      "-- Epoch 6 done -- Train loss: 0.022430752338946654, train ACC: 0.9933333333333333, val loss: 0.01927141531952657, val ACC: 0.9946875\n",
      "<--- 12024.20096540451 seconds --->\n",
      "Training iteration 1 loss: 0.0011363993398845196, ACC:1.0\n",
      "Training iteration 2 loss: 0.0004945763503201306, ACC:1.0\n",
      "Training iteration 3 loss: 0.0013658716343343258, ACC:1.0\n",
      "Training iteration 4 loss: 0.00044253122177906334, ACC:1.0\n",
      "Training iteration 5 loss: 0.00026458242791704834, ACC:1.0\n",
      "Training iteration 6 loss: 0.000460811541415751, ACC:1.0\n",
      "Training iteration 7 loss: 0.00018092722166329622, ACC:1.0\n",
      "Training iteration 8 loss: 0.000347190914908424, ACC:1.0\n",
      "Training iteration 9 loss: 0.0005534965894185007, ACC:1.0\n",
      "Training iteration 10 loss: 0.0009580528712831438, ACC:1.0\n",
      "Training iteration 11 loss: 0.0007543722749687731, ACC:1.0\n",
      "Training iteration 12 loss: 0.00016811095701996237, ACC:1.0\n",
      "Training iteration 13 loss: 0.0003872259985655546, ACC:1.0\n",
      "Training iteration 14 loss: 0.0002636427525430918, ACC:1.0\n",
      "Training iteration 15 loss: 0.0005891760811209679, ACC:1.0\n",
      "Training iteration 16 loss: 0.002518824767321348, ACC:1.0\n",
      "Training iteration 17 loss: 0.001427750918082893, ACC:1.0\n",
      "Training iteration 18 loss: 0.0004218565300107002, ACC:1.0\n",
      "Training iteration 19 loss: 0.00011499747051857412, ACC:1.0\n",
      "Training iteration 20 loss: 0.004622654058039188, ACC:1.0\n",
      "Training iteration 21 loss: 0.0003808095643762499, ACC:1.0\n",
      "Training iteration 22 loss: 0.00015240401262417436, ACC:1.0\n",
      "Training iteration 23 loss: 0.00046826450852677226, ACC:1.0\n",
      "Training iteration 24 loss: 0.013997921720147133, ACC:0.984375\n",
      "Training iteration 25 loss: 0.008169641718268394, ACC:1.0\n",
      "Training iteration 26 loss: 0.0003645085380412638, ACC:1.0\n",
      "Training iteration 27 loss: 0.0005835834890604019, ACC:1.0\n",
      "Training iteration 28 loss: 0.0004740913864225149, ACC:1.0\n",
      "Training iteration 29 loss: 0.007142213173210621, ACC:1.0\n",
      "Training iteration 30 loss: 0.0007696330430917442, ACC:1.0\n",
      "Training iteration 31 loss: 0.0005230569513514638, ACC:1.0\n",
      "Training iteration 32 loss: 0.0038513478357344866, ACC:1.0\n",
      "Training iteration 33 loss: 0.015654543414711952, ACC:1.0\n",
      "Training iteration 34 loss: 0.1348869502544403, ACC:0.984375\n",
      "Training iteration 35 loss: 0.08997897803783417, ACC:0.984375\n",
      "Training iteration 36 loss: 0.0002442261320538819, ACC:1.0\n",
      "Training iteration 37 loss: 0.00442847004160285, ACC:1.0\n",
      "Training iteration 38 loss: 0.06906713545322418, ACC:0.984375\n",
      "Training iteration 39 loss: 0.00017550709890201688, ACC:1.0\n",
      "Training iteration 40 loss: 0.013786878436803818, ACC:1.0\n",
      "Training iteration 41 loss: 0.019009804353117943, ACC:0.984375\n",
      "Training iteration 42 loss: 0.014144498854875565, ACC:0.984375\n",
      "Training iteration 43 loss: 0.08596355468034744, ACC:0.96875\n",
      "Training iteration 44 loss: 0.0914616584777832, ACC:0.953125\n",
      "Training iteration 45 loss: 0.0022646458819508553, ACC:1.0\n",
      "Training iteration 46 loss: 0.01177431270480156, ACC:1.0\n",
      "Training iteration 47 loss: 0.007484334520995617, ACC:1.0\n",
      "Training iteration 48 loss: 0.04117236286401749, ACC:0.984375\n",
      "Training iteration 49 loss: 0.0043028551153838634, ACC:1.0\n",
      "Training iteration 50 loss: 0.0019151708111166954, ACC:1.0\n",
      "Training iteration 51 loss: 0.0037863729521632195, ACC:1.0\n",
      "Training iteration 52 loss: 0.00274959160014987, ACC:1.0\n",
      "Training iteration 53 loss: 0.0033134836703538895, ACC:1.0\n",
      "Training iteration 54 loss: 0.008592643775045872, ACC:1.0\n",
      "Training iteration 55 loss: 0.0028826207853853703, ACC:1.0\n",
      "Training iteration 56 loss: 0.0008040547254495323, ACC:1.0\n",
      "Training iteration 57 loss: 0.10396763682365417, ACC:0.96875\n",
      "Training iteration 58 loss: 0.0005600008880719543, ACC:1.0\n",
      "Training iteration 59 loss: 0.00019321126455906779, ACC:1.0\n",
      "Training iteration 60 loss: 0.0019724976737052202, ACC:1.0\n",
      "Training iteration 61 loss: 0.026029525324702263, ACC:0.984375\n",
      "Training iteration 62 loss: 0.004692547954618931, ACC:1.0\n",
      "Training iteration 63 loss: 0.01393248699605465, ACC:0.984375\n",
      "Training iteration 64 loss: 0.000436229893239215, ACC:1.0\n",
      "Training iteration 65 loss: 0.10937824845314026, ACC:0.953125\n",
      "Training iteration 66 loss: 0.000826301402412355, ACC:1.0\n",
      "Training iteration 67 loss: 0.00014627208292949945, ACC:1.0\n",
      "Training iteration 68 loss: 0.017442453652620316, ACC:0.984375\n",
      "Training iteration 69 loss: 0.00039482821011915803, ACC:1.0\n",
      "Training iteration 70 loss: 0.0969584733247757, ACC:0.96875\n",
      "Training iteration 71 loss: 0.06372369080781937, ACC:0.984375\n",
      "Training iteration 72 loss: 0.16191384196281433, ACC:0.984375\n",
      "Training iteration 73 loss: 0.0007357286522164941, ACC:1.0\n",
      "Training iteration 74 loss: 0.007015831768512726, ACC:1.0\n",
      "Training iteration 75 loss: 0.04504093900322914, ACC:0.984375\n",
      "Training iteration 76 loss: 0.005895365960896015, ACC:1.0\n",
      "Training iteration 77 loss: 0.008825275115668774, ACC:1.0\n",
      "Training iteration 78 loss: 0.03739878535270691, ACC:0.984375\n",
      "Training iteration 79 loss: 0.01692492514848709, ACC:0.984375\n",
      "Training iteration 80 loss: 0.16431793570518494, ACC:0.96875\n",
      "Training iteration 81 loss: 0.002107583684846759, ACC:1.0\n",
      "Training iteration 82 loss: 0.0180693157017231, ACC:0.984375\n",
      "Training iteration 83 loss: 0.0033099716529250145, ACC:1.0\n",
      "Training iteration 84 loss: 0.04850080981850624, ACC:0.984375\n",
      "Training iteration 85 loss: 0.08319617062807083, ACC:0.984375\n",
      "Training iteration 86 loss: 0.0866473838686943, ACC:0.984375\n",
      "Training iteration 87 loss: 0.07705489546060562, ACC:0.96875\n",
      "Training iteration 88 loss: 0.024153396487236023, ACC:0.984375\n",
      "Training iteration 89 loss: 0.019164031371474266, ACC:0.984375\n",
      "Training iteration 90 loss: 0.003919493407011032, ACC:1.0\n",
      "Training iteration 91 loss: 0.0018229145789518952, ACC:1.0\n",
      "Training iteration 92 loss: 0.0004879377083852887, ACC:1.0\n",
      "Training iteration 93 loss: 0.017185483127832413, ACC:1.0\n",
      "Training iteration 94 loss: 0.000563702778890729, ACC:1.0\n",
      "Training iteration 95 loss: 0.0029672409873455763, ACC:1.0\n",
      "Training iteration 96 loss: 0.013138617388904095, ACC:0.984375\n",
      "Training iteration 97 loss: 0.009574659168720245, ACC:1.0\n",
      "Training iteration 98 loss: 0.03523716330528259, ACC:0.984375\n",
      "Training iteration 99 loss: 0.16610276699066162, ACC:0.984375\n",
      "Training iteration 100 loss: 0.0007126859272830188, ACC:1.0\n",
      "Training iteration 101 loss: 0.014195822179317474, ACC:1.0\n",
      "Training iteration 102 loss: 0.21142029762268066, ACC:0.984375\n",
      "Training iteration 103 loss: 0.1311200112104416, ACC:0.96875\n",
      "Training iteration 104 loss: 0.0005232015973888338, ACC:1.0\n",
      "Training iteration 105 loss: 0.0015721176750957966, ACC:1.0\n",
      "Training iteration 106 loss: 0.07835827767848969, ACC:0.984375\n",
      "Training iteration 107 loss: 0.010250949300825596, ACC:1.0\n",
      "Training iteration 108 loss: 0.006863683927804232, ACC:1.0\n",
      "Training iteration 109 loss: 0.009682521224021912, ACC:1.0\n",
      "Training iteration 110 loss: 0.019275130704045296, ACC:1.0\n",
      "Training iteration 111 loss: 0.011392912827432156, ACC:1.0\n",
      "Training iteration 112 loss: 0.0071457489393651485, ACC:1.0\n",
      "Training iteration 113 loss: 0.010904407128691673, ACC:1.0\n",
      "Training iteration 114 loss: 0.0064184158109128475, ACC:1.0\n",
      "Training iteration 115 loss: 0.00235220929607749, ACC:1.0\n",
      "Training iteration 116 loss: 0.0034921776968985796, ACC:1.0\n",
      "Training iteration 117 loss: 0.0019900186453014612, ACC:1.0\n",
      "Training iteration 118 loss: 0.0014550291234627366, ACC:1.0\n",
      "Training iteration 119 loss: 0.006239897571504116, ACC:1.0\n",
      "Training iteration 120 loss: 0.0031203986145555973, ACC:1.0\n",
      "Training iteration 121 loss: 0.0021453280933201313, ACC:1.0\n",
      "Training iteration 122 loss: 0.00024208448303397745, ACC:1.0\n",
      "Training iteration 123 loss: 0.0598684698343277, ACC:0.984375\n",
      "Training iteration 124 loss: 0.012911343947052956, ACC:0.984375\n",
      "Training iteration 125 loss: 0.0012488524662330747, ACC:1.0\n",
      "Training iteration 126 loss: 0.0024427631869912148, ACC:1.0\n",
      "Training iteration 127 loss: 0.0007260513957589865, ACC:1.0\n",
      "Training iteration 128 loss: 0.0005320016643963754, ACC:1.0\n",
      "Training iteration 129 loss: 0.11914964765310287, ACC:0.984375\n",
      "Training iteration 130 loss: 0.015549120493233204, ACC:0.984375\n",
      "Training iteration 131 loss: 0.00047661527059972286, ACC:1.0\n",
      "Training iteration 132 loss: 0.0660426989197731, ACC:0.984375\n",
      "Training iteration 133 loss: 0.0004871353739872575, ACC:1.0\n",
      "Training iteration 134 loss: 0.0014889423036947846, ACC:1.0\n",
      "Training iteration 135 loss: 0.01071983389556408, ACC:1.0\n",
      "Training iteration 136 loss: 0.0003532043192535639, ACC:1.0\n",
      "Training iteration 137 loss: 0.0019748283084481955, ACC:1.0\n",
      "Training iteration 138 loss: 0.0006541050388477743, ACC:1.0\n",
      "Training iteration 139 loss: 0.0568365603685379, ACC:0.984375\n",
      "Training iteration 140 loss: 0.005497169215232134, ACC:1.0\n",
      "Training iteration 141 loss: 0.0011323639191687107, ACC:1.0\n",
      "Training iteration 142 loss: 0.03176567703485489, ACC:0.984375\n",
      "Training iteration 143 loss: 0.000979508040472865, ACC:1.0\n",
      "Training iteration 144 loss: 0.0993194580078125, ACC:0.984375\n",
      "Training iteration 145 loss: 0.05072562396526337, ACC:0.984375\n",
      "Training iteration 146 loss: 0.0003075662534683943, ACC:1.0\n",
      "Training iteration 147 loss: 0.009855668060481548, ACC:1.0\n",
      "Training iteration 148 loss: 0.016765829175710678, ACC:0.984375\n",
      "Training iteration 149 loss: 0.0005600361037068069, ACC:1.0\n",
      "Training iteration 150 loss: 0.00041977420914918184, ACC:1.0\n",
      "Training iteration 151 loss: 0.07607272267341614, ACC:0.96875\n",
      "Training iteration 152 loss: 0.03764911741018295, ACC:0.984375\n",
      "Training iteration 153 loss: 0.0012896009720861912, ACC:1.0\n",
      "Training iteration 154 loss: 0.013472162187099457, ACC:1.0\n",
      "Training iteration 155 loss: 0.016816936433315277, ACC:1.0\n",
      "Training iteration 156 loss: 0.056128282099962234, ACC:0.984375\n",
      "Training iteration 157 loss: 0.030717041343450546, ACC:0.984375\n",
      "Training iteration 158 loss: 0.02126329205930233, ACC:1.0\n",
      "Training iteration 159 loss: 0.003106958232820034, ACC:1.0\n",
      "Training iteration 160 loss: 0.0011702347546815872, ACC:1.0\n",
      "Training iteration 161 loss: 0.003138497471809387, ACC:1.0\n",
      "Training iteration 162 loss: 0.0017675079870969057, ACC:1.0\n",
      "Training iteration 163 loss: 0.0031015502754598856, ACC:1.0\n",
      "Training iteration 164 loss: 0.001797546399757266, ACC:1.0\n",
      "Training iteration 165 loss: 0.0005827043787576258, ACC:1.0\n",
      "Training iteration 166 loss: 0.0014042479451745749, ACC:1.0\n",
      "Training iteration 167 loss: 0.0026814094744622707, ACC:1.0\n",
      "Training iteration 168 loss: 0.0013284076703712344, ACC:1.0\n",
      "Training iteration 169 loss: 0.00010168994049308822, ACC:1.0\n",
      "Training iteration 170 loss: 0.0005868148291483521, ACC:1.0\n",
      "Training iteration 171 loss: 0.0010977855417877436, ACC:1.0\n",
      "Training iteration 172 loss: 0.012366246432065964, ACC:0.984375\n",
      "Training iteration 173 loss: 0.027043938636779785, ACC:0.984375\n",
      "Training iteration 174 loss: 0.07442258298397064, ACC:0.984375\n",
      "Training iteration 175 loss: 0.0007493909215554595, ACC:1.0\n",
      "Training iteration 176 loss: 0.030891817063093185, ACC:0.984375\n",
      "Training iteration 177 loss: 0.0006996391457505524, ACC:1.0\n",
      "Training iteration 178 loss: 0.002890782430768013, ACC:1.0\n",
      "Training iteration 179 loss: 0.00042144799954257905, ACC:1.0\n",
      "Training iteration 180 loss: 0.04236319288611412, ACC:0.984375\n",
      "Training iteration 181 loss: 0.0715659111738205, ACC:0.984375\n",
      "Training iteration 182 loss: 0.00570646720007062, ACC:1.0\n",
      "Training iteration 183 loss: 0.010854845866560936, ACC:1.0\n",
      "Training iteration 184 loss: 0.02000885270535946, ACC:0.984375\n",
      "Training iteration 185 loss: 0.0069646225310862064, ACC:1.0\n",
      "Training iteration 186 loss: 0.06385347247123718, ACC:0.96875\n",
      "Training iteration 187 loss: 0.0009023159509524703, ACC:1.0\n",
      "Training iteration 188 loss: 0.006057909689843655, ACC:1.0\n",
      "Training iteration 189 loss: 0.030282393097877502, ACC:0.984375\n",
      "Training iteration 190 loss: 0.004011834971606731, ACC:1.0\n",
      "Training iteration 191 loss: 0.05530568212270737, ACC:0.984375\n",
      "Training iteration 192 loss: 0.004426687490195036, ACC:1.0\n",
      "Training iteration 193 loss: 0.0008165236213244498, ACC:1.0\n",
      "Training iteration 194 loss: 0.039799369871616364, ACC:0.984375\n",
      "Training iteration 195 loss: 0.05166032165288925, ACC:0.984375\n",
      "Training iteration 196 loss: 0.0021301335655152798, ACC:1.0\n",
      "Training iteration 197 loss: 0.005057246889919043, ACC:1.0\n",
      "Training iteration 198 loss: 0.0025168652646243572, ACC:1.0\n",
      "Training iteration 199 loss: 0.009704331867396832, ACC:1.0\n",
      "Training iteration 200 loss: 0.0027201238553971052, ACC:1.0\n",
      "Training iteration 201 loss: 0.0018244789680466056, ACC:1.0\n",
      "Training iteration 202 loss: 0.0775594487786293, ACC:0.984375\n",
      "Training iteration 203 loss: 0.11883541196584702, ACC:0.96875\n",
      "Training iteration 204 loss: 0.001724031986668706, ACC:1.0\n",
      "Training iteration 205 loss: 0.02816263772547245, ACC:0.984375\n",
      "Training iteration 206 loss: 0.002821176778525114, ACC:1.0\n",
      "Training iteration 207 loss: 0.11971157789230347, ACC:0.96875\n",
      "Training iteration 208 loss: 0.0022033797577023506, ACC:1.0\n",
      "Training iteration 209 loss: 0.014706891030073166, ACC:0.984375\n",
      "Training iteration 210 loss: 0.00543868076056242, ACC:1.0\n",
      "Training iteration 211 loss: 0.0008310634293593466, ACC:1.0\n",
      "Training iteration 212 loss: 0.006424691528081894, ACC:1.0\n",
      "Training iteration 213 loss: 0.21954147517681122, ACC:0.953125\n",
      "Training iteration 214 loss: 0.225803405046463, ACC:0.9375\n",
      "Training iteration 215 loss: 0.001418844796717167, ACC:1.0\n",
      "Training iteration 216 loss: 0.08791091293096542, ACC:0.96875\n",
      "Training iteration 217 loss: 0.04437905177474022, ACC:1.0\n",
      "Training iteration 218 loss: 0.10481201857328415, ACC:0.96875\n",
      "Training iteration 219 loss: 0.041605185717344284, ACC:0.984375\n",
      "Training iteration 220 loss: 0.004597974009811878, ACC:1.0\n",
      "Training iteration 221 loss: 0.03134080767631531, ACC:0.984375\n",
      "Training iteration 222 loss: 0.0006745176506228745, ACC:1.0\n",
      "Training iteration 223 loss: 0.003885388607159257, ACC:1.0\n",
      "Training iteration 224 loss: 0.020538076758384705, ACC:0.984375\n",
      "Training iteration 225 loss: 0.0006071901298128068, ACC:1.0\n",
      "Training iteration 226 loss: 0.03517322987318039, ACC:0.984375\n",
      "Training iteration 227 loss: 0.0013532238081097603, ACC:1.0\n",
      "Training iteration 228 loss: 0.006805291399359703, ACC:1.0\n",
      "Training iteration 229 loss: 0.008480194956064224, ACC:1.0\n",
      "Training iteration 230 loss: 0.004035767633467913, ACC:1.0\n",
      "Training iteration 231 loss: 0.07804659754037857, ACC:0.96875\n",
      "Training iteration 232 loss: 0.0013024071231484413, ACC:1.0\n",
      "Training iteration 233 loss: 0.1547052264213562, ACC:0.96875\n",
      "Training iteration 234 loss: 0.03433799371123314, ACC:0.984375\n",
      "Training iteration 235 loss: 0.027217281982302666, ACC:0.984375\n",
      "Training iteration 236 loss: 0.012440281920135021, ACC:0.984375\n",
      "Training iteration 237 loss: 0.007038010284304619, ACC:1.0\n",
      "Training iteration 238 loss: 0.007567049469798803, ACC:1.0\n",
      "Training iteration 239 loss: 0.028870433568954468, ACC:0.984375\n",
      "Training iteration 240 loss: 0.018784508109092712, ACC:0.984375\n",
      "Training iteration 241 loss: 0.0009173261933028698, ACC:1.0\n",
      "Training iteration 242 loss: 0.0008715861476957798, ACC:1.0\n",
      "Training iteration 243 loss: 0.0005649933591485023, ACC:1.0\n",
      "Training iteration 244 loss: 0.0004274501116015017, ACC:1.0\n",
      "Training iteration 245 loss: 0.04143533855676651, ACC:0.984375\n",
      "Training iteration 246 loss: 0.03287641331553459, ACC:0.984375\n",
      "Training iteration 247 loss: 0.004870534408837557, ACC:1.0\n",
      "Training iteration 248 loss: 0.0016112737357616425, ACC:1.0\n",
      "Training iteration 249 loss: 0.06013234704732895, ACC:0.96875\n",
      "Training iteration 250 loss: 0.0012637852923944592, ACC:1.0\n",
      "Training iteration 251 loss: 0.0024661049246788025, ACC:1.0\n",
      "Training iteration 252 loss: 0.0007441084017045796, ACC:1.0\n",
      "Training iteration 253 loss: 0.002923917258158326, ACC:1.0\n",
      "Training iteration 254 loss: 0.0004446087696123868, ACC:1.0\n",
      "Training iteration 255 loss: 0.002596678212285042, ACC:1.0\n",
      "Training iteration 256 loss: 0.0012552248081192374, ACC:1.0\n",
      "Training iteration 257 loss: 0.0007093494641594589, ACC:1.0\n",
      "Training iteration 258 loss: 0.03648601472377777, ACC:0.984375\n",
      "Training iteration 259 loss: 0.2717157304286957, ACC:0.96875\n",
      "Training iteration 260 loss: 0.009511033073067665, ACC:1.0\n",
      "Training iteration 261 loss: 0.0012516146525740623, ACC:1.0\n",
      "Training iteration 262 loss: 0.002547080395743251, ACC:1.0\n",
      "Training iteration 263 loss: 0.1331542730331421, ACC:0.984375\n",
      "Training iteration 264 loss: 0.04983007162809372, ACC:0.984375\n",
      "Training iteration 265 loss: 0.0015210876008495688, ACC:1.0\n",
      "Training iteration 266 loss: 0.04732856526970863, ACC:0.984375\n",
      "Training iteration 267 loss: 0.001872278400696814, ACC:1.0\n",
      "Training iteration 268 loss: 0.0015326457796618342, ACC:1.0\n",
      "Training iteration 269 loss: 0.042320616543293, ACC:0.96875\n",
      "Training iteration 270 loss: 0.0005795398610644042, ACC:1.0\n",
      "Training iteration 271 loss: 0.011669705621898174, ACC:1.0\n",
      "Training iteration 272 loss: 0.0014250590465962887, ACC:1.0\n",
      "Training iteration 273 loss: 0.010717390105128288, ACC:1.0\n",
      "Training iteration 274 loss: 0.08301950246095657, ACC:0.96875\n",
      "Training iteration 275 loss: 0.09110259264707565, ACC:0.984375\n",
      "Training iteration 276 loss: 0.017116665840148926, ACC:0.984375\n",
      "Training iteration 277 loss: 0.0061813835054636, ACC:1.0\n",
      "Training iteration 278 loss: 0.002662136685103178, ACC:1.0\n",
      "Training iteration 279 loss: 0.023325320333242416, ACC:0.984375\n",
      "Training iteration 280 loss: 0.0011761641362681985, ACC:1.0\n",
      "Training iteration 281 loss: 0.016484547406435013, ACC:0.984375\n",
      "Training iteration 282 loss: 0.0011067986488342285, ACC:1.0\n",
      "Training iteration 283 loss: 0.007480488158762455, ACC:1.0\n",
      "Training iteration 284 loss: 0.003263195976614952, ACC:1.0\n",
      "Training iteration 285 loss: 0.011807521805167198, ACC:1.0\n",
      "Training iteration 286 loss: 0.07385735958814621, ACC:0.984375\n",
      "Training iteration 287 loss: 0.00838396418839693, ACC:1.0\n",
      "Training iteration 288 loss: 0.0518355667591095, ACC:0.984375\n",
      "Training iteration 289 loss: 0.09169705957174301, ACC:0.984375\n",
      "Training iteration 290 loss: 0.0003651347942650318, ACC:1.0\n",
      "Training iteration 291 loss: 0.022020777687430382, ACC:0.984375\n",
      "Training iteration 292 loss: 0.0037009604275226593, ACC:1.0\n",
      "Training iteration 293 loss: 0.04948612302541733, ACC:0.984375\n",
      "Training iteration 294 loss: 0.01392095722258091, ACC:1.0\n",
      "Training iteration 295 loss: 0.0016281278803944588, ACC:1.0\n",
      "Training iteration 296 loss: 0.015345749445259571, ACC:0.984375\n",
      "Training iteration 297 loss: 0.011362407356500626, ACC:1.0\n",
      "Training iteration 298 loss: 0.0017799270572140813, ACC:1.0\n",
      "Training iteration 299 loss: 0.025577330961823463, ACC:0.984375\n",
      "Training iteration 300 loss: 0.0009033618262037635, ACC:1.0\n",
      "Training iteration 301 loss: 0.00030732754385098815, ACC:1.0\n",
      "Training iteration 302 loss: 0.0008538291440345347, ACC:1.0\n",
      "Training iteration 303 loss: 0.0002859345986507833, ACC:1.0\n",
      "Training iteration 304 loss: 0.019680572673678398, ACC:0.984375\n",
      "Training iteration 305 loss: 0.0012515769340097904, ACC:1.0\n",
      "Training iteration 306 loss: 0.004870866425335407, ACC:1.0\n",
      "Training iteration 307 loss: 0.000625300221145153, ACC:1.0\n",
      "Training iteration 308 loss: 0.08241958916187286, ACC:0.984375\n",
      "Training iteration 309 loss: 0.015097860246896744, ACC:0.984375\n",
      "Training iteration 310 loss: 0.0006862379959784448, ACC:1.0\n",
      "Training iteration 311 loss: 0.10407812893390656, ACC:0.984375\n",
      "Training iteration 312 loss: 0.1314496248960495, ACC:0.96875\n",
      "Training iteration 313 loss: 0.0008770485292188823, ACC:1.0\n",
      "Training iteration 314 loss: 0.0005701545742340386, ACC:1.0\n",
      "Training iteration 315 loss: 0.0022532488219439983, ACC:1.0\n",
      "Training iteration 316 loss: 0.019052663818001747, ACC:0.984375\n",
      "Training iteration 317 loss: 0.07958918809890747, ACC:0.96875\n",
      "Training iteration 318 loss: 0.0014989563496783376, ACC:1.0\n",
      "Training iteration 319 loss: 0.053334254771471024, ACC:0.984375\n",
      "Training iteration 320 loss: 0.0007108511636033654, ACC:1.0\n",
      "Training iteration 321 loss: 0.00338232540525496, ACC:1.0\n",
      "Training iteration 322 loss: 0.0071906233206391335, ACC:1.0\n",
      "Training iteration 323 loss: 0.0014036349020898342, ACC:1.0\n",
      "Training iteration 324 loss: 0.0034810537472367287, ACC:1.0\n",
      "Training iteration 325 loss: 0.002578981453552842, ACC:1.0\n",
      "Training iteration 326 loss: 0.0031581358052790165, ACC:1.0\n",
      "Training iteration 327 loss: 0.002193179214373231, ACC:1.0\n",
      "Training iteration 328 loss: 0.011009982787072659, ACC:1.0\n",
      "Training iteration 329 loss: 0.0007240228587761521, ACC:1.0\n",
      "Training iteration 330 loss: 0.0010439383331686258, ACC:1.0\n",
      "Training iteration 331 loss: 0.0011140176793560386, ACC:1.0\n",
      "Training iteration 332 loss: 0.0012329901801422238, ACC:1.0\n",
      "Training iteration 333 loss: 0.05778844282031059, ACC:0.96875\n",
      "Training iteration 334 loss: 0.03716916963458061, ACC:0.984375\n",
      "Training iteration 335 loss: 0.0041351402178406715, ACC:1.0\n",
      "Training iteration 336 loss: 0.02217603661119938, ACC:1.0\n",
      "Training iteration 337 loss: 0.07523465156555176, ACC:0.984375\n",
      "Training iteration 338 loss: 0.07121281325817108, ACC:0.984375\n",
      "Training iteration 339 loss: 0.09837575256824493, ACC:0.984375\n",
      "Training iteration 340 loss: 0.0004637008532881737, ACC:1.0\n",
      "Training iteration 341 loss: 0.0005421587266027927, ACC:1.0\n",
      "Training iteration 342 loss: 0.035318952053785324, ACC:0.984375\n",
      "Training iteration 343 loss: 0.06953015178442001, ACC:0.96875\n",
      "Training iteration 344 loss: 0.08080672472715378, ACC:0.984375\n",
      "Training iteration 345 loss: 0.06150614842772484, ACC:0.96875\n",
      "Training iteration 346 loss: 0.009985802695155144, ACC:1.0\n",
      "Training iteration 347 loss: 0.06084057316184044, ACC:0.96875\n",
      "Training iteration 348 loss: 0.002399275777861476, ACC:1.0\n",
      "Training iteration 349 loss: 0.04356255382299423, ACC:0.984375\n",
      "Training iteration 350 loss: 0.0015192498685792089, ACC:1.0\n",
      "Training iteration 351 loss: 0.0011497567174956203, ACC:1.0\n",
      "Training iteration 352 loss: 0.08282730728387833, ACC:0.96875\n",
      "Training iteration 353 loss: 0.035140689462423325, ACC:1.0\n",
      "Training iteration 354 loss: 0.007391138467937708, ACC:1.0\n",
      "Training iteration 355 loss: 0.024253251031041145, ACC:0.984375\n",
      "Training iteration 356 loss: 0.0003748672897927463, ACC:1.0\n",
      "Training iteration 357 loss: 0.0003915800480172038, ACC:1.0\n",
      "Training iteration 358 loss: 0.0003183098160661757, ACC:1.0\n",
      "Training iteration 359 loss: 0.0031288680620491505, ACC:1.0\n",
      "Training iteration 360 loss: 0.26947563886642456, ACC:0.96875\n",
      "Training iteration 361 loss: 0.0003120238543488085, ACC:1.0\n",
      "Training iteration 362 loss: 0.16577669978141785, ACC:0.96875\n",
      "Training iteration 363 loss: 0.006859810557216406, ACC:1.0\n",
      "Training iteration 364 loss: 0.030627211555838585, ACC:0.984375\n",
      "Training iteration 365 loss: 0.0023456234484910965, ACC:1.0\n",
      "Training iteration 366 loss: 0.05181673914194107, ACC:0.984375\n",
      "Training iteration 367 loss: 0.011161016300320625, ACC:1.0\n",
      "Training iteration 368 loss: 0.0036328709684312344, ACC:1.0\n",
      "Training iteration 369 loss: 0.008893814869225025, ACC:1.0\n",
      "Training iteration 370 loss: 0.009324646554887295, ACC:1.0\n",
      "Training iteration 371 loss: 0.0013984492979943752, ACC:1.0\n",
      "Training iteration 372 loss: 0.004693659488111734, ACC:1.0\n",
      "Training iteration 373 loss: 0.08566974848508835, ACC:0.984375\n",
      "Training iteration 374 loss: 0.0011157938279211521, ACC:1.0\n",
      "Training iteration 375 loss: 0.012775510549545288, ACC:1.0\n",
      "Training iteration 376 loss: 0.0015240531647577882, ACC:1.0\n",
      "Training iteration 377 loss: 0.029112257063388824, ACC:0.984375\n",
      "Training iteration 378 loss: 0.07193385064601898, ACC:0.953125\n",
      "Training iteration 379 loss: 0.009473806247115135, ACC:1.0\n",
      "Training iteration 380 loss: 0.0004756604030262679, ACC:1.0\n",
      "Training iteration 381 loss: 0.0023048152215778828, ACC:1.0\n",
      "Training iteration 382 loss: 0.018908284604549408, ACC:0.984375\n",
      "Training iteration 383 loss: 0.006068534683436155, ACC:1.0\n",
      "Training iteration 384 loss: 0.06126750260591507, ACC:0.984375\n",
      "Training iteration 385 loss: 0.03134749457240105, ACC:0.984375\n",
      "Training iteration 386 loss: 0.06602216511964798, ACC:0.984375\n",
      "Training iteration 387 loss: 0.080119788646698, ACC:0.96875\n",
      "Training iteration 388 loss: 0.004579730331897736, ACC:1.0\n",
      "Training iteration 389 loss: 0.03579028695821762, ACC:0.984375\n",
      "Training iteration 390 loss: 0.034153085201978683, ACC:0.984375\n",
      "Training iteration 391 loss: 0.02972021885216236, ACC:0.984375\n",
      "Training iteration 392 loss: 0.017431631684303284, ACC:1.0\n",
      "Training iteration 393 loss: 0.01801532693207264, ACC:1.0\n",
      "Training iteration 394 loss: 0.05848283693194389, ACC:0.984375\n",
      "Training iteration 395 loss: 0.014321595430374146, ACC:1.0\n",
      "Training iteration 396 loss: 0.003492268966510892, ACC:1.0\n",
      "Training iteration 397 loss: 0.1154501810669899, ACC:0.96875\n",
      "Training iteration 398 loss: 0.04479337856173515, ACC:0.984375\n",
      "Training iteration 399 loss: 0.006230540573596954, ACC:1.0\n",
      "Training iteration 400 loss: 0.0004654201620724052, ACC:1.0\n",
      "Training iteration 401 loss: 0.08814722299575806, ACC:0.984375\n",
      "Training iteration 402 loss: 0.046094756573438644, ACC:0.984375\n",
      "Training iteration 403 loss: 0.0005809206631965935, ACC:1.0\n",
      "Training iteration 404 loss: 0.12379999458789825, ACC:0.984375\n",
      "Training iteration 405 loss: 0.0032415036112070084, ACC:1.0\n",
      "Training iteration 406 loss: 0.0044859712943434715, ACC:1.0\n",
      "Training iteration 407 loss: 0.07325557619333267, ACC:0.984375\n",
      "Training iteration 408 loss: 0.0059797922149300575, ACC:1.0\n",
      "Training iteration 409 loss: 0.0004034648591186851, ACC:1.0\n",
      "Training iteration 410 loss: 0.0005760203930549324, ACC:1.0\n",
      "Training iteration 411 loss: 0.08761513233184814, ACC:0.984375\n",
      "Training iteration 412 loss: 0.06268728524446487, ACC:0.984375\n",
      "Training iteration 413 loss: 0.024614645168185234, ACC:0.96875\n",
      "Training iteration 414 loss: 0.0046614850871264935, ACC:1.0\n",
      "Training iteration 415 loss: 0.0006362967542372644, ACC:1.0\n",
      "Training iteration 416 loss: 0.008219250477850437, ACC:1.0\n",
      "Training iteration 417 loss: 0.0009636841714382172, ACC:1.0\n",
      "Training iteration 418 loss: 0.0018839571857824922, ACC:1.0\n",
      "Training iteration 419 loss: 0.0005820380174554884, ACC:1.0\n",
      "Training iteration 420 loss: 0.040776435285806656, ACC:0.984375\n",
      "Training iteration 421 loss: 0.0010299228597432375, ACC:1.0\n",
      "Training iteration 422 loss: 0.0015199037734419107, ACC:1.0\n",
      "Training iteration 423 loss: 0.005648649297654629, ACC:1.0\n",
      "Training iteration 424 loss: 0.15489548444747925, ACC:0.984375\n",
      "Training iteration 425 loss: 0.0021958970464766026, ACC:1.0\n",
      "Training iteration 426 loss: 0.002009582705795765, ACC:1.0\n",
      "Training iteration 427 loss: 0.005487522576004267, ACC:1.0\n",
      "Training iteration 428 loss: 0.009131588973104954, ACC:1.0\n",
      "Training iteration 429 loss: 0.0036439625546336174, ACC:1.0\n",
      "Training iteration 430 loss: 0.004715005401521921, ACC:1.0\n",
      "Training iteration 431 loss: 0.0030070729553699493, ACC:1.0\n",
      "Training iteration 432 loss: 0.0741666629910469, ACC:0.984375\n",
      "Training iteration 433 loss: 0.010578390210866928, ACC:1.0\n",
      "Training iteration 434 loss: 0.003442397341132164, ACC:1.0\n",
      "Training iteration 435 loss: 0.03540850430727005, ACC:0.984375\n",
      "Training iteration 436 loss: 0.009843629784882069, ACC:1.0\n",
      "Training iteration 437 loss: 0.0018356845248490572, ACC:1.0\n",
      "Training iteration 438 loss: 0.0001779466838343069, ACC:1.0\n",
      "Training iteration 439 loss: 0.014199681580066681, ACC:0.984375\n",
      "Training iteration 440 loss: 0.00036332590389065444, ACC:1.0\n",
      "Training iteration 441 loss: 0.029712075367569923, ACC:0.984375\n",
      "Training iteration 442 loss: 0.003303559496998787, ACC:1.0\n",
      "Training iteration 443 loss: 0.00025177275529131293, ACC:1.0\n",
      "Training iteration 444 loss: 0.0002554357924964279, ACC:1.0\n",
      "Training iteration 445 loss: 0.0005844664992764592, ACC:1.0\n",
      "Training iteration 446 loss: 0.0053979335352778435, ACC:1.0\n",
      "Training iteration 447 loss: 0.0010651177726686, ACC:1.0\n",
      "Training iteration 448 loss: 0.028412917628884315, ACC:0.984375\n",
      "Training iteration 449 loss: 0.0008523027063347399, ACC:1.0\n",
      "Training iteration 450 loss: 0.0025676684454083443, ACC:1.0\n",
      "Validation iteration 451 loss: 0.0006073549739085138, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.0005891447071917355, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.12204594910144806, ACC: 0.96875\n",
      "Validation iteration 454 loss: 0.0003921674797311425, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.0011772231664508581, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.026553872972726822, ACC: 0.984375\n",
      "Validation iteration 457 loss: 0.002173463348299265, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.03675911948084831, ACC: 0.984375\n",
      "Validation iteration 459 loss: 0.06644219905138016, ACC: 0.953125\n",
      "Validation iteration 460 loss: 0.05093597248196602, ACC: 0.984375\n",
      "Validation iteration 461 loss: 0.006402193568646908, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.050427548587322235, ACC: 0.984375\n",
      "Validation iteration 463 loss: 0.16151511669158936, ACC: 0.953125\n",
      "Validation iteration 464 loss: 0.0005547394976019859, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.07779697328805923, ACC: 0.984375\n",
      "Validation iteration 466 loss: 0.03973652794957161, ACC: 0.984375\n",
      "Validation iteration 467 loss: 0.09669344127178192, ACC: 0.96875\n",
      "Validation iteration 468 loss: 0.06914127618074417, ACC: 0.984375\n",
      "Validation iteration 469 loss: 0.020519249141216278, ACC: 0.984375\n",
      "Validation iteration 470 loss: 0.010591235011816025, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.0007545951521024108, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.0960521325469017, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.07546775788068771, ACC: 0.984375\n",
      "Validation iteration 474 loss: 0.03413303941488266, ACC: 0.984375\n",
      "Validation iteration 475 loss: 0.05800098553299904, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.005620538257062435, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.01951850764453411, ACC: 0.984375\n",
      "Validation iteration 478 loss: 0.1370888203382492, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.006119825411587954, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.0006739178206771612, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.01379407662898302, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.04612051323056221, ACC: 0.984375\n",
      "Validation iteration 483 loss: 0.0010170548921450973, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.017373885959386826, ACC: 1.0\n",
      "Validation iteration 485 loss: 0.045349232852458954, ACC: 0.984375\n",
      "Validation iteration 486 loss: 0.0021740433294326067, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.0006432925583794713, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.04199499636888504, ACC: 0.96875\n",
      "Validation iteration 489 loss: 0.025723107159137726, ACC: 0.96875\n",
      "Validation iteration 490 loss: 0.0012856756802648306, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.0009542082552798092, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.0012039561988785863, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.004105267580598593, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.015591195784509182, ACC: 0.984375\n",
      "Validation iteration 495 loss: 0.009067077189683914, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.00032515297061763704, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.0013037130702286959, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.0023980082478374243, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.0021222762297838926, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.0009900566656142473, ACC: 1.0\n",
      "-- Epoch 7 done -- Train loss: 0.02452168603901454, train ACC: 0.9932291666666667, val loss: 0.030160433576093056, val ACC: 0.9903125\n",
      "<--- 13151.575439214706 seconds --->\n",
      "Training iteration 1 loss: 0.0012026476906612515, ACC:1.0\n",
      "Training iteration 2 loss: 0.03273987025022507, ACC:0.984375\n",
      "Training iteration 3 loss: 0.0005510621704161167, ACC:1.0\n",
      "Training iteration 4 loss: 0.010536745190620422, ACC:1.0\n",
      "Training iteration 5 loss: 0.0022903592325747013, ACC:1.0\n",
      "Training iteration 6 loss: 0.04450872540473938, ACC:0.984375\n",
      "Training iteration 7 loss: 0.029459981247782707, ACC:0.984375\n",
      "Training iteration 8 loss: 0.00059875677106902, ACC:1.0\n",
      "Training iteration 9 loss: 0.05405198410153389, ACC:0.984375\n",
      "Training iteration 10 loss: 0.028389599174261093, ACC:0.984375\n",
      "Training iteration 11 loss: 0.00010485052916919813, ACC:1.0\n",
      "Training iteration 12 loss: 0.017815550789237022, ACC:0.984375\n",
      "Training iteration 13 loss: 0.002460590796545148, ACC:1.0\n",
      "Training iteration 14 loss: 0.003804663196206093, ACC:1.0\n",
      "Training iteration 15 loss: 0.0288203414529562, ACC:0.984375\n",
      "Training iteration 16 loss: 0.0009613605216145515, ACC:1.0\n",
      "Training iteration 17 loss: 0.0008563821902498603, ACC:1.0\n",
      "Training iteration 18 loss: 0.0003643372911028564, ACC:1.0\n",
      "Training iteration 19 loss: 0.00029658142011612654, ACC:1.0\n",
      "Training iteration 20 loss: 0.0005283631617203355, ACC:1.0\n",
      "Training iteration 21 loss: 0.010677543468773365, ACC:1.0\n",
      "Training iteration 22 loss: 0.0003213692980352789, ACC:1.0\n",
      "Training iteration 23 loss: 0.12319092452526093, ACC:0.984375\n",
      "Training iteration 24 loss: 0.00024799731909297407, ACC:1.0\n",
      "Training iteration 25 loss: 0.0006079471204429865, ACC:1.0\n",
      "Training iteration 26 loss: 0.0006940450984984636, ACC:1.0\n",
      "Training iteration 27 loss: 0.002110228408128023, ACC:1.0\n",
      "Training iteration 28 loss: 0.034322138875722885, ACC:0.984375\n",
      "Training iteration 29 loss: 0.0012435371754691005, ACC:1.0\n",
      "Training iteration 30 loss: 0.0024402241688221693, ACC:1.0\n",
      "Training iteration 31 loss: 0.0015757110668346286, ACC:1.0\n",
      "Training iteration 32 loss: 0.0358378142118454, ACC:0.984375\n",
      "Training iteration 33 loss: 0.05034944787621498, ACC:0.984375\n",
      "Training iteration 34 loss: 0.0008367727277800441, ACC:1.0\n",
      "Training iteration 35 loss: 0.006600986700505018, ACC:1.0\n",
      "Training iteration 36 loss: 0.006991453468799591, ACC:1.0\n",
      "Training iteration 37 loss: 0.011335829272866249, ACC:1.0\n",
      "Training iteration 38 loss: 0.0005331612192094326, ACC:1.0\n",
      "Training iteration 39 loss: 0.19456496834754944, ACC:0.984375\n",
      "Training iteration 40 loss: 0.02286272495985031, ACC:0.984375\n",
      "Training iteration 41 loss: 0.0009203474037349224, ACC:1.0\n",
      "Training iteration 42 loss: 0.000502185255754739, ACC:1.0\n",
      "Training iteration 43 loss: 0.0006321032997220755, ACC:1.0\n",
      "Training iteration 44 loss: 0.00017652384121902287, ACC:1.0\n",
      "Training iteration 45 loss: 0.0011453782208263874, ACC:1.0\n",
      "Training iteration 46 loss: 0.0002095153322443366, ACC:1.0\n",
      "Training iteration 47 loss: 0.0038106676656752825, ACC:1.0\n",
      "Training iteration 48 loss: 0.001979759894311428, ACC:1.0\n",
      "Training iteration 49 loss: 0.003970144782215357, ACC:1.0\n",
      "Training iteration 50 loss: 5.702063936041668e-05, ACC:1.0\n",
      "Training iteration 51 loss: 0.0007507242262363434, ACC:1.0\n",
      "Training iteration 52 loss: 0.0016481662169098854, ACC:1.0\n",
      "Training iteration 53 loss: 0.09161122888326645, ACC:0.984375\n",
      "Training iteration 54 loss: 0.010804073885083199, ACC:1.0\n",
      "Training iteration 55 loss: 4.716337571153417e-05, ACC:1.0\n",
      "Training iteration 56 loss: 0.0100778266787529, ACC:1.0\n",
      "Training iteration 57 loss: 0.0001914720342028886, ACC:1.0\n",
      "Training iteration 58 loss: 0.0004975215415470302, ACC:1.0\n",
      "Training iteration 59 loss: 0.00012352604244370013, ACC:1.0\n",
      "Training iteration 60 loss: 0.0009713052422739565, ACC:1.0\n",
      "Training iteration 61 loss: 0.0017967363819479942, ACC:1.0\n",
      "Training iteration 62 loss: 0.002624256070703268, ACC:1.0\n",
      "Training iteration 63 loss: 0.006537436507642269, ACC:1.0\n",
      "Training iteration 64 loss: 0.0008789841085672379, ACC:1.0\n",
      "Training iteration 65 loss: 0.0004100350779481232, ACC:1.0\n",
      "Training iteration 66 loss: 0.003197846934199333, ACC:1.0\n",
      "Training iteration 67 loss: 0.00021950551308691502, ACC:1.0\n",
      "Training iteration 68 loss: 0.00012132958363508806, ACC:1.0\n",
      "Training iteration 69 loss: 4.887752220383845e-05, ACC:1.0\n",
      "Training iteration 70 loss: 0.0015949320513755083, ACC:1.0\n",
      "Training iteration 71 loss: 0.1487637609243393, ACC:0.96875\n",
      "Training iteration 72 loss: 0.0004883194924332201, ACC:1.0\n",
      "Training iteration 73 loss: 0.00424368679523468, ACC:1.0\n",
      "Training iteration 74 loss: 0.11326345056295395, ACC:0.984375\n",
      "Training iteration 75 loss: 0.008865186013281345, ACC:1.0\n",
      "Training iteration 76 loss: 0.09195350110530853, ACC:0.984375\n",
      "Training iteration 77 loss: 0.026405178010463715, ACC:0.984375\n",
      "Training iteration 78 loss: 0.005416072905063629, ACC:1.0\n",
      "Training iteration 79 loss: 0.017075538635253906, ACC:1.0\n",
      "Training iteration 80 loss: 0.08636850863695145, ACC:0.96875\n",
      "Training iteration 81 loss: 0.0017461752286180854, ACC:1.0\n",
      "Training iteration 82 loss: 0.11578908562660217, ACC:0.984375\n",
      "Training iteration 83 loss: 0.01629147119820118, ACC:0.984375\n",
      "Training iteration 84 loss: 0.10971035063266754, ACC:0.984375\n",
      "Training iteration 85 loss: 0.10674470663070679, ACC:0.96875\n",
      "Training iteration 86 loss: 0.01698973774909973, ACC:1.0\n",
      "Training iteration 87 loss: 0.03018394112586975, ACC:0.984375\n",
      "Training iteration 88 loss: 0.0016671644989401102, ACC:1.0\n",
      "Training iteration 89 loss: 0.03867786377668381, ACC:0.984375\n",
      "Training iteration 90 loss: 0.00811424758285284, ACC:1.0\n",
      "Training iteration 91 loss: 0.0026889366563409567, ACC:1.0\n",
      "Training iteration 92 loss: 0.006878692656755447, ACC:1.0\n",
      "Training iteration 93 loss: 0.008491682820022106, ACC:1.0\n",
      "Training iteration 94 loss: 0.007318342104554176, ACC:1.0\n",
      "Training iteration 95 loss: 0.0024661696515977383, ACC:1.0\n",
      "Training iteration 96 loss: 0.016518007963895798, ACC:0.984375\n",
      "Training iteration 97 loss: 0.0004924319800920784, ACC:1.0\n",
      "Training iteration 98 loss: 0.004851253237575293, ACC:1.0\n",
      "Training iteration 99 loss: 0.001362217590212822, ACC:1.0\n",
      "Training iteration 100 loss: 0.00044759295997209847, ACC:1.0\n",
      "Training iteration 101 loss: 0.039125096052885056, ACC:0.984375\n",
      "Training iteration 102 loss: 0.0006168649415485561, ACC:1.0\n",
      "Training iteration 103 loss: 0.004076274577528238, ACC:1.0\n",
      "Training iteration 104 loss: 0.000838319945614785, ACC:1.0\n",
      "Training iteration 105 loss: 0.00016367417993023992, ACC:1.0\n",
      "Training iteration 106 loss: 0.08792150765657425, ACC:0.984375\n",
      "Training iteration 107 loss: 7.910806016298011e-05, ACC:1.0\n",
      "Training iteration 108 loss: 0.0007435871521010995, ACC:1.0\n",
      "Training iteration 109 loss: 0.0004748553619720042, ACC:1.0\n",
      "Training iteration 110 loss: 0.026606077328324318, ACC:0.984375\n",
      "Training iteration 111 loss: 0.0014186142943799496, ACC:1.0\n",
      "Training iteration 112 loss: 0.0029927941504865885, ACC:1.0\n",
      "Training iteration 113 loss: 0.005204240791499615, ACC:1.0\n",
      "Training iteration 114 loss: 0.00027046113973483443, ACC:1.0\n",
      "Training iteration 115 loss: 0.00010136970377061516, ACC:1.0\n",
      "Training iteration 116 loss: 0.1408064067363739, ACC:0.984375\n",
      "Training iteration 117 loss: 0.006070885807275772, ACC:1.0\n",
      "Training iteration 118 loss: 0.017258213832974434, ACC:0.984375\n",
      "Training iteration 119 loss: 0.031637370586395264, ACC:0.984375\n",
      "Training iteration 120 loss: 0.0026190471835434437, ACC:1.0\n",
      "Training iteration 121 loss: 0.0033154732082039118, ACC:1.0\n",
      "Training iteration 122 loss: 0.018237119540572166, ACC:0.984375\n",
      "Training iteration 123 loss: 0.0021801532711833715, ACC:1.0\n",
      "Training iteration 124 loss: 0.0019421330653131008, ACC:1.0\n",
      "Training iteration 125 loss: 0.052559591829776764, ACC:0.984375\n",
      "Training iteration 126 loss: 0.007037059869617224, ACC:1.0\n",
      "Training iteration 127 loss: 0.0026913518086075783, ACC:1.0\n",
      "Training iteration 128 loss: 0.0025687329471111298, ACC:1.0\n",
      "Training iteration 129 loss: 0.0012561489129438996, ACC:1.0\n",
      "Training iteration 130 loss: 0.0035151829943060875, ACC:1.0\n",
      "Training iteration 131 loss: 0.009230325929820538, ACC:1.0\n",
      "Training iteration 132 loss: 0.05822720006108284, ACC:0.984375\n",
      "Training iteration 133 loss: 0.07663927227258682, ACC:0.984375\n",
      "Training iteration 134 loss: 0.006407351233065128, ACC:1.0\n",
      "Training iteration 135 loss: 0.000955907569732517, ACC:1.0\n",
      "Training iteration 136 loss: 0.043175749480724335, ACC:0.984375\n",
      "Training iteration 137 loss: 0.0032575337681919336, ACC:1.0\n",
      "Training iteration 138 loss: 0.0001832755224313587, ACC:1.0\n",
      "Training iteration 139 loss: 0.06377455592155457, ACC:0.96875\n",
      "Training iteration 140 loss: 0.005814314354211092, ACC:1.0\n",
      "Training iteration 141 loss: 0.003370191901922226, ACC:1.0\n",
      "Training iteration 142 loss: 0.00011378034832887352, ACC:1.0\n",
      "Training iteration 143 loss: 0.08478666096925735, ACC:0.984375\n",
      "Training iteration 144 loss: 0.021514814347028732, ACC:0.984375\n",
      "Training iteration 145 loss: 0.03978731483221054, ACC:0.984375\n",
      "Training iteration 146 loss: 0.002871626755222678, ACC:1.0\n",
      "Training iteration 147 loss: 0.0022919857874512672, ACC:1.0\n",
      "Training iteration 148 loss: 0.002214982407167554, ACC:1.0\n",
      "Training iteration 149 loss: 0.013017546385526657, ACC:0.984375\n",
      "Training iteration 150 loss: 0.008837049826979637, ACC:1.0\n",
      "Training iteration 151 loss: 0.002588492352515459, ACC:1.0\n",
      "Training iteration 152 loss: 0.00044016350875608623, ACC:1.0\n",
      "Training iteration 153 loss: 0.002155059715732932, ACC:1.0\n",
      "Training iteration 154 loss: 0.018524574115872383, ACC:0.984375\n",
      "Training iteration 155 loss: 0.08149107545614243, ACC:0.96875\n",
      "Training iteration 156 loss: 0.0014911644393578172, ACC:1.0\n",
      "Training iteration 157 loss: 0.003187325084581971, ACC:1.0\n",
      "Training iteration 158 loss: 0.007882561534643173, ACC:1.0\n",
      "Training iteration 159 loss: 0.00042990167276002467, ACC:1.0\n",
      "Training iteration 160 loss: 0.0010376960271969438, ACC:1.0\n",
      "Training iteration 161 loss: 0.0002179899311158806, ACC:1.0\n",
      "Training iteration 162 loss: 0.0009983013151213527, ACC:1.0\n",
      "Training iteration 163 loss: 0.005493519827723503, ACC:1.0\n",
      "Training iteration 164 loss: 0.13255621492862701, ACC:0.984375\n",
      "Training iteration 165 loss: 0.0020644254982471466, ACC:1.0\n",
      "Training iteration 166 loss: 0.0025347445625811815, ACC:1.0\n",
      "Training iteration 167 loss: 0.04642905667424202, ACC:0.984375\n",
      "Training iteration 168 loss: 0.004373237490653992, ACC:1.0\n",
      "Training iteration 169 loss: 0.0014330071862787008, ACC:1.0\n",
      "Training iteration 170 loss: 0.013944469392299652, ACC:0.984375\n",
      "Training iteration 171 loss: 0.07543843239545822, ACC:0.953125\n",
      "Training iteration 172 loss: 0.00148314842954278, ACC:1.0\n",
      "Training iteration 173 loss: 0.06796859204769135, ACC:0.984375\n",
      "Training iteration 174 loss: 0.09136781096458435, ACC:0.984375\n",
      "Training iteration 175 loss: 0.00522452499717474, ACC:1.0\n",
      "Training iteration 176 loss: 0.036505430936813354, ACC:0.984375\n",
      "Training iteration 177 loss: 0.012219265103340149, ACC:1.0\n",
      "Training iteration 178 loss: 0.015801947563886642, ACC:1.0\n",
      "Training iteration 179 loss: 0.003917423542588949, ACC:1.0\n",
      "Training iteration 180 loss: 0.08094727993011475, ACC:0.984375\n",
      "Training iteration 181 loss: 0.03919381648302078, ACC:0.984375\n",
      "Training iteration 182 loss: 0.0008316452731378376, ACC:1.0\n",
      "Training iteration 183 loss: 0.03725992143154144, ACC:0.984375\n",
      "Training iteration 184 loss: 0.0010250679915770888, ACC:1.0\n",
      "Training iteration 185 loss: 0.009997463785111904, ACC:1.0\n",
      "Training iteration 186 loss: 0.14493446052074432, ACC:0.984375\n",
      "Training iteration 187 loss: 0.0006126334192231297, ACC:1.0\n",
      "Training iteration 188 loss: 0.00042547870543785393, ACC:1.0\n",
      "Training iteration 189 loss: 0.0005482047563418746, ACC:1.0\n",
      "Training iteration 190 loss: 0.08483321219682693, ACC:0.984375\n",
      "Training iteration 191 loss: 0.0065610469318926334, ACC:1.0\n",
      "Training iteration 192 loss: 0.022548355162143707, ACC:0.984375\n",
      "Training iteration 193 loss: 0.014425111934542656, ACC:1.0\n",
      "Training iteration 194 loss: 0.01924409531056881, ACC:1.0\n",
      "Training iteration 195 loss: 0.02732224203646183, ACC:1.0\n",
      "Training iteration 196 loss: 0.004105524625629187, ACC:1.0\n",
      "Training iteration 197 loss: 0.0006217822665348649, ACC:1.0\n",
      "Training iteration 198 loss: 0.002930043265223503, ACC:1.0\n",
      "Training iteration 199 loss: 0.0006558243185281754, ACC:1.0\n",
      "Training iteration 200 loss: 0.0002769298735074699, ACC:1.0\n",
      "Training iteration 201 loss: 0.00012873327068518847, ACC:1.0\n",
      "Training iteration 202 loss: 0.0019073187140747905, ACC:1.0\n",
      "Training iteration 203 loss: 0.09924507141113281, ACC:0.96875\n",
      "Training iteration 204 loss: 0.00010846061923075467, ACC:1.0\n",
      "Training iteration 205 loss: 0.0003318742965348065, ACC:1.0\n",
      "Training iteration 206 loss: 0.01001229789108038, ACC:1.0\n",
      "Training iteration 207 loss: 0.00021160108735784888, ACC:1.0\n",
      "Training iteration 208 loss: 0.14802120625972748, ACC:0.984375\n",
      "Training iteration 209 loss: 0.004096987657248974, ACC:1.0\n",
      "Training iteration 210 loss: 0.0008461566758342087, ACC:1.0\n",
      "Training iteration 211 loss: 0.029909664765000343, ACC:0.96875\n",
      "Training iteration 212 loss: 0.0374491885304451, ACC:0.984375\n",
      "Training iteration 213 loss: 0.09139666706323624, ACC:0.984375\n",
      "Training iteration 214 loss: 0.0021873307414352894, ACC:1.0\n",
      "Training iteration 215 loss: 0.005481536965817213, ACC:1.0\n",
      "Training iteration 216 loss: 9.77919771685265e-05, ACC:1.0\n",
      "Training iteration 217 loss: 0.042273808270692825, ACC:0.984375\n",
      "Training iteration 218 loss: 6.90849483362399e-05, ACC:1.0\n",
      "Training iteration 219 loss: 0.09590054303407669, ACC:0.984375\n",
      "Training iteration 220 loss: 0.0009478159481659532, ACC:1.0\n",
      "Training iteration 221 loss: 0.017888665199279785, ACC:0.984375\n",
      "Training iteration 222 loss: 0.03301706910133362, ACC:0.984375\n",
      "Training iteration 223 loss: 0.0007128434372134507, ACC:1.0\n",
      "Training iteration 224 loss: 0.002601602580398321, ACC:1.0\n",
      "Training iteration 225 loss: 0.003649461315944791, ACC:1.0\n",
      "Training iteration 226 loss: 0.003020257456228137, ACC:1.0\n",
      "Training iteration 227 loss: 0.007104154676198959, ACC:1.0\n",
      "Training iteration 228 loss: 0.0028337358962744474, ACC:1.0\n",
      "Training iteration 229 loss: 0.010394260287284851, ACC:1.0\n",
      "Training iteration 230 loss: 0.01385237742215395, ACC:1.0\n",
      "Training iteration 231 loss: 0.047069910913705826, ACC:0.984375\n",
      "Training iteration 232 loss: 0.008883892558515072, ACC:1.0\n",
      "Training iteration 233 loss: 0.0007047889521345496, ACC:1.0\n",
      "Training iteration 234 loss: 0.0016958145424723625, ACC:1.0\n",
      "Training iteration 235 loss: 0.0010223702993243933, ACC:1.0\n",
      "Training iteration 236 loss: 0.001111657009460032, ACC:1.0\n",
      "Training iteration 237 loss: 0.0007942324155010283, ACC:1.0\n",
      "Training iteration 238 loss: 0.000644292333163321, ACC:1.0\n",
      "Training iteration 239 loss: 0.08941841870546341, ACC:0.984375\n",
      "Training iteration 240 loss: 0.00028869937523268163, ACC:1.0\n",
      "Training iteration 241 loss: 0.00032680557342246175, ACC:1.0\n",
      "Training iteration 242 loss: 0.00010389928502263501, ACC:1.0\n",
      "Training iteration 243 loss: 0.0006612215074710548, ACC:1.0\n",
      "Training iteration 244 loss: 0.0004231827042531222, ACC:1.0\n",
      "Training iteration 245 loss: 0.000357186101609841, ACC:1.0\n",
      "Training iteration 246 loss: 0.07685195654630661, ACC:0.984375\n",
      "Training iteration 247 loss: 0.0010982970707118511, ACC:1.0\n",
      "Training iteration 248 loss: 0.0004292161902412772, ACC:1.0\n",
      "Training iteration 249 loss: 0.001020899973809719, ACC:1.0\n",
      "Training iteration 250 loss: 0.012984229251742363, ACC:1.0\n",
      "Training iteration 251 loss: 0.010937836021184921, ACC:1.0\n",
      "Training iteration 252 loss: 0.4262004792690277, ACC:0.96875\n",
      "Training iteration 253 loss: 0.001790309906937182, ACC:1.0\n",
      "Training iteration 254 loss: 0.002239419147372246, ACC:1.0\n",
      "Training iteration 255 loss: 0.02034260891377926, ACC:0.984375\n",
      "Training iteration 256 loss: 0.0626826360821724, ACC:0.96875\n",
      "Training iteration 257 loss: 0.007900487631559372, ACC:1.0\n",
      "Training iteration 258 loss: 0.004555684048682451, ACC:1.0\n",
      "Training iteration 259 loss: 0.020503606647253036, ACC:0.984375\n",
      "Training iteration 260 loss: 0.007487139664590359, ACC:1.0\n",
      "Training iteration 261 loss: 0.006236211862415075, ACC:1.0\n",
      "Training iteration 262 loss: 0.008006084710359573, ACC:1.0\n",
      "Training iteration 263 loss: 0.004558105021715164, ACC:1.0\n",
      "Training iteration 264 loss: 0.037438470870256424, ACC:0.984375\n",
      "Training iteration 265 loss: 0.01765594258904457, ACC:0.984375\n",
      "Training iteration 266 loss: 0.004694322124123573, ACC:1.0\n",
      "Training iteration 267 loss: 0.0017166582401841879, ACC:1.0\n",
      "Training iteration 268 loss: 0.0028157304041087627, ACC:1.0\n",
      "Training iteration 269 loss: 0.00045438355300575495, ACC:1.0\n",
      "Training iteration 270 loss: 0.16859684884548187, ACC:0.984375\n",
      "Training iteration 271 loss: 0.10395121574401855, ACC:0.96875\n",
      "Training iteration 272 loss: 0.0746319442987442, ACC:0.984375\n",
      "Training iteration 273 loss: 0.0013513407902792096, ACC:1.0\n",
      "Training iteration 274 loss: 0.004157114773988724, ACC:1.0\n",
      "Training iteration 275 loss: 0.006289995275437832, ACC:1.0\n",
      "Training iteration 276 loss: 0.05624914914369583, ACC:0.984375\n",
      "Training iteration 277 loss: 0.011491275392472744, ACC:1.0\n",
      "Training iteration 278 loss: 0.017292380332946777, ACC:1.0\n",
      "Training iteration 279 loss: 0.01686626859009266, ACC:1.0\n",
      "Training iteration 280 loss: 0.005733097903430462, ACC:1.0\n",
      "Training iteration 281 loss: 0.004831502679735422, ACC:1.0\n",
      "Training iteration 282 loss: 0.00575289037078619, ACC:1.0\n",
      "Training iteration 283 loss: 0.0063366056419909, ACC:1.0\n",
      "Training iteration 284 loss: 0.04483664035797119, ACC:0.984375\n",
      "Training iteration 285 loss: 0.11816051602363586, ACC:0.984375\n",
      "Training iteration 286 loss: 0.01772463135421276, ACC:0.984375\n",
      "Training iteration 287 loss: 0.0008893798221834004, ACC:1.0\n",
      "Training iteration 288 loss: 0.0691390261054039, ACC:0.984375\n",
      "Training iteration 289 loss: 0.0004998577642254531, ACC:1.0\n",
      "Training iteration 290 loss: 0.04091093689203262, ACC:0.984375\n",
      "Training iteration 291 loss: 0.10768390446901321, ACC:0.96875\n",
      "Training iteration 292 loss: 0.0014823805540800095, ACC:1.0\n",
      "Training iteration 293 loss: 0.0004369234084151685, ACC:1.0\n",
      "Training iteration 294 loss: 0.004378421697765589, ACC:1.0\n",
      "Training iteration 295 loss: 0.07967542111873627, ACC:0.984375\n",
      "Training iteration 296 loss: 0.0012223900994285941, ACC:1.0\n",
      "Training iteration 297 loss: 0.08692724257707596, ACC:0.984375\n",
      "Training iteration 298 loss: 0.01926017925143242, ACC:0.984375\n",
      "Training iteration 299 loss: 0.01072907168418169, ACC:1.0\n",
      "Training iteration 300 loss: 0.12019664794206619, ACC:0.984375\n",
      "Training iteration 301 loss: 0.07909122109413147, ACC:0.984375\n",
      "Training iteration 302 loss: 0.024662446230649948, ACC:0.984375\n",
      "Training iteration 303 loss: 0.0007023970829322934, ACC:1.0\n",
      "Training iteration 304 loss: 0.04422391206026077, ACC:0.984375\n",
      "Training iteration 305 loss: 0.08729095757007599, ACC:0.96875\n",
      "Training iteration 306 loss: 0.05336408317089081, ACC:0.984375\n",
      "Training iteration 307 loss: 0.0034650417510420084, ACC:1.0\n",
      "Training iteration 308 loss: 0.007090168073773384, ACC:1.0\n",
      "Training iteration 309 loss: 0.013824552297592163, ACC:1.0\n",
      "Training iteration 310 loss: 0.03271748498082161, ACC:0.984375\n",
      "Training iteration 311 loss: 0.014707357622683048, ACC:1.0\n",
      "Training iteration 312 loss: 0.003261754522100091, ACC:1.0\n",
      "Training iteration 313 loss: 0.038141556084156036, ACC:0.984375\n",
      "Training iteration 314 loss: 0.22343701124191284, ACC:0.96875\n",
      "Training iteration 315 loss: 0.07294619828462601, ACC:0.984375\n",
      "Training iteration 316 loss: 0.02829727903008461, ACC:0.984375\n",
      "Training iteration 317 loss: 0.001458378043025732, ACC:1.0\n",
      "Training iteration 318 loss: 0.000693506037350744, ACC:1.0\n",
      "Training iteration 319 loss: 0.00302270264364779, ACC:1.0\n",
      "Training iteration 320 loss: 0.01665712147951126, ACC:0.984375\n",
      "Training iteration 321 loss: 0.0007405285723507404, ACC:1.0\n",
      "Training iteration 322 loss: 0.07105787843465805, ACC:0.96875\n",
      "Training iteration 323 loss: 0.0007831311086192727, ACC:1.0\n",
      "Training iteration 324 loss: 0.0011306380620226264, ACC:1.0\n",
      "Training iteration 325 loss: 0.11254359781742096, ACC:0.96875\n",
      "Training iteration 326 loss: 0.003986337222158909, ACC:1.0\n",
      "Training iteration 327 loss: 0.01552373543381691, ACC:1.0\n",
      "Training iteration 328 loss: 0.05841802805662155, ACC:0.984375\n",
      "Training iteration 329 loss: 0.0071310484781861305, ACC:1.0\n",
      "Training iteration 330 loss: 0.014482995495200157, ACC:1.0\n",
      "Training iteration 331 loss: 0.019125668331980705, ACC:1.0\n",
      "Training iteration 332 loss: 0.012702396139502525, ACC:1.0\n",
      "Training iteration 333 loss: 0.013745402917265892, ACC:1.0\n",
      "Training iteration 334 loss: 0.044795773923397064, ACC:0.984375\n",
      "Training iteration 335 loss: 0.00539844436571002, ACC:1.0\n",
      "Training iteration 336 loss: 0.05850644037127495, ACC:0.984375\n",
      "Training iteration 337 loss: 0.0020540431141853333, ACC:1.0\n",
      "Training iteration 338 loss: 0.001025583827868104, ACC:1.0\n",
      "Training iteration 339 loss: 0.002210521139204502, ACC:1.0\n",
      "Training iteration 340 loss: 0.0067454450763762, ACC:1.0\n",
      "Training iteration 341 loss: 0.001737915794365108, ACC:1.0\n",
      "Training iteration 342 loss: 0.0016025510849431157, ACC:1.0\n",
      "Training iteration 343 loss: 0.00018963802722282708, ACC:1.0\n",
      "Training iteration 344 loss: 0.0004116334021091461, ACC:1.0\n",
      "Training iteration 345 loss: 0.15539412200450897, ACC:0.984375\n",
      "Training iteration 346 loss: 0.10249267518520355, ACC:0.96875\n",
      "Training iteration 347 loss: 0.0844668298959732, ACC:0.984375\n",
      "Training iteration 348 loss: 0.021793151274323463, ACC:0.984375\n",
      "Training iteration 349 loss: 0.0035679549910128117, ACC:1.0\n",
      "Training iteration 350 loss: 0.11562157422304153, ACC:0.96875\n",
      "Training iteration 351 loss: 0.006140497513115406, ACC:1.0\n",
      "Training iteration 352 loss: 0.0015412840293720365, ACC:1.0\n",
      "Training iteration 353 loss: 0.0013556117191910744, ACC:1.0\n",
      "Training iteration 354 loss: 0.10866491496562958, ACC:0.96875\n",
      "Training iteration 355 loss: 0.0015244000824168324, ACC:1.0\n",
      "Training iteration 356 loss: 0.0038011844735592604, ACC:1.0\n",
      "Training iteration 357 loss: 0.004083122126758099, ACC:1.0\n",
      "Training iteration 358 loss: 0.04450041800737381, ACC:0.984375\n",
      "Training iteration 359 loss: 0.012265271507203579, ACC:0.984375\n",
      "Training iteration 360 loss: 0.002813595812767744, ACC:1.0\n",
      "Training iteration 361 loss: 0.00035084428964182734, ACC:1.0\n",
      "Training iteration 362 loss: 0.00020742413471452892, ACC:1.0\n",
      "Training iteration 363 loss: 0.007510781288146973, ACC:1.0\n",
      "Training iteration 364 loss: 0.10812541842460632, ACC:0.984375\n",
      "Training iteration 365 loss: 0.020012252032756805, ACC:0.984375\n",
      "Training iteration 366 loss: 0.00045285464148037136, ACC:1.0\n",
      "Training iteration 367 loss: 0.008166613057255745, ACC:1.0\n",
      "Training iteration 368 loss: 0.0019312790827825665, ACC:1.0\n",
      "Training iteration 369 loss: 0.0012132541742175817, ACC:1.0\n",
      "Training iteration 370 loss: 0.020669812336564064, ACC:0.984375\n",
      "Training iteration 371 loss: 0.055693574249744415, ACC:0.984375\n",
      "Training iteration 372 loss: 0.0015813313657417893, ACC:1.0\n",
      "Training iteration 373 loss: 0.0016640865942463279, ACC:1.0\n",
      "Training iteration 374 loss: 0.003802211955189705, ACC:1.0\n",
      "Training iteration 375 loss: 0.003163183107972145, ACC:1.0\n",
      "Training iteration 376 loss: 0.0024474419187754393, ACC:1.0\n",
      "Training iteration 377 loss: 0.0010969194117933512, ACC:1.0\n",
      "Training iteration 378 loss: 0.0006508939550258219, ACC:1.0\n",
      "Training iteration 379 loss: 0.054231565445661545, ACC:0.984375\n",
      "Training iteration 380 loss: 0.0043364958837628365, ACC:1.0\n",
      "Training iteration 381 loss: 0.1417275369167328, ACC:0.96875\n",
      "Training iteration 382 loss: 0.000472228042781353, ACC:1.0\n",
      "Training iteration 383 loss: 0.0008551308419555426, ACC:1.0\n",
      "Training iteration 384 loss: 0.22073255479335785, ACC:0.953125\n",
      "Training iteration 385 loss: 0.0011967129539698362, ACC:1.0\n",
      "Training iteration 386 loss: 0.008906921371817589, ACC:1.0\n",
      "Training iteration 387 loss: 0.0008480840479023755, ACC:1.0\n",
      "Training iteration 388 loss: 0.0007581787649542093, ACC:1.0\n",
      "Training iteration 389 loss: 0.037144411355257034, ACC:0.984375\n",
      "Training iteration 390 loss: 0.03967519849538803, ACC:0.984375\n",
      "Training iteration 391 loss: 0.03918934613466263, ACC:0.984375\n",
      "Training iteration 392 loss: 0.09079886227846146, ACC:0.953125\n",
      "Training iteration 393 loss: 0.06388621777296066, ACC:0.984375\n",
      "Training iteration 394 loss: 0.07882627099752426, ACC:0.96875\n",
      "Training iteration 395 loss: 0.022242825478315353, ACC:1.0\n",
      "Training iteration 396 loss: 0.006604819092899561, ACC:1.0\n",
      "Training iteration 397 loss: 0.0016150408191606402, ACC:1.0\n",
      "Training iteration 398 loss: 0.07940652221441269, ACC:0.953125\n",
      "Training iteration 399 loss: 0.002670824294909835, ACC:1.0\n",
      "Training iteration 400 loss: 0.030867302790284157, ACC:0.984375\n",
      "Training iteration 401 loss: 0.12334056198596954, ACC:0.984375\n",
      "Training iteration 402 loss: 0.002004713751375675, ACC:1.0\n",
      "Training iteration 403 loss: 0.03708688169717789, ACC:0.984375\n",
      "Training iteration 404 loss: 0.2031514048576355, ACC:0.953125\n",
      "Training iteration 405 loss: 0.10765363276004791, ACC:0.96875\n",
      "Training iteration 406 loss: 0.011017311364412308, ACC:1.0\n",
      "Training iteration 407 loss: 0.057630784809589386, ACC:0.96875\n",
      "Training iteration 408 loss: 0.0052705747075378895, ACC:1.0\n",
      "Training iteration 409 loss: 0.004972201306372881, ACC:1.0\n",
      "Training iteration 410 loss: 0.026249095797538757, ACC:0.984375\n",
      "Training iteration 411 loss: 0.008093900978565216, ACC:1.0\n",
      "Training iteration 412 loss: 0.017627635970711708, ACC:0.984375\n",
      "Training iteration 413 loss: 0.0032591696362942457, ACC:1.0\n",
      "Training iteration 414 loss: 0.0781954973936081, ACC:0.984375\n",
      "Training iteration 415 loss: 0.08236146718263626, ACC:0.984375\n",
      "Training iteration 416 loss: 0.09457255899906158, ACC:0.953125\n",
      "Training iteration 417 loss: 0.14634662866592407, ACC:0.96875\n",
      "Training iteration 418 loss: 0.07129043340682983, ACC:0.96875\n",
      "Training iteration 419 loss: 0.07036986947059631, ACC:0.984375\n",
      "Training iteration 420 loss: 0.26952099800109863, ACC:0.984375\n",
      "Training iteration 421 loss: 0.0020225513726472855, ACC:1.0\n",
      "Training iteration 422 loss: 0.03778887167572975, ACC:0.984375\n",
      "Training iteration 423 loss: 0.002076490316540003, ACC:1.0\n",
      "Training iteration 424 loss: 0.06225958094000816, ACC:0.984375\n",
      "Training iteration 425 loss: 0.09090598672628403, ACC:0.984375\n",
      "Training iteration 426 loss: 0.0016864328645169735, ACC:1.0\n",
      "Training iteration 427 loss: 0.010928552597761154, ACC:1.0\n",
      "Training iteration 428 loss: 0.147597536444664, ACC:0.9375\n",
      "Training iteration 429 loss: 0.03197187930345535, ACC:0.984375\n",
      "Training iteration 430 loss: 0.11137992888689041, ACC:0.96875\n",
      "Training iteration 431 loss: 0.10589451342821121, ACC:0.96875\n",
      "Training iteration 432 loss: 0.04148591309785843, ACC:0.984375\n",
      "Training iteration 433 loss: 0.007324840873479843, ACC:1.0\n",
      "Training iteration 434 loss: 0.004951710347086191, ACC:1.0\n",
      "Training iteration 435 loss: 0.026253147050738335, ACC:0.984375\n",
      "Training iteration 436 loss: 0.0019526861142367125, ACC:1.0\n",
      "Training iteration 437 loss: 0.00020624315948225558, ACC:1.0\n",
      "Training iteration 438 loss: 0.02337801270186901, ACC:0.984375\n",
      "Training iteration 439 loss: 0.00088409025920555, ACC:1.0\n",
      "Training iteration 440 loss: 0.00053485541138798, ACC:1.0\n",
      "Training iteration 441 loss: 0.00025474303402006626, ACC:1.0\n",
      "Training iteration 442 loss: 9.924920595949516e-05, ACC:1.0\n",
      "Training iteration 443 loss: 0.0001740895095281303, ACC:1.0\n",
      "Training iteration 444 loss: 0.004861884284764528, ACC:1.0\n",
      "Training iteration 445 loss: 0.006272774655371904, ACC:1.0\n",
      "Training iteration 446 loss: 0.05455810949206352, ACC:0.984375\n",
      "Training iteration 447 loss: 0.0022525081876665354, ACC:1.0\n",
      "Training iteration 448 loss: 0.023670528084039688, ACC:0.984375\n",
      "Training iteration 449 loss: 0.053924474865198135, ACC:0.984375\n",
      "Training iteration 450 loss: 0.0011504238937050104, ACC:1.0\n",
      "Validation iteration 451 loss: 0.004374315962195396, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.05071207508444786, ACC: 0.96875\n",
      "Validation iteration 453 loss: 0.030211295932531357, ACC: 0.984375\n",
      "Validation iteration 454 loss: 0.009096456691622734, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.005826948676258326, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.014516469091176987, ACC: 0.984375\n",
      "Validation iteration 457 loss: 0.00793426763266325, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.04547441750764847, ACC: 0.984375\n",
      "Validation iteration 459 loss: 0.01924346387386322, ACC: 0.984375\n",
      "Validation iteration 460 loss: 0.034133054316043854, ACC: 0.984375\n",
      "Validation iteration 461 loss: 0.007710717618465424, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.01143853459507227, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.034918371587991714, ACC: 0.984375\n",
      "Validation iteration 464 loss: 0.016464605927467346, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.043920326977968216, ACC: 0.984375\n",
      "Validation iteration 466 loss: 0.06665115803480148, ACC: 0.984375\n",
      "Validation iteration 467 loss: 0.0587739422917366, ACC: 0.984375\n",
      "Validation iteration 468 loss: 0.00235688011161983, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.019032809883356094, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.011017734184861183, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.0021619810722768307, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.0529477559030056, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.003908555954694748, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.00669400067999959, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.002473190426826477, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.00302239996381104, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.08048111200332642, ACC: 0.96875\n",
      "Validation iteration 478 loss: 0.005362371448427439, ACC: 1.0\n",
      "Validation iteration 479 loss: 0.015319899655878544, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.0037895035929977894, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.0036046220920979977, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.0033387907315045595, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.11725346744060516, ACC: 0.984375\n",
      "Validation iteration 484 loss: 0.025925347581505775, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.0018136007711291313, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.0024784940760582685, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.018273459747433662, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.002285397844389081, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.14575085043907166, ACC: 0.96875\n",
      "Validation iteration 490 loss: 0.0027108483482152224, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.014862747862935066, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.061369288712739944, ACC: 0.984375\n",
      "Validation iteration 493 loss: 0.022020990028977394, ACC: 0.984375\n",
      "Validation iteration 494 loss: 0.01171582005918026, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.13392433524131775, ACC: 0.984375\n",
      "Validation iteration 496 loss: 0.12200288474559784, ACC: 0.96875\n",
      "Validation iteration 497 loss: 0.028175218030810356, ACC: 0.984375\n",
      "Validation iteration 498 loss: 0.2476763129234314, ACC: 0.953125\n",
      "Validation iteration 499 loss: 0.006976558826863766, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.0007044037920422852, ACC: 1.0\n",
      "-- Epoch 8 done -- Train loss: 0.026781974285428684, train ACC: 0.9931944444444445, val loss: 0.03285664111957885, val ACC: 0.9915625\n",
      "<--- 14712.43992137909 seconds --->\n",
      "Training iteration 1 loss: 0.00154096819460392, ACC:1.0\n",
      "Training iteration 2 loss: 0.027296453714370728, ACC:0.984375\n",
      "Training iteration 3 loss: 0.004178491421043873, ACC:1.0\n",
      "Training iteration 4 loss: 0.06662425398826599, ACC:0.984375\n",
      "Training iteration 5 loss: 0.027579886838793755, ACC:0.984375\n",
      "Training iteration 6 loss: 0.00035421858774498105, ACC:1.0\n",
      "Training iteration 7 loss: 0.0002971506037283689, ACC:1.0\n",
      "Training iteration 8 loss: 0.0018139717867597938, ACC:1.0\n",
      "Training iteration 9 loss: 0.0002739447227213532, ACC:1.0\n",
      "Training iteration 10 loss: 0.025001218542456627, ACC:0.984375\n",
      "Training iteration 11 loss: 0.16948656737804413, ACC:0.984375\n",
      "Training iteration 12 loss: 0.17936065793037415, ACC:0.953125\n",
      "Training iteration 13 loss: 0.0007527638226747513, ACC:1.0\n",
      "Training iteration 14 loss: 0.0026836004108190536, ACC:1.0\n",
      "Training iteration 15 loss: 0.005312282592058182, ACC:1.0\n",
      "Training iteration 16 loss: 0.07367163151502609, ACC:0.984375\n",
      "Training iteration 17 loss: 0.06780364364385605, ACC:0.953125\n",
      "Training iteration 18 loss: 0.004603227600455284, ACC:1.0\n",
      "Training iteration 19 loss: 0.005846839863806963, ACC:1.0\n",
      "Training iteration 20 loss: 0.12109898030757904, ACC:0.953125\n",
      "Training iteration 21 loss: 0.006666301749646664, ACC:1.0\n",
      "Training iteration 22 loss: 0.029490357264876366, ACC:0.984375\n",
      "Training iteration 23 loss: 0.007427644915878773, ACC:1.0\n",
      "Training iteration 24 loss: 0.007125855889171362, ACC:1.0\n",
      "Training iteration 25 loss: 0.00551319494843483, ACC:1.0\n",
      "Training iteration 26 loss: 0.006335914600640535, ACC:1.0\n",
      "Training iteration 27 loss: 0.019334452226758003, ACC:0.984375\n",
      "Training iteration 28 loss: 0.00227406807243824, ACC:1.0\n",
      "Training iteration 29 loss: 0.004666677676141262, ACC:1.0\n",
      "Training iteration 30 loss: 0.010829336941242218, ACC:1.0\n",
      "Training iteration 31 loss: 0.0019449129467830062, ACC:1.0\n",
      "Training iteration 32 loss: 0.002580808475613594, ACC:1.0\n",
      "Training iteration 33 loss: 0.2005409449338913, ACC:0.984375\n",
      "Training iteration 34 loss: 0.0009866656037047505, ACC:1.0\n",
      "Training iteration 35 loss: 0.00045871484326198697, ACC:1.0\n",
      "Training iteration 36 loss: 0.06426854431629181, ACC:0.984375\n",
      "Training iteration 37 loss: 0.18297827243804932, ACC:0.96875\n",
      "Training iteration 38 loss: 0.13040685653686523, ACC:0.96875\n",
      "Training iteration 39 loss: 0.02813764289021492, ACC:0.984375\n",
      "Training iteration 40 loss: 0.010935883969068527, ACC:1.0\n",
      "Training iteration 41 loss: 0.0036845291033387184, ACC:1.0\n",
      "Training iteration 42 loss: 0.03122936561703682, ACC:0.984375\n",
      "Training iteration 43 loss: 0.0029942309483885765, ACC:1.0\n",
      "Training iteration 44 loss: 0.009128759615123272, ACC:1.0\n",
      "Training iteration 45 loss: 0.021508021280169487, ACC:0.984375\n",
      "Training iteration 46 loss: 0.0027427608147263527, ACC:1.0\n",
      "Training iteration 47 loss: 0.003586016595363617, ACC:1.0\n",
      "Training iteration 48 loss: 0.01511783991008997, ACC:1.0\n",
      "Training iteration 49 loss: 0.07923007011413574, ACC:0.984375\n",
      "Training iteration 50 loss: 0.06565257161855698, ACC:0.984375\n",
      "Training iteration 51 loss: 0.007158963941037655, ACC:1.0\n",
      "Training iteration 52 loss: 0.0002493360370863229, ACC:1.0\n",
      "Training iteration 53 loss: 0.03613002598285675, ACC:0.984375\n",
      "Training iteration 54 loss: 0.0057573504745960236, ACC:1.0\n",
      "Training iteration 55 loss: 0.0023339407052844763, ACC:1.0\n",
      "Training iteration 56 loss: 0.0037432557437568903, ACC:1.0\n",
      "Training iteration 57 loss: 0.0069870962761342525, ACC:1.0\n",
      "Training iteration 58 loss: 0.0008073776843957603, ACC:1.0\n",
      "Training iteration 59 loss: 0.021722910925745964, ACC:0.984375\n",
      "Training iteration 60 loss: 0.10254064202308655, ACC:0.984375\n",
      "Training iteration 61 loss: 0.003862583078444004, ACC:1.0\n",
      "Training iteration 62 loss: 0.010189074091613293, ACC:1.0\n",
      "Training iteration 63 loss: 0.014280742965638638, ACC:0.984375\n",
      "Training iteration 64 loss: 0.02861878089606762, ACC:0.984375\n",
      "Training iteration 65 loss: 0.0012889510253444314, ACC:1.0\n",
      "Training iteration 66 loss: 0.006033852696418762, ACC:1.0\n",
      "Training iteration 67 loss: 0.0005815992481075227, ACC:1.0\n",
      "Training iteration 68 loss: 0.00936309527605772, ACC:1.0\n",
      "Training iteration 69 loss: 0.0001279456919291988, ACC:1.0\n",
      "Training iteration 70 loss: 0.00929341558367014, ACC:1.0\n",
      "Training iteration 71 loss: 0.004424122162163258, ACC:1.0\n",
      "Training iteration 72 loss: 0.00027522153686732054, ACC:1.0\n",
      "Training iteration 73 loss: 0.0001502254162915051, ACC:1.0\n",
      "Training iteration 74 loss: 0.00016862402844708413, ACC:1.0\n",
      "Training iteration 75 loss: 0.00012707654968835413, ACC:1.0\n",
      "Training iteration 76 loss: 0.0011151143116876483, ACC:1.0\n",
      "Training iteration 77 loss: 0.002537680324167013, ACC:1.0\n",
      "Training iteration 78 loss: 0.00020628732454497367, ACC:1.0\n",
      "Training iteration 79 loss: 0.00017849853611551225, ACC:1.0\n",
      "Training iteration 80 loss: 0.00018959424050990492, ACC:1.0\n",
      "Training iteration 81 loss: 0.001130108255892992, ACC:1.0\n",
      "Training iteration 82 loss: 0.014916898682713509, ACC:0.984375\n",
      "Training iteration 83 loss: 0.0004988152650184929, ACC:1.0\n",
      "Training iteration 84 loss: 0.05606558173894882, ACC:0.984375\n",
      "Training iteration 85 loss: 0.10142337530851364, ACC:0.984375\n",
      "Training iteration 86 loss: 0.0008140271529555321, ACC:1.0\n",
      "Training iteration 87 loss: 0.000990812317468226, ACC:1.0\n",
      "Training iteration 88 loss: 0.0009461662266403437, ACC:1.0\n",
      "Training iteration 89 loss: 0.006448214873671532, ACC:1.0\n",
      "Training iteration 90 loss: 0.004014667589217424, ACC:1.0\n",
      "Training iteration 91 loss: 0.004198136739432812, ACC:1.0\n",
      "Training iteration 92 loss: 0.18032580614089966, ACC:0.984375\n",
      "Training iteration 93 loss: 0.00038738123839721084, ACC:1.0\n",
      "Training iteration 94 loss: 0.0002025929425144568, ACC:1.0\n",
      "Training iteration 95 loss: 0.00029900926165282726, ACC:1.0\n",
      "Training iteration 96 loss: 0.00012637182953767478, ACC:1.0\n",
      "Training iteration 97 loss: 0.0002755661844275892, ACC:1.0\n",
      "Training iteration 98 loss: 0.00044090888695791364, ACC:1.0\n",
      "Training iteration 99 loss: 0.04249720275402069, ACC:0.984375\n",
      "Training iteration 100 loss: 0.0003032722161151469, ACC:1.0\n",
      "Training iteration 101 loss: 9.673894965089858e-05, ACC:1.0\n",
      "Training iteration 102 loss: 0.010071896947920322, ACC:1.0\n",
      "Training iteration 103 loss: 0.12199001014232635, ACC:0.984375\n",
      "Training iteration 104 loss: 0.00023084015992935747, ACC:1.0\n",
      "Training iteration 105 loss: 0.007353884167969227, ACC:1.0\n",
      "Training iteration 106 loss: 0.0022093146108090878, ACC:1.0\n",
      "Training iteration 107 loss: 0.002796287415549159, ACC:1.0\n",
      "Training iteration 108 loss: 0.04471152275800705, ACC:0.984375\n",
      "Training iteration 109 loss: 0.02180076576769352, ACC:0.984375\n",
      "Training iteration 110 loss: 0.011766689829528332, ACC:1.0\n",
      "Training iteration 111 loss: 0.015295750461518764, ACC:1.0\n",
      "Training iteration 112 loss: 0.03285066410899162, ACC:0.96875\n",
      "Training iteration 113 loss: 0.03593432158231735, ACC:0.984375\n",
      "Training iteration 114 loss: 0.0046488093212246895, ACC:1.0\n",
      "Training iteration 115 loss: 0.007554174400866032, ACC:1.0\n",
      "Training iteration 116 loss: 0.0023070794995874166, ACC:1.0\n",
      "Training iteration 117 loss: 0.00031820483854971826, ACC:1.0\n",
      "Training iteration 118 loss: 0.027099890634417534, ACC:0.984375\n",
      "Training iteration 119 loss: 0.0003172654251102358, ACC:1.0\n",
      "Training iteration 120 loss: 0.10922370851039886, ACC:0.984375\n",
      "Training iteration 121 loss: 8.301369234686717e-05, ACC:1.0\n",
      "Training iteration 122 loss: 0.002676856704056263, ACC:1.0\n",
      "Training iteration 123 loss: 0.003967571537941694, ACC:1.0\n",
      "Training iteration 124 loss: 0.00036751097650267184, ACC:1.0\n",
      "Training iteration 125 loss: 0.037122633308172226, ACC:0.984375\n",
      "Training iteration 126 loss: 0.06899607181549072, ACC:0.984375\n",
      "Training iteration 127 loss: 0.05663345009088516, ACC:0.984375\n",
      "Training iteration 128 loss: 0.00200686976313591, ACC:1.0\n",
      "Training iteration 129 loss: 0.03142417594790459, ACC:0.984375\n",
      "Training iteration 130 loss: 0.03289886936545372, ACC:0.984375\n",
      "Training iteration 131 loss: 0.058963097631931305, ACC:0.984375\n",
      "Training iteration 132 loss: 0.0016735551180317998, ACC:1.0\n",
      "Training iteration 133 loss: 0.0008751106215640903, ACC:1.0\n",
      "Training iteration 134 loss: 0.0016952131409198046, ACC:1.0\n",
      "Training iteration 135 loss: 0.0017354098381474614, ACC:1.0\n",
      "Training iteration 136 loss: 0.10128895938396454, ACC:0.984375\n",
      "Training iteration 137 loss: 0.0014100465923547745, ACC:1.0\n",
      "Training iteration 138 loss: 0.11663453280925751, ACC:0.984375\n",
      "Training iteration 139 loss: 0.07630901038646698, ACC:0.96875\n",
      "Training iteration 140 loss: 0.0008092408534139395, ACC:1.0\n",
      "Training iteration 141 loss: 0.09979115426540375, ACC:0.984375\n",
      "Training iteration 142 loss: 0.006176590919494629, ACC:1.0\n",
      "Training iteration 143 loss: 0.006450916640460491, ACC:1.0\n",
      "Training iteration 144 loss: 0.028667382895946503, ACC:0.984375\n",
      "Training iteration 145 loss: 0.0004844526993110776, ACC:1.0\n",
      "Training iteration 146 loss: 0.04716510325670242, ACC:0.984375\n",
      "Training iteration 147 loss: 0.00015567184891551733, ACC:1.0\n",
      "Training iteration 148 loss: 5.176036211196333e-05, ACC:1.0\n",
      "Training iteration 149 loss: 5.389943908085115e-05, ACC:1.0\n",
      "Training iteration 150 loss: 0.00011128692858619615, ACC:1.0\n",
      "Training iteration 151 loss: 0.0001520862424513325, ACC:1.0\n",
      "Training iteration 152 loss: 3.4359054552624e-05, ACC:1.0\n",
      "Training iteration 153 loss: 0.0015453281812369823, ACC:1.0\n",
      "Training iteration 154 loss: 0.004017962142825127, ACC:1.0\n",
      "Training iteration 155 loss: 0.0007021483033895493, ACC:1.0\n",
      "Training iteration 156 loss: 0.02071831375360489, ACC:0.984375\n",
      "Training iteration 157 loss: 0.00024402111012022942, ACC:1.0\n",
      "Training iteration 158 loss: 5.8855453971773386e-05, ACC:1.0\n",
      "Training iteration 159 loss: 0.0027889993507415056, ACC:1.0\n",
      "Training iteration 160 loss: 0.0703270435333252, ACC:0.984375\n",
      "Training iteration 161 loss: 3.3585041819605976e-05, ACC:1.0\n",
      "Training iteration 162 loss: 0.0004388461238704622, ACC:1.0\n",
      "Training iteration 163 loss: 0.0004301021690480411, ACC:1.0\n",
      "Training iteration 164 loss: 0.046978093683719635, ACC:0.984375\n",
      "Training iteration 165 loss: 4.803417687071487e-05, ACC:1.0\n",
      "Training iteration 166 loss: 0.1906784474849701, ACC:0.96875\n",
      "Training iteration 167 loss: 0.009829235263168812, ACC:1.0\n",
      "Training iteration 168 loss: 0.0008182000019587576, ACC:1.0\n",
      "Training iteration 169 loss: 0.07065216451883316, ACC:0.984375\n",
      "Training iteration 170 loss: 0.14158909022808075, ACC:0.96875\n",
      "Training iteration 171 loss: 0.00459270179271698, ACC:1.0\n",
      "Training iteration 172 loss: 0.11326240003108978, ACC:0.96875\n",
      "Training iteration 173 loss: 0.09460961073637009, ACC:0.96875\n",
      "Training iteration 174 loss: 0.1414351612329483, ACC:0.96875\n",
      "Training iteration 175 loss: 0.006610475014895201, ACC:1.0\n",
      "Training iteration 176 loss: 0.14233332872390747, ACC:0.96875\n",
      "Training iteration 177 loss: 0.01634391024708748, ACC:1.0\n",
      "Training iteration 178 loss: 0.01243144553154707, ACC:1.0\n",
      "Training iteration 179 loss: 0.0399995818734169, ACC:0.96875\n",
      "Training iteration 180 loss: 0.03634217008948326, ACC:0.984375\n",
      "Training iteration 181 loss: 0.02326304279267788, ACC:0.984375\n",
      "Training iteration 182 loss: 0.004761487245559692, ACC:1.0\n",
      "Training iteration 183 loss: 0.02206646278500557, ACC:0.984375\n",
      "Training iteration 184 loss: 0.006357837468385696, ACC:1.0\n",
      "Training iteration 185 loss: 0.0444730743765831, ACC:0.984375\n",
      "Training iteration 186 loss: 0.04267605021595955, ACC:0.984375\n",
      "Training iteration 187 loss: 0.022100193426012993, ACC:0.984375\n",
      "Training iteration 188 loss: 0.03437836840748787, ACC:0.984375\n",
      "Training iteration 189 loss: 0.002550099277868867, ACC:1.0\n",
      "Training iteration 190 loss: 0.040099501609802246, ACC:0.984375\n",
      "Training iteration 191 loss: 0.012372259050607681, ACC:1.0\n",
      "Training iteration 192 loss: 0.01625888980925083, ACC:0.984375\n",
      "Training iteration 193 loss: 0.0018694752361625433, ACC:1.0\n",
      "Training iteration 194 loss: 0.0010134225012734532, ACC:1.0\n",
      "Training iteration 195 loss: 0.00024301621306221932, ACC:1.0\n",
      "Training iteration 196 loss: 0.0002158725546905771, ACC:1.0\n",
      "Training iteration 197 loss: 0.0006663362728431821, ACC:1.0\n",
      "Training iteration 198 loss: 7.583948172396049e-05, ACC:1.0\n",
      "Training iteration 199 loss: 0.0007961222436279058, ACC:1.0\n",
      "Training iteration 200 loss: 0.010111181996762753, ACC:1.0\n",
      "Training iteration 201 loss: 0.1276804357767105, ACC:0.984375\n",
      "Training iteration 202 loss: 0.00019789474026765674, ACC:1.0\n",
      "Training iteration 203 loss: 0.07759673148393631, ACC:0.984375\n",
      "Training iteration 204 loss: 0.00043746328447014093, ACC:1.0\n",
      "Training iteration 205 loss: 0.00012113904813304543, ACC:1.0\n",
      "Training iteration 206 loss: 0.0004988130531273782, ACC:1.0\n",
      "Training iteration 207 loss: 0.0008454533526673913, ACC:1.0\n",
      "Training iteration 208 loss: 0.0705496296286583, ACC:0.984375\n",
      "Training iteration 209 loss: 0.07235771417617798, ACC:0.96875\n",
      "Training iteration 210 loss: 0.06247849762439728, ACC:0.984375\n",
      "Training iteration 211 loss: 0.07445000857114792, ACC:0.984375\n",
      "Training iteration 212 loss: 0.0007970094447955489, ACC:1.0\n",
      "Training iteration 213 loss: 0.0005396371707320213, ACC:1.0\n",
      "Training iteration 214 loss: 0.00016403027984779328, ACC:1.0\n",
      "Training iteration 215 loss: 0.0006264755502343178, ACC:1.0\n",
      "Training iteration 216 loss: 0.019634317606687546, ACC:1.0\n",
      "Training iteration 217 loss: 0.01838991604745388, ACC:0.984375\n",
      "Training iteration 218 loss: 0.08329372107982635, ACC:0.96875\n",
      "Training iteration 219 loss: 8.651595999253914e-05, ACC:1.0\n",
      "Training iteration 220 loss: 0.0006058784201741219, ACC:1.0\n",
      "Training iteration 221 loss: 0.0005077634705230594, ACC:1.0\n",
      "Training iteration 222 loss: 0.06340372562408447, ACC:0.984375\n",
      "Training iteration 223 loss: 0.00027596193831413984, ACC:1.0\n",
      "Training iteration 224 loss: 0.0189667921513319, ACC:0.984375\n",
      "Training iteration 225 loss: 0.04903063923120499, ACC:0.984375\n",
      "Training iteration 226 loss: 0.06668668240308762, ACC:0.984375\n",
      "Training iteration 227 loss: 0.08348127454519272, ACC:0.984375\n",
      "Training iteration 228 loss: 0.00393262505531311, ACC:1.0\n",
      "Training iteration 229 loss: 0.016325298696756363, ACC:0.984375\n",
      "Training iteration 230 loss: 0.014755227603018284, ACC:1.0\n",
      "Training iteration 231 loss: 0.015024034306406975, ACC:1.0\n",
      "Training iteration 232 loss: 0.01824135147035122, ACC:1.0\n",
      "Training iteration 233 loss: 0.031208205968141556, ACC:0.984375\n",
      "Training iteration 234 loss: 0.49785205721855164, ACC:0.984375\n",
      "Training iteration 235 loss: 0.00081965874414891, ACC:1.0\n",
      "Training iteration 236 loss: 0.10676086694002151, ACC:0.984375\n",
      "Training iteration 237 loss: 0.06778918951749802, ACC:0.984375\n",
      "Training iteration 238 loss: 0.007592853158712387, ACC:1.0\n",
      "Training iteration 239 loss: 0.01819404773414135, ACC:0.984375\n",
      "Training iteration 240 loss: 0.19000625610351562, ACC:0.96875\n",
      "Training iteration 241 loss: 0.00029624486342072487, ACC:1.0\n",
      "Training iteration 242 loss: 0.0006796345696784556, ACC:1.0\n",
      "Training iteration 243 loss: 0.004654326010495424, ACC:1.0\n",
      "Training iteration 244 loss: 0.06826138496398926, ACC:0.984375\n",
      "Training iteration 245 loss: 0.0022129963617771864, ACC:1.0\n",
      "Training iteration 246 loss: 0.00926568266004324, ACC:1.0\n",
      "Training iteration 247 loss: 0.056442201137542725, ACC:0.96875\n",
      "Training iteration 248 loss: 0.07442456483840942, ACC:0.96875\n",
      "Training iteration 249 loss: 0.009339088574051857, ACC:1.0\n",
      "Training iteration 250 loss: 0.014385313726961613, ACC:1.0\n",
      "Training iteration 251 loss: 0.006931812968105078, ACC:1.0\n",
      "Training iteration 252 loss: 0.1264973133802414, ACC:0.96875\n",
      "Training iteration 253 loss: 0.06976715475320816, ACC:0.984375\n",
      "Training iteration 254 loss: 0.09098482877016068, ACC:0.96875\n",
      "Training iteration 255 loss: 0.002823745599016547, ACC:1.0\n",
      "Training iteration 256 loss: 0.005514061078429222, ACC:1.0\n",
      "Training iteration 257 loss: 0.0020849204156547785, ACC:1.0\n",
      "Training iteration 258 loss: 0.01528011541813612, ACC:1.0\n",
      "Training iteration 259 loss: 0.005245185922831297, ACC:1.0\n",
      "Training iteration 260 loss: 0.020709212869405746, ACC:1.0\n",
      "Training iteration 261 loss: 0.009535551071166992, ACC:1.0\n",
      "Training iteration 262 loss: 0.03473450616002083, ACC:0.984375\n",
      "Training iteration 263 loss: 0.14336653053760529, ACC:0.96875\n",
      "Training iteration 264 loss: 0.02366957813501358, ACC:0.984375\n",
      "Training iteration 265 loss: 0.017935002222657204, ACC:1.0\n",
      "Training iteration 266 loss: 0.0006367778405547142, ACC:1.0\n",
      "Training iteration 267 loss: 0.0034393195528537035, ACC:1.0\n",
      "Training iteration 268 loss: 0.003931718412786722, ACC:1.0\n",
      "Training iteration 269 loss: 0.019236596301198006, ACC:0.984375\n",
      "Training iteration 270 loss: 0.0015735859051346779, ACC:1.0\n",
      "Training iteration 271 loss: 0.2858949899673462, ACC:0.984375\n",
      "Training iteration 272 loss: 0.03782833740115166, ACC:0.984375\n",
      "Training iteration 273 loss: 0.0018176388693973422, ACC:1.0\n",
      "Training iteration 274 loss: 0.0016586089041084051, ACC:1.0\n",
      "Training iteration 275 loss: 0.0004442696226760745, ACC:1.0\n",
      "Training iteration 276 loss: 0.00018147706578020006, ACC:1.0\n",
      "Training iteration 277 loss: 0.12144996970891953, ACC:0.984375\n",
      "Training iteration 278 loss: 0.022610483691096306, ACC:0.984375\n",
      "Training iteration 279 loss: 0.009423443116247654, ACC:1.0\n",
      "Training iteration 280 loss: 0.19417645037174225, ACC:0.9375\n",
      "Training iteration 281 loss: 0.004680355079472065, ACC:1.0\n",
      "Training iteration 282 loss: 0.01776835508644581, ACC:1.0\n",
      "Training iteration 283 loss: 0.05159987509250641, ACC:0.984375\n",
      "Training iteration 284 loss: 0.01901545189321041, ACC:1.0\n",
      "Training iteration 285 loss: 0.02313489094376564, ACC:1.0\n",
      "Training iteration 286 loss: 0.01705026999115944, ACC:1.0\n",
      "Training iteration 287 loss: 0.08428694307804108, ACC:0.984375\n",
      "Training iteration 288 loss: 0.0013820334570482373, ACC:1.0\n",
      "Training iteration 289 loss: 0.0013318270212039351, ACC:1.0\n",
      "Training iteration 290 loss: 0.00023398704070132226, ACC:1.0\n",
      "Training iteration 291 loss: 0.0006747770239599049, ACC:1.0\n",
      "Training iteration 292 loss: 6.804618169553578e-05, ACC:1.0\n",
      "Training iteration 293 loss: 0.0003366093442309648, ACC:1.0\n",
      "Training iteration 294 loss: 0.09187938272953033, ACC:0.984375\n",
      "Training iteration 295 loss: 0.00019299527048133314, ACC:1.0\n",
      "Training iteration 296 loss: 0.025073351338505745, ACC:0.984375\n",
      "Training iteration 297 loss: 2.655620664882008e-05, ACC:1.0\n",
      "Training iteration 298 loss: 0.00012887819320894778, ACC:1.0\n",
      "Training iteration 299 loss: 0.0062560830265283585, ACC:1.0\n",
      "Training iteration 300 loss: 0.00018521856691222638, ACC:1.0\n",
      "Training iteration 301 loss: 0.07668297737836838, ACC:0.984375\n",
      "Training iteration 302 loss: 0.08010707050561905, ACC:0.984375\n",
      "Training iteration 303 loss: 0.00037765727029182017, ACC:1.0\n",
      "Training iteration 304 loss: 0.029114821925759315, ACC:0.984375\n",
      "Training iteration 305 loss: 0.22390411794185638, ACC:0.953125\n",
      "Training iteration 306 loss: 0.04785120114684105, ACC:0.96875\n",
      "Training iteration 307 loss: 0.0002010637690545991, ACC:1.0\n",
      "Training iteration 308 loss: 6.665145338047296e-05, ACC:1.0\n",
      "Training iteration 309 loss: 0.0041997176595032215, ACC:1.0\n",
      "Training iteration 310 loss: 4.515894397627562e-05, ACC:1.0\n",
      "Training iteration 311 loss: 0.08396212756633759, ACC:0.984375\n",
      "Training iteration 312 loss: 0.0033507938496768475, ACC:1.0\n",
      "Training iteration 313 loss: 0.009522542357444763, ACC:1.0\n",
      "Training iteration 314 loss: 0.00018220837228000164, ACC:1.0\n",
      "Training iteration 315 loss: 0.005748997908085585, ACC:1.0\n",
      "Training iteration 316 loss: 0.004269040655344725, ACC:1.0\n",
      "Training iteration 317 loss: 0.041936244815588, ACC:0.96875\n",
      "Training iteration 318 loss: 0.042313992977142334, ACC:0.984375\n",
      "Training iteration 319 loss: 0.10944299399852753, ACC:0.984375\n",
      "Training iteration 320 loss: 0.012109500356018543, ACC:1.0\n",
      "Training iteration 321 loss: 0.025213560089468956, ACC:1.0\n",
      "Training iteration 322 loss: 0.009536126628518105, ACC:1.0\n",
      "Training iteration 323 loss: 0.034220095723867416, ACC:0.984375\n",
      "Training iteration 324 loss: 0.0011812207521870732, ACC:1.0\n",
      "Training iteration 325 loss: 0.015463754534721375, ACC:0.984375\n",
      "Training iteration 326 loss: 0.09771835058927536, ACC:0.96875\n",
      "Training iteration 327 loss: 0.03275720775127411, ACC:0.984375\n",
      "Training iteration 328 loss: 0.0019049779511988163, ACC:1.0\n",
      "Training iteration 329 loss: 0.019626373425126076, ACC:0.984375\n",
      "Training iteration 330 loss: 0.013937569223344326, ACC:1.0\n",
      "Training iteration 331 loss: 0.004802077077329159, ACC:1.0\n",
      "Training iteration 332 loss: 0.014724034816026688, ACC:0.984375\n",
      "Training iteration 333 loss: 0.0015764336567372084, ACC:1.0\n",
      "Training iteration 334 loss: 0.0015946169150993228, ACC:1.0\n",
      "Training iteration 335 loss: 0.0758545845746994, ACC:0.984375\n",
      "Training iteration 336 loss: 0.004461070988327265, ACC:1.0\n",
      "Training iteration 337 loss: 0.002493348903954029, ACC:1.0\n",
      "Training iteration 338 loss: 0.0002776931505650282, ACC:1.0\n",
      "Training iteration 339 loss: 0.00240508490242064, ACC:1.0\n",
      "Training iteration 340 loss: 0.0014233930269256234, ACC:1.0\n",
      "Training iteration 341 loss: 0.027822773903608322, ACC:0.984375\n",
      "Training iteration 342 loss: 0.001451463787816465, ACC:1.0\n",
      "Training iteration 343 loss: 0.0009138704044744372, ACC:1.0\n",
      "Training iteration 344 loss: 0.0006871312507428229, ACC:1.0\n",
      "Training iteration 345 loss: 0.0009072251850739121, ACC:1.0\n",
      "Training iteration 346 loss: 0.0026283676270395517, ACC:1.0\n",
      "Training iteration 347 loss: 0.0031774407252669334, ACC:1.0\n",
      "Training iteration 348 loss: 0.1241857036948204, ACC:0.984375\n",
      "Training iteration 349 loss: 0.0015977700240910053, ACC:1.0\n",
      "Training iteration 350 loss: 0.017794229090213776, ACC:0.984375\n",
      "Training iteration 351 loss: 0.0018749847076833248, ACC:1.0\n",
      "Training iteration 352 loss: 1.4208930224413052e-05, ACC:1.0\n",
      "Training iteration 353 loss: 4.536561027634889e-05, ACC:1.0\n",
      "Training iteration 354 loss: 0.0340605191886425, ACC:0.984375\n",
      "Training iteration 355 loss: 9.651140862843022e-05, ACC:1.0\n",
      "Training iteration 356 loss: 0.00013110219151712954, ACC:1.0\n",
      "Training iteration 357 loss: 0.11671631783246994, ACC:0.96875\n",
      "Training iteration 358 loss: 4.7498615458607674e-05, ACC:1.0\n",
      "Training iteration 359 loss: 3.7906480429228395e-05, ACC:1.0\n",
      "Training iteration 360 loss: 0.00045688304817304015, ACC:1.0\n",
      "Training iteration 361 loss: 0.18494419753551483, ACC:0.96875\n",
      "Training iteration 362 loss: 0.0003552210982888937, ACC:1.0\n",
      "Training iteration 363 loss: 0.19673068821430206, ACC:0.96875\n",
      "Training iteration 364 loss: 0.00850940216332674, ACC:1.0\n",
      "Training iteration 365 loss: 0.03634083271026611, ACC:0.984375\n",
      "Training iteration 366 loss: 0.0053477901965379715, ACC:1.0\n",
      "Training iteration 367 loss: 0.050999343395233154, ACC:0.96875\n",
      "Training iteration 368 loss: 0.0005208927323110402, ACC:1.0\n",
      "Training iteration 369 loss: 0.02812090702354908, ACC:0.984375\n",
      "Training iteration 370 loss: 0.07609035074710846, ACC:0.984375\n",
      "Training iteration 371 loss: 6.0553938965313137e-05, ACC:1.0\n",
      "Training iteration 372 loss: 0.0006139583420008421, ACC:1.0\n",
      "Training iteration 373 loss: 0.002853803103789687, ACC:1.0\n",
      "Training iteration 374 loss: 0.0007463350775651634, ACC:1.0\n",
      "Training iteration 375 loss: 0.00016756060358602554, ACC:1.0\n",
      "Training iteration 376 loss: 0.025221651419997215, ACC:0.984375\n",
      "Training iteration 377 loss: 0.007107525132596493, ACC:1.0\n",
      "Training iteration 378 loss: 0.0017794002778828144, ACC:1.0\n",
      "Training iteration 379 loss: 0.0008871901081874967, ACC:1.0\n",
      "Training iteration 380 loss: 0.03223538026213646, ACC:0.984375\n",
      "Training iteration 381 loss: 0.00046144891530275345, ACC:1.0\n",
      "Training iteration 382 loss: 0.0007988958386704326, ACC:1.0\n",
      "Training iteration 383 loss: 0.0008166384068317711, ACC:1.0\n",
      "Training iteration 384 loss: 0.025972040370106697, ACC:0.984375\n",
      "Training iteration 385 loss: 0.004292617551982403, ACC:1.0\n",
      "Training iteration 386 loss: 0.008878606371581554, ACC:1.0\n",
      "Training iteration 387 loss: 0.002680764300748706, ACC:1.0\n",
      "Training iteration 388 loss: 0.00726154912263155, ACC:1.0\n",
      "Training iteration 389 loss: 0.06077605113387108, ACC:0.984375\n",
      "Training iteration 390 loss: 0.018735909834504128, ACC:1.0\n",
      "Training iteration 391 loss: 0.0004693258961196989, ACC:1.0\n",
      "Training iteration 392 loss: 0.13297119736671448, ACC:0.984375\n",
      "Training iteration 393 loss: 0.002353447489440441, ACC:1.0\n",
      "Training iteration 394 loss: 0.0076895589008927345, ACC:1.0\n",
      "Training iteration 395 loss: 0.00016405204951297492, ACC:1.0\n",
      "Training iteration 396 loss: 6.859060522401705e-05, ACC:1.0\n",
      "Training iteration 397 loss: 0.016536355018615723, ACC:0.984375\n",
      "Training iteration 398 loss: 0.030696088448166847, ACC:0.984375\n",
      "Training iteration 399 loss: 0.03469798341393471, ACC:0.984375\n",
      "Training iteration 400 loss: 0.0007935784524306655, ACC:1.0\n",
      "Training iteration 401 loss: 0.0361606702208519, ACC:0.984375\n",
      "Training iteration 402 loss: 0.049723800271749496, ACC:0.96875\n",
      "Training iteration 403 loss: 0.0008718896424397826, ACC:1.0\n",
      "Training iteration 404 loss: 0.0002957932010758668, ACC:1.0\n",
      "Training iteration 405 loss: 0.056299395859241486, ACC:0.984375\n",
      "Training iteration 406 loss: 0.00848812609910965, ACC:1.0\n",
      "Training iteration 407 loss: 0.039036281406879425, ACC:0.984375\n",
      "Training iteration 408 loss: 0.05015456676483154, ACC:0.984375\n",
      "Training iteration 409 loss: 0.04702969267964363, ACC:0.984375\n",
      "Training iteration 410 loss: 0.017310544848442078, ACC:0.984375\n",
      "Training iteration 411 loss: 8.562336006434634e-05, ACC:1.0\n",
      "Training iteration 412 loss: 0.00024492101510986686, ACC:1.0\n",
      "Training iteration 413 loss: 0.009261607192456722, ACC:1.0\n",
      "Training iteration 414 loss: 0.012685621157288551, ACC:1.0\n",
      "Training iteration 415 loss: 0.002642168663442135, ACC:1.0\n",
      "Training iteration 416 loss: 0.00035583641147240996, ACC:1.0\n",
      "Training iteration 417 loss: 0.0002810928563121706, ACC:1.0\n",
      "Training iteration 418 loss: 0.1291825771331787, ACC:0.984375\n",
      "Training iteration 419 loss: 0.004672884475439787, ACC:1.0\n",
      "Training iteration 420 loss: 0.0022751225624233484, ACC:1.0\n",
      "Training iteration 421 loss: 0.03953839838504791, ACC:0.96875\n",
      "Training iteration 422 loss: 0.04067279398441315, ACC:0.984375\n",
      "Training iteration 423 loss: 0.03958885744214058, ACC:0.96875\n",
      "Training iteration 424 loss: 0.009587941691279411, ACC:1.0\n",
      "Training iteration 425 loss: 0.056440360844135284, ACC:0.984375\n",
      "Training iteration 426 loss: 0.0005939223919995129, ACC:1.0\n",
      "Training iteration 427 loss: 0.00013252907956484705, ACC:1.0\n",
      "Training iteration 428 loss: 0.0031531495042145252, ACC:1.0\n",
      "Training iteration 429 loss: 0.00729000847786665, ACC:1.0\n",
      "Training iteration 430 loss: 0.18151164054870605, ACC:0.984375\n",
      "Training iteration 431 loss: 5.284562212182209e-05, ACC:1.0\n",
      "Training iteration 432 loss: 0.14672328531742096, ACC:0.96875\n",
      "Training iteration 433 loss: 0.1782231479883194, ACC:0.953125\n",
      "Training iteration 434 loss: 6.704204861307517e-05, ACC:1.0\n",
      "Training iteration 435 loss: 0.028809772804379463, ACC:0.984375\n",
      "Training iteration 436 loss: 7.73493229644373e-05, ACC:1.0\n",
      "Training iteration 437 loss: 0.08713182806968689, ACC:0.984375\n",
      "Training iteration 438 loss: 0.09224524348974228, ACC:0.984375\n",
      "Training iteration 439 loss: 0.00379531504586339, ACC:1.0\n",
      "Training iteration 440 loss: 0.008013137616217136, ACC:1.0\n",
      "Training iteration 441 loss: 0.0021090295631438494, ACC:1.0\n",
      "Training iteration 442 loss: 0.03378700837492943, ACC:0.984375\n",
      "Training iteration 443 loss: 0.0014335901942104101, ACC:1.0\n",
      "Training iteration 444 loss: 0.0009544561617076397, ACC:1.0\n",
      "Training iteration 445 loss: 0.003984595183283091, ACC:1.0\n",
      "Training iteration 446 loss: 0.02547522820532322, ACC:0.984375\n",
      "Training iteration 447 loss: 0.009466933086514473, ACC:1.0\n",
      "Training iteration 448 loss: 0.004499694332480431, ACC:1.0\n",
      "Training iteration 449 loss: 0.002792450599372387, ACC:1.0\n",
      "Training iteration 450 loss: 0.0031858154106885195, ACC:1.0\n",
      "Validation iteration 451 loss: 0.2873135805130005, ACC: 0.984375\n",
      "Validation iteration 452 loss: 0.0016857782611623406, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.0003428697236813605, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.0014468325534835458, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.1034078299999237, ACC: 0.96875\n",
      "Validation iteration 456 loss: 0.0005549368797801435, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.01302487961947918, ACC: 0.984375\n",
      "Validation iteration 458 loss: 0.030602263286709785, ACC: 0.984375\n",
      "Validation iteration 459 loss: 0.012861374765634537, ACC: 0.984375\n",
      "Validation iteration 460 loss: 0.00601155823096633, ACC: 1.0\n",
      "Validation iteration 461 loss: 0.000575772428419441, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.0011406325502321124, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.000504205992911011, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.0011397870257496834, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.0007235581870190799, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.004141998942941427, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.007893349044024944, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.011798272840678692, ACC: 0.984375\n",
      "Validation iteration 469 loss: 0.0002624023472890258, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.08261575549840927, ACC: 0.984375\n",
      "Validation iteration 471 loss: 8.940794941736385e-05, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.08389376848936081, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.15069837868213654, ACC: 0.96875\n",
      "Validation iteration 474 loss: 0.0003739375388249755, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.00048157377750612795, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.002242515329271555, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.004724063910543919, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.11959826946258545, ACC: 0.984375\n",
      "Validation iteration 479 loss: 9.376765228807926e-05, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.0021890823263674974, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.012156669050455093, ACC: 0.984375\n",
      "Validation iteration 482 loss: 0.002614694181829691, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.020196791738271713, ACC: 0.984375\n",
      "Validation iteration 484 loss: 0.00012181886268081143, ACC: 1.0\n",
      "Validation iteration 485 loss: 0.006723177619278431, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.5456593632698059, ACC: 0.984375\n",
      "Validation iteration 487 loss: 0.002367291832342744, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.0013241150882095098, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.00027102886815555394, ACC: 1.0\n",
      "Validation iteration 490 loss: 0.0747191533446312, ACC: 0.984375\n",
      "Validation iteration 491 loss: 0.001538387150503695, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.02813643403351307, ACC: 0.984375\n",
      "Validation iteration 493 loss: 0.0006589801632799208, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.0003242047387175262, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.00010143787221750244, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.07906296849250793, ACC: 0.984375\n",
      "Validation iteration 497 loss: 0.05148615688085556, ACC: 0.96875\n",
      "Validation iteration 498 loss: 0.08567991107702255, ACC: 0.984375\n",
      "Validation iteration 499 loss: 0.008114946074783802, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.005109291989356279, ACC: 1.0\n",
      "-- Epoch 9 done -- Train loss: 0.02856658899339689, train ACC: 0.9928472222222222, val loss: 0.03717598452276434, val ACC: 0.9934375\n",
      "<--- 16542.932386636734 seconds --->\n",
      "Training iteration 1 loss: 0.20472091436386108, ACC:0.984375\n",
      "Training iteration 2 loss: 0.0005249478854238987, ACC:1.0\n",
      "Training iteration 3 loss: 0.00046075458521954715, ACC:1.0\n",
      "Training iteration 4 loss: 0.00010868690878851339, ACC:1.0\n",
      "Training iteration 5 loss: 0.027174491435289383, ACC:0.984375\n",
      "Training iteration 6 loss: 0.001863220240920782, ACC:1.0\n",
      "Training iteration 7 loss: 0.0632241889834404, ACC:0.984375\n",
      "Training iteration 8 loss: 0.004068214911967516, ACC:1.0\n",
      "Training iteration 9 loss: 0.11338746547698975, ACC:0.96875\n",
      "Training iteration 10 loss: 0.05853687971830368, ACC:0.984375\n",
      "Training iteration 11 loss: 0.008024473674595356, ACC:1.0\n",
      "Training iteration 12 loss: 0.0005751028656959534, ACC:1.0\n",
      "Training iteration 13 loss: 0.0046882000751793385, ACC:1.0\n",
      "Training iteration 14 loss: 0.0006173048168420792, ACC:1.0\n",
      "Training iteration 15 loss: 0.07285283505916595, ACC:0.984375\n",
      "Training iteration 16 loss: 0.0035568655002862215, ACC:1.0\n",
      "Training iteration 17 loss: 0.11297588795423508, ACC:0.96875\n",
      "Training iteration 18 loss: 0.008700384758412838, ACC:1.0\n",
      "Training iteration 19 loss: 0.08349289745092392, ACC:0.984375\n",
      "Training iteration 20 loss: 0.0702030211687088, ACC:0.96875\n",
      "Training iteration 21 loss: 0.012963838875293732, ACC:1.0\n",
      "Training iteration 22 loss: 0.001298021525144577, ACC:1.0\n",
      "Training iteration 23 loss: 0.0021692654117941856, ACC:1.0\n",
      "Training iteration 24 loss: 0.08437532931566238, ACC:0.984375\n",
      "Training iteration 25 loss: 0.004164776764810085, ACC:1.0\n",
      "Training iteration 26 loss: 0.004439417272806168, ACC:1.0\n",
      "Training iteration 27 loss: 0.006332693621516228, ACC:1.0\n",
      "Training iteration 28 loss: 0.013304187916219234, ACC:1.0\n",
      "Training iteration 29 loss: 0.019911859184503555, ACC:0.984375\n",
      "Training iteration 30 loss: 0.05169422924518585, ACC:0.984375\n",
      "Training iteration 31 loss: 0.00031342756119556725, ACC:1.0\n",
      "Training iteration 32 loss: 0.1935158669948578, ACC:0.96875\n",
      "Training iteration 33 loss: 0.0005695032887160778, ACC:1.0\n",
      "Training iteration 34 loss: 0.009423837065696716, ACC:1.0\n",
      "Training iteration 35 loss: 0.04166422039270401, ACC:0.984375\n",
      "Training iteration 36 loss: 0.005694391205906868, ACC:1.0\n",
      "Training iteration 37 loss: 0.015490586869418621, ACC:0.984375\n",
      "Training iteration 38 loss: 0.003098702058196068, ACC:1.0\n",
      "Training iteration 39 loss: 0.0038722376339137554, ACC:1.0\n",
      "Training iteration 40 loss: 0.012995362281799316, ACC:1.0\n",
      "Training iteration 41 loss: 0.014091433957219124, ACC:1.0\n",
      "Training iteration 42 loss: 0.08998560160398483, ACC:0.96875\n",
      "Training iteration 43 loss: 0.0019439855823293328, ACC:1.0\n",
      "Training iteration 44 loss: 0.015425220131874084, ACC:1.0\n",
      "Training iteration 45 loss: 0.1783849447965622, ACC:0.984375\n",
      "Training iteration 46 loss: 0.032890185713768005, ACC:0.984375\n",
      "Training iteration 47 loss: 0.0013675556983798742, ACC:1.0\n",
      "Training iteration 48 loss: 0.005363912787288427, ACC:1.0\n",
      "Training iteration 49 loss: 0.002897677244618535, ACC:1.0\n",
      "Training iteration 50 loss: 0.001972225494682789, ACC:1.0\n",
      "Training iteration 51 loss: 0.002799518406391144, ACC:1.0\n",
      "Training iteration 52 loss: 0.0005418080254457891, ACC:1.0\n",
      "Training iteration 53 loss: 0.02367342635989189, ACC:0.984375\n",
      "Training iteration 54 loss: 0.002270639408379793, ACC:1.0\n",
      "Training iteration 55 loss: 0.00018630706472322345, ACC:1.0\n",
      "Training iteration 56 loss: 0.0004236970853526145, ACC:1.0\n",
      "Training iteration 57 loss: 0.065943643450737, ACC:0.984375\n",
      "Training iteration 58 loss: 0.0010019292822107673, ACC:1.0\n",
      "Training iteration 59 loss: 0.017022136598825455, ACC:0.984375\n",
      "Training iteration 60 loss: 0.0004462585784494877, ACC:1.0\n",
      "Training iteration 61 loss: 0.001980341738089919, ACC:1.0\n",
      "Training iteration 62 loss: 0.00601333798840642, ACC:1.0\n",
      "Training iteration 63 loss: 0.04061150923371315, ACC:0.984375\n",
      "Training iteration 64 loss: 0.005757355596870184, ACC:1.0\n",
      "Training iteration 65 loss: 0.00849368516355753, ACC:1.0\n",
      "Training iteration 66 loss: 0.03737182170152664, ACC:0.984375\n",
      "Training iteration 67 loss: 0.00015614740550518036, ACC:1.0\n",
      "Training iteration 68 loss: 0.021160414442420006, ACC:0.984375\n",
      "Training iteration 69 loss: 0.1914321631193161, ACC:0.984375\n",
      "Training iteration 70 loss: 0.1345127373933792, ACC:0.984375\n",
      "Training iteration 71 loss: 0.020441705361008644, ACC:0.984375\n",
      "Training iteration 72 loss: 0.0025776419788599014, ACC:1.0\n",
      "Training iteration 73 loss: 0.07657894492149353, ACC:0.96875\n",
      "Training iteration 74 loss: 0.0006727005238644779, ACC:1.0\n",
      "Training iteration 75 loss: 0.00023766931553836912, ACC:1.0\n",
      "Training iteration 76 loss: 0.000789411598816514, ACC:1.0\n",
      "Training iteration 77 loss: 0.04613944888114929, ACC:0.984375\n",
      "Training iteration 78 loss: 0.0005037625087425113, ACC:1.0\n",
      "Training iteration 79 loss: 0.001071867998689413, ACC:1.0\n",
      "Training iteration 80 loss: 0.0053055682219564915, ACC:1.0\n",
      "Training iteration 81 loss: 0.004322810564190149, ACC:1.0\n",
      "Training iteration 82 loss: 0.0032717627473175526, ACC:1.0\n",
      "Training iteration 83 loss: 0.002280246466398239, ACC:1.0\n",
      "Training iteration 84 loss: 0.0030039341654628515, ACC:1.0\n",
      "Training iteration 85 loss: 0.006069515366107225, ACC:1.0\n",
      "Training iteration 86 loss: 0.002734661102294922, ACC:1.0\n",
      "Training iteration 87 loss: 0.0020135845988988876, ACC:1.0\n",
      "Training iteration 88 loss: 0.002178344177082181, ACC:1.0\n",
      "Training iteration 89 loss: 0.004217094741761684, ACC:1.0\n",
      "Training iteration 90 loss: 0.009512204676866531, ACC:1.0\n",
      "Training iteration 91 loss: 0.0003215146716684103, ACC:1.0\n",
      "Training iteration 92 loss: 0.00025423808256164193, ACC:1.0\n",
      "Training iteration 93 loss: 0.0005153226666152477, ACC:1.0\n",
      "Training iteration 94 loss: 0.016608992591500282, ACC:0.984375\n",
      "Training iteration 95 loss: 0.0003540879406500608, ACC:1.0\n",
      "Training iteration 96 loss: 0.00018937193090096116, ACC:1.0\n",
      "Training iteration 97 loss: 0.0001880166819319129, ACC:1.0\n",
      "Training iteration 98 loss: 0.04280390217900276, ACC:0.984375\n",
      "Training iteration 99 loss: 0.002687342930585146, ACC:1.0\n",
      "Training iteration 100 loss: 0.00022874002752359957, ACC:1.0\n",
      "Training iteration 101 loss: 6.258462235564366e-05, ACC:1.0\n",
      "Training iteration 102 loss: 0.0007861341582611203, ACC:1.0\n",
      "Training iteration 103 loss: 0.07112617790699005, ACC:0.984375\n",
      "Training iteration 104 loss: 0.002044165274128318, ACC:1.0\n",
      "Training iteration 105 loss: 8.444459672318771e-05, ACC:1.0\n",
      "Training iteration 106 loss: 6.532736733788624e-05, ACC:1.0\n",
      "Training iteration 107 loss: 3.211685179849155e-05, ACC:1.0\n",
      "Training iteration 108 loss: 0.00012133183918194845, ACC:1.0\n",
      "Training iteration 109 loss: 0.001492424518801272, ACC:1.0\n",
      "Training iteration 110 loss: 3.50486152456142e-05, ACC:1.0\n",
      "Training iteration 111 loss: 4.885520411335165e-06, ACC:1.0\n",
      "Training iteration 112 loss: 0.0021020222920924425, ACC:1.0\n",
      "Training iteration 113 loss: 0.0001516399352112785, ACC:1.0\n",
      "Training iteration 114 loss: 0.17498022317886353, ACC:0.96875\n",
      "Training iteration 115 loss: 0.13121002912521362, ACC:0.984375\n",
      "Training iteration 116 loss: 0.03021615743637085, ACC:0.96875\n",
      "Training iteration 117 loss: 0.05033249408006668, ACC:0.984375\n",
      "Training iteration 118 loss: 0.00015288898430299014, ACC:1.0\n",
      "Training iteration 119 loss: 0.05264389514923096, ACC:0.984375\n",
      "Training iteration 120 loss: 0.04671303555369377, ACC:0.984375\n",
      "Training iteration 121 loss: 0.009403827600181103, ACC:1.0\n",
      "Training iteration 122 loss: 0.009783292189240456, ACC:1.0\n",
      "Training iteration 123 loss: 0.004218084737658501, ACC:1.0\n",
      "Training iteration 124 loss: 0.036253247410058975, ACC:0.984375\n",
      "Training iteration 125 loss: 0.0002850788296200335, ACC:1.0\n",
      "Training iteration 126 loss: 0.0018866527825593948, ACC:1.0\n",
      "Training iteration 127 loss: 0.0011409864528104663, ACC:1.0\n",
      "Training iteration 128 loss: 0.001255782670341432, ACC:1.0\n",
      "Training iteration 129 loss: 0.012945713475346565, ACC:1.0\n",
      "Training iteration 130 loss: 0.00031209454755298793, ACC:1.0\n",
      "Training iteration 131 loss: 0.00020912772743031383, ACC:1.0\n",
      "Training iteration 132 loss: 0.00034791234065778553, ACC:1.0\n",
      "Training iteration 133 loss: 2.097834658343345e-05, ACC:1.0\n",
      "Training iteration 134 loss: 0.0008689939859323204, ACC:1.0\n",
      "Training iteration 135 loss: 0.0007991094025783241, ACC:1.0\n",
      "Training iteration 136 loss: 0.0026982296258211136, ACC:1.0\n",
      "Training iteration 137 loss: 5.2363466238602996e-05, ACC:1.0\n",
      "Training iteration 138 loss: 0.008848924189805984, ACC:1.0\n",
      "Training iteration 139 loss: 0.07964518666267395, ACC:0.984375\n",
      "Training iteration 140 loss: 0.01541074551641941, ACC:0.984375\n",
      "Training iteration 141 loss: 0.00022533029550686479, ACC:1.0\n",
      "Training iteration 142 loss: 0.0011256997240707278, ACC:1.0\n",
      "Training iteration 143 loss: 0.002008328214287758, ACC:1.0\n",
      "Training iteration 144 loss: 0.037217285484075546, ACC:0.984375\n",
      "Training iteration 145 loss: 0.005280866287648678, ACC:1.0\n",
      "Training iteration 146 loss: 0.005455686245113611, ACC:1.0\n",
      "Training iteration 147 loss: 0.0020322557538747787, ACC:1.0\n",
      "Training iteration 148 loss: 0.009024359285831451, ACC:1.0\n",
      "Training iteration 149 loss: 0.005878113675862551, ACC:1.0\n",
      "Training iteration 150 loss: 9.488705109106377e-05, ACC:1.0\n",
      "Training iteration 151 loss: 0.015397352166473866, ACC:0.984375\n",
      "Training iteration 152 loss: 0.13445410132408142, ACC:0.96875\n",
      "Training iteration 153 loss: 0.0014506777515634894, ACC:1.0\n",
      "Training iteration 154 loss: 1.0586553798930254e-05, ACC:1.0\n",
      "Training iteration 155 loss: 0.0003891384694725275, ACC:1.0\n",
      "Training iteration 156 loss: 0.11103668808937073, ACC:0.984375\n",
      "Training iteration 157 loss: 0.0002180522569688037, ACC:1.0\n",
      "Training iteration 158 loss: 0.00014494993956759572, ACC:1.0\n",
      "Training iteration 159 loss: 0.0011350675486028194, ACC:1.0\n",
      "Training iteration 160 loss: 0.029099516570568085, ACC:0.984375\n",
      "Training iteration 161 loss: 0.5911881327629089, ACC:0.984375\n",
      "Training iteration 162 loss: 5.712691563530825e-05, ACC:1.0\n",
      "Training iteration 163 loss: 0.00046475898125208914, ACC:1.0\n",
      "Training iteration 164 loss: 0.06198764964938164, ACC:0.984375\n",
      "Training iteration 165 loss: 1.359248399734497, ACC:0.84375\n",
      "Training iteration 166 loss: 0.00017421573284082115, ACC:1.0\n",
      "Training iteration 167 loss: 0.26475831866264343, ACC:0.921875\n",
      "Training iteration 168 loss: 11.383633613586426, ACC:0.453125\n",
      "Training iteration 169 loss: 0.7203106880187988, ACC:0.828125\n",
      "Training iteration 170 loss: 0.34301215410232544, ACC:0.875\n",
      "Training iteration 171 loss: 0.7205224633216858, ACC:0.84375\n",
      "Training iteration 172 loss: 0.3333762288093567, ACC:0.921875\n",
      "Training iteration 173 loss: 0.6008357405662537, ACC:0.890625\n",
      "Training iteration 174 loss: 0.7142714858055115, ACC:0.671875\n",
      "Training iteration 175 loss: 0.5359765291213989, ACC:0.65625\n",
      "Training iteration 176 loss: 0.4175684452056885, ACC:0.828125\n",
      "Training iteration 177 loss: 0.6356515288352966, ACC:0.78125\n",
      "Training iteration 178 loss: 0.4793781638145447, ACC:0.78125\n",
      "Training iteration 179 loss: 0.7833905220031738, ACC:0.640625\n",
      "Training iteration 180 loss: 0.43542617559432983, ACC:0.828125\n",
      "Training iteration 181 loss: 4.1795783042907715, ACC:0.484375\n",
      "Training iteration 182 loss: 0.9572095274925232, ACC:0.6875\n",
      "Training iteration 183 loss: 2.487114191055298, ACC:0.71875\n",
      "Training iteration 184 loss: 1.16115403175354, ACC:0.71875\n",
      "Training iteration 185 loss: 1.907162070274353, ACC:0.59375\n",
      "Training iteration 186 loss: 1.341395378112793, ACC:0.484375\n",
      "Training iteration 187 loss: 1.2121607065200806, ACC:0.5625\n",
      "Training iteration 188 loss: 1.1019012928009033, ACC:0.546875\n",
      "Training iteration 189 loss: 0.9420458674430847, ACC:0.5\n",
      "Training iteration 190 loss: 0.8116861581802368, ACC:0.515625\n",
      "Training iteration 191 loss: 1.119618535041809, ACC:0.46875\n",
      "Training iteration 192 loss: 0.9426864385604858, ACC:0.484375\n",
      "Training iteration 193 loss: 0.7646895051002502, ACC:0.625\n",
      "Training iteration 194 loss: 0.8637359142303467, ACC:0.5625\n",
      "Training iteration 195 loss: 0.5581029653549194, ACC:0.703125\n",
      "Training iteration 196 loss: 0.7394806742668152, ACC:0.625\n",
      "Training iteration 197 loss: 0.9023730754852295, ACC:0.5625\n",
      "Training iteration 198 loss: 0.781784176826477, ACC:0.5625\n",
      "Training iteration 199 loss: 1.0273131132125854, ACC:0.515625\n",
      "Training iteration 200 loss: 1.195818305015564, ACC:0.5\n",
      "Training iteration 201 loss: 0.700810968875885, ACC:0.609375\n",
      "Training iteration 202 loss: 0.8763172626495361, ACC:0.65625\n",
      "Training iteration 203 loss: 0.8053042888641357, ACC:0.53125\n",
      "Training iteration 204 loss: 0.7822667956352234, ACC:0.5\n",
      "Training iteration 205 loss: 0.8204004764556885, ACC:0.46875\n",
      "Training iteration 206 loss: 0.8355459570884705, ACC:0.59375\n",
      "Training iteration 207 loss: 0.9088490605354309, ACC:0.46875\n",
      "Training iteration 208 loss: 0.7282156944274902, ACC:0.453125\n",
      "Training iteration 209 loss: 0.6026149988174438, ACC:0.671875\n",
      "Training iteration 210 loss: 0.7094531655311584, ACC:0.609375\n",
      "Training iteration 211 loss: 0.8192510604858398, ACC:0.515625\n",
      "Training iteration 212 loss: 0.8841726183891296, ACC:0.578125\n",
      "Training iteration 213 loss: 0.8302648663520813, ACC:0.59375\n",
      "Training iteration 214 loss: 0.8560187220573425, ACC:0.546875\n",
      "Training iteration 215 loss: 0.8936079740524292, ACC:0.40625\n",
      "Training iteration 216 loss: 0.8692690134048462, ACC:0.515625\n",
      "Training iteration 217 loss: 0.9067533016204834, ACC:0.578125\n",
      "Training iteration 218 loss: 0.8393439054489136, ACC:0.5625\n",
      "Training iteration 219 loss: 0.9700203537940979, ACC:0.3125\n",
      "Training iteration 220 loss: 0.7932568192481995, ACC:0.546875\n",
      "Training iteration 221 loss: 0.8101814985275269, ACC:0.546875\n",
      "Training iteration 222 loss: 0.8933387398719788, ACC:0.421875\n",
      "Training iteration 223 loss: 0.8334868550300598, ACC:0.515625\n",
      "Training iteration 224 loss: 0.671934187412262, ACC:0.53125\n",
      "Training iteration 225 loss: 0.8089808225631714, ACC:0.4375\n",
      "Training iteration 226 loss: 0.8882285952568054, ACC:0.484375\n",
      "Training iteration 227 loss: 0.7448626160621643, ACC:0.5\n",
      "Training iteration 228 loss: 0.7235156893730164, ACC:0.5625\n",
      "Training iteration 229 loss: 0.8233514428138733, ACC:0.53125\n",
      "Training iteration 230 loss: 0.8188402652740479, ACC:0.484375\n",
      "Training iteration 231 loss: 0.7612272500991821, ACC:0.453125\n",
      "Training iteration 232 loss: 0.6930915117263794, ACC:0.578125\n",
      "Training iteration 233 loss: 0.732929527759552, ACC:0.546875\n",
      "Training iteration 234 loss: 0.6719512343406677, ACC:0.53125\n",
      "Training iteration 235 loss: 0.715390145778656, ACC:0.5625\n",
      "Training iteration 236 loss: 0.7451893091201782, ACC:0.5\n",
      "Training iteration 237 loss: 0.6140109896659851, ACC:0.671875\n",
      "Training iteration 238 loss: 0.8818676471710205, ACC:0.484375\n",
      "Training iteration 239 loss: 0.7694699168205261, ACC:0.46875\n",
      "Training iteration 240 loss: 0.7810633182525635, ACC:0.421875\n",
      "Training iteration 241 loss: 0.7747948169708252, ACC:0.546875\n",
      "Training iteration 242 loss: 0.8056420087814331, ACC:0.453125\n",
      "Training iteration 243 loss: 0.7362794876098633, ACC:0.515625\n",
      "Training iteration 244 loss: 0.7270917892456055, ACC:0.53125\n",
      "Training iteration 245 loss: 0.7810737490653992, ACC:0.46875\n",
      "Training iteration 246 loss: 0.774246871471405, ACC:0.46875\n",
      "Training iteration 247 loss: 0.7461130023002625, ACC:0.59375\n",
      "Training iteration 248 loss: 0.8963512182235718, ACC:0.484375\n",
      "Training iteration 249 loss: 0.7303870320320129, ACC:0.546875\n",
      "Training iteration 250 loss: 0.8101850748062134, ACC:0.53125\n",
      "Training iteration 251 loss: 0.7452941536903381, ACC:0.484375\n",
      "Training iteration 252 loss: 0.7994577884674072, ACC:0.421875\n",
      "Training iteration 253 loss: 0.8282397389411926, ACC:0.453125\n",
      "Training iteration 254 loss: 0.7151318192481995, ACC:0.53125\n",
      "Training iteration 255 loss: 0.7111204862594604, ACC:0.53125\n",
      "Training iteration 256 loss: 0.5768324732780457, ACC:0.703125\n",
      "Training iteration 257 loss: 0.7865240573883057, ACC:0.5\n",
      "Training iteration 258 loss: 0.6976926326751709, ACC:0.609375\n",
      "Training iteration 259 loss: 0.8512268662452698, ACC:0.53125\n",
      "Training iteration 260 loss: 0.7746561169624329, ACC:0.515625\n",
      "Training iteration 261 loss: 0.7519125938415527, ACC:0.5625\n",
      "Training iteration 262 loss: 0.7410921454429626, ACC:0.515625\n",
      "Training iteration 263 loss: 0.7880706787109375, ACC:0.453125\n",
      "Training iteration 264 loss: 0.7200675010681152, ACC:0.515625\n",
      "Training iteration 265 loss: 0.7354796528816223, ACC:0.5\n",
      "Training iteration 266 loss: 0.7015888094902039, ACC:0.640625\n",
      "Training iteration 267 loss: 0.7467274069786072, ACC:0.5\n",
      "Training iteration 268 loss: 0.7429516315460205, ACC:0.5\n",
      "Training iteration 269 loss: 0.7437047362327576, ACC:0.484375\n",
      "Training iteration 270 loss: 0.7293597459793091, ACC:0.5625\n",
      "Training iteration 271 loss: 0.5941863656044006, ACC:0.6875\n",
      "Training iteration 272 loss: 0.6856367588043213, ACC:0.546875\n",
      "Training iteration 273 loss: 0.6269097328186035, ACC:0.578125\n",
      "Training iteration 274 loss: 0.6046189665794373, ACC:0.671875\n",
      "Training iteration 275 loss: 0.649279773235321, ACC:0.6875\n",
      "Training iteration 276 loss: 0.5150293707847595, ACC:0.703125\n",
      "Training iteration 277 loss: 0.5447788834571838, ACC:0.765625\n",
      "Training iteration 278 loss: 0.5071064233779907, ACC:0.8125\n",
      "Training iteration 279 loss: 0.44900426268577576, ACC:0.78125\n",
      "Training iteration 280 loss: 0.6785584092140198, ACC:0.796875\n",
      "Training iteration 281 loss: 0.8202853798866272, ACC:0.65625\n",
      "Training iteration 282 loss: 0.9207799434661865, ACC:0.5625\n",
      "Training iteration 283 loss: 0.637047290802002, ACC:0.734375\n",
      "Training iteration 284 loss: 0.7999621629714966, ACC:0.515625\n",
      "Training iteration 285 loss: 0.5469698905944824, ACC:0.71875\n",
      "Training iteration 286 loss: 0.5733516812324524, ACC:0.71875\n",
      "Training iteration 287 loss: 0.6466178297996521, ACC:0.625\n",
      "Training iteration 288 loss: 0.75615394115448, ACC:0.546875\n",
      "Training iteration 289 loss: 0.6062929034233093, ACC:0.609375\n",
      "Training iteration 290 loss: 0.7497653961181641, ACC:0.46875\n",
      "Training iteration 291 loss: 0.49759969115257263, ACC:0.703125\n",
      "Training iteration 292 loss: 0.46665292978286743, ACC:0.734375\n",
      "Training iteration 293 loss: 0.37025174498558044, ACC:0.84375\n",
      "Training iteration 294 loss: 0.4052175283432007, ACC:0.875\n",
      "Training iteration 295 loss: 0.33783066272735596, ACC:0.859375\n",
      "Training iteration 296 loss: 0.20533178746700287, ACC:0.90625\n",
      "Training iteration 297 loss: 0.19691798090934753, ACC:0.953125\n",
      "Training iteration 298 loss: 0.8293760418891907, ACC:0.875\n",
      "Training iteration 299 loss: 0.8976640105247498, ACC:0.765625\n",
      "Training iteration 300 loss: 1.1091300249099731, ACC:0.703125\n",
      "Training iteration 301 loss: 0.6189793944358826, ACC:0.78125\n",
      "Training iteration 302 loss: 0.32582229375839233, ACC:0.921875\n",
      "Training iteration 303 loss: 0.5917502045631409, ACC:0.734375\n",
      "Training iteration 304 loss: 0.4869652986526489, ACC:0.828125\n",
      "Training iteration 305 loss: 0.28934693336486816, ACC:0.921875\n",
      "Training iteration 306 loss: 0.25066080689430237, ACC:0.875\n",
      "Training iteration 307 loss: 0.3866019546985626, ACC:0.84375\n",
      "Training iteration 308 loss: 0.25571736693382263, ACC:0.875\n",
      "Training iteration 309 loss: 0.23698462545871735, ACC:0.921875\n",
      "Training iteration 310 loss: 0.45372068881988525, ACC:0.796875\n",
      "Training iteration 311 loss: 0.3988654613494873, ACC:0.875\n",
      "Training iteration 312 loss: 0.5366071462631226, ACC:0.734375\n",
      "Training iteration 313 loss: 0.5659554600715637, ACC:0.75\n",
      "Training iteration 314 loss: 0.5349952578544617, ACC:0.734375\n",
      "Training iteration 315 loss: 0.49640709161758423, ACC:0.6875\n",
      "Training iteration 316 loss: 0.7024926543235779, ACC:0.59375\n",
      "Training iteration 317 loss: 0.4653676450252533, ACC:0.78125\n",
      "Training iteration 318 loss: 0.2885423004627228, ACC:0.90625\n",
      "Training iteration 319 loss: 0.3003697991371155, ACC:0.828125\n",
      "Training iteration 320 loss: 0.42634332180023193, ACC:0.8125\n",
      "Training iteration 321 loss: 0.5151636600494385, ACC:0.71875\n",
      "Training iteration 322 loss: 0.3678027391433716, ACC:0.828125\n",
      "Training iteration 323 loss: 0.43980586528778076, ACC:0.84375\n",
      "Training iteration 324 loss: 0.30673810839653015, ACC:0.859375\n",
      "Training iteration 325 loss: 0.22612665593624115, ACC:0.921875\n",
      "Training iteration 326 loss: 0.35145777463912964, ACC:0.890625\n",
      "Training iteration 327 loss: 0.18744370341300964, ACC:0.9375\n",
      "Training iteration 328 loss: 0.23165971040725708, ACC:0.875\n",
      "Training iteration 329 loss: 0.11402903497219086, ACC:0.96875\n",
      "Training iteration 330 loss: 0.4300469160079956, ACC:0.9375\n",
      "Training iteration 331 loss: 0.08948086947202682, ACC:0.96875\n",
      "Training iteration 332 loss: 0.15431977808475494, ACC:0.953125\n",
      "Training iteration 333 loss: 0.3337040841579437, ACC:0.875\n",
      "Training iteration 334 loss: 0.4285556375980377, ACC:0.859375\n",
      "Training iteration 335 loss: 0.329288125038147, ACC:0.859375\n",
      "Training iteration 336 loss: 0.2571577727794647, ACC:0.890625\n",
      "Training iteration 337 loss: 0.15171748399734497, ACC:0.953125\n",
      "Training iteration 338 loss: 0.2903251349925995, ACC:0.921875\n",
      "Training iteration 339 loss: 0.13136443495750427, ACC:0.953125\n",
      "Training iteration 340 loss: 0.09322784841060638, ACC:0.96875\n",
      "Training iteration 341 loss: 0.10179630666971207, ACC:0.984375\n",
      "Training iteration 342 loss: 0.03306896239519119, ACC:0.984375\n",
      "Training iteration 343 loss: 0.22410935163497925, ACC:0.96875\n",
      "Training iteration 344 loss: 0.07414816319942474, ACC:0.984375\n",
      "Training iteration 345 loss: 0.04704545438289642, ACC:0.984375\n",
      "Training iteration 346 loss: 0.04902130365371704, ACC:0.96875\n",
      "Training iteration 347 loss: 0.650619626045227, ACC:0.859375\n",
      "Training iteration 348 loss: 0.11420750617980957, ACC:0.96875\n",
      "Training iteration 349 loss: 0.402091920375824, ACC:0.875\n",
      "Training iteration 350 loss: 0.1301218420267105, ACC:0.953125\n",
      "Training iteration 351 loss: 0.07648283988237381, ACC:0.984375\n",
      "Training iteration 352 loss: 0.05894220247864723, ACC:0.984375\n",
      "Training iteration 353 loss: 0.09096500277519226, ACC:0.96875\n",
      "Training iteration 354 loss: 0.19286619126796722, ACC:0.953125\n",
      "Training iteration 355 loss: 0.30744799971580505, ACC:0.96875\n",
      "Training iteration 356 loss: 0.07864658534526825, ACC:0.984375\n",
      "Training iteration 357 loss: 0.10665902495384216, ACC:0.96875\n",
      "Training iteration 358 loss: 0.20817311108112335, ACC:0.984375\n",
      "Training iteration 359 loss: 0.0690954178571701, ACC:0.984375\n",
      "Training iteration 360 loss: 0.20345008373260498, ACC:0.9375\n",
      "Training iteration 361 loss: 0.17936311662197113, ACC:0.953125\n",
      "Training iteration 362 loss: 0.11686056107282639, ACC:0.96875\n",
      "Training iteration 363 loss: 0.16036461293697357, ACC:0.921875\n",
      "Training iteration 364 loss: 0.17230558395385742, ACC:0.9375\n",
      "Training iteration 365 loss: 0.16770830750465393, ACC:0.953125\n",
      "Training iteration 366 loss: 0.12507346272468567, ACC:0.953125\n",
      "Training iteration 367 loss: 0.06017748638987541, ACC:0.984375\n",
      "Training iteration 368 loss: 0.035546787083148956, ACC:1.0\n",
      "Training iteration 369 loss: 0.10590959340333939, ACC:0.953125\n",
      "Training iteration 370 loss: 0.04379851371049881, ACC:0.984375\n",
      "Training iteration 371 loss: 0.1298593133687973, ACC:0.96875\n",
      "Training iteration 372 loss: 0.02998240292072296, ACC:0.984375\n",
      "Training iteration 373 loss: 0.005763324908912182, ACC:1.0\n",
      "Training iteration 374 loss: 0.02768474817276001, ACC:0.984375\n",
      "Training iteration 375 loss: 0.007027134299278259, ACC:1.0\n",
      "Training iteration 376 loss: 0.046074867248535156, ACC:0.984375\n",
      "Training iteration 377 loss: 0.21831707656383514, ACC:0.984375\n",
      "Training iteration 378 loss: 0.879620373249054, ACC:0.953125\n",
      "Training iteration 379 loss: 0.08559241145849228, ACC:0.984375\n",
      "Training iteration 380 loss: 0.31852903962135315, ACC:0.921875\n",
      "Training iteration 381 loss: 0.21039016544818878, ACC:0.953125\n",
      "Training iteration 382 loss: 0.3016168177127838, ACC:0.859375\n",
      "Training iteration 383 loss: 0.1956232488155365, ACC:0.9375\n",
      "Training iteration 384 loss: 0.3175789415836334, ACC:0.859375\n",
      "Training iteration 385 loss: 0.5136851072311401, ACC:0.8125\n",
      "Training iteration 386 loss: 0.3790700435638428, ACC:0.828125\n",
      "Training iteration 387 loss: 0.38082629442214966, ACC:0.84375\n",
      "Training iteration 388 loss: 0.4966842532157898, ACC:0.796875\n",
      "Training iteration 389 loss: 0.3492647111415863, ACC:0.84375\n",
      "Training iteration 390 loss: 0.23705554008483887, ACC:0.875\n",
      "Training iteration 391 loss: 0.28539910912513733, ACC:0.890625\n",
      "Training iteration 392 loss: 0.37904590368270874, ACC:0.875\n",
      "Training iteration 393 loss: 0.2581251859664917, ACC:0.921875\n",
      "Training iteration 394 loss: 0.25923383235931396, ACC:0.890625\n",
      "Training iteration 395 loss: 0.20218028128147125, ACC:0.90625\n",
      "Training iteration 396 loss: 0.17139649391174316, ACC:0.953125\n",
      "Training iteration 397 loss: 0.08258484303951263, ACC:1.0\n",
      "Training iteration 398 loss: 0.08100194483995438, ACC:0.984375\n",
      "Training iteration 399 loss: 0.2110745757818222, ACC:0.921875\n",
      "Training iteration 400 loss: 0.18141458928585052, ACC:0.96875\n",
      "Training iteration 401 loss: 0.05510566383600235, ACC:0.984375\n",
      "Training iteration 402 loss: 0.19982899725437164, ACC:0.9375\n",
      "Training iteration 403 loss: 0.05154871568083763, ACC:1.0\n",
      "Training iteration 404 loss: 0.1430831104516983, ACC:0.953125\n",
      "Training iteration 405 loss: 0.1322510689496994, ACC:0.96875\n",
      "Training iteration 406 loss: 0.1910276710987091, ACC:0.921875\n",
      "Training iteration 407 loss: 0.008442483842372894, ACC:1.0\n",
      "Training iteration 408 loss: 0.06684447079896927, ACC:0.984375\n",
      "Training iteration 409 loss: 0.04598388820886612, ACC:0.984375\n",
      "Training iteration 410 loss: 0.13868048787117004, ACC:0.96875\n",
      "Training iteration 411 loss: 0.08617880195379257, ACC:0.953125\n",
      "Training iteration 412 loss: 0.05682928487658501, ACC:0.984375\n",
      "Training iteration 413 loss: 0.0509631410241127, ACC:0.984375\n",
      "Training iteration 414 loss: 0.045186176896095276, ACC:0.984375\n",
      "Training iteration 415 loss: 0.3126576542854309, ACC:0.921875\n",
      "Training iteration 416 loss: 0.10705476999282837, ACC:0.96875\n",
      "Training iteration 417 loss: 0.08552997559309006, ACC:0.984375\n",
      "Training iteration 418 loss: 0.06781814992427826, ACC:0.96875\n",
      "Training iteration 419 loss: 0.07567888498306274, ACC:0.984375\n",
      "Training iteration 420 loss: 0.016232572495937347, ACC:1.0\n",
      "Training iteration 421 loss: 0.11703772842884064, ACC:0.984375\n",
      "Training iteration 422 loss: 0.28581616282463074, ACC:0.9375\n",
      "Training iteration 423 loss: 0.016782283782958984, ACC:0.984375\n",
      "Training iteration 424 loss: 0.19307073950767517, ACC:0.96875\n",
      "Training iteration 425 loss: 0.11258497089147568, ACC:0.953125\n",
      "Training iteration 426 loss: 0.13302460312843323, ACC:0.96875\n",
      "Training iteration 427 loss: 0.015923097729682922, ACC:1.0\n",
      "Training iteration 428 loss: 0.09548109769821167, ACC:0.953125\n",
      "Training iteration 429 loss: 0.18940022587776184, ACC:0.953125\n",
      "Training iteration 430 loss: 0.041175298392772675, ACC:1.0\n",
      "Training iteration 431 loss: 0.19725826382637024, ACC:0.9375\n",
      "Training iteration 432 loss: 0.044144049286842346, ACC:0.96875\n",
      "Training iteration 433 loss: 0.018660474568605423, ACC:1.0\n",
      "Training iteration 434 loss: 0.03976687043905258, ACC:0.984375\n",
      "Training iteration 435 loss: 0.05401844158768654, ACC:0.984375\n",
      "Training iteration 436 loss: 0.02420957386493683, ACC:1.0\n",
      "Training iteration 437 loss: 0.07871086150407791, ACC:0.953125\n",
      "Training iteration 438 loss: 0.04299786314368248, ACC:0.984375\n",
      "Training iteration 439 loss: 0.11922615021467209, ACC:0.96875\n",
      "Training iteration 440 loss: 0.03651221841573715, ACC:1.0\n",
      "Training iteration 441 loss: 0.02081390842795372, ACC:1.0\n",
      "Training iteration 442 loss: 0.012766432948410511, ACC:1.0\n",
      "Training iteration 443 loss: 0.04847617447376251, ACC:0.984375\n",
      "Training iteration 444 loss: 0.01886921003460884, ACC:0.984375\n",
      "Training iteration 445 loss: 0.03523235023021698, ACC:0.984375\n",
      "Training iteration 446 loss: 0.009611246176064014, ACC:1.0\n",
      "Training iteration 447 loss: 0.0684417337179184, ACC:0.96875\n",
      "Training iteration 448 loss: 0.17371854186058044, ACC:0.953125\n",
      "Training iteration 449 loss: 0.006323644891381264, ACC:1.0\n",
      "Training iteration 450 loss: 0.21827971935272217, ACC:0.953125\n",
      "Validation iteration 451 loss: 0.016213135793805122, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.025854915380477905, ACC: 0.984375\n",
      "Validation iteration 453 loss: 0.04899657517671585, ACC: 0.984375\n",
      "Validation iteration 454 loss: 0.10927220433950424, ACC: 0.984375\n",
      "Validation iteration 455 loss: 0.030709464102983475, ACC: 0.984375\n",
      "Validation iteration 456 loss: 0.054553624242544174, ACC: 0.984375\n",
      "Validation iteration 457 loss: 0.03350471332669258, ACC: 0.984375\n",
      "Validation iteration 458 loss: 0.09006180614233017, ACC: 0.984375\n",
      "Validation iteration 459 loss: 0.01308506540954113, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.020629774779081345, ACC: 1.0\n",
      "Validation iteration 461 loss: 0.017871402204036713, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.012782484292984009, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.02114684507250786, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.013190893456339836, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.13180343806743622, ACC: 0.96875\n",
      "Validation iteration 466 loss: 0.01995077170431614, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.015752917155623436, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.03765176981687546, ACC: 0.984375\n",
      "Validation iteration 469 loss: 0.27414387464523315, ACC: 0.90625\n",
      "Validation iteration 470 loss: 0.01281803473830223, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.2137361466884613, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.09151345491409302, ACC: 0.96875\n",
      "Validation iteration 473 loss: 0.11186910420656204, ACC: 0.96875\n",
      "Validation iteration 474 loss: 0.005591873079538345, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.0930570736527443, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.020120520144701004, ACC: 0.984375\n",
      "Validation iteration 477 loss: 0.012176674790680408, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.39400285482406616, ACC: 0.953125\n",
      "Validation iteration 479 loss: 0.10697319358587265, ACC: 0.96875\n",
      "Validation iteration 480 loss: 0.09651561826467514, ACC: 0.984375\n",
      "Validation iteration 481 loss: 0.041384074836969376, ACC: 0.984375\n",
      "Validation iteration 482 loss: 0.03971905633807182, ACC: 0.984375\n",
      "Validation iteration 483 loss: 0.19112469255924225, ACC: 0.984375\n",
      "Validation iteration 484 loss: 0.034200962632894516, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.12719060480594635, ACC: 0.96875\n",
      "Validation iteration 486 loss: 0.015535646118223667, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.04074352979660034, ACC: 0.984375\n",
      "Validation iteration 488 loss: 0.20093126595020294, ACC: 0.953125\n",
      "Validation iteration 489 loss: 0.11376630514860153, ACC: 0.984375\n",
      "Validation iteration 490 loss: 0.012974454089999199, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.011788851581513882, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.06668642163276672, ACC: 0.984375\n",
      "Validation iteration 493 loss: 0.009418463334441185, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.038547202944755554, ACC: 0.984375\n",
      "Validation iteration 495 loss: 0.009098921902477741, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.06858446449041367, ACC: 0.984375\n",
      "Validation iteration 497 loss: 0.1211453452706337, ACC: 0.96875\n",
      "Validation iteration 498 loss: 0.09748916327953339, ACC: 0.984375\n",
      "Validation iteration 499 loss: 0.4147365093231201, ACC: 0.9375\n",
      "Validation iteration 500 loss: 0.13914495706558228, ACC: 0.953125\n",
      "-- Epoch 10 done -- Train loss: 0.33726467236424873, train ACC: 0.8551041666666667, val loss: 0.07879522234201432, val ACC: 0.9834375\n",
      "<--- 17924.024205207825 seconds --->\n"
     ]
    }
   ],
   "source": [
    "# for timing model training purposes\n",
    "start_time = time.time()\n",
    "\n",
    "# input variables\n",
    "epoch_num = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# train model\n",
    "model = OLeNet()\n",
    "cost_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# to store loss for training & validation set\n",
    "jw_train_epoch = []  ## to store loss for training set by epoch (average loss of iterations)\n",
    "jw_val_epoch = []    ## to store loss for validation set by epoch (average loss of iterations)\n",
    "\n",
    "# to store accuracy of training & validation set\n",
    "acc_train_epoch = []\n",
    "acc_val_epoch = []\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "\n",
    "    # track train & validation loss & accuracy by iteration for each epoch\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    test_counter = 1\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ''' include below IF statement if want to limit number of batches '''\n",
    "        if i+1 == 501:  # should have total of 500 batches after train & val sets\n",
    "            break\n",
    "        \n",
    "        # use 80% for training, 20% for testing, and 10% for validation of the 40k training samples\n",
    "        ## batch size=64 so 625 batches total: 450 train batches, 50 val batches, and 125 test batches\n",
    "\n",
    "        if i+1 > 450:  # validate model with validation set (10% of total train data)\n",
    "            inputs, labels = data\n",
    "            \n",
    "            logits, outputs = model(inputs)\n",
    "            cost = cost_fn(logits, labels)\n",
    "            \n",
    "            jw_val = float(cost)\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "            correct = pred == labels\n",
    "            acc = sum(np.array(correct))/len(np.array(correct))\n",
    "\n",
    "            acc_val = acc\n",
    "\n",
    "            val_loss.append(jw_val)\n",
    "            val_acc.append(acc_val)\n",
    "            \n",
    "            print(f'Validation iteration {i+1} loss: {jw_val}, ACC: {acc_val}')\n",
    "            \n",
    "        else:  # train model with training set\n",
    "\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()            # zero the parameter gradients\n",
    "            logits, outputs = model(inputs)  # forward\n",
    "            cost = cost_fn(logits, labels)   # input logits prior to softmax activation into cost function\n",
    "            cost.backward()                  # backward\n",
    "            optimizer.step()                 # optimize\n",
    "\n",
    "            jw_train = float(cost)\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred = torch.Tensor.argmax(outputs, dim=1)            # get labels of prediction with highest probability\n",
    "            correct = pred == labels                              # compare to actual labels and see which was predicted correctly\n",
    "\n",
    "            acc = sum(np.array(correct))/len(np.array(correct))   # calculate accuracy\n",
    "\n",
    "            acc_train = acc\n",
    "\n",
    "            train_loss.append(jw_train)\n",
    "            train_acc.append(acc_train)\n",
    "            \n",
    "            print(f'Training iteration {i+1} loss: {jw_train}, ACC:{acc_train}')\n",
    "\n",
    "    # to save time, epoch loss = the lowest loss, epoch acc = highest acc in training\n",
    "    epoch_jw = np.mean(np.array(train_loss))\n",
    "    epoch_acc = np.mean(np.array(train_acc))\n",
    "\n",
    "    jw_train_epoch.append(epoch_jw)\n",
    "    jw_val_epoch.append(np.mean(val_loss))\n",
    "    acc_train_epoch.append(epoch_acc)\n",
    "    acc_val_epoch.append(np.mean(val_acc))\n",
    "    \n",
    "    print(f'-- Epoch {epoch+1} done -- Train loss: {epoch_jw}, train ACC: {epoch_acc}, val loss: {np.mean(val_loss)}, val ACC: {np.mean(val_acc)}')\n",
    "    \n",
    "    print(\"<--- %s seconds --->\" % (time.time() - start_time))\n",
    "\n",
    "    # save model at every epoch\n",
    "    path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/OLeNet_model_saves/01_selu_lr0.001/olenet_lr0.001_cpu_epoch{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwdS7oqJweUo"
   },
   "source": [
    "Plot loss and accuracy over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1636947922694,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "hZPUGYuZYBLH",
    "outputId": "2596b8f8-a62c-4f15-f0e9-77282375feb6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU5fXA8e/JTja2BEQCJiC7CMEIVUSxaotLcVdQW6m21tbdqlVr1dran63WWltstdbS1oW6l1bUqnXBrYLsgUQWQQIEkrBkyL6c3x/3JtzECWSb3JnkfJ5nnrn7nJnAPfe+73vfV1QVY4wxprkovwMwxhgTnixBGGOMCcoShDHGmKAsQRhjjAnKEoQxxpigLEEYY4wJyhKEiRgi8qqIXHqA9fNE5OetPFamiKiIxHRehK0jIreLyONd/bnGtJUlCNNIRDaJyMl+x9ESVT1VVf8KICJzROR9v2NqD1X9hap+x+84AETkbhF5soPHuEFECkWkVESeEJH4A2x7kojkiUi5iLwtIod51sW7+5e6x7vRsy5ORJ53/42qiEzvSMymdSxBGNOJ/LgjaUlXxCIiXwduBU4CDgOGAT9tYds04EXgJ0A/YAnwD88mdwMj3OOcCNwiIjM8698HLgEKO/VLmJapqr3shaoCbAJODrI8HngI2Oa+HgLi3XVpwL+BPcAuYBEQ5a77EbAVCAD5wElBjp3l7tuwz5+AnZ71fweud6ffAb4DjAEqgTpgH7DHXT8PmAu84n7m/4DhLXzXTECBGHe+N/BnYLsb88+BaHfdcOC/QAlQDDwF9Gn2u/0IWAlUAYe7x74U+MLd58ee7e8GnmwWR0vb9gL+CuwG1gK3AAUH+BsqcBWwDvjcXfZbYAtQCnwKTHOXzwCqgRr3d1xxsN8iyOc9DfzCM38SUNjCtlcAH3rmk4AKYLQ7vw34mmf9z4D5QY5TAEz3+/9LT3jZHYRpjR8DXwEmAhOAycAd7rof4vyHTQcGArcDKiKjgKuBo1U1Bfg6zom0CVX9HOfEle0uOh7YJyJj3PkTgHeb7bMWuBL4SFWTVbWPZ/UsnCvYvsB64N5Wfsd5QC3OyT0b+BpOMgIQ4P+AQ3GS0xCck7zXbOB0oI97HIDjgFE4J807Pd8pmJa2vQsniQwDTsG5gj6Ys4ApwFh3fjHO364fzgn9ORFJUNXXgF8A/3B/xwnu9vNo+bdobhywwjO/AhgoIv0Ptq2qlgEbgHEi0hcYFORY41rxfU2IWIIwrXExcI+q7lTVIpwT8DfddTU4/7EPU9UaVV2kzmVeHc6dx1gRiVXVTaq6oYXjvwucICKHuPPPu/NZQCpNTxoH85KqfqKqtThX+hMPtoOIDAROw7lTKVPVncBvcJINqrpeVd9Q1Sr3+z+Ik7i8HlbVLapa4Vn2U1WtUNUV7neYQMta2vYCnCv03apaADx8sO8D/J+q7mqIRVWfVNUSVa1V1V/j/F1Gtee3CCIZ2OuZb5hOacW2DdunuOvgy8cKdhzTRcKmvNSEtUOBzZ75ze4ygPtxrqb/IyIAj6nqfaq6XkSud9eNE5HXgRtVdVuQ478LzMS5E3kPpyjpmzjFSItUtb4NsXrLp8vZf+I5kMOAWGC7+x3AuXjaAo0nzd8C03BOWFE4RT5eWzoYS0vbHtrs2ME+p7km24jITcDl7rEUJ+mmtbDvAX+LIPa5x2vQMB1oxbYN2wfcdQ3zlc3WGZ/YHYRpjW04J44GQ91lqGpAVX+oqsNwTvI3ishJ7rqnVfU4d18FftnC8d/FOflOd6ffB6YSpHjJozO7Id6CU3eQpqp93FeqqjYUb/zC/bzxqpqKU8wjzY4Rqm6RtwMZnvkhrdinMRYRmYZTb3EB0NctjtvL/vibx32w36K5XJreGU0AdqhqycG2FZEknPqdXFXdjfNdmx8r94Df1ISUJQjTXKyIJHheMcAzwB0iku62RLkTeBJARM4QkcPFudzci1O0VC8io0Tkq26Tx0qcysigdwKqus5dfwnwrqqWAjuAc2k5QewAMkQkrqNfWFW3A/8Bfi0iqSISJSLDRaShGCkF5wp3r4gMBm7u6Ge2wbPAbSLS1/3sq9u4fwpOfUIRECMid9L0Kn4HkCkiUdCq36K5vwGXi8hYEemDUzc1r4VtXwKOEJFzRSQB59/RSlXN8xzrDve7jga+6z2W2ww2wZ2Nc/99Nk/UphNZgjDNLcQ5WTe87sZpxbIEp5XOKmCpuwycZolv4pxAPwIeUdW3ccq578NplVMIDABuO8DnvguUqOoWz7y4nxXMf3GuLgtFpLitXzKIbwFxwBqc4qPncepWwKlzmYSTAF/BaarZVe7BKXr7HOd3fh7nCr+1XgdeAz7DKRqspGlx0XPue4mINPzWB/otmnArun8FvI3TCmszTsU6ACKSKyIXu9sW4ST9e93jTqFp3cZdOJXWm3H+/ve7x2+Qj/NvcrD7vSpoemdrOpk49YnGmEggIt8HZqlqS1f0xnQau4MwJoyJyCARmeoW9YzCaVb8kt9xmZ7BWjEZE97igEfZ/0DhfOARXyMyPYYVMRljjAkqpEVMIjJDRPJFZL2I3Bpk/ZUiskpElovI+yIy1l2eKSIV7vLlIvLHUMZpjDHmy0J2ByEi0TgtJ07BaYWxGJitqms826S6TRoRkZnAD1R1hohkAv9W1SNa+3lpaWmamZnZeV/AGGN6gE8//bRYVdODrQtlHcRkYL2qbgQQkfnAmThN5wBoSA6uJDrwsFFmZiZLlixp7+7GGNMjicjmltaFsohpME3bWxe4y5oQkatEZANOW+prPauyRGSZiLzrPg36JSJyhYgsEZElRUVFnRm7Mcb0eL43c1XVuao6HKe75IYeQrcDQ1U1G7gReFpEmvfhgqo+pqo5qpqTnh70DskYY0w7hTJBbKVpvzEZ7rKWzMfpphi318wSd/pTnKcrR4YoTmOMMUGEsg5iMTDC7bJ5K84j9Rd5NxCREW4/POD0pb/OXZ4O7FLVOhEZhtOdw8a2BlBTU0NBQQGVlZUH39i0SkJCAhkZGcTGxvodijEmxEKWIFS1VkSuxukzJRp4QlVzReQeYImqLgCuFmcM5BqcvlkaBqQ/HrhHRGpwOni7UlV3tTWGgoICUlJSyMzMxPr06jhVpaSkhIKCArKysvwOxxgTYiF9klpVF+J0/uZddqdn+roW9nsBeKGjn19ZWWnJoROJCP3798caBBjTM/heSR1qlhw6l/2exvQc3T5BGGNMd/b8pwXM/+SLkBzbEkQIlZSUMHHiRCZOnMghhxzC4MGDG+erq6sPuO+SJUu49tprD7gNwLHHHttZ4RpjItCTH2/mpWUHaiDaftabawj179+f5cuXA3D33XeTnJzMTTfd1Li+traWmJjgf4KcnBxycnIO+hkffvhh5wRrjIk49fXKZzsCnH9UxsE3bge7g+hic+bM4corr2TKlCnccsstfPLJJxxzzDFkZ2dz7LHHkp+fD8A777zDGWecATjJ5bLLLmP69OkMGzaMhx9+uPF4ycnJjdtPnz6d8847j9GjR3PxxRfT0M/WwoULGT16NEcddRTXXntt43GNMZGtYHcF5dV1jDrkS88Rd4oecwfx03/lsmZb6cE3bIOxh6Zy1zdaGsu9ZQUFBXz44YdER0dTWlrKokWLiImJ4c033+T222/nhRe+3IArLy+Pt99+m0AgwKhRo/j+97//pWcRli1bRm5uLoceeihTp07lgw8+ICcnh+9973u89957ZGVlMXv27HZ/X2NMeMkrdM5pow5JCcnxe0yCCCfnn38+0dHRAOzdu5dLL72UdevWISLU1NQE3ef0008nPj6e+Ph4BgwYwI4dO8jIaHpbOXny5MZlEydOZNOmTSQnJzNs2LDG5xZmz57NY489FsJvZ4zpKvmFAcASRIe150o/VJKSkhqnf/KTn3DiiSfy0ksvsWnTJqZPnx50n/j4+Mbp6Ohoamtr27WNMab7yNsRYEi/XiTHh+ZUbnUQPtu7dy+DBzud3M6bN6/Tjz9q1Cg2btzIpk2bAPjHP/7R6Z9hjPFHfmGAUQNDU/8AliB8d8stt3DbbbeRnZ0dkiv+Xr168cgjjzBjxgyOOuooUlJS6N27d6d/jjGma1XW1PF5cRmjQ1S8BN1oTOqcnBxtPmDQ2rVrGTNmjE8RhY99+/aRnJyMqnLVVVcxYsQIbrjhhnYfz35XY/y3eutezvjd+/xudjbfmHBou48jIp+qatA29XYH0QP86U9/YuLEiYwbN469e/fyve99z++QjDEd1FBBHco7iB5TSd2T3XDDDR26YzDGhJ/8HQHioqPITEs6+MbtZHcQxhgTgfIKAwwfkExsdOhO45YgjDEmAuUXloa0eAksQRhjTMTZU17NjtIqSxDGGGOaygvxE9QNLEGE2Iknnsjrr7/eZNlDDz3E97///aDbT58+nYbmuqeddhp79uz50jZ33303DzzwwAE/9+WXX2bNmjWN83feeSdvvvlmW8M3xoSh/S2YQveQHFiCCLnZs2czf/78Jsvmz5/fqk7zFi5cSJ8+fdr1uc0TxD333MPJJ5/crmMZY8JLXmGA3r1iGZgaf/CNO8ASRIidd955vPLKK40DBG3atIlt27bxzDPPkJOTw7hx47jrrruC7puZmUlxcTEA9957LyNHjuS4445r7BIcnGccjj76aCZMmMC5555LeXk5H374IQsWLODmm29m4sSJbNiwgTlz5vD8888D8NZbb5Gdnc348eO57LLLqKqqavy8u+66i0mTJjF+/Hjy8vJC+dMYY9opr7CUUYekhHwI4J7zHMSrt0Lhqs495iHj4dT7DrhJv379mDx5Mq+++ipnnnkm8+fP54ILLuD222+nX79+1NXVcdJJJ7Fy5UqOPPLIoMf49NNPmT9/PsuXL6e2tpZJkyZx1FFHAXDOOefw3e9+F4A77riDP//5z1xzzTXMnDmTM844g/POO6/JsSorK5kzZw5vvfUWI0eO5Fvf+hZ/+MMfuP766wFIS0tj6dKlPPLIIzzwwAM8/vjjHf2VjDGdqL5e+awwwLkhGiTIy+4guoC3mKmheOnZZ59l0qRJZGdnk5ub26Q4qLlFixZx9tlnk5iYSGpqKjNnzmxct3r1aqZNm8b48eN56qmnyM3NPWAs+fn5ZGVlMXLkSAAuvfRS3nvvvcb155xzDgBHHXVUYwd/xpjwsXVPBWXVdSGvoIYQ30GIyAzgt0A08Liq3tds/ZXAVUAdsA+4QlXXuOtuAy53112rqk1retvqIFf6oXTmmWdyww03sHTpUsrLy+nXrx8PPPAAixcvpm/fvsyZM4fKysp2HXvOnDm8/PLLTJgwgXnz5vHOO+90KNaGLsOtu3BjwlNeF3Sx0SBkdxAiEg3MBU4FxgKzRWRss82eVtXxqjoR+BXwoLvvWGAWMA6YATziHi8iJScnc+KJJ3LZZZcxe/ZsSktLSUpKonfv3uzYsYNXX331gPsff/zxvPzyy1RUVBAIBPjXv/7VuC4QCDBo0CBqamp46qmnGpenpKQQCAS+dKxRo0axadMm1q9fD8Df//53TjjhhE76psaYUMt3R5EbOTCCEwQwGVivqhtVtRqYD5zp3UBVvWOAJgENXcueCcxX1SpV/RxY7x4vYs2ePZsVK1Ywe/ZsJkyYQHZ2NqNHj+aiiy5i6tSpB9x30qRJXHjhhUyYMIFTTz2Vo48+unHdz372M6ZMmcLUqVMZPXp04/JZs2Zx//33k52dzYYNGxqXJyQk8Je//IXzzz+f8ePHExUVxZVXXtn5X9gYExJ5hQEy+vYiJSH24Bt3UMi6+xaR84AZqvodd/6bwBRVvbrZdlcBNwJxwFdVdZ2I/B74WFWfdLf5M/Cqqj7fbN8rgCsAhg4detTmzZubxGDdUoeG/a7G+OeUB9/lsP6JPH7p0QffuBXCurtvVZ2rqsOBHwF3tHHfx1Q1R1Vz0tPTQxOgMcaEiaraOjYWl3VJBTWENkFsBYZ45jPcZS2ZD5zVzn2NMabb27CzjLp6ZVSIn6BuEMoEsRgYISJZIhKHU+m8wLuBiIzwzJ4OrHOnFwCzRCReRLKAEcAn7Qmiu4yYFy7s9zTGP3luBXVXtGCCEDZzVdVaEbkaeB2nmesTqporIvcAS1R1AXC1iJwM1AC7gUvdfXNF5FlgDVALXKWqdW2NISEhgZKSEvr37x/yJw57AlWlpKSEhIQEv0MxpkfKLwwQGy1khXCQIK+QPgehqguBhc2W3emZvu4A+94L3NuRz8/IyKCgoICioqKOHMZ4JCQkkJER+ic4jTFfllcYYHh6aAcJ8urWXW3ExsaSlZXldxjGGNMp8gsDfGVYvy77PN9bMRljjDm4veU1FJZWMnpQ11RQgyUIY4yJCA0V1F3VxBUsQRhjTETI39F1fTA1sARhjDERIK8wQGpCDIekdl0rQksQxhgTAfILA4w+JLVLm+xbgjDGmDCnquQXBrq0/gEsQRhjTNgr2F3BvqpaSxDGGGOayu/CQYK8LEEYY0yYa2jBNNIShDHGGK+8wgCD+/QitQsGCfKyBGGMMWEuv7C0y4uXwBKEMcaEteraejYWdd0gQV6WIIwxJoxtKNpHbb1agjDGGNPU/hZMXddJXwNLEMYYE8bWFpYSGy0MS++aQYK8LEEYY0wYy+/iQYK8LEEYY0wY86OLjQaWIIwxJkztLa9h+95KX+ofwBKEMcaELT/GgPCyBGGMMWEq34dR5LwsQRhjTJjKKwyQkhDDoN5dN0iQV0gThIjMEJF8EVkvIrcGWX+jiKwRkZUi8paIHOZZVyciy93XglDGaYwx4cgZJCilSwcJ8gpZghCRaGAucCowFpgtImObbbYMyFHVI4HngV951lWo6kT3NTNUcRpjTDjya5Agr1DeQUwG1qvqRlWtBuYDZ3o3UNW3VbXcnf0YyAhhPMYYEzG27qkgUFXLKJ9aMEFoE8RgYItnvsBd1pLLgVc98wkiskREPhaRs4LtICJXuNssKSoq6njExhgTJvwaJMgrxrdP9hCRS4Ac4ATP4sNUdauIDAP+KyKrVHWDdz9VfQx4DCAnJ0e7LGBjjAmxPDdBjBzYPYuYtgJDPPMZ7rImRORk4MfATFWtaliuqlvd943AO0B2CGM1xpiwku8OEtS7V9cOEuQVygSxGBghIlkiEgfMApq0RhKRbOBRnOSw07O8r4jEu9NpwFRgTQhjNcaYsOJ3BTWEMEGoai1wNfA6sBZ4VlVzReQeEWlolXQ/kAw816w56xhgiYisAN4G7lNVSxDGmB6huraeDUX7fE8QIa2DUNWFwMJmy+70TJ/cwn4fAuNDGZsxxoSrjcXOIEF+VlCDPUltjDFhp6EFk993EJYgjDEmzOQVBoiJEoalJfsahyUIY4wJM3nbSxmenkxcjL+naEsQxhgTZsKhBRNYgjDGmLCyt6KGbXsrLUEYY4xp6jN3kKAxgyxBGGOM8chrbMHkXyd9DSxBGGNMGMkvLCUlIYZDfRokyMsShDHGhJH8wgCjBvo3SJCXJQhjjAkTqkpemLRgAksQxhgTNrbtrSRQWet7FxsNLEEYY0yYyC8sBcKjghosQRhjTNhobMHk4yBBXpYgjDEmTOQXBji0dwK9E/0bJMjLEoQxxoSJcOlio4ElCGOMCQM1dQ2DBIVH/QNYgjDGmLCwsaiMmjr/BwnysgRhjDFhIK+xBZMlCGOMMR757iBBw9P9HSTIyxKEMcaEgbzCAMPSk3wfJMgrfCIxxpgezGnBFD4V1GAJwhhjfFdaWcPWPRVhVUENIU4QIjJDRPJFZL2I3Bpk/Y0iskZEVorIWyJymGfdpSKyzn1dGso4jTHGT5+5T1D3mAQhItHAXOBUYCwwW0TGNttsGZCjqkcCzwO/cvftB9wFTAEmA3eJSN9QxWqMMX7aP0hQD0kQOCf29aq6UVWrgfnAmd4NVPVtVS13Zz8GMtzprwNvqOouVd0NvAHMCGGsxhjjm/zCACnxMQzu08vvUJoIZYIYDGzxzBe4y1pyOfBqO/c1xpiIlV8YYOQh4TFIkFdYVFKLyCVADnB/G/e7QkSWiMiSoqKi0ARnjDEh5AwSVBp2xUsQ2gSxFRjimc9wlzUhIicDPwZmqmpVW/ZV1cdUNUdVc9LT0zstcGOM6SqFpZWUhtEgQV6hTBCLgREikiUiccAsYIF3AxHJBh7FSQ47PateB74mIn3dyumvucuMMaZbydseXmNAeMWE6sCqWisiV+Oc2KOBJ1Q1V0TuAZao6gKcIqVk4Dm37O0LVZ2pqrtE5Gc4SQbgHlXdFapYjTHGL3mNTVzD6yE5CGGCAFDVhcDCZsvu9EyffIB9nwCeCF10xhjjv/zCUgaF0SBBXq0qYhKRJBGJcqdHishMEQm/b2OMMREmL8wGCfJqbR3Ee0CCiAwG/gN8E5gXqqCMMaYn2D9IUGQnCHEfaDsHeERVzwfGhS4sY4zp/j4vDr9BgrxanSBE5BjgYuAVd1l0aEIyxpieobGLjYHhV0ENrU8Q1wO3AS+5LZGGAW+HLixjjOn+8gtLiY4Shg9I8juUoFrViklV3wXeBXArq4tV9dpQBmaMMd1dfmGAYWlJxMeEZ4FMa1sxPS0iqSKSBKwG1ojIzaENzRhjure128O3BRO0vohprKqWAmfhdKiXhdOSyRhjTDsEwnSQIK/WJohY97mHs4AFqloDaOjCMsaY7u2zHeH7BHWD1iaIR4FNQBLwnjvyW2mogjLGmO4uXAcJ8mptJfXDwMOeRZtF5MTQhGSMMd1ffmGA5PgYMvqG1yBBXq2tpO4tIg82jL0gIr/GuZswxhjTDnmFAUYOTA67QYK8WlvE9AQQAC5wX6XAX0IVlDHGdGeqSn5hgFFhXP8Are/NdbiqnuuZ/6mILA9FQMYY093tKK1ib0VNWLdggtbfQVSIyHENMyIyFagITUjGGNO95RU6bXzCuYIaWn8HcSXwNxHp7c7vBi4NTUjGGNO97R8kqBskCFVdAUwQkVR3vlRErgdWhjI4Y4zpjvILAxySmkCfxDi/QzmgNo1Jraql7hPVADeGIB5jjOn2wnmQIK82JYhmwrdtljHGhKmauno27NwX9sVL0LEEYV1tGGNMG20qLqO6rj4i7iAOWAchIgGCJwIBwvfxP2OMCVOR0MVGgwMmCFUN/29gjDERJL8wQHSUcPiAZL9DOaiOFDEdlIjMEJF8EVkvIrcGWX+8iCwVkVoROa/ZujoRWe6+FoQyTmOM6Sp5hQGywniQIK/WPgfRZiISDcwFTgEKgMUiskBV13g2+wKYA9wU5BAVqjoxVPEZY4wf8neUcmRGH7/DaJVQ3kFMBtar6kZVrQbmA2d6N1DVTaq6EqgPYRzGGBMW9lXVsmVXBaMHRkbpfSgTxGBgi2e+wF3WWgluz7Efi8hZnRuaMcZ0vfyGJ6gHhXcnfQ1CVsTUCQ5T1a0iMgz4r4isUtUN3g1E5ArgCoChQ4f6EaMxxrRafoR0sdEglHcQW4EhnvkMd1mrqOpW930j8A6QHWSbx1Q1R1Vz0tPTOxatMcaEWH5hKUlx0QzuExlPCYQyQSwGRohIlojEAbOAVrVGEpG+IhLvTqcBU4E1B97LGGPCW15hgJGHpBAVFRkdUYQsQahqLXA18DqwFnhWVXNF5B4RmQkgIkeLSAFwPvCoiOS6u48BlojICuBt4L5mrZ+MMSaiqCr5OwIRU7wEIa6DUNWFwMJmy+70TC/GKXpqvt+HwPhQxtbs88J62D9jTOTbGahiT3kNoyKkBROE+EG5SLCjtJILHv2IxZt2+R2KMaYb29/FRmS0YAJLEPSKi6YoUMXVTy+lZF+V3+EYY7qpfHcUuUgqYurxCSI1IZa5F09id3kN1/9jOXX11kmtMabz5W0PMDA1nr5J4T1IkFePTxAA4w7tzU9njmPRumLmvr3e73CMMd2QM0hQ5BQvgSWIRrOOHsLZ2YP5zZuf8cH6Yr/DMcZ0I7V19awvioxBgrwsQbhEhHvPPoLD05O5bv4ydpRW+h2SMaab2FRSRnVtfUS1YAJLEE0kxsXwyMWTKKuq45pnllFbZ30IGmM6LpIGCfKyBNHMiIEp/OKcI/jk8108+MZnfodjjOkGImmQIC9LEEGcnZ3B7MlDeOSdDbydt9PvcIwxES6vMEBm/0QSYsN/kCAvSxAtuOsb4xg7KJUbnl3O1j0VfodjjIlg+YUBRkdYCyawBNGihNhoHrl4ErV1ylVPLaW61uojjDFtV1ZVyxe7yiOu/gEsQRxQZloSvzrvSJZv2cN9r+b5HY4xJgLl74isMSC8LEEcxGnjBzHn2Eye+OBzXlu93e9wjDERZv8gQVbE1C3dftoYJgzpw83PrWRzSZnf4RhjIkh+YYDEuGgy+kbGIEFeliBaIS4mirkXZRMVJfzgqaVU1tT5HZIxJkLkFZYycmDkDBLkZQmilTL6JvLgBRPI3VbKPf+2sYuMMQenqm4LpsirfwBLEG1y0piBXHnCcJ7+3xe8vKzVw2sbY3qookAVu8trIrIFE1iCaLObvjaSyZn9uP2lVazfGfA7HGNMGIvULjYaWIJoo5joKH53UTa9YqP5wVNLKa+u9TskY0yYiuQWTGAJol0Gpibw21nZrNu5jzteXo2qDTJkjPmyvMIA6Snx9IugQYK8LEG003Ej0rjupBG8uHQrzy7Z4nc4xpgwlFdYGrEV1GAJokOu+eoIjjs8jTv/mcuabaV+h2OMCSO1dfWs2xl5gwR5WYLogOgo4aFZE+mTGMtVTy8lUFnjd0jGmDCxqaTcGSQoQusfIMQJQkRmiEi+iKwXkVuDrD9eRJaKSK2InNds3aUiss59XRrKODsiLTme382exBe7yrn1hVVWH2GMAbwV1HYH8SUiEg3MBU4FxgKzRWRss82+AOYATzfbtx9wFzAFmAzcJSJ9QxVrR03O6sfNXx/FK6u28/ePN/sdjjEmDOQXlhIlRNwgQV6hvIOYDKxX1Y2qWg3MB870bqCqm1R1JdC8L+2vA2+o6i5V3Q28AcwIYawddsW0YZw0egA/+/caVmzZ43c4xhif5RUGyExLirhBgrxCmSAGA97mPQXusk7bV0SuEJElInPJSfYAABk5SURBVLKkqKio3YF2hqgo4dcXTGBASgI/eGope8utPsKYnix/R+R2sdEgoiupVfUxVc1R1Zz09HS/w6FPYhxzL57EzkAlP3xuudVHGNNDlVe7gwQNjNwKaghtgtgKDPHMZ7jLQr2vryYO6cOPTxvDm2t38qdFG/0Oxxjjg8927EMVRg+yO4iWLAZGiEiWiMQBs4AFrdz3deBrItLXrZz+mrssIlx6bCanjT+EX76Wz5JNu/wOxxjTxfK2O89FWRFTC1S1Frga58S+FnhWVXNF5B4RmQkgIkeLSAFwPvCoiOS6++4CfoaTZBYD97jLIoKIcN+5RzKkby+ufnoZJfuq/A7JGNOF8txBgob0TfQ7lA6R7lJOnpOTo0uWLPE7jCZyt+3l7Ec+ZEpWP+Z9ezLREThgiDGm7WY/9jHlNXX886qpfodyUCLyqarmBFsX0ZXU4W7cob25Z+Y4Fq0rZu7b6/0OxxjTBVTVacE0MLKLl8ASRMhdePQQzskezG/e/IwP1hf7HY4xJsSK9lWxq6w6YseA8LIEEWIiws/PPoLD05O5bv4ydpZW+h2SMSaEukMXGw0sQXSBxLgYHrl4EmVVdVz9zDJq65o/OG6M6S7yI3wUOS9LEF1kxMAUfnHOEXzy+S4efOMzv8MxxoRIXmGAtOR4+ifH+x1Kh1mC6EJnZ2cwe/JQHnlnA2/n7fQ7HGNMCOQXBhjTlQ/IqUJdaLr2sQTRxe76xljGDkrlhmeXs3VPhd/hGGM6UV298tmOAKO6qgVTdTm8/AN48btOouhkliC6WEJsNI9cPInaOuXqp5dSXWv1EcZ0F5tKyqiqre+a+ofidfD4SbDiGUgbZQmiu8hMS+L+845k2Rd7uO/VPL/DMcZ0kv0tmELcSd/qF+Gx6RAohEtegBNvg6jOP51bgvDJqeMH8e2pmTzxwee8tnq73+EYYzpBXmGAKIERA0M0SFBtNbz6I3j+2zBgLFy5CA4/KTSfhSUIX9126hgmDOnDzc+tZHNJmd/hGGM6KL+wlMz+IRokaM8W+Mup8L8/wld+AHNegd4Znf85HpYgfBQXE8Xci7KJihJ+8NRSKmvq/A7JGNMB+YWB0NQ/rHsTHp0GRflwwd9gxv9BTFznf04zliB8ltE3kQcvmEDutlJ+9u81fodjjGmn8upaNu8q79wEUV8H/70XnjoPUgfDFe/A2DMPtleniemyTzItOmnMQK48YTh/fHcDUSL8+PQxET2OrTE90bqGQYI6q4J6XxG8cDl8/i5MvAROux/iurb7cEsQYeKmr42kXpXH3tvIp5t3M/fiSWSlJfkdljGmlTq1D6YvPobn5kDFbpj5e5j0zY4fsx2siClMxERHcftpY3hiTg7b91ZwxsOL+OfyiBhl1RgDrC0spVdsNEP7deAqXxU+/B385TSISYDL3/AtOYAliLDz1dEDWXjdNMYemsp185dz6wsrqai2ymtjwl1+YYCRA5OJau/AYJV74R+XwH/ugNGnwffehUFHdm6QbWQJIgwN6t2LZ777Fa46cTj/WLKFs+Z+wPqdAb/DMsYcQIdaMG1fCY+eAJ+9Bl+7Fy74OyT07twA28ESRJiKiY7i5q+P5q/fnkzxviq+8bsPeP7TAr/DMsYEURSooqSsmlFtraBWhaV/g8dPhtoq59mGY68GCY/hiS1BhLnjR6bz6nXTmDikDzc9t4Ibn11OWVWt32EZYzzaVUFdXQ7/vAoWXAOHHQPfew+GfiVEEbaPJYgIMCA1gSe/M4XrTx7BS8u2MvP377N2e6nfYRljXHmFzv/HVhcxFa937hqWPw0n/AgueRGS00MYYftYglCFT/7klAHWh2/PqtFRwvUnj+Sp70whUFnLWXM/4On/fYGGoAdHY0zb5BcGSEuOI601gwTlvuR2tLcdLnkeTrwdosLzuaeQJggRmSEi+SKyXkRuDbI+XkT+4a7/n4hkusszRaRCRJa7rz+GLMg9m2HhTc5j7A8c7rQ9/nQe7Po8ZB/ZEccOT2PhddOYnNWP219axbXzlxOoDM1gIcaY1snfETj4A3K11fDqrc45ZsBot6O9k7skvvYK2YNyIhINzAVOAQqAxSKyQFW9/UlcDuxW1cNFZBbwS+BCd90GVZ0Yqvga9c2EG9fC5+/BxnecV+5Lzro+h8GwE2DYdMg6AZLSQh5Oa6Qlx/PXb0/mj+9t4Nf/+YxVBXv4/UWTOGKw/60ejOlpGgYJunjKYS1vtLfASQwFi2HK9+GUe7qkL6WOCuWT1JOB9aq6EUBE5gNnAt4EcSZwtzv9PPB7ER+q71MPhQmznJeqMxDHxnecR9xz/+m0MgAYOH5/wjjsWIjz70nnqCjhB9MP5+jMflz7zDLOeeRDfnz6GL51zGH48RMa01NtLimjsuYAgwStfxNe+C7UVcP582Dc2V0aX0eEMkEMBrZ45guAKS1to6q1IrIX6O+uyxKRZUApcIeqLmr+ASJyBXAFwNChQzsnahFIH+m8plwBdbWwfQVsfNtJGp88Bh/9HqJiYchk585i2HQYPAmiYzsnhjY4OrMfC6+dxk3PreCuBbl8tKGEX553JL17dX0sxvRELbZgqq+Dd38J7/4KBoxxemFNG+FDhO0Xrn0xbQeGqmqJiBwFvCwi41S1SdMdVX0MeAwgJycnNLW10TGQcZTzOv4mp2nalo/d4qh34Z3/g3d+AXEpkDl1f3HUgDFd1pa5b1Icj1+aw5/f/5z7Xs3j9IcX8fuLJjFxSJ8u+fxuq7bauYvs1RcycvyOxoSpvMIAIjBigCdBlBU7He1tfAcmXASn/7rLO9rrDKFMEFuBIZ75DHdZsG0KRCQG6A2UqNM0pwpAVT8VkQ3ASGBJCONtnbhEGP5V5wVQvgs2Ldpff/HZa87y5IHu3cUJznufIS0dsVOICN+ZNoyjDuvL1U8v47w/fMitp47m8uOyrMipLVRh+3JYMR9WPQflJc7yrOOd5oiZx/kbnwk7+YUBMvsn0SvObYn0xcfw3LedfzszfwfZ3wybB9/aKpQJYjEwQkSycBLBLOCiZtssAC4FPgLOA/6rqioi6cAuVa0TkWHACGBjCGNtv8R+Tv/sDX207/nCubP4/F2nWGrVs87yfsOdu4thJ0DmNGe/EMge2peF107jlhdW8PNX1vLRhhIeOH8CfZPCv0LMV6Xbnb/V8megaC1Ex8Go05x6qV0b4YPfwrzTYeixcMItzt8yQv/Tm86VvyPAqIEpzsXFx4/AG3dC7yHwnTdg0AS/w+sQCWU7ehE5DXgIiAaeUNV7ReQeYImqLhCRBODvQDawC5ilqhtF5FzgHqAGqAfuUtV/HeizcnJydMkS/28wmlCFnWuchLHxHdj8AVTvAwQOnbi//mLoVyC2Vyd/tPK3jzZz7ytr6Z8cx+9mZ5OTGZqkFLFqKiDvFedhpY1vg9ZDxtEwYTYccY5TtOTddunf4P2HILDN2e6EHznNFC1R9FgV1XWMves1bjp+EFeVPghr/wWjz4Az50KvyCjiFZFPVTVoGWpIE0RXCssE0VxdDWz9dH/9RcEnUF8L0fEwdIqTLMbM7NSKrFUFe7n6maUU7K7gxlNG8v0Thre/t8nuQNUpAljxNOS+DFWlkJrhtmKbDWmHH3j/2ipY9iS8/xvYuwUOzYbjb4FRp1qi6IFWFuzh1rlP8Vz/R0kqK4BTfgrHhE9fSq1hCSJcVe2DzR+6xVHvwI7VzvKB42HcWU5zuP7DO/wxgcoabntxFf9euZ1pI9L4zYUTW/fEZ3eye5NTr7DiGWc6NgnGznSSQuY0iGrjM6O11bByPiz6tXO8gePhhJth9DfafiwTsRa/+FvGr/gZMcn9iblgntOnUoSxBBEpSrfBmn/C6heduwtwyjDHneMkjL6Z7T60qvLMJ1u4+1+59OkVy29nZXPM8P4H3zGSVZY6v+eKZ5ziPQSypjmtSsZ8A+KTO/4ZdbVOZfaiB6BkPaSPcVq7jTs7bLtPMAdQXQ7lxU4Fc3kJlJXsn25cvstppeTOf6hHMOWml4hOGeB39O1iCSIS7dkCa152nure+qmzbPBRzoln3NnQO6Ndh127vZSrnl7KpuIyrj1pBNd8dQTR3anIqb7OuRtb8Qys/TfUVjgNBCbOhiNnha41WX2d87d6734oyoP+I5xEccR5TlNp0/Xqap0hO5uc8IudE3yTE74nEdRWBD+WREGvfk5vCon9nUYmiWnM25DIy9Ff5+VrTuja79aJLEFEut2bnPLy3Bedh/YAMiY7Faljz4LUQW06XFlVLT95eTUvLtvKscP789CFExmQmtD5cXelonynsnnls04lckJvOOJc524hI6fryoTr62HtAidR7FgNfbNg2g+dOg4fHqSMeKpOA4HqfVAVgOqy/dNNTvrulX3DSb+sGCr3tHzcuBRI6u+e7PtDYpp70u/vSQKe5Ql9ghYd5vz8DU4cNYD7z4/c1kqWILqTkg3OlWruS26dhcDQY5xkMWYmpAxs9aGeW7KFO/+ZS1J8NL+5cCLTRoRfd8MHVL4LVj3v3C1sWwoSDSNOcU7GI0+FWB+TXn09fPaq8xTt9uXQZygcdwNMvBhiunH9T3091JQ5J/KqfVAd8Ey7r2DTX1pWtn9eD9LLclTsl67sG0/wSZ4Tf6Jnm074GxTvqyLn529yx+lj+M60YR0+nl8sQXRXRZ/tTxZFawFxHuQad7bzXEYrOhdctyPAVU8vZd3OfVw1/XCuP3kEMdFhXMlaWw3r33DuFj57HeprnAriibNh/PmQHGblwKqw7g2ny4WtSyB1MEy9HiZ9y98E1hY1lbAzF7Ythx25zpV540k84Dm5u4mBVp5TYno59UBx7ive+57kXOU3TidDfIpnOnX/iT8+xZdWQx+sL+bix//Hk5dP4bgR4dGRZ3tYgugJdq51EsXqF6FknXM1nTXNqeAe840DPphXUV3HT/+Vy/zFW5ic2Y/fzp7IoN6d+1xGhzQ83bz8GVj9vFOEkDQAjrzAuVs4ZLzfER6cqvOsxbu/gi8+guRDYOq1cNS3w6sLhtoq585023LnN9+2zPm3Ve+OYhjf27nwiEtyT9juCTw++SAndHd947bJvlXiqypVtfVU1dRTVVtHVW09lTVN36tq66is8bw3rt+/bM32vXy8cReLf3wy6SmRe1doCaInUXWu8nJfdJLF7s8hKsZ5xmLc2TD69KYPgHn8c/lWbn9xFdV19QxMTeCQ1AQG9nbem08PSI0nITbE/8Fberp54kUw/KROr/xVVUorayneV0VxoIrifdUU76siSpz+rvolxjnvSXH0SYwlPqYd318VNr3v3FFsWgRJ6XDsNZBzeee0qmqL2irn38q2ZW4yWO4mA3d8kYQ+zgOdh2bDoInOdJ/DQnK1XlevVNTUUV5dS0V1nTtd50xX11FeU0dltbO+YbrpCb31J/mq2o4NDBYbLSTERBMfG8URg3sz79uTO+lX8IcliJ5K1anUzn3JSRh7vnDKa4d/1amzGHWqU5nr8XlxGc8u2ULh3krnVeq8V9TUfenwfRNjnUTiJo5g030TY5v2BVVf57QsKStyKhK97+XN5ndtdJ9unuwUIY07u8Xk1pL6emVPRQ0l+6oo2uee9ANVThLYtz8JNCSE6rrWnzyS42Pol+QmjcTYLyWRvolx9E923vslxdG7V2zTFmObP4L3fgUb/uu0kDnmKph8BSS0ceD71mhIBg13BW1MBjV19VQ0nqTdk3dNnedk3vTEXlmzf7tKd713ecO+DdPVbTxpi0B8TBQJsdFN3uNjokmIbfoeHxNFfPPtYqMaT/IN7y3t6/2MuJio7tXqD0sQBpxksXWpkyhyX4bSAueK/PBTnBPvqBlOkUDQXZ0r6x2l+5PGjsb3CgJ7S6gpLSKqopj+lNJfSunPXvpJgAFRpQyK2UdaVCl9tJTkur1EEexkIG6lYrpThJGUBmkjYfwFX3q6ua5e2VVW7TnJV1EccOabJ4FdZdXU1n/533hMlNDfHSKy8ZUSR3qz+bTkeFRhd3k1u8qq2V1Wza5y972shl1lVewqr3Hnq9ldXk159ZeTKTgntT699ieSfm4iGVv/GdML5zG0ZBE1sakUH3EZtTlX0Kf/AJLjYw7Y2aL3xF1R41wpV1aWIzvXEL9zJb2KV5GyazWpgXVEq1NMVBGdyvbEUXyRMJJNcSNZF304BZpOpXv1XdHwqt4/XxfkNzwQEUiMjaZXXAy94qJIjI2hV1w0vWKjSYyLbjYd08LyhmnnGL3iYtxjOids64Syc1iCME3V1zsVpqtfdJ61CGyHmASnBdC4c5w+oqoDTtvwsqL9r/Jm8w3r64MPeVoVk8K+mL7skd6U1KdQWJfCluokdtSlsEtTKSGVYu1NiaYSndiX9N5JHNI7obF4Kz42ynO1vz8h7CqrJtj5Ki46yhkXOKXhJO9JACnOfEMC6N0rNmRdjlTW1HkSSg0lZVVuYqlplmCqG7erqVOOkI1cE/MyX49eQqn24q91X+dvehqS2J++iXEouj8JuAkhqr6akbKFI6M+5wjZyPiozxklW4gTJ0nt0SRW1WexWrNYVZ/FSh3GdhlAYmwM8bHRzok31jkRJ7gn34QY991d3rBNgvtKjGtY7p68G6f3L7cTeOSwBGFaVl/vjG+R+5JzZ1G2s+VtY5P2X903XumnO80HvfNJ6c7dQJAhFVWV0opap+jKcyfS5K6ktJLifdUAJMRGNbnKT09pdtXvSQipCQe+2g5Xqsq+qlp2l9Wwq7ya6q0rOXTF7xm8/T/URCXwUf+z+VfiOVRGJ5NV/wVZNes4rOozBlfkkV6+ofHOoDo2ldK+R1DW/wgq04+kbuAEovod1niFnuCewGPDuZWa6XKWIEzr1Nc5fUNtW+a0emo46Sem7W+50kWqa+uprqsnKS46Ik/6nWJnntOFx+oXnLojrffUGfRuWl8waKLTFUtP/a1Mu1mCMCaSFa+HxX9yigEtGZhOdqAEYZ3EGBPu0g6HU3/pdxSmB7LCSGOMMUFZgjDGGBOUJQhjjDFBWYIwxhgTlCUIY4wxQVmCMMYYE5QlCGOMMUFZgjDGGBNUt3mSWkSKgM1+x9FBaUCx30GEEfs9mrLfYz/7LZrqyO9xmKoGHW+42ySI7kBElrT0yHtPZL9HU/Z77Ge/RVOh+j2siMkYY0xQliCMMcYEZQkivDzmdwBhxn6Ppuz32M9+i6ZC8ntYHYQxxpig7A7CGGNMUJYgjDHGBGUJIgyIyBAReVtE1ohIrohc53dMfhORaBFZJiL/9jsWv4lIHxF5XkTyRGStiBzjd0x+EpEb3P8nq0XkGRFJ8DumriQiT4jIThFZ7VnWT0TeEJF17nvfzvgsSxDhoRb4oaqOBb4CXCUiY32OyW/XAWv9DiJM/BZ4TVVHAxPowb+LiAwGrgVyVPUIIBqY5W9UXW4eMKPZsluBt1R1BPCWO99hliDCgKpuV9Wl7nQA5wQw2N+o/CMiGcDpwON+x+I3EekNHA/8GUBVq1V1j79R+S4G6CUiMUAisM3neLqUqr4H7Gq2+Ezgr+70X4GzOuOzLEGEGRHJBLKB//kbia8eAm4B6v0OJAxkAUXAX9wit8dFJMnvoPyiqluBB4AvgO3AXlX9j79RhYWBqrrdnS4EBnbGQS1BhBERSQZeAK5X1VK/4/GDiJwB7FTVT/2OJUzEAJOAP6hqNlBGJxUfRCK3bP1MnMR5KJAkIpf4G1V4UefZhU55fsESRJgQkVic5PCUqr7odzw+mgrMFJFNwHzgqyLypL8h+aoAKFDVhjvK53ESRk91MvC5qhapag3wInCszzGFgx0iMgjAfd/ZGQe1BBEGRERwypjXquqDfsfjJ1W9TVUzVDUTp/Lxv6raY68QVbUQ2CIio9xFJwFrfAzJb18AXxGRRPf/zUn04Ep7jwXApe70pcA/O+OgliDCw1TgmzhXy8vd12l+B2XCxjXAUyKyEpgI/MLneHzj3kk9DywFVuGcw3pUtxsi8gzwETBKRApE5HLgPuAUEVmHc5d1X6d8lnW1YYwxJhi7gzDGGBOUJQhjjDFBWYIwxhgTlCUIY4wxQVmCMMYYE5QlCGPaQETqPE2Rl4tIpz3VLCKZ3h46jfFbjN8BGBNhKlR1ot9BGNMV7A7CmE4gIptE5FciskpEPhGRw93lmSLyXxFZKSJvichQd/lAEXlJRFa4r4buIqJF5E/ueAf/EZFevn0p0+NZgjCmbXo1K2K60LNur6qOB36P0yMtwO+Av6rqkcBTwMPu8oeBd1V1Ak7fSrnu8hHAXFUdB+wBzg3x9zGmRfYktTFtICL7VDU5yPJNwFdVdaPb8WKhqvYXkWJgkKrWuMu3q2qaiBQBGapa5TlGJvCGO+gLIvIjIFZVfx76b2bMl9kdhDGdR1uYbosqz3QdVk9ofGQJwpjOc6Hn/SN3+kP2D4l5MbDInX4L+D40jr/du6uCNKa17OrEmLbpJSLLPfOvqWpDU9e+bo+rVcBsd9k1OKPB3YwzMty33eXXAY+5PXHW4SSL7RgTRqwOwphO4NZB5Khqsd+xGNNZrIjJGGNMUHYHYYwxJii7gzDGGBOUJQhjjDFBWYIwxhgTlCUIY4wxQVmCMMYYE9T/A4XptbKxMqiSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss over epoch\n",
    "plt.plot([i+1 for i in range(len(jw_train_epoch))],jw_train_epoch)\n",
    "plt.plot([i+1 for i in range(len(jw_val_epoch))],jw_val_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Loss with learning rate {learning_rate}')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1636947922938,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "j8coUlN-YBLI",
    "outputId": "a359b205-273e-47be-fb6c-590fede51029"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracy over epoch\n",
    "plt.plot([i+1 for i in range(len(acc_train_epoch))],acc_train_epoch)\n",
    "plt.plot([i+1 for i in range(len(acc_val_epoch))],acc_val_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Accuracy with learning rate {learning_rate}')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJQvqXWnYBLI"
   },
   "source": [
    "**Calculate test accuracy for model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234815,
     "status": "ok",
     "timestamp": 1636948764231,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "EoO1l_KoBvaf",
    "outputId": "9400d418-333e-420f-c15a-dba85ced032a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLeNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=576, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      " Testing accuracy = 99.7%\n"
     ]
    }
   ],
   "source": [
    "# load saved model \n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/OLeNet_model_saves/01_selu_lr0.001/olenet_lr0.001_cpu_epoch{4}.pth\"\n",
    "\n",
    "model = OLeNet()\n",
    "model.load_state_dict(torch.load(path))\n",
    "print(model.eval())\n",
    "\n",
    "# or use model in current session\n",
    "# model = model\n",
    "\n",
    "# calculate test accuracy of model at a given epoch\n",
    "\n",
    "correct_counter = 0\n",
    "data_counter = 0\n",
    "\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    if i+1 == 626:\n",
    "        break\n",
    "    else:\n",
    "        if i+1 > 500:\n",
    "            inputs, labels = data\n",
    "            logits, outputs = model(inputs)\n",
    "\n",
    "            # calculate overall accuracy across all classes\n",
    "            pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "            correct = pred == labels\n",
    "\n",
    "            correct_cnt = sum(np.array(correct))\n",
    "            data_cnt = len(np.array(correct))\n",
    "\n",
    "            correct_counter += correct_cnt \n",
    "            data_counter += data_cnt\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "print(f'\\n Testing accuracy = {(correct_counter/data_counter)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20910,
     "status": "ok",
     "timestamp": 1636948431380,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "a62LgfqSQkEI",
    "outputId": "ebe5939f-72a4-4fa7-c47a-49d8bfbcafc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLeNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=576, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      " Testing accuracy = 92.2%\n"
     ]
    }
   ],
   "source": [
    "# test accuracy with 1000 augmented images to see if model can handle \"new\" inputs\n",
    "\n",
    "# load saved model \n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/OLeNet_model_saves/01_selu_lr0.001/olenet_lr0.001_cpu_epoch{4}.pth\"\n",
    "\n",
    "model = OLeNet()\n",
    "model.load_state_dict(torch.load(path))\n",
    "print(model.eval())\n",
    "\n",
    "# or use model in current session\n",
    "# model = model\n",
    "\n",
    "# calculate test accuracy of model at a given epoch\n",
    "\n",
    "testset_path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/test_sets/small_1000_set.pth\"\n",
    "inputs, labels = torch.load(testset_path)\n",
    "\n",
    "logits, outputs = model(inputs)\n",
    "\n",
    "# calculate overall accuracy across all classes\n",
    "pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "correct = pred == labels\n",
    "\n",
    "correct_cnt = sum(np.array(correct))\n",
    "data_cnt = len(np.array(correct))\n",
    "\n",
    "print(f'\\n Testing accuracy = {(correct_cnt/data_cnt)*100:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OLeNet_model_selu_noAugmentation_lr0.001.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
