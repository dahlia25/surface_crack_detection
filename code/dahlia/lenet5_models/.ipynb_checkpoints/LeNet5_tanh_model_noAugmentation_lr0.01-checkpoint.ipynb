{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLEh-LthYBKv"
   },
   "source": [
    "# Surface Crack Images - LeNet5 model\n",
    "This notebook contains the code training the LeNet5 model (without augmented data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEb3XLxjYBK4"
   },
   "source": [
    "**Load packages/modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27371,
     "status": "ok",
     "timestamp": 1636904545575,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "H-oXyKDyYEED",
    "outputId": "b0ba67e8-c3d7-4166-910c-6b0ecf03e13f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27219,
     "status": "ok",
     "timestamp": 1636904608647,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "9AKkpfhtYBK5",
    "outputId": "3530f74b-eb28-4260-8412-6b23431f76fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.10.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print(f'Torch version: {torch .__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3792,
     "status": "ok",
     "timestamp": 1636904612432,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "i_OfDGEmYBK8",
    "outputId": "59304a18-4242-4663-a8f6-99d9cf65aefa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "%pip install torchsummary\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysKmpc9cYBK8"
   },
   "source": [
    "**Load data into tensors first**<br>\n",
    "Then concatenate the tensors with original and augmented data into one for ease of model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 685,
     "status": "ok",
     "timestamp": 1636904789994,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "syPC_CMNYBK_"
   },
   "outputs": [],
   "source": [
    "# load data into tensors first\n",
    "data_dir = '/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/archive/original'\n",
    "# aug_data_dir = '/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/archive/augmented'\n",
    "batch_size = 64 \n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "data = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(data, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True, \n",
    "                                         pin_memory=True)\n",
    "\n",
    "# aug_dataloader = torch.utils.data.DataLoader(aug_data,\n",
    "#                                              batch_size=batch_size*9,  # multiply by 9 since augmented x9 images for each original\n",
    "#                                              shuffle=True,\n",
    "#                                              pin_memory=True)\n",
    "\n",
    "# images, labels = next(iter(dataloader))\n",
    "# aug_images, aug_labels = next(iter(aug_dataloader))\n",
    "\n",
    "# print(f'Original images shape: {images.shape}')\n",
    "# print(f'Augmented images shape: {aug_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9C4r55Yj-Pf"
   },
   "outputs": [],
   "source": [
    "# generate and save test set\n",
    "test_counter = 1\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    if i+1 > 500:\n",
    "        if test_counter == 1:\n",
    "            test_inputs, test_labels = data \n",
    "            test_counter += 1\n",
    "        else:\n",
    "            new_inputs, new_labels = data\n",
    "            test_inputs = torch.concat((test_inputs, new_inputs), 0)\n",
    "            test_labels = torch.concat((test_labels, new_labels), 0)\n",
    "            test_counter += 1\n",
    "\n",
    "        # print(f'Batch {i+1} for test set complete.')\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# print(f'Saving test inputs {test_inputs.shape}, test labels {test_labels.shape}')\n",
    "\n",
    "# save test set at the end of training\n",
    "testset = [test_inputs, test_labels]\n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_testset.pth\"\n",
    "torch.save(testset, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZaeGsiNYBLC"
   },
   "source": [
    "## Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1636904798554,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "-0gG2OoGYBLD"
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, padding=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=2, stride=2)       \n",
    "        self.conv3 = nn.Conv2d(16, 120, 5, padding=2, stride=2)\n",
    "\n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(120*49, 84)\n",
    "        self.fc2 = nn.Linear(84, 2)   # have two classes (has crack/no crack)\n",
    "        \n",
    "        # softmax layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # pooling layer\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2)  # Average pool 2x2\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = F.tanh(self.conv1(x)) # layer 1\n",
    "        x = self.pool(x)          # layer 2\n",
    "        \n",
    "        x = F.tanh(self.conv2(x)) # layer 3\n",
    "        x = self.pool(x)          # layer 4\n",
    "        \n",
    "        x = F.tanh(self.conv3(x)) # layer 5\n",
    "        \n",
    "        x = torch.flatten(x, 1)   # flatten\n",
    "        \n",
    "        x = F.tanh(self.fc1(x))   # reshape layer\n",
    "        logit = self.fc2(x)       # layer 6\n",
    "        \n",
    "        output = self.softmax(logit) # layer 7\n",
    "\n",
    "        return logit, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 586,
     "status": "ok",
     "timestamp": 1636904801710,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "0gbcU53BYBLE",
    "outputId": "8fb81373-6d19-441e-d218-44528a5aba86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 114, 114]             456\n",
      "         AvgPool2d-2            [-1, 6, 57, 57]               0\n",
      "            Conv2d-3           [-1, 16, 29, 29]           2,416\n",
      "         AvgPool2d-4           [-1, 16, 14, 14]               0\n",
      "            Conv2d-5            [-1, 120, 7, 7]          48,120\n",
      "            Linear-6                   [-1, 84]         494,004\n",
      "            Linear-7                    [-1, 2]             170\n",
      "           Softmax-8                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 545,166\n",
      "Trainable params: 545,166\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 0.92\n",
      "Params size (MB): 2.08\n",
      "Estimated Total Size (MB): 3.59\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "# print model summary\n",
    "\n",
    "model = LeNet5()\n",
    "\n",
    "channels = 3\n",
    "H = 227\n",
    "W = 227\n",
    "\n",
    "summary(model, (channels, H, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp89_6KnjowJ"
   },
   "source": [
    "### Find optimal learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiN5vfU2U_-j"
   },
   "source": [
    "**Train model with learning rates by starting with small number of batches**, so can confirm that architecture is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10204989,
     "status": "ok",
     "timestamp": 1636915078657,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "0acPeyXXsDdk",
    "outputId": "b4812644-f510-4985-c5fb-c892399f6a5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 1 loss: 0.6897343993186951, ACC:0.578125\n",
      "Training iteration 2 loss: 2.830310583114624, ACC:0.5\n",
      "Training iteration 3 loss: 2.485419988632202, ACC:0.40625\n",
      "Training iteration 4 loss: 1.560442566871643, ACC:0.515625\n",
      "Training iteration 5 loss: 1.0268293619155884, ACC:0.4375\n",
      "Training iteration 6 loss: 0.7051945924758911, ACC:0.359375\n",
      "Training iteration 7 loss: 0.9102673530578613, ACC:0.46875\n",
      "Training iteration 8 loss: 1.2797346115112305, ACC:0.40625\n",
      "Training iteration 9 loss: 1.1754283905029297, ACC:0.5\n",
      "Training iteration 10 loss: 1.0374184846878052, ACC:0.546875\n",
      "Training iteration 11 loss: 0.9085361957550049, ACC:0.5625\n",
      "Training iteration 12 loss: 0.7929279208183289, ACC:0.5625\n",
      "Training iteration 13 loss: 0.7066138982772827, ACC:0.5625\n",
      "Training iteration 14 loss: 0.6965560913085938, ACC:0.484375\n",
      "Training iteration 15 loss: 0.7410632967948914, ACC:0.4375\n",
      "Training iteration 16 loss: 0.8061846494674683, ACC:0.453125\n",
      "Training iteration 17 loss: 0.8065280914306641, ACC:0.5\n",
      "Training iteration 18 loss: 0.8416385650634766, ACC:0.46875\n",
      "Training iteration 19 loss: 0.7529415488243103, ACC:0.53125\n",
      "Training iteration 20 loss: 0.7097262144088745, ACC:0.546875\n",
      "Training iteration 21 loss: 0.7214401960372925, ACC:0.453125\n",
      "Training iteration 22 loss: 0.6936314105987549, ACC:0.453125\n",
      "Training iteration 23 loss: 0.698665976524353, ACC:0.515625\n",
      "Training iteration 24 loss: 0.7543011903762817, ACC:0.4375\n",
      "Training iteration 25 loss: 0.7326170206069946, ACC:0.5\n",
      "Training iteration 26 loss: 0.7543527483940125, ACC:0.453125\n",
      "Training iteration 27 loss: 0.7313822507858276, ACC:0.453125\n",
      "Training iteration 28 loss: 0.6941567063331604, ACC:0.515625\n",
      "Training iteration 29 loss: 0.6894580721855164, ACC:0.578125\n",
      "Training iteration 30 loss: 0.6996371150016785, ACC:0.515625\n",
      "Training iteration 31 loss: 0.7298119068145752, ACC:0.484375\n",
      "Training iteration 32 loss: 0.7492225766181946, ACC:0.46875\n",
      "Training iteration 33 loss: 0.7013748288154602, ACC:0.546875\n",
      "Training iteration 34 loss: 0.6819697022438049, ACC:0.578125\n",
      "Training iteration 35 loss: 0.7186472415924072, ACC:0.453125\n",
      "Training iteration 36 loss: 0.6857384443283081, ACC:0.578125\n",
      "Training iteration 37 loss: 0.6926684975624084, ACC:0.53125\n",
      "Training iteration 38 loss: 0.6839421391487122, ACC:0.578125\n",
      "Training iteration 39 loss: 0.7005631327629089, ACC:0.515625\n",
      "Training iteration 40 loss: 0.6820020079612732, ACC:0.578125\n",
      "Training iteration 41 loss: 0.699217677116394, ACC:0.546875\n",
      "Training iteration 42 loss: 0.7074987888336182, ACC:0.53125\n",
      "Training iteration 43 loss: 0.7159302830696106, ACC:0.5\n",
      "Training iteration 44 loss: 0.6627886891365051, ACC:0.640625\n",
      "Training iteration 45 loss: 0.7300540208816528, ACC:0.359375\n",
      "Training iteration 46 loss: 0.6931127309799194, ACC:0.53125\n",
      "Training iteration 47 loss: 0.6887748837471008, ACC:0.546875\n",
      "Training iteration 48 loss: 0.6965922713279724, ACC:0.53125\n",
      "Training iteration 49 loss: 0.7101457715034485, ACC:0.515625\n",
      "Training iteration 50 loss: 0.6833458542823792, ACC:0.578125\n",
      "Training iteration 51 loss: 0.7527293562889099, ACC:0.421875\n",
      "Training iteration 52 loss: 0.7204686403274536, ACC:0.4375\n",
      "Training iteration 53 loss: 0.6944426894187927, ACC:0.4375\n",
      "Training iteration 54 loss: 0.6811041831970215, ACC:0.578125\n",
      "Training iteration 55 loss: 0.7131182551383972, ACC:0.53125\n",
      "Training iteration 56 loss: 0.7991882562637329, ACC:0.4375\n",
      "Training iteration 57 loss: 0.7357912659645081, ACC:0.515625\n",
      "Training iteration 58 loss: 0.7451438307762146, ACC:0.453125\n",
      "Training iteration 59 loss: 0.7095041871070862, ACC:0.4375\n",
      "Training iteration 60 loss: 0.7103927135467529, ACC:0.421875\n",
      "Training iteration 61 loss: 0.681684672832489, ACC:0.578125\n",
      "Training iteration 62 loss: 0.6893877387046814, ACC:0.578125\n",
      "Training iteration 63 loss: 0.6979232430458069, ACC:0.578125\n",
      "Training iteration 64 loss: 0.6888379454612732, ACC:0.59375\n",
      "Training iteration 65 loss: 0.6957336068153381, ACC:0.578125\n",
      "Training iteration 66 loss: 0.7390078902244568, ACC:0.484375\n",
      "Training iteration 67 loss: 0.680896520614624, ACC:0.578125\n",
      "Training iteration 68 loss: 0.7054315209388733, ACC:0.375\n",
      "Training iteration 69 loss: 0.7153853178024292, ACC:0.4375\n",
      "Training iteration 70 loss: 0.6891012191772461, ACC:0.5625\n",
      "Training iteration 71 loss: 0.71410071849823, ACC:0.53125\n",
      "Training iteration 72 loss: 0.7796339392662048, ACC:0.421875\n",
      "Training iteration 73 loss: 0.7125930786132812, ACC:0.5\n",
      "Training iteration 74 loss: 0.6999536156654358, ACC:0.46875\n",
      "Training iteration 75 loss: 0.6792895197868347, ACC:0.609375\n",
      "Training iteration 76 loss: 0.7223700881004333, ACC:0.5\n",
      "Training iteration 77 loss: 0.717282235622406, ACC:0.546875\n",
      "Training iteration 78 loss: 0.7247092127799988, ACC:0.546875\n",
      "Training iteration 79 loss: 0.7686510682106018, ACC:0.46875\n",
      "Training iteration 80 loss: 0.7487075924873352, ACC:0.421875\n",
      "Training iteration 81 loss: 0.6938099265098572, ACC:0.484375\n",
      "Training iteration 82 loss: 0.7024751305580139, ACC:0.515625\n",
      "Training iteration 83 loss: 0.7305623888969421, ACC:0.515625\n",
      "Training iteration 84 loss: 0.7596545815467834, ACC:0.5\n",
      "Training iteration 85 loss: 0.7701721787452698, ACC:0.46875\n",
      "Training iteration 86 loss: 0.696107804775238, ACC:0.546875\n",
      "Training iteration 87 loss: 0.6790985465049744, ACC:0.609375\n",
      "Training iteration 88 loss: 0.6932296752929688, ACC:0.5\n",
      "Training iteration 89 loss: 0.6887478828430176, ACC:0.546875\n",
      "Training iteration 90 loss: 0.7168686985969543, ACC:0.46875\n",
      "Training iteration 91 loss: 0.6810240745544434, ACC:0.578125\n",
      "Training iteration 92 loss: 0.7034860849380493, ACC:0.515625\n",
      "Training iteration 93 loss: 0.7269105911254883, ACC:0.421875\n",
      "Training iteration 94 loss: 0.6913818120956421, ACC:0.53125\n",
      "Training iteration 95 loss: 0.6985373497009277, ACC:0.453125\n",
      "Training iteration 96 loss: 0.6916134357452393, ACC:0.53125\n",
      "Training iteration 97 loss: 0.7167009115219116, ACC:0.4375\n",
      "Training iteration 98 loss: 0.7148389220237732, ACC:0.40625\n",
      "Training iteration 99 loss: 0.6931779980659485, ACC:0.5\n",
      "Training iteration 100 loss: 0.7129796743392944, ACC:0.421875\n",
      "Training iteration 101 loss: 0.6783643364906311, ACC:0.59375\n",
      "Training iteration 102 loss: 0.6946496367454529, ACC:0.53125\n",
      "Training iteration 103 loss: 0.70512455701828, ACC:0.5\n",
      "Training iteration 104 loss: 0.7084371447563171, ACC:0.46875\n",
      "Training iteration 105 loss: 0.6898727416992188, ACC:0.546875\n",
      "Training iteration 106 loss: 0.6945481896400452, ACC:0.4375\n",
      "Training iteration 107 loss: 0.6918331384658813, ACC:0.53125\n",
      "Training iteration 108 loss: 0.7007620334625244, ACC:0.4375\n",
      "Training iteration 109 loss: 0.6950872540473938, ACC:0.46875\n",
      "Training iteration 110 loss: 0.69525146484375, ACC:0.453125\n",
      "Training iteration 111 loss: 0.6897174715995789, ACC:0.5625\n",
      "Training iteration 112 loss: 0.7015986442565918, ACC:0.453125\n",
      "Training iteration 113 loss: 0.6877713203430176, ACC:0.5625\n",
      "Training iteration 114 loss: 0.6911936402320862, ACC:0.53125\n",
      "Training iteration 115 loss: 0.6845490336418152, ACC:0.578125\n",
      "Training iteration 116 loss: 0.7109181880950928, ACC:0.4375\n",
      "Training iteration 117 loss: 0.6891225576400757, ACC:0.546875\n",
      "Training iteration 118 loss: 0.698047399520874, ACC:0.453125\n",
      "Training iteration 119 loss: 0.691730260848999, ACC:0.546875\n",
      "Training iteration 120 loss: 0.680178165435791, ACC:0.59375\n",
      "Training iteration 121 loss: 0.7103167176246643, ACC:0.5\n",
      "Training iteration 122 loss: 0.7239870429039001, ACC:0.484375\n",
      "Training iteration 123 loss: 0.6924991011619568, ACC:0.546875\n",
      "Training iteration 124 loss: 0.7006381154060364, ACC:0.5\n",
      "Training iteration 125 loss: 0.6882722973823547, ACC:0.578125\n",
      "Training iteration 126 loss: 0.6913937926292419, ACC:0.59375\n",
      "Training iteration 127 loss: 0.6793582439422607, ACC:0.59375\n",
      "Training iteration 128 loss: 0.7274122834205627, ACC:0.46875\n",
      "Training iteration 129 loss: 0.7330437898635864, ACC:0.46875\n",
      "Training iteration 130 loss: 0.7014713287353516, ACC:0.515625\n",
      "Training iteration 131 loss: 0.6803022623062134, ACC:0.609375\n",
      "Training iteration 132 loss: 0.6901230812072754, ACC:0.5625\n",
      "Training iteration 133 loss: 0.6938069462776184, ACC:0.484375\n",
      "Training iteration 134 loss: 0.6917216777801514, ACC:0.609375\n",
      "Training iteration 135 loss: 0.6998761296272278, ACC:0.484375\n",
      "Training iteration 136 loss: 0.6973631381988525, ACC:0.515625\n",
      "Training iteration 137 loss: 0.6938740015029907, ACC:0.53125\n",
      "Training iteration 138 loss: 0.689125657081604, ACC:0.546875\n",
      "Training iteration 139 loss: 0.6982095837593079, ACC:0.5\n",
      "Training iteration 140 loss: 0.6977708339691162, ACC:0.46875\n",
      "Training iteration 141 loss: 0.6966529488563538, ACC:0.4375\n",
      "Training iteration 142 loss: 0.7008209824562073, ACC:0.4375\n",
      "Training iteration 143 loss: 0.6933853030204773, ACC:0.5\n",
      "Training iteration 144 loss: 0.6914851665496826, ACC:0.578125\n",
      "Training iteration 145 loss: 0.7045544385910034, ACC:0.453125\n",
      "Training iteration 146 loss: 0.697820246219635, ACC:0.5\n",
      "Training iteration 147 loss: 0.7086683511734009, ACC:0.421875\n",
      "Training iteration 148 loss: 0.6934006810188293, ACC:0.484375\n",
      "Training iteration 149 loss: 0.6958674788475037, ACC:0.5\n",
      "Training iteration 150 loss: 0.6819254159927368, ACC:0.578125\n",
      "Training iteration 151 loss: 0.6759104132652283, ACC:0.59375\n",
      "Training iteration 152 loss: 0.715364933013916, ACC:0.5\n",
      "Training iteration 153 loss: 0.6754775643348694, ACC:0.59375\n",
      "Training iteration 154 loss: 0.6978252530097961, ACC:0.53125\n",
      "Training iteration 155 loss: 0.7053797841072083, ACC:0.484375\n",
      "Training iteration 156 loss: 0.6926634907722473, ACC:0.515625\n",
      "Training iteration 157 loss: 0.6880300045013428, ACC:0.5625\n",
      "Training iteration 158 loss: 0.7141371965408325, ACC:0.46875\n",
      "Training iteration 159 loss: 0.6975692510604858, ACC:0.53125\n",
      "Training iteration 160 loss: 0.7012979388237, ACC:0.515625\n",
      "Training iteration 161 loss: 0.7165563106536865, ACC:0.421875\n",
      "Training iteration 162 loss: 0.6927251219749451, ACC:0.515625\n",
      "Training iteration 163 loss: 0.6986638307571411, ACC:0.515625\n",
      "Training iteration 164 loss: 0.7299227714538574, ACC:0.46875\n",
      "Training iteration 165 loss: 0.7187278270721436, ACC:0.484375\n",
      "Training iteration 166 loss: 0.7093945741653442, ACC:0.453125\n",
      "Training iteration 167 loss: 0.6905480027198792, ACC:0.546875\n",
      "Training iteration 168 loss: 0.6925686001777649, ACC:0.546875\n",
      "Training iteration 169 loss: 0.6530577540397644, ACC:0.640625\n",
      "Training iteration 170 loss: 0.7414114475250244, ACC:0.53125\n",
      "Training iteration 171 loss: 0.6798112988471985, ACC:0.609375\n",
      "Training iteration 172 loss: 0.6936935782432556, ACC:0.578125\n",
      "Training iteration 173 loss: 0.7529383897781372, ACC:0.421875\n",
      "Training iteration 174 loss: 0.6943219900131226, ACC:0.46875\n",
      "Training iteration 175 loss: 0.7366103529930115, ACC:0.4375\n",
      "Training iteration 176 loss: 0.7421351671218872, ACC:0.484375\n",
      "Training iteration 177 loss: 0.7117218375205994, ACC:0.53125\n",
      "Training iteration 178 loss: 0.7282965779304504, ACC:0.453125\n",
      "Training iteration 179 loss: 0.6909485459327698, ACC:0.546875\n",
      "Training iteration 180 loss: 0.6887484192848206, ACC:0.546875\n",
      "Training iteration 181 loss: 0.6820762157440186, ACC:0.578125\n",
      "Training iteration 182 loss: 0.6621772646903992, ACC:0.625\n",
      "Training iteration 183 loss: 0.8003454208374023, ACC:0.4375\n",
      "Training iteration 184 loss: 0.73648601770401, ACC:0.484375\n",
      "Training iteration 185 loss: 0.7086545825004578, ACC:0.4375\n",
      "Training iteration 186 loss: 0.7193590402603149, ACC:0.4375\n",
      "Training iteration 187 loss: 0.7338209748268127, ACC:0.484375\n",
      "Training iteration 188 loss: 0.782143771648407, ACC:0.40625\n",
      "Training iteration 189 loss: 0.731261670589447, ACC:0.40625\n",
      "Training iteration 190 loss: 0.6936230659484863, ACC:0.515625\n",
      "Training iteration 191 loss: 0.7023977637290955, ACC:0.546875\n",
      "Training iteration 192 loss: 0.7509081959724426, ACC:0.515625\n",
      "Training iteration 193 loss: 0.8330791592597961, ACC:0.40625\n",
      "Training iteration 194 loss: 0.6945885419845581, ACC:0.546875\n",
      "Training iteration 195 loss: 0.6928304433822632, ACC:0.515625\n",
      "Training iteration 196 loss: 0.704833984375, ACC:0.5\n",
      "Training iteration 197 loss: 0.7410653829574585, ACC:0.46875\n",
      "Training iteration 198 loss: 0.7000930309295654, ACC:0.546875\n",
      "Training iteration 199 loss: 0.6994678974151611, ACC:0.53125\n",
      "Training iteration 200 loss: 0.6823813915252686, ACC:0.578125\n",
      "Training iteration 201 loss: 0.6975477933883667, ACC:0.453125\n",
      "Training iteration 202 loss: 0.6969073414802551, ACC:0.484375\n",
      "Training iteration 203 loss: 0.7080806493759155, ACC:0.46875\n",
      "Training iteration 204 loss: 0.7072561383247375, ACC:0.46875\n",
      "Training iteration 205 loss: 0.6895005106925964, ACC:0.546875\n",
      "Training iteration 206 loss: 0.6906254887580872, ACC:0.609375\n",
      "Training iteration 207 loss: 0.6914493441581726, ACC:0.53125\n",
      "Training iteration 208 loss: 0.7071120738983154, ACC:0.421875\n",
      "Training iteration 209 loss: 0.692665159702301, ACC:0.515625\n",
      "Training iteration 210 loss: 0.6917616724967957, ACC:0.5625\n",
      "Training iteration 211 loss: 0.6861675977706909, ACC:0.5625\n",
      "Training iteration 212 loss: 0.6910859942436218, ACC:0.546875\n",
      "Training iteration 213 loss: 0.748511552810669, ACC:0.421875\n",
      "Training iteration 214 loss: 0.6896308660507202, ACC:0.546875\n",
      "Training iteration 215 loss: 0.6929293870925903, ACC:0.515625\n",
      "Training iteration 216 loss: 0.6901459693908691, ACC:0.5625\n",
      "Training iteration 217 loss: 0.710218608379364, ACC:0.46875\n",
      "Training iteration 218 loss: 0.7167572379112244, ACC:0.46875\n",
      "Training iteration 219 loss: 0.7074159979820251, ACC:0.46875\n",
      "Training iteration 220 loss: 0.6969459652900696, ACC:0.390625\n",
      "Training iteration 221 loss: 0.6856564283370972, ACC:0.5625\n",
      "Training iteration 222 loss: 0.671563446521759, ACC:0.609375\n",
      "Training iteration 223 loss: 0.8224033117294312, ACC:0.4375\n",
      "Training iteration 224 loss: 0.7365346550941467, ACC:0.515625\n",
      "Training iteration 225 loss: 0.707730770111084, ACC:0.5\n",
      "Training iteration 226 loss: 0.6962314248085022, ACC:0.453125\n",
      "Training iteration 227 loss: 0.7072305083274841, ACC:0.5\n",
      "Training iteration 228 loss: 0.7363505363464355, ACC:0.46875\n",
      "Training iteration 229 loss: 0.7776644825935364, ACC:0.34375\n",
      "Training iteration 230 loss: 0.6939716935157776, ACC:0.40625\n",
      "Training iteration 231 loss: 0.7512478828430176, ACC:0.4375\n",
      "Training iteration 232 loss: 0.770818293094635, ACC:0.46875\n",
      "Training iteration 233 loss: 0.7475025057792664, ACC:0.484375\n",
      "Training iteration 234 loss: 0.7367650866508484, ACC:0.40625\n",
      "Training iteration 235 loss: 0.6936774253845215, ACC:0.515625\n",
      "Training iteration 236 loss: 0.7224429845809937, ACC:0.515625\n",
      "Training iteration 237 loss: 0.728024959564209, ACC:0.546875\n",
      "Training iteration 238 loss: 0.717217206954956, ACC:0.5625\n",
      "Training iteration 239 loss: 0.7209251523017883, ACC:0.53125\n",
      "Training iteration 240 loss: 0.6757798790931702, ACC:0.59375\n",
      "Training iteration 241 loss: 0.6978830099105835, ACC:0.453125\n",
      "Training iteration 242 loss: 0.678275465965271, ACC:0.59375\n",
      "Training iteration 243 loss: 0.7139451503753662, ACC:0.53125\n",
      "Training iteration 244 loss: 0.7556961178779602, ACC:0.5\n",
      "Training iteration 245 loss: 0.7817060947418213, ACC:0.4375\n",
      "Training iteration 246 loss: 0.6982790231704712, ACC:0.515625\n",
      "Training iteration 247 loss: 0.6966264247894287, ACC:0.484375\n",
      "Training iteration 248 loss: 0.7011697292327881, ACC:0.53125\n",
      "Training iteration 249 loss: 0.7349863052368164, ACC:0.5\n",
      "Training iteration 250 loss: 0.7048609256744385, ACC:0.546875\n",
      "Training iteration 251 loss: 0.7202250361442566, ACC:0.484375\n",
      "Training iteration 252 loss: 0.6911929249763489, ACC:0.53125\n",
      "Training iteration 253 loss: 0.6831174492835999, ACC:0.59375\n",
      "Training iteration 254 loss: 0.6827974915504456, ACC:0.578125\n",
      "Training iteration 255 loss: 0.7181364297866821, ACC:0.546875\n",
      "Training iteration 256 loss: 0.7721129059791565, ACC:0.484375\n",
      "Training iteration 257 loss: 0.7378352284431458, ACC:0.484375\n",
      "Training iteration 258 loss: 0.6991519331932068, ACC:0.484375\n",
      "Training iteration 259 loss: 0.6814239025115967, ACC:0.578125\n",
      "Training iteration 260 loss: 0.7341077923774719, ACC:0.515625\n",
      "Training iteration 261 loss: 0.7865099906921387, ACC:0.484375\n",
      "Training iteration 262 loss: 0.7731353044509888, ACC:0.46875\n",
      "Training iteration 263 loss: 0.6759067177772522, ACC:0.59375\n",
      "Training iteration 264 loss: 0.6928115487098694, ACC:0.59375\n",
      "Training iteration 265 loss: 0.6814648509025574, ACC:0.59375\n",
      "Training iteration 266 loss: 0.7223833203315735, ACC:0.46875\n",
      "Training iteration 267 loss: 0.7073365449905396, ACC:0.515625\n",
      "Training iteration 268 loss: 0.6960269212722778, ACC:0.53125\n",
      "Training iteration 269 loss: 0.7029829621315002, ACC:0.46875\n",
      "Training iteration 270 loss: 0.6948748826980591, ACC:0.46875\n",
      "Training iteration 271 loss: 0.6918107271194458, ACC:0.53125\n",
      "Training iteration 272 loss: 0.6713141798973083, ACC:0.609375\n",
      "Training iteration 273 loss: 0.6760855913162231, ACC:0.59375\n",
      "Training iteration 274 loss: 0.7404144406318665, ACC:0.484375\n",
      "Training iteration 275 loss: 0.7466933727264404, ACC:0.4375\n",
      "Training iteration 276 loss: 0.700836718082428, ACC:0.453125\n",
      "Training iteration 277 loss: 0.6853929162025452, ACC:0.5625\n",
      "Training iteration 278 loss: 0.7025030255317688, ACC:0.5625\n",
      "Training iteration 279 loss: 0.7115722894668579, ACC:0.578125\n",
      "Training iteration 280 loss: 0.8591776490211487, ACC:0.40625\n",
      "Training iteration 281 loss: 0.7280737161636353, ACC:0.484375\n",
      "Training iteration 282 loss: 0.6964420676231384, ACC:0.421875\n",
      "Training iteration 283 loss: 0.7113639712333679, ACC:0.5\n",
      "Training iteration 284 loss: 0.6707576513290405, ACC:0.609375\n",
      "Training iteration 285 loss: 0.8169238567352295, ACC:0.40625\n",
      "Training iteration 286 loss: 0.7265845537185669, ACC:0.484375\n",
      "Training iteration 287 loss: 0.6926965117454529, ACC:0.515625\n",
      "Training iteration 288 loss: 0.7273649573326111, ACC:0.40625\n",
      "Training iteration 289 loss: 0.7446289658546448, ACC:0.40625\n",
      "Training iteration 290 loss: 0.7221155762672424, ACC:0.390625\n",
      "Training iteration 291 loss: 0.7001227140426636, ACC:0.46875\n",
      "Training iteration 292 loss: 0.6929966807365417, ACC:0.546875\n",
      "Training iteration 293 loss: 0.7839722037315369, ACC:0.390625\n",
      "Training iteration 294 loss: 0.707633912563324, ACC:0.5\n",
      "Training iteration 295 loss: 0.6968003511428833, ACC:0.453125\n",
      "Training iteration 296 loss: 0.7310284972190857, ACC:0.390625\n",
      "Training iteration 297 loss: 0.701656699180603, ACC:0.515625\n",
      "Training iteration 298 loss: 0.7187612056732178, ACC:0.453125\n",
      "Training iteration 299 loss: 0.6894106864929199, ACC:0.546875\n",
      "Training iteration 300 loss: 0.6961107850074768, ACC:0.40625\n",
      "Training iteration 301 loss: 0.6944712400436401, ACC:0.453125\n",
      "Training iteration 302 loss: 0.6941981911659241, ACC:0.484375\n",
      "Training iteration 303 loss: 0.6902260780334473, ACC:0.546875\n",
      "Training iteration 304 loss: 0.7202187776565552, ACC:0.328125\n",
      "Training iteration 305 loss: 0.6914854645729065, ACC:0.53125\n",
      "Training iteration 306 loss: 0.6718782186508179, ACC:0.609375\n",
      "Training iteration 307 loss: 0.6699574589729309, ACC:0.609375\n",
      "Training iteration 308 loss: 0.7573265433311462, ACC:0.5\n",
      "Training iteration 309 loss: 0.7471844553947449, ACC:0.5\n",
      "Training iteration 310 loss: 0.7495230436325073, ACC:0.40625\n",
      "Training iteration 311 loss: 0.6849288940429688, ACC:0.59375\n",
      "Training iteration 312 loss: 0.7334831357002258, ACC:0.515625\n",
      "Training iteration 313 loss: 0.7408036589622498, ACC:0.5625\n",
      "Training iteration 314 loss: 0.7661772966384888, ACC:0.546875\n",
      "Training iteration 315 loss: 0.7095044851303101, ACC:0.578125\n",
      "Training iteration 316 loss: 0.7601513266563416, ACC:0.421875\n",
      "Training iteration 317 loss: 0.6926609873771667, ACC:0.515625\n",
      "Training iteration 318 loss: 0.7498196363449097, ACC:0.46875\n",
      "Training iteration 319 loss: 0.6701470613479614, ACC:0.625\n",
      "Training iteration 320 loss: 0.834109902381897, ACC:0.453125\n",
      "Training iteration 321 loss: 0.747694730758667, ACC:0.5\n",
      "Training iteration 322 loss: 0.7155267596244812, ACC:0.4375\n",
      "Training iteration 323 loss: 0.6903786659240723, ACC:0.546875\n",
      "Training iteration 324 loss: 0.7985296845436096, ACC:0.453125\n",
      "Training iteration 325 loss: 0.8239688873291016, ACC:0.453125\n",
      "Training iteration 326 loss: 0.7616490721702576, ACC:0.46875\n",
      "Training iteration 327 loss: 0.716851532459259, ACC:0.390625\n",
      "Training iteration 328 loss: 0.7224370837211609, ACC:0.484375\n",
      "Training iteration 329 loss: 0.8376578092575073, ACC:0.421875\n",
      "Training iteration 330 loss: 0.7096132636070251, ACC:0.578125\n",
      "Training iteration 331 loss: 0.7868523001670837, ACC:0.4375\n",
      "Training iteration 332 loss: 0.699799656867981, ACC:0.5\n",
      "Training iteration 333 loss: 0.7136750221252441, ACC:0.4375\n",
      "Training iteration 334 loss: 0.7145184874534607, ACC:0.515625\n",
      "Training iteration 335 loss: 0.7801413536071777, ACC:0.421875\n",
      "Training iteration 336 loss: 0.7234510779380798, ACC:0.46875\n",
      "Training iteration 337 loss: 0.6906943321228027, ACC:0.5625\n",
      "Training iteration 338 loss: 0.7010754346847534, ACC:0.484375\n",
      "Training iteration 339 loss: 0.6809142231941223, ACC:0.578125\n",
      "Training iteration 340 loss: 0.745303213596344, ACC:0.4375\n",
      "Training iteration 341 loss: 0.69605553150177, ACC:0.53125\n",
      "Training iteration 342 loss: 0.6865556836128235, ACC:0.5625\n",
      "Training iteration 343 loss: 0.695883572101593, ACC:0.40625\n",
      "Training iteration 344 loss: 0.692179262638092, ACC:0.53125\n",
      "Training iteration 345 loss: 0.6944668292999268, ACC:0.546875\n",
      "Training iteration 346 loss: 0.6457664370536804, ACC:0.65625\n",
      "Training iteration 347 loss: 0.7334896922111511, ACC:0.515625\n",
      "Training iteration 348 loss: 0.7524431347846985, ACC:0.46875\n",
      "Training iteration 349 loss: 0.7134594321250916, ACC:0.46875\n",
      "Training iteration 350 loss: 0.6884055733680725, ACC:0.5625\n",
      "Training iteration 351 loss: 0.7159346342086792, ACC:0.515625\n",
      "Training iteration 352 loss: 0.6666383743286133, ACC:0.625\n",
      "Training iteration 353 loss: 0.7559924721717834, ACC:0.53125\n",
      "Training iteration 354 loss: 0.7279868721961975, ACC:0.546875\n",
      "Training iteration 355 loss: 0.6769954562187195, ACC:0.59375\n",
      "Training iteration 356 loss: 0.7125189900398254, ACC:0.4375\n",
      "Training iteration 357 loss: 0.6887471079826355, ACC:0.546875\n",
      "Training iteration 358 loss: 0.7731678485870361, ACC:0.421875\n",
      "Training iteration 359 loss: 0.7554888129234314, ACC:0.46875\n",
      "Training iteration 360 loss: 0.6882752180099487, ACC:0.5625\n",
      "Training iteration 361 loss: 0.7062851190567017, ACC:0.453125\n",
      "Training iteration 362 loss: 0.6953944563865662, ACC:0.5\n",
      "Training iteration 363 loss: 0.6873245239257812, ACC:0.5625\n",
      "Training iteration 364 loss: 0.6705800294876099, ACC:0.609375\n",
      "Training iteration 365 loss: 0.7615651488304138, ACC:0.484375\n",
      "Training iteration 366 loss: 0.7407534718513489, ACC:0.484375\n",
      "Training iteration 367 loss: 0.704664409160614, ACC:0.484375\n",
      "Training iteration 368 loss: 0.7104323506355286, ACC:0.390625\n",
      "Training iteration 369 loss: 0.6809496879577637, ACC:0.578125\n",
      "Training iteration 370 loss: 0.6828099489212036, ACC:0.578125\n",
      "Training iteration 371 loss: 0.6699739694595337, ACC:0.609375\n",
      "Training iteration 372 loss: 0.763741135597229, ACC:0.453125\n",
      "Training iteration 373 loss: 0.7240428924560547, ACC:0.46875\n",
      "Training iteration 374 loss: 0.6945400834083557, ACC:0.46875\n",
      "Training iteration 375 loss: 0.7171282172203064, ACC:0.46875\n",
      "Training iteration 376 loss: 0.7498342394828796, ACC:0.453125\n",
      "Training iteration 377 loss: 0.6760456562042236, ACC:0.59375\n",
      "Training iteration 378 loss: 0.7158148288726807, ACC:0.484375\n",
      "Training iteration 379 loss: 0.7058072090148926, ACC:0.4375\n",
      "Training iteration 380 loss: 0.7022424340248108, ACC:0.46875\n",
      "Training iteration 381 loss: 0.7100102305412292, ACC:0.5\n",
      "Training iteration 382 loss: 0.728655219078064, ACC:0.46875\n",
      "Training iteration 383 loss: 0.7221502661705017, ACC:0.4375\n",
      "Training iteration 384 loss: 0.6931496858596802, ACC:0.5\n",
      "Training iteration 385 loss: 0.7065463066101074, ACC:0.484375\n",
      "Training iteration 386 loss: 0.6880391836166382, ACC:0.5625\n",
      "Training iteration 387 loss: 0.7423527240753174, ACC:0.453125\n",
      "Training iteration 388 loss: 0.7250473499298096, ACC:0.4375\n",
      "Training iteration 389 loss: 0.6939495801925659, ACC:0.4375\n",
      "Training iteration 390 loss: 0.6820187568664551, ACC:0.578125\n",
      "Training iteration 391 loss: 0.6963347792625427, ACC:0.546875\n",
      "Training iteration 392 loss: 0.7479796409606934, ACC:0.46875\n",
      "Training iteration 393 loss: 0.69613116979599, ACC:0.546875\n",
      "Training iteration 394 loss: 0.6891708374023438, ACC:0.546875\n",
      "Training iteration 395 loss: 0.6945005059242249, ACC:0.484375\n",
      "Training iteration 396 loss: 0.6912765502929688, ACC:0.53125\n",
      "Training iteration 397 loss: 0.6912893056869507, ACC:0.546875\n",
      "Training iteration 398 loss: 0.7318753004074097, ACC:0.46875\n",
      "Training iteration 399 loss: 0.6980392336845398, ACC:0.53125\n",
      "Training iteration 400 loss: 0.6855837106704712, ACC:0.5625\n",
      "Training iteration 401 loss: 0.6926950812339783, ACC:0.515625\n",
      "Training iteration 402 loss: 0.6939426064491272, ACC:0.484375\n",
      "Training iteration 403 loss: 0.705124020576477, ACC:0.40625\n",
      "Training iteration 404 loss: 0.6924247145652771, ACC:0.5625\n",
      "Training iteration 405 loss: 0.6929615139961243, ACC:0.5625\n",
      "Training iteration 406 loss: 0.7013154029846191, ACC:0.40625\n",
      "Training iteration 407 loss: 0.6931533217430115, ACC:0.5\n",
      "Training iteration 408 loss: 0.6978222131729126, ACC:0.453125\n",
      "Training iteration 409 loss: 0.6881512999534607, ACC:0.59375\n",
      "Training iteration 410 loss: 0.6979904770851135, ACC:0.484375\n",
      "Training iteration 411 loss: 0.7102171778678894, ACC:0.40625\n",
      "Training iteration 412 loss: 0.6945312023162842, ACC:0.4375\n",
      "Training iteration 413 loss: 0.6887669563293457, ACC:0.5625\n",
      "Training iteration 414 loss: 0.6671278476715088, ACC:0.65625\n",
      "Training iteration 415 loss: 0.6899441480636597, ACC:0.5625\n",
      "Training iteration 416 loss: 0.7288236021995544, ACC:0.515625\n",
      "Training iteration 417 loss: 0.7953075766563416, ACC:0.390625\n",
      "Training iteration 418 loss: 0.6980557441711426, ACC:0.5\n",
      "Training iteration 419 loss: 0.6824270486831665, ACC:0.578125\n",
      "Training iteration 420 loss: 0.7190273404121399, ACC:0.53125\n",
      "Training iteration 421 loss: 0.7846054434776306, ACC:0.484375\n",
      "Training iteration 422 loss: 0.7319633960723877, ACC:0.53125\n",
      "Training iteration 423 loss: 0.7490447759628296, ACC:0.421875\n",
      "Training iteration 424 loss: 0.7040285468101501, ACC:0.359375\n",
      "Training iteration 425 loss: 0.7095596790313721, ACC:0.484375\n",
      "Training iteration 426 loss: 0.7060360312461853, ACC:0.515625\n",
      "Training iteration 427 loss: 0.7258429527282715, ACC:0.453125\n",
      "Training iteration 428 loss: 0.6934785842895508, ACC:0.515625\n",
      "Training iteration 429 loss: 0.6926852464675903, ACC:0.515625\n",
      "Training iteration 430 loss: 0.6853617429733276, ACC:0.5625\n",
      "Training iteration 431 loss: 0.6964768767356873, ACC:0.546875\n",
      "Training iteration 432 loss: 0.6774062514305115, ACC:0.59375\n",
      "Training iteration 433 loss: 0.678042471408844, ACC:0.59375\n",
      "Training iteration 434 loss: 0.7391062378883362, ACC:0.46875\n",
      "Training iteration 435 loss: 0.7096331715583801, ACC:0.46875\n",
      "Training iteration 436 loss: 0.6953875422477722, ACC:0.484375\n",
      "Training iteration 437 loss: 0.7133466005325317, ACC:0.484375\n",
      "Training iteration 438 loss: 0.6896578073501587, ACC:0.5625\n",
      "Training iteration 439 loss: 0.7124448418617249, ACC:0.515625\n",
      "Training iteration 440 loss: 0.7190980315208435, ACC:0.46875\n",
      "Training iteration 441 loss: 0.6953151822090149, ACC:0.484375\n",
      "Training iteration 442 loss: 0.6984226107597351, ACC:0.5\n",
      "Training iteration 443 loss: 0.7063699960708618, ACC:0.515625\n",
      "Training iteration 444 loss: 0.6973356008529663, ACC:0.546875\n",
      "Training iteration 445 loss: 0.7275720238685608, ACC:0.46875\n",
      "Training iteration 446 loss: 0.6887567043304443, ACC:0.546875\n",
      "Training iteration 447 loss: 0.6931890249252319, ACC:0.484375\n",
      "Training iteration 448 loss: 0.6988561749458313, ACC:0.484375\n",
      "Training iteration 449 loss: 0.7170481085777283, ACC:0.421875\n",
      "Training iteration 450 loss: 0.6897701621055603, ACC:0.546875\n",
      "Validation iteration 451 loss: 0.6927496194839478, ACC: 0.53125\n",
      "Validation iteration 452 loss: 0.6931697726249695, ACC: 0.5\n",
      "Validation iteration 453 loss: 0.6935899257659912, ACC: 0.46875\n",
      "Validation iteration 454 loss: 0.6927496194839478, ACC: 0.53125\n",
      "Validation iteration 455 loss: 0.6929596662521362, ACC: 0.515625\n",
      "Validation iteration 456 loss: 0.6927496790885925, ACC: 0.53125\n",
      "Validation iteration 457 loss: 0.6944301724433899, ACC: 0.40625\n",
      "Validation iteration 458 loss: 0.6940100789070129, ACC: 0.4375\n",
      "Validation iteration 459 loss: 0.6944302320480347, ACC: 0.40625\n",
      "Validation iteration 460 loss: 0.6944301724433899, ACC: 0.40625\n",
      "Validation iteration 461 loss: 0.6931697726249695, ACC: 0.5\n",
      "Validation iteration 462 loss: 0.6927496194839478, ACC: 0.53125\n",
      "Validation iteration 463 loss: 0.6938000321388245, ACC: 0.453125\n",
      "Validation iteration 464 loss: 0.6914892196655273, ACC: 0.625\n",
      "Validation iteration 465 loss: 0.6925395131111145, ACC: 0.546875\n",
      "Validation iteration 466 loss: 0.693379819393158, ACC: 0.484375\n",
      "Validation iteration 467 loss: 0.6944302320480347, ACC: 0.40625\n",
      "Validation iteration 468 loss: 0.6935899257659912, ACC: 0.46875\n",
      "Validation iteration 469 loss: 0.6925395727157593, ACC: 0.546875\n",
      "Validation iteration 470 loss: 0.6925395131111145, ACC: 0.546875\n",
      "Validation iteration 471 loss: 0.692329466342926, ACC: 0.5625\n",
      "Validation iteration 472 loss: 0.6914892196655273, ACC: 0.625\n",
      "Validation iteration 473 loss: 0.6912791132926941, ACC: 0.640625\n",
      "Validation iteration 474 loss: 0.6933798789978027, ACC: 0.484375\n",
      "Validation iteration 475 loss: 0.6935899257659912, ACC: 0.46875\n",
      "Validation iteration 476 loss: 0.6921194195747375, ACC: 0.578125\n",
      "Validation iteration 477 loss: 0.693379819393158, ACC: 0.484375\n",
      "Validation iteration 478 loss: 0.6919093132019043, ACC: 0.59375\n",
      "Validation iteration 479 loss: 0.6923295259475708, ACC: 0.5625\n",
      "Validation iteration 480 loss: 0.6940100193023682, ACC: 0.4375\n",
      "Validation iteration 481 loss: 0.692959725856781, ACC: 0.515625\n",
      "Validation iteration 482 loss: 0.6927496194839478, ACC: 0.53125\n",
      "Validation iteration 483 loss: 0.6937999725341797, ACC: 0.453125\n",
      "Validation iteration 484 loss: 0.6914892196655273, ACC: 0.625\n",
      "Validation iteration 485 loss: 0.6940100789070129, ACC: 0.4375\n",
      "Validation iteration 486 loss: 0.6935898661613464, ACC: 0.46875\n",
      "Validation iteration 487 loss: 0.6927496790885925, ACC: 0.53125\n",
      "Validation iteration 488 loss: 0.692329466342926, ACC: 0.5625\n",
      "Validation iteration 489 loss: 0.6921193599700928, ACC: 0.578125\n",
      "Validation iteration 490 loss: 0.6931697130203247, ACC: 0.5\n",
      "Validation iteration 491 loss: 0.6933798789978027, ACC: 0.484375\n",
      "Validation iteration 492 loss: 0.6942201256752014, ACC: 0.421875\n",
      "Validation iteration 493 loss: 0.6937999725341797, ACC: 0.453125\n",
      "Validation iteration 494 loss: 0.691699206829071, ACC: 0.609375\n",
      "Validation iteration 495 loss: 0.6931697726249695, ACC: 0.5\n",
      "Validation iteration 496 loss: 0.691699206829071, ACC: 0.609375\n",
      "Validation iteration 497 loss: 0.6935898661613464, ACC: 0.46875\n",
      "Validation iteration 498 loss: 0.6937999725341797, ACC: 0.453125\n",
      "Validation iteration 499 loss: 0.6933798789978027, ACC: 0.484375\n",
      "Validation iteration 500 loss: 0.692959725856781, ACC: 0.515625\n",
      "-- Epoch 1 done -- Train loss: 0.7268049388461643, train ACC: 0.5044097222222222, val loss: 0.6930395233631134, val ACC: 0.5096875\n",
      "<--- 7383.89350271225 seconds --->\n",
      "Training iteration 1 loss: 0.693379819393158, ACC:0.484375\n",
      "Training iteration 2 loss: 0.6968914866447449, ACC:0.46875\n",
      "Training iteration 3 loss: 0.6945518851280212, ACC:0.5\n",
      "Training iteration 4 loss: 0.6993533968925476, ACC:0.4375\n",
      "Training iteration 5 loss: 0.6934719681739807, ACC:0.5\n",
      "Training iteration 6 loss: 0.6767193675041199, ACC:0.625\n",
      "Training iteration 7 loss: 0.704409658908844, ACC:0.515625\n",
      "Training iteration 8 loss: 0.7333882451057434, ACC:0.46875\n",
      "Training iteration 9 loss: 0.7489770650863647, ACC:0.375\n",
      "Training iteration 10 loss: 0.6957178115844727, ACC:0.46875\n",
      "Training iteration 11 loss: 0.748518168926239, ACC:0.390625\n",
      "Training iteration 12 loss: 0.6972922086715698, ACC:0.53125\n",
      "Training iteration 13 loss: 0.7015697956085205, ACC:0.5\n",
      "Training iteration 14 loss: 0.6927604675292969, ACC:0.515625\n",
      "Training iteration 15 loss: 0.6960011720657349, ACC:0.46875\n",
      "Training iteration 16 loss: 0.6935826539993286, ACC:0.515625\n",
      "Training iteration 17 loss: 0.7064029574394226, ACC:0.453125\n",
      "Training iteration 18 loss: 0.7028923034667969, ACC:0.40625\n",
      "Training iteration 19 loss: 0.6937869191169739, ACC:0.515625\n",
      "Training iteration 20 loss: 0.691967248916626, ACC:0.546875\n",
      "Training iteration 21 loss: 0.7347740530967712, ACC:0.46875\n",
      "Training iteration 22 loss: 0.6984521746635437, ACC:0.53125\n",
      "Training iteration 23 loss: 0.6826895475387573, ACC:0.578125\n",
      "Training iteration 24 loss: 0.687726616859436, ACC:0.578125\n",
      "Training iteration 25 loss: 0.6935753226280212, ACC:0.5\n",
      "Training iteration 26 loss: 0.6925989985466003, ACC:0.53125\n",
      "Training iteration 27 loss: 0.6948274970054626, ACC:0.421875\n",
      "Training iteration 28 loss: 0.700247585773468, ACC:0.4375\n",
      "Training iteration 29 loss: 0.6999890804290771, ACC:0.421875\n",
      "Training iteration 30 loss: 0.6938709616661072, ACC:0.5\n",
      "Training iteration 31 loss: 0.7004896998405457, ACC:0.484375\n",
      "Training iteration 32 loss: 0.7043853402137756, ACC:0.46875\n",
      "Training iteration 33 loss: 0.6883113384246826, ACC:0.5625\n",
      "Training iteration 34 loss: 0.6944317817687988, ACC:0.484375\n",
      "Training iteration 35 loss: 0.6935306787490845, ACC:0.484375\n",
      "Training iteration 36 loss: 0.6983938813209534, ACC:0.421875\n",
      "Training iteration 37 loss: 0.6899437308311462, ACC:0.59375\n",
      "Training iteration 38 loss: 0.681503176689148, ACC:0.578125\n",
      "Training iteration 39 loss: 0.6761833429336548, ACC:0.59375\n",
      "Training iteration 40 loss: 0.7504591345787048, ACC:0.484375\n",
      "Training iteration 41 loss: 0.7198857069015503, ACC:0.515625\n",
      "Training iteration 42 loss: 0.712559700012207, ACC:0.46875\n",
      "Training iteration 43 loss: 0.6902510523796082, ACC:0.546875\n",
      "Training iteration 44 loss: 0.7110010385513306, ACC:0.515625\n",
      "Training iteration 45 loss: 0.7846563458442688, ACC:0.4375\n",
      "Training iteration 46 loss: 0.6615632772445679, ACC:0.625\n",
      "Training iteration 47 loss: 0.7363389730453491, ACC:0.4375\n",
      "Training iteration 48 loss: 0.6901402473449707, ACC:0.5625\n",
      "Training iteration 49 loss: 0.7082643508911133, ACC:0.4375\n",
      "Training iteration 50 loss: 0.7109962105751038, ACC:0.453125\n",
      "Training iteration 51 loss: 0.6912035942077637, ACC:0.53125\n",
      "Training iteration 52 loss: 0.6869363188743591, ACC:0.65625\n",
      "Training iteration 53 loss: 0.6965874433517456, ACC:0.5\n",
      "Training iteration 54 loss: 0.6957162618637085, ACC:0.515625\n",
      "Training iteration 55 loss: 0.692113995552063, ACC:0.53125\n",
      "Training iteration 56 loss: 0.7158694863319397, ACC:0.390625\n",
      "Training iteration 57 loss: 0.6964272856712341, ACC:0.46875\n",
      "Training iteration 58 loss: 0.6963248252868652, ACC:0.515625\n",
      "Training iteration 59 loss: 0.7198920249938965, ACC:0.453125\n",
      "Training iteration 60 loss: 0.6983124017715454, ACC:0.5\n",
      "Training iteration 61 loss: 0.6963464021682739, ACC:0.40625\n",
      "Training iteration 62 loss: 0.7020830512046814, ACC:0.5\n",
      "Training iteration 63 loss: 0.765831708908081, ACC:0.390625\n",
      "Training iteration 64 loss: 0.7033244967460632, ACC:0.5\n",
      "Training iteration 65 loss: 0.6913761496543884, ACC:0.546875\n",
      "Training iteration 66 loss: 0.6950790286064148, ACC:0.5\n",
      "Training iteration 67 loss: 0.7033445835113525, ACC:0.484375\n",
      "Training iteration 68 loss: 0.688873827457428, ACC:0.546875\n",
      "Training iteration 69 loss: 0.6793987154960632, ACC:0.59375\n",
      "Training iteration 70 loss: 0.7197662591934204, ACC:0.421875\n",
      "Training iteration 71 loss: 0.6940434575080872, ACC:0.5\n",
      "Training iteration 72 loss: 0.6927554607391357, ACC:0.515625\n",
      "Training iteration 73 loss: 0.6927732229232788, ACC:0.53125\n",
      "Training iteration 74 loss: 0.7273927330970764, ACC:0.4375\n",
      "Training iteration 75 loss: 0.6823898553848267, ACC:0.578125\n",
      "Training iteration 76 loss: 0.6871222257614136, ACC:0.5625\n",
      "Training iteration 77 loss: 0.6962536573410034, ACC:0.484375\n",
      "Training iteration 78 loss: 0.6928390264511108, ACC:0.515625\n",
      "Training iteration 79 loss: 0.6916196346282959, ACC:0.546875\n",
      "Training iteration 80 loss: 0.6866891384124756, ACC:0.5625\n",
      "Training iteration 81 loss: 0.6898549199104309, ACC:0.546875\n",
      "Training iteration 82 loss: 0.7161723375320435, ACC:0.484375\n",
      "Training iteration 83 loss: 0.6947815418243408, ACC:0.53125\n",
      "Training iteration 84 loss: 0.6939594745635986, ACC:0.515625\n",
      "Training iteration 85 loss: 0.6933096051216125, ACC:0.484375\n",
      "Training iteration 86 loss: 0.6937592625617981, ACC:0.515625\n",
      "Training iteration 87 loss: 0.7235839366912842, ACC:0.421875\n",
      "Training iteration 88 loss: 0.7090662717819214, ACC:0.421875\n",
      "Training iteration 89 loss: 0.6877560615539551, ACC:0.5625\n",
      "Training iteration 90 loss: 0.6946668028831482, ACC:0.546875\n",
      "Training iteration 91 loss: 0.6996713876724243, ACC:0.5625\n",
      "Training iteration 92 loss: 0.6938828229904175, ACC:0.578125\n",
      "Training iteration 93 loss: 0.6710146069526672, ACC:0.609375\n",
      "Training iteration 94 loss: 0.7419381141662598, ACC:0.453125\n",
      "Training iteration 95 loss: 0.6932523250579834, ACC:0.515625\n",
      "Training iteration 96 loss: 0.6918075680732727, ACC:0.53125\n",
      "Training iteration 97 loss: 0.7053514122962952, ACC:0.53125\n",
      "Training iteration 98 loss: 0.7169724106788635, ACC:0.53125\n",
      "Training iteration 99 loss: 0.6781250238418579, ACC:0.59375\n",
      "Training iteration 100 loss: 0.7157003879547119, ACC:0.5\n",
      "Training iteration 101 loss: 0.685958981513977, ACC:0.5625\n",
      "Training iteration 102 loss: 0.6937289834022522, ACC:0.46875\n",
      "Training iteration 103 loss: 0.6934401988983154, ACC:0.515625\n",
      "Training iteration 104 loss: 0.7169568538665771, ACC:0.421875\n",
      "Training iteration 105 loss: 0.6959689259529114, ACC:0.484375\n",
      "Training iteration 106 loss: 0.7005358338356018, ACC:0.40625\n",
      "Training iteration 107 loss: 0.6907487511634827, ACC:0.546875\n",
      "Training iteration 108 loss: 0.7035751342773438, ACC:0.40625\n",
      "Training iteration 109 loss: 0.6933876872062683, ACC:0.5\n",
      "Training iteration 110 loss: 0.6913087964057922, ACC:0.53125\n",
      "Training iteration 111 loss: 0.7006415724754333, ACC:0.5\n",
      "Training iteration 112 loss: 0.689017653465271, ACC:0.546875\n",
      "Training iteration 113 loss: 0.7115888595581055, ACC:0.4375\n",
      "Training iteration 114 loss: 0.6923030018806458, ACC:0.5625\n",
      "Training iteration 115 loss: 0.691529393196106, ACC:0.53125\n",
      "Training iteration 116 loss: 0.6887820363044739, ACC:0.546875\n",
      "Training iteration 117 loss: 0.697783887386322, ACC:0.515625\n",
      "Training iteration 118 loss: 0.6767847537994385, ACC:0.59375\n",
      "Training iteration 119 loss: 0.726172149181366, ACC:0.4375\n",
      "Training iteration 120 loss: 0.6959893107414246, ACC:0.5\n",
      "Training iteration 121 loss: 0.6833251118659973, ACC:0.671875\n",
      "Training iteration 122 loss: 0.7070397734642029, ACC:0.53125\n",
      "Training iteration 123 loss: 0.7362765073776245, ACC:0.53125\n",
      "Training iteration 124 loss: 0.8190088868141174, ACC:0.421875\n",
      "Training iteration 125 loss: 0.7102301120758057, ACC:0.5\n",
      "Training iteration 126 loss: 0.691257894039154, ACC:0.53125\n",
      "Training iteration 127 loss: 0.7371611595153809, ACC:0.484375\n",
      "Training iteration 128 loss: 0.7547163367271423, ACC:0.5\n",
      "Training iteration 129 loss: 0.7008400559425354, ACC:0.5625\n",
      "Training iteration 130 loss: 0.7000903487205505, ACC:0.53125\n",
      "Training iteration 131 loss: 0.6991084814071655, ACC:0.453125\n",
      "Training iteration 132 loss: 0.7107862830162048, ACC:0.46875\n",
      "Training iteration 133 loss: 0.7048597931861877, ACC:0.53125\n",
      "Training iteration 134 loss: 0.7010485529899597, ACC:0.546875\n",
      "Training iteration 135 loss: 0.6893318295478821, ACC:0.5625\n",
      "Training iteration 136 loss: 0.6809123158454895, ACC:0.578125\n",
      "Training iteration 137 loss: 0.682864248752594, ACC:0.578125\n",
      "Training iteration 138 loss: 0.6849696040153503, ACC:0.578125\n",
      "Training iteration 139 loss: 0.7053884267807007, ACC:0.4375\n",
      "Training iteration 140 loss: 0.6929267644882202, ACC:0.515625\n",
      "Training iteration 141 loss: 0.6954694390296936, ACC:0.484375\n",
      "Training iteration 142 loss: 0.6931375861167908, ACC:0.515625\n",
      "Training iteration 143 loss: 0.6954079270362854, ACC:0.5\n",
      "Training iteration 144 loss: 0.7000382542610168, ACC:0.4375\n",
      "Training iteration 145 loss: 0.6949795484542847, ACC:0.484375\n",
      "Training iteration 146 loss: 0.6887760162353516, ACC:0.546875\n",
      "Training iteration 147 loss: 0.7271220684051514, ACC:0.40625\n",
      "Training iteration 148 loss: 0.6946790218353271, ACC:0.5\n",
      "Training iteration 149 loss: 0.6904146075248718, ACC:0.546875\n",
      "Training iteration 150 loss: 0.6810351014137268, ACC:0.578125\n",
      "Training iteration 151 loss: 0.6842216849327087, ACC:0.578125\n",
      "Training iteration 152 loss: 0.7094971537590027, ACC:0.546875\n",
      "Training iteration 153 loss: 0.7404839396476746, ACC:0.484375\n",
      "Training iteration 154 loss: 0.6946120262145996, ACC:0.53125\n",
      "Training iteration 155 loss: 0.6916114091873169, ACC:0.625\n",
      "Training iteration 156 loss: 0.7352755665779114, ACC:0.46875\n",
      "Training iteration 157 loss: 0.787918210029602, ACC:0.4375\n",
      "Training iteration 158 loss: 0.7467168569564819, ACC:0.453125\n",
      "Training iteration 159 loss: 0.6913187503814697, ACC:0.53125\n",
      "Training iteration 160 loss: 0.7013660669326782, ACC:0.5\n",
      "Training iteration 161 loss: 0.7137346267700195, ACC:0.515625\n",
      "Training iteration 162 loss: 0.6859440803527832, ACC:0.578125\n",
      "Training iteration 163 loss: 0.7526980638504028, ACC:0.4375\n",
      "Training iteration 164 loss: 0.697222113609314, ACC:0.5\n",
      "Training iteration 165 loss: 0.7065632343292236, ACC:0.4375\n",
      "Training iteration 166 loss: 0.7104312181472778, ACC:0.484375\n",
      "Training iteration 167 loss: 0.7149127721786499, ACC:0.46875\n",
      "Training iteration 168 loss: 0.6822673678398132, ACC:0.59375\n",
      "Training iteration 169 loss: 0.6939516663551331, ACC:0.5\n",
      "Training iteration 170 loss: 0.6938203573226929, ACC:0.40625\n",
      "Training iteration 171 loss: 0.6908157467842102, ACC:0.546875\n",
      "Training iteration 172 loss: 0.6940287947654724, ACC:0.515625\n",
      "Training iteration 173 loss: 0.692437469959259, ACC:0.53125\n",
      "Training iteration 174 loss: 0.7040583491325378, ACC:0.484375\n",
      "Training iteration 175 loss: 0.6955338716506958, ACC:0.5\n",
      "Training iteration 176 loss: 0.6936874985694885, ACC:0.40625\n",
      "Training iteration 177 loss: 0.69376140832901, ACC:0.4375\n",
      "Training iteration 178 loss: 0.6915007829666138, ACC:0.53125\n",
      "Training iteration 179 loss: 0.696799099445343, ACC:0.5\n",
      "Training iteration 180 loss: 0.6977477669715881, ACC:0.5\n",
      "Training iteration 181 loss: 0.689060389995575, ACC:0.546875\n",
      "Training iteration 182 loss: 0.6861680150032043, ACC:0.578125\n",
      "Training iteration 183 loss: 0.6639242768287659, ACC:0.703125\n",
      "Training iteration 184 loss: 0.733113706111908, ACC:0.46875\n",
      "Training iteration 185 loss: 0.7480321526527405, ACC:0.453125\n",
      "Training iteration 186 loss: 0.703347384929657, ACC:0.5\n",
      "Training iteration 187 loss: 0.694292426109314, ACC:0.4375\n",
      "Training iteration 188 loss: 0.6917005777359009, ACC:0.53125\n",
      "Training iteration 189 loss: 0.740738034248352, ACC:0.390625\n",
      "Training iteration 190 loss: 0.6819931268692017, ACC:0.59375\n",
      "Training iteration 191 loss: 0.7002972364425659, ACC:0.421875\n",
      "Training iteration 192 loss: 0.6972816586494446, ACC:0.484375\n",
      "Training iteration 193 loss: 0.7007110118865967, ACC:0.5\n",
      "Training iteration 194 loss: 0.7094506621360779, ACC:0.46875\n",
      "Training iteration 195 loss: 0.6930939555168152, ACC:0.515625\n",
      "Training iteration 196 loss: 0.694311797618866, ACC:0.453125\n",
      "Training iteration 197 loss: 0.6892470121383667, ACC:0.5625\n",
      "Training iteration 198 loss: 0.7090649604797363, ACC:0.4375\n",
      "Training iteration 199 loss: 0.694949746131897, ACC:0.5\n",
      "Training iteration 200 loss: 0.6927095651626587, ACC:0.53125\n",
      "Training iteration 201 loss: 0.6956713199615479, ACC:0.4375\n",
      "Training iteration 202 loss: 0.6940388083457947, ACC:0.46875\n",
      "Training iteration 203 loss: 0.6932245492935181, ACC:0.5\n",
      "Training iteration 204 loss: 0.6933982968330383, ACC:0.484375\n",
      "Training iteration 205 loss: 0.6936096549034119, ACC:0.484375\n",
      "Training iteration 206 loss: 0.6920024752616882, ACC:0.546875\n",
      "Training iteration 207 loss: 0.6955658793449402, ACC:0.484375\n",
      "Training iteration 208 loss: 0.6913610100746155, ACC:0.53125\n",
      "Training iteration 209 loss: 0.6944267749786377, ACC:0.5\n",
      "Training iteration 210 loss: 0.6961185336112976, ACC:0.46875\n",
      "Training iteration 211 loss: 0.6950303316116333, ACC:0.4375\n",
      "Training iteration 212 loss: 0.6935464143753052, ACC:0.46875\n",
      "Training iteration 213 loss: 0.6891737580299377, ACC:0.578125\n",
      "Training iteration 214 loss: 0.6956299543380737, ACC:0.515625\n",
      "Training iteration 215 loss: 0.6903564929962158, ACC:0.546875\n",
      "Training iteration 216 loss: 0.686178982257843, ACC:0.5625\n",
      "Training iteration 217 loss: 0.6861318945884705, ACC:0.5625\n",
      "Training iteration 218 loss: 0.6996480226516724, ACC:0.515625\n",
      "Training iteration 219 loss: 0.7026530504226685, ACC:0.46875\n",
      "Training iteration 220 loss: 0.6942434310913086, ACC:0.484375\n",
      "Training iteration 221 loss: 0.7053260207176208, ACC:0.46875\n",
      "Training iteration 222 loss: 0.6889166831970215, ACC:0.546875\n",
      "Training iteration 223 loss: 0.6958746910095215, ACC:0.515625\n",
      "Training iteration 224 loss: 0.6962948441505432, ACC:0.5\n",
      "Training iteration 225 loss: 0.6921291351318359, ACC:0.53125\n",
      "Training iteration 226 loss: 0.6966675519943237, ACC:0.40625\n",
      "Training iteration 227 loss: 0.6880725622177124, ACC:0.59375\n",
      "Training iteration 228 loss: 0.7063071131706238, ACC:0.484375\n",
      "Training iteration 229 loss: 0.7064880728721619, ACC:0.5\n",
      "Training iteration 230 loss: 0.6734573245048523, ACC:0.609375\n",
      "Training iteration 231 loss: 0.7136614322662354, ACC:0.453125\n",
      "Training iteration 232 loss: 0.6913530230522156, ACC:0.53125\n",
      "Training iteration 233 loss: 0.6934544444084167, ACC:0.5\n",
      "Training iteration 234 loss: 0.7010008096694946, ACC:0.46875\n",
      "Training iteration 235 loss: 0.691218912601471, ACC:0.53125\n",
      "Training iteration 236 loss: 0.6949474215507507, ACC:0.5\n",
      "Training iteration 237 loss: 0.6952399015426636, ACC:0.46875\n",
      "Training iteration 238 loss: 0.6879907250404358, ACC:0.578125\n",
      "Training iteration 239 loss: 0.6899339556694031, ACC:0.546875\n",
      "Training iteration 240 loss: 0.7579959630966187, ACC:0.40625\n",
      "Training iteration 241 loss: 0.6932958364486694, ACC:0.53125\n",
      "Training iteration 242 loss: 0.6883749961853027, ACC:0.625\n",
      "Training iteration 243 loss: 0.694119930267334, ACC:0.46875\n",
      "Training iteration 244 loss: 0.6974337100982666, ACC:0.40625\n",
      "Training iteration 245 loss: 0.6926609873771667, ACC:0.515625\n",
      "Training iteration 246 loss: 0.6983866691589355, ACC:0.484375\n",
      "Training iteration 247 loss: 0.6842114329338074, ACC:0.578125\n",
      "Training iteration 248 loss: 0.7122622728347778, ACC:0.4375\n",
      "Training iteration 249 loss: 0.6940999031066895, ACC:0.5\n",
      "Training iteration 250 loss: 0.6897662878036499, ACC:0.5625\n",
      "Training iteration 251 loss: 0.7065244913101196, ACC:0.484375\n",
      "Training iteration 252 loss: 0.6757960915565491, ACC:0.59375\n",
      "Training iteration 253 loss: 0.7563585042953491, ACC:0.390625\n",
      "Training iteration 254 loss: 0.6809121966362, ACC:0.609375\n",
      "Training iteration 255 loss: 0.692537784576416, ACC:0.546875\n",
      "Training iteration 256 loss: 0.6954402327537537, ACC:0.40625\n",
      "Training iteration 257 loss: 0.694434404373169, ACC:0.5\n",
      "Training iteration 258 loss: 0.686053991317749, ACC:0.5625\n",
      "Training iteration 259 loss: 0.7106333374977112, ACC:0.46875\n",
      "Training iteration 260 loss: 0.688761293888092, ACC:0.546875\n",
      "Training iteration 261 loss: 0.6911972761154175, ACC:0.53125\n",
      "Training iteration 262 loss: 0.6947737336158752, ACC:0.484375\n",
      "Training iteration 263 loss: 0.6940162181854248, ACC:0.484375\n",
      "Training iteration 264 loss: 0.7005101442337036, ACC:0.4375\n",
      "Training iteration 265 loss: 0.6939412355422974, ACC:0.421875\n",
      "Training iteration 266 loss: 0.6949736475944519, ACC:0.515625\n",
      "Training iteration 267 loss: 0.7172969579696655, ACC:0.46875\n",
      "Training iteration 268 loss: 0.7191888689994812, ACC:0.4375\n",
      "Training iteration 269 loss: 0.693371593952179, ACC:0.46875\n",
      "Training iteration 270 loss: 0.7120139598846436, ACC:0.46875\n",
      "Training iteration 271 loss: 0.711256742477417, ACC:0.5\n",
      "Training iteration 272 loss: 0.7207621932029724, ACC:0.453125\n",
      "Training iteration 273 loss: 0.6917152404785156, ACC:0.53125\n",
      "Training iteration 274 loss: 0.6706404089927673, ACC:0.671875\n",
      "Training iteration 275 loss: 0.6961053609848022, ACC:0.5625\n",
      "Training iteration 276 loss: 0.8580089807510376, ACC:0.390625\n",
      "Training iteration 277 loss: 0.7295842170715332, ACC:0.5\n",
      "Training iteration 278 loss: 0.6848381757736206, ACC:0.59375\n",
      "Training iteration 279 loss: 0.7090243697166443, ACC:0.4375\n",
      "Training iteration 280 loss: 0.7107948660850525, ACC:0.46875\n",
      "Training iteration 281 loss: 0.6887474060058594, ACC:0.546875\n",
      "Training iteration 282 loss: 0.6911994218826294, ACC:0.53125\n",
      "Training iteration 283 loss: 0.6935206651687622, ACC:0.5\n",
      "Training iteration 284 loss: 0.6954800486564636, ACC:0.40625\n",
      "Training iteration 285 loss: 0.6896310448646545, ACC:0.5625\n",
      "Training iteration 286 loss: 0.6991150975227356, ACC:0.5\n",
      "Training iteration 287 loss: 0.6895813941955566, ACC:0.546875\n",
      "Training iteration 288 loss: 0.7076480388641357, ACC:0.484375\n",
      "Training iteration 289 loss: 0.6740024089813232, ACC:0.640625\n",
      "Training iteration 290 loss: 0.7017577290534973, ACC:0.484375\n",
      "Training iteration 291 loss: 0.7069350481033325, ACC:0.421875\n",
      "Training iteration 292 loss: 0.6867536306381226, ACC:0.578125\n",
      "Training iteration 293 loss: 0.7130107879638672, ACC:0.5\n",
      "Training iteration 294 loss: 0.7660303711891174, ACC:0.421875\n",
      "Training iteration 295 loss: 0.6991027593612671, ACC:0.515625\n",
      "Training iteration 296 loss: 0.6931802034378052, ACC:0.484375\n",
      "Training iteration 297 loss: 0.674106240272522, ACC:0.609375\n",
      "Training iteration 298 loss: 0.7699360251426697, ACC:0.421875\n",
      "Training iteration 299 loss: 0.7197304368019104, ACC:0.5\n",
      "Training iteration 300 loss: 0.7024815082550049, ACC:0.484375\n",
      "Training iteration 301 loss: 0.6858237385749817, ACC:0.578125\n",
      "Training iteration 302 loss: 0.7773480415344238, ACC:0.390625\n",
      "Training iteration 303 loss: 0.7345685362815857, ACC:0.46875\n",
      "Training iteration 304 loss: 0.7115339636802673, ACC:0.4375\n",
      "Training iteration 305 loss: 0.7016957998275757, ACC:0.484375\n",
      "Training iteration 306 loss: 0.6912717819213867, ACC:0.5625\n",
      "Training iteration 307 loss: 0.7414600849151611, ACC:0.5\n",
      "Training iteration 308 loss: 0.6860964298248291, ACC:0.578125\n",
      "Training iteration 309 loss: 0.7248072028160095, ACC:0.453125\n",
      "Training iteration 310 loss: 0.6926091909408569, ACC:0.53125\n",
      "Training iteration 311 loss: 0.7000669836997986, ACC:0.53125\n",
      "Training iteration 312 loss: 0.7412620186805725, ACC:0.5\n",
      "Training iteration 313 loss: 0.680837094783783, ACC:0.59375\n",
      "Training iteration 314 loss: 0.7294503450393677, ACC:0.484375\n",
      "Training iteration 315 loss: 0.6835054755210876, ACC:0.578125\n",
      "Training iteration 316 loss: 0.6987098455429077, ACC:0.40625\n",
      "Training iteration 317 loss: 0.6913920640945435, ACC:0.53125\n",
      "Training iteration 318 loss: 0.7032009959220886, ACC:0.4375\n",
      "Training iteration 319 loss: 0.693418025970459, ACC:0.484375\n",
      "Training iteration 320 loss: 0.6857032775878906, ACC:0.578125\n",
      "Training iteration 321 loss: 0.7055087685585022, ACC:0.5\n",
      "Training iteration 322 loss: 0.675480842590332, ACC:0.59375\n",
      "Training iteration 323 loss: 0.7150295972824097, ACC:0.5\n",
      "Training iteration 324 loss: 0.6856123805046082, ACC:0.5625\n",
      "Training iteration 325 loss: 0.7023389935493469, ACC:0.46875\n",
      "Training iteration 326 loss: 0.6898215413093567, ACC:0.5625\n",
      "Training iteration 327 loss: 0.6918857097625732, ACC:0.546875\n",
      "Training iteration 328 loss: 0.687151312828064, ACC:0.578125\n",
      "Training iteration 329 loss: 0.7520744800567627, ACC:0.484375\n",
      "Training iteration 330 loss: 0.682774543762207, ACC:0.578125\n",
      "Training iteration 331 loss: 0.6854459643363953, ACC:0.5625\n",
      "Training iteration 332 loss: 0.6963093876838684, ACC:0.40625\n",
      "Training iteration 333 loss: 0.6949601173400879, ACC:0.53125\n",
      "Training iteration 334 loss: 0.7194528579711914, ACC:0.515625\n",
      "Training iteration 335 loss: 0.7566447854042053, ACC:0.453125\n",
      "Training iteration 336 loss: 0.7248678207397461, ACC:0.421875\n",
      "Training iteration 337 loss: 0.7076554298400879, ACC:0.453125\n",
      "Training iteration 338 loss: 0.6766530871391296, ACC:0.59375\n",
      "Training iteration 339 loss: 0.7732475996017456, ACC:0.46875\n",
      "Training iteration 340 loss: 0.741169810295105, ACC:0.484375\n",
      "Training iteration 341 loss: 0.679218053817749, ACC:0.59375\n",
      "Training iteration 342 loss: 0.6918754577636719, ACC:0.53125\n",
      "Training iteration 343 loss: 0.7148694396018982, ACC:0.46875\n",
      "Training iteration 344 loss: 0.6523488163948059, ACC:0.65625\n",
      "Training iteration 345 loss: 0.7275636792182922, ACC:0.5\n",
      "Training iteration 346 loss: 0.6835076212882996, ACC:0.578125\n",
      "Training iteration 347 loss: 0.6972358822822571, ACC:0.53125\n",
      "Training iteration 348 loss: 0.7009224891662598, ACC:0.46875\n",
      "Training iteration 349 loss: 0.6932494044303894, ACC:0.515625\n",
      "Training iteration 350 loss: 0.709320604801178, ACC:0.5\n",
      "Training iteration 351 loss: 0.7418249845504761, ACC:0.4375\n",
      "Training iteration 352 loss: 0.6953976154327393, ACC:0.515625\n",
      "Training iteration 353 loss: 0.6952233910560608, ACC:0.453125\n",
      "Training iteration 354 loss: 0.7078153491020203, ACC:0.4375\n",
      "Training iteration 355 loss: 0.7028106451034546, ACC:0.4375\n",
      "Training iteration 356 loss: 0.691544234752655, ACC:0.53125\n",
      "Training iteration 357 loss: 0.7233247756958008, ACC:0.421875\n",
      "Training iteration 358 loss: 0.6921651363372803, ACC:0.53125\n",
      "Training iteration 359 loss: 0.7011456489562988, ACC:0.453125\n",
      "Training iteration 360 loss: 0.7047790288925171, ACC:0.359375\n",
      "Training iteration 361 loss: 0.6932514905929565, ACC:0.46875\n",
      "Training iteration 362 loss: 0.6930059790611267, ACC:0.515625\n",
      "Training iteration 363 loss: 0.7013146281242371, ACC:0.484375\n",
      "Training iteration 364 loss: 0.6914607882499695, ACC:0.53125\n",
      "Training iteration 365 loss: 0.699105978012085, ACC:0.46875\n",
      "Training iteration 366 loss: 0.692194938659668, ACC:0.5625\n",
      "Training iteration 367 loss: 0.6778692603111267, ACC:0.59375\n",
      "Training iteration 368 loss: 0.6925593018531799, ACC:0.5625\n",
      "Training iteration 369 loss: 0.8308488130569458, ACC:0.359375\n",
      "Training iteration 370 loss: 0.6941245198249817, ACC:0.53125\n",
      "Training iteration 371 loss: 0.7006115913391113, ACC:0.4375\n",
      "Training iteration 372 loss: 0.6949474215507507, ACC:0.53125\n",
      "Training iteration 373 loss: 0.7139669060707092, ACC:0.5\n",
      "Training iteration 374 loss: 0.7296005487442017, ACC:0.4375\n",
      "Training iteration 375 loss: 0.6911512613296509, ACC:0.546875\n",
      "Training iteration 376 loss: 0.7091048955917358, ACC:0.421875\n",
      "Training iteration 377 loss: 0.7064041495323181, ACC:0.4375\n",
      "Training iteration 378 loss: 0.692946195602417, ACC:0.53125\n",
      "Training iteration 379 loss: 0.6951908469200134, ACC:0.515625\n",
      "Training iteration 380 loss: 0.7166099548339844, ACC:0.46875\n",
      "Training iteration 381 loss: 0.6972286105155945, ACC:0.515625\n",
      "Training iteration 382 loss: 0.6789639592170715, ACC:0.640625\n",
      "Training iteration 383 loss: 0.6809985637664795, ACC:0.59375\n",
      "Training iteration 384 loss: 0.6996804475784302, ACC:0.515625\n",
      "Training iteration 385 loss: 0.7367907762527466, ACC:0.40625\n",
      "Training iteration 386 loss: 0.6936812400817871, ACC:0.5\n",
      "Training iteration 387 loss: 0.7100940942764282, ACC:0.4375\n",
      "Training iteration 388 loss: 0.6970101594924927, ACC:0.515625\n",
      "Training iteration 389 loss: 0.6958742737770081, ACC:0.515625\n",
      "Training iteration 390 loss: 0.6996234655380249, ACC:0.46875\n",
      "Training iteration 391 loss: 0.6918915510177612, ACC:0.53125\n",
      "Training iteration 392 loss: 0.7007720470428467, ACC:0.5\n",
      "Training iteration 393 loss: 0.7316352725028992, ACC:0.421875\n",
      "Training iteration 394 loss: 0.689072847366333, ACC:0.546875\n",
      "Training iteration 395 loss: 0.6922398209571838, ACC:0.578125\n",
      "Training iteration 396 loss: 0.7020876407623291, ACC:0.5\n",
      "Training iteration 397 loss: 0.7364281415939331, ACC:0.4375\n",
      "Training iteration 398 loss: 0.6966655850410461, ACC:0.515625\n",
      "Training iteration 399 loss: 0.6916202306747437, ACC:0.546875\n",
      "Training iteration 400 loss: 0.6941100358963013, ACC:0.5\n",
      "Training iteration 401 loss: 0.711826741695404, ACC:0.40625\n",
      "Training iteration 402 loss: 0.6933671236038208, ACC:0.484375\n",
      "Training iteration 403 loss: 0.6985014081001282, ACC:0.484375\n",
      "Training iteration 404 loss: 0.7158037424087524, ACC:0.421875\n",
      "Training iteration 405 loss: 0.6935479044914246, ACC:0.5\n",
      "Training iteration 406 loss: 0.7032257318496704, ACC:0.421875\n",
      "Training iteration 407 loss: 0.6886396408081055, ACC:0.5625\n",
      "Training iteration 408 loss: 0.6932893991470337, ACC:0.515625\n",
      "Training iteration 409 loss: 0.7049636840820312, ACC:0.4375\n",
      "Training iteration 410 loss: 0.6929614543914795, ACC:0.546875\n",
      "Training iteration 411 loss: 0.6888317465782166, ACC:0.546875\n",
      "Training iteration 412 loss: 0.6939175724983215, ACC:0.546875\n",
      "Training iteration 413 loss: 0.7723877429962158, ACC:0.390625\n",
      "Training iteration 414 loss: 0.6860266327857971, ACC:0.5625\n",
      "Training iteration 415 loss: 0.6917010545730591, ACC:0.53125\n",
      "Training iteration 416 loss: 0.7043929100036621, ACC:0.5\n",
      "Training iteration 417 loss: 0.7124212980270386, ACC:0.5\n",
      "Training iteration 418 loss: 0.6808918118476868, ACC:0.578125\n",
      "Training iteration 419 loss: 0.7204670310020447, ACC:0.421875\n",
      "Training iteration 420 loss: 0.6946635246276855, ACC:0.46875\n",
      "Training iteration 421 loss: 0.7000952363014221, ACC:0.5\n",
      "Training iteration 422 loss: 0.7153358459472656, ACC:0.46875\n",
      "Training iteration 423 loss: 0.6887900233268738, ACC:0.546875\n",
      "Training iteration 424 loss: 0.6960370540618896, ACC:0.484375\n",
      "Training iteration 425 loss: 0.6935974359512329, ACC:0.5\n",
      "Training iteration 426 loss: 0.6944159865379333, ACC:0.515625\n",
      "Training iteration 427 loss: 0.6966629028320312, ACC:0.515625\n",
      "Training iteration 428 loss: 0.6958752870559692, ACC:0.515625\n",
      "Training iteration 429 loss: 0.6846269965171814, ACC:0.578125\n",
      "Training iteration 430 loss: 0.686923623085022, ACC:0.5625\n",
      "Training iteration 431 loss: 0.7000395059585571, ACC:0.484375\n",
      "Training iteration 432 loss: 0.6968059539794922, ACC:0.484375\n",
      "Training iteration 433 loss: 0.6936416029930115, ACC:0.453125\n",
      "Training iteration 434 loss: 0.6991158723831177, ACC:0.375\n",
      "Training iteration 435 loss: 0.6981715559959412, ACC:0.484375\n",
      "Training iteration 436 loss: 0.7037044763565063, ACC:0.484375\n",
      "Training iteration 437 loss: 0.6827690005302429, ACC:0.578125\n",
      "Training iteration 438 loss: 0.7298638224601746, ACC:0.34375\n",
      "Training iteration 439 loss: 0.7012338042259216, ACC:0.453125\n",
      "Training iteration 440 loss: 0.7179503440856934, ACC:0.453125\n",
      "Training iteration 441 loss: 0.6891979575157166, ACC:0.546875\n",
      "Training iteration 442 loss: 0.7085450291633606, ACC:0.4375\n",
      "Training iteration 443 loss: 0.6935585737228394, ACC:0.5\n",
      "Training iteration 444 loss: 0.6891961097717285, ACC:0.546875\n",
      "Training iteration 445 loss: 0.7311549186706543, ACC:0.453125\n",
      "Training iteration 446 loss: 0.699073314666748, ACC:0.515625\n",
      "Training iteration 447 loss: 0.6912649273872375, ACC:0.53125\n",
      "Training iteration 448 loss: 0.6954184174537659, ACC:0.46875\n",
      "Training iteration 449 loss: 0.6951566338539124, ACC:0.5\n",
      "Training iteration 450 loss: 0.6827284097671509, ACC:0.59375\n",
      "Validation iteration 451 loss: 0.69291090965271, ACC: 0.53125\n",
      "Validation iteration 452 loss: 0.7080865502357483, ACC: 0.46875\n",
      "Validation iteration 453 loss: 0.7232622504234314, ACC: 0.40625\n",
      "Validation iteration 454 loss: 0.7042926549911499, ACC: 0.484375\n",
      "Validation iteration 455 loss: 0.6739412546157837, ACC: 0.609375\n",
      "Validation iteration 456 loss: 0.7080866098403931, ACC: 0.46875\n",
      "Validation iteration 457 loss: 0.69291090965271, ACC: 0.53125\n",
      "Validation iteration 458 loss: 0.7042927742004395, ACC: 0.484375\n",
      "Validation iteration 459 loss: 0.7156744599342346, ACC: 0.4375\n",
      "Validation iteration 460 loss: 0.7004987597465515, ACC: 0.5\n",
      "Validation iteration 461 loss: 0.6853230595588684, ACC: 0.5625\n",
      "Validation iteration 462 loss: 0.7080866098403931, ACC: 0.46875\n",
      "Validation iteration 463 loss: 0.7004987597465515, ACC: 0.5\n",
      "Validation iteration 464 loss: 0.6739412546157837, ACC: 0.609375\n",
      "Validation iteration 465 loss: 0.6853230595588684, ACC: 0.5625\n",
      "Validation iteration 466 loss: 0.7384379506111145, ACC: 0.34375\n",
      "Validation iteration 467 loss: 0.7118805050849915, ACC: 0.453125\n",
      "Validation iteration 468 loss: 0.6891169548034668, ACC: 0.546875\n",
      "Validation iteration 469 loss: 0.7384379506111145, ACC: 0.34375\n",
      "Validation iteration 470 loss: 0.6967048048973083, ACC: 0.515625\n",
      "Validation iteration 471 loss: 0.6777352094650269, ACC: 0.59375\n",
      "Validation iteration 472 loss: 0.7156744003295898, ACC: 0.4375\n",
      "Validation iteration 473 loss: 0.7156744599342346, ACC: 0.4375\n",
      "Validation iteration 474 loss: 0.68152916431427, ACC: 0.578125\n",
      "Validation iteration 475 loss: 0.6967048048973083, ACC: 0.515625\n",
      "Validation iteration 476 loss: 0.6587656736373901, ACC: 0.671875\n",
      "Validation iteration 477 loss: 0.6929108500480652, ACC: 0.53125\n",
      "Validation iteration 478 loss: 0.7080866098403931, ACC: 0.46875\n",
      "Validation iteration 479 loss: 0.7080866098403931, ACC: 0.46875\n",
      "Validation iteration 480 loss: 0.719468355178833, ACC: 0.421875\n",
      "Validation iteration 481 loss: 0.7042926549911499, ACC: 0.484375\n",
      "Validation iteration 482 loss: 0.6891169548034668, ACC: 0.546875\n",
      "Validation iteration 483 loss: 0.7042927145957947, ACC: 0.484375\n",
      "Validation iteration 484 loss: 0.69291090965271, ACC: 0.53125\n",
      "Validation iteration 485 loss: 0.7118805050849915, ACC: 0.453125\n",
      "Validation iteration 486 loss: 0.7042926549911499, ACC: 0.484375\n",
      "Validation iteration 487 loss: 0.7004988193511963, ACC: 0.5\n",
      "Validation iteration 488 loss: 0.7080866098403931, ACC: 0.46875\n",
      "Validation iteration 489 loss: 0.6967048048973083, ACC: 0.515625\n",
      "Validation iteration 490 loss: 0.6815291047096252, ACC: 0.578125\n",
      "Validation iteration 491 loss: 0.7004987001419067, ACC: 0.5\n",
      "Validation iteration 492 loss: 0.6853230595588684, ACC: 0.5625\n",
      "Validation iteration 493 loss: 0.7042926549911499, ACC: 0.484375\n",
      "Validation iteration 494 loss: 0.6663534045219421, ACC: 0.640625\n",
      "Validation iteration 495 loss: 0.6739413142204285, ACC: 0.609375\n",
      "Validation iteration 496 loss: 0.6967048048973083, ACC: 0.515625\n",
      "Validation iteration 497 loss: 0.7080865502357483, ACC: 0.46875\n",
      "Validation iteration 498 loss: 0.6929108500480652, ACC: 0.53125\n",
      "Validation iteration 499 loss: 0.6891169548034668, ACC: 0.546875\n",
      "Validation iteration 500 loss: 0.7156744003295898, ACC: 0.4375\n",
      "-- Epoch 2 done -- Train loss: 0.7019815393288931, train ACC: 0.5034375, val loss: 0.6990570521354675, val ACC: 0.5059375\n",
      "<--- 8993.927365541458 seconds --->\n",
      "Training iteration 1 loss: 0.7118804454803467, ACC:0.453125\n",
      "Training iteration 2 loss: 0.7014515995979309, ACC:0.46875\n",
      "Training iteration 3 loss: 0.6956262588500977, ACC:0.421875\n",
      "Training iteration 4 loss: 0.698363184928894, ACC:0.421875\n",
      "Training iteration 5 loss: 0.6864367127418518, ACC:0.609375\n",
      "Training iteration 6 loss: 0.7175547480583191, ACC:0.46875\n",
      "Training iteration 7 loss: 0.6996583938598633, ACC:0.53125\n",
      "Training iteration 8 loss: 0.6706796288490295, ACC:0.609375\n",
      "Training iteration 9 loss: 0.665870189666748, ACC:0.625\n",
      "Training iteration 10 loss: 0.6876667141914368, ACC:0.5625\n",
      "Training iteration 11 loss: 0.7250606417655945, ACC:0.46875\n",
      "Training iteration 12 loss: 0.6858122944831848, ACC:0.5625\n",
      "Training iteration 13 loss: 0.6940395832061768, ACC:0.46875\n",
      "Training iteration 14 loss: 0.677330493927002, ACC:0.609375\n",
      "Training iteration 15 loss: 0.7362005710601807, ACC:0.46875\n",
      "Training iteration 16 loss: 0.684779942035675, ACC:0.578125\n",
      "Training iteration 17 loss: 0.7307634949684143, ACC:0.46875\n",
      "Training iteration 18 loss: 0.6839492917060852, ACC:0.578125\n",
      "Training iteration 19 loss: 0.6942644715309143, ACC:0.46875\n",
      "Training iteration 20 loss: 0.7168348431587219, ACC:0.34375\n",
      "Training iteration 21 loss: 0.7007457613945007, ACC:0.421875\n",
      "Training iteration 22 loss: 0.689318835735321, ACC:0.546875\n",
      "Training iteration 23 loss: 0.6999664306640625, ACC:0.484375\n",
      "Training iteration 24 loss: 0.6788012981414795, ACC:0.625\n",
      "Training iteration 25 loss: 0.6815261244773865, ACC:0.578125\n",
      "Training iteration 26 loss: 0.7103634476661682, ACC:0.5\n",
      "Training iteration 27 loss: 0.7333104014396667, ACC:0.421875\n",
      "Training iteration 28 loss: 0.6927627921104431, ACC:0.515625\n",
      "Training iteration 29 loss: 0.6924532055854797, ACC:0.53125\n",
      "Training iteration 30 loss: 0.6952388286590576, ACC:0.546875\n",
      "Training iteration 31 loss: 0.7082163095474243, ACC:0.53125\n",
      "Training iteration 32 loss: 0.7147983908653259, ACC:0.5\n",
      "Training iteration 33 loss: 0.6806042194366455, ACC:0.59375\n",
      "Training iteration 34 loss: 0.6927282810211182, ACC:0.515625\n",
      "Training iteration 35 loss: 0.7040912508964539, ACC:0.359375\n",
      "Training iteration 36 loss: 0.7027606964111328, ACC:0.40625\n",
      "Training iteration 37 loss: 0.6912950277328491, ACC:0.546875\n",
      "Training iteration 38 loss: 0.6936355233192444, ACC:0.5\n",
      "Training iteration 39 loss: 0.6943060755729675, ACC:0.484375\n",
      "Training iteration 40 loss: 0.6931502819061279, ACC:0.5\n",
      "Training iteration 41 loss: 0.6952158212661743, ACC:0.46875\n",
      "Training iteration 42 loss: 0.693590521812439, ACC:0.484375\n",
      "Training iteration 43 loss: 0.6945914626121521, ACC:0.46875\n",
      "Training iteration 44 loss: 0.6954405307769775, ACC:0.421875\n",
      "Training iteration 45 loss: 0.6872755289077759, ACC:0.5625\n",
      "Training iteration 46 loss: 0.730255126953125, ACC:0.421875\n",
      "Training iteration 47 loss: 0.6889141201972961, ACC:0.546875\n",
      "Training iteration 48 loss: 0.7011616230010986, ACC:0.453125\n",
      "Training iteration 49 loss: 0.6927686929702759, ACC:0.515625\n",
      "Training iteration 50 loss: 0.7114346027374268, ACC:0.46875\n",
      "Training iteration 51 loss: 0.7026562690734863, ACC:0.5\n",
      "Training iteration 52 loss: 0.7063009142875671, ACC:0.4375\n",
      "Training iteration 53 loss: 0.689365804195404, ACC:0.546875\n",
      "Training iteration 54 loss: 0.6754992604255676, ACC:0.59375\n",
      "Training iteration 55 loss: 0.7636605501174927, ACC:0.46875\n",
      "Training iteration 56 loss: 0.7558342218399048, ACC:0.453125\n",
      "Training iteration 57 loss: 0.6913530230522156, ACC:0.53125\n",
      "Training iteration 58 loss: 0.7108736038208008, ACC:0.4375\n",
      "Training iteration 59 loss: 0.6915692687034607, ACC:0.546875\n",
      "Training iteration 60 loss: 0.7068214416503906, ACC:0.515625\n",
      "Training iteration 61 loss: 0.7253264784812927, ACC:0.4375\n",
      "Training iteration 62 loss: 0.6931486129760742, ACC:0.5\n",
      "Training iteration 63 loss: 0.6941509246826172, ACC:0.53125\n",
      "Training iteration 64 loss: 0.749931275844574, ACC:0.4375\n",
      "Training iteration 65 loss: 0.6863414645195007, ACC:0.5625\n",
      "Training iteration 66 loss: 0.6917240023612976, ACC:0.53125\n",
      "Training iteration 67 loss: 0.6936380863189697, ACC:0.484375\n",
      "Training iteration 68 loss: 0.6913385987281799, ACC:0.53125\n",
      "Training iteration 69 loss: 0.7003948092460632, ACC:0.515625\n",
      "Training iteration 70 loss: 0.7295852899551392, ACC:0.4375\n",
      "Training iteration 71 loss: 0.6931679844856262, ACC:0.515625\n",
      "Training iteration 72 loss: 0.694341242313385, ACC:0.5\n",
      "Training iteration 73 loss: 0.7014762759208679, ACC:0.5\n",
      "Training iteration 74 loss: 0.740829586982727, ACC:0.375\n",
      "Training iteration 75 loss: 0.6936180591583252, ACC:0.421875\n",
      "Training iteration 76 loss: 0.6890004277229309, ACC:0.546875\n",
      "Training iteration 77 loss: 0.7433321475982666, ACC:0.359375\n",
      "Training iteration 78 loss: 0.6890715956687927, ACC:0.578125\n",
      "Training iteration 79 loss: 0.6927210092544556, ACC:0.515625\n",
      "Training iteration 80 loss: 0.6976606845855713, ACC:0.484375\n",
      "Training iteration 81 loss: 0.6912245750427246, ACC:0.53125\n",
      "Training iteration 82 loss: 0.6740044951438904, ACC:0.65625\n",
      "Training iteration 83 loss: 0.7075080871582031, ACC:0.5\n",
      "Training iteration 84 loss: 0.7526524662971497, ACC:0.390625\n",
      "Training iteration 85 loss: 0.6868448257446289, ACC:0.59375\n",
      "Training iteration 86 loss: 0.6961462497711182, ACC:0.46875\n",
      "Training iteration 87 loss: 0.7086704969406128, ACC:0.390625\n",
      "Training iteration 88 loss: 0.6905832886695862, ACC:0.546875\n",
      "Training iteration 89 loss: 0.7216379046440125, ACC:0.4375\n",
      "Training iteration 90 loss: 0.6935765743255615, ACC:0.53125\n",
      "Training iteration 91 loss: 0.7059240937232971, ACC:0.453125\n",
      "Training iteration 92 loss: 0.6898841857910156, ACC:0.578125\n",
      "Training iteration 93 loss: 0.7152028679847717, ACC:0.484375\n",
      "Training iteration 94 loss: 0.6617417335510254, ACC:0.625\n",
      "Training iteration 95 loss: 0.7353789210319519, ACC:0.5\n",
      "Training iteration 96 loss: 0.7740201354026794, ACC:0.375\n",
      "Training iteration 97 loss: 0.6937519311904907, ACC:0.5\n",
      "Training iteration 98 loss: 0.7090529799461365, ACC:0.53125\n",
      "Training iteration 99 loss: 0.8306035995483398, ACC:0.40625\n",
      "Training iteration 100 loss: 0.7638082504272461, ACC:0.421875\n",
      "Training iteration 101 loss: 0.6949570178985596, ACC:0.46875\n",
      "Training iteration 102 loss: 0.7060878276824951, ACC:0.53125\n",
      "Training iteration 103 loss: 0.7109224200248718, ACC:0.5625\n",
      "Training iteration 104 loss: 0.6914830207824707, ACC:0.59375\n",
      "Training iteration 105 loss: 0.7233330011367798, ACC:0.53125\n",
      "Training iteration 106 loss: 0.7174511551856995, ACC:0.46875\n",
      "Training iteration 107 loss: 0.7095271944999695, ACC:0.390625\n",
      "Training iteration 108 loss: 0.719821572303772, ACC:0.453125\n",
      "Training iteration 109 loss: 0.7355365753173828, ACC:0.375\n",
      "Training iteration 110 loss: 0.6927525401115417, ACC:0.515625\n",
      "Training iteration 111 loss: 0.7318603992462158, ACC:0.453125\n",
      "Training iteration 112 loss: 0.7306122779846191, ACC:0.46875\n",
      "Training iteration 113 loss: 0.7116369605064392, ACC:0.453125\n",
      "Training iteration 114 loss: 0.6858488917350769, ACC:0.578125\n",
      "Training iteration 115 loss: 0.6857659816741943, ACC:0.578125\n",
      "Training iteration 116 loss: 0.7083956599235535, ACC:0.578125\n",
      "Training iteration 117 loss: 0.8112941384315491, ACC:0.46875\n",
      "Training iteration 118 loss: 0.6868718862533569, ACC:0.578125\n",
      "Training iteration 119 loss: 0.7153587341308594, ACC:0.375\n",
      "Training iteration 120 loss: 0.726793646812439, ACC:0.484375\n",
      "Training iteration 121 loss: 0.784060001373291, ACC:0.484375\n",
      "Training iteration 122 loss: 0.7770466208457947, ACC:0.484375\n",
      "Training iteration 123 loss: 0.7203245759010315, ACC:0.484375\n",
      "Training iteration 124 loss: 0.6994650363922119, ACC:0.453125\n",
      "Training iteration 125 loss: 0.7460013031959534, ACC:0.4375\n",
      "Training iteration 126 loss: 0.7501790523529053, ACC:0.4375\n",
      "Training iteration 127 loss: 0.7012717723846436, ACC:0.484375\n",
      "Training iteration 128 loss: 0.6912826895713806, ACC:0.53125\n",
      "Training iteration 129 loss: 0.7120069861412048, ACC:0.515625\n",
      "Training iteration 130 loss: 0.742789089679718, ACC:0.484375\n",
      "Training iteration 131 loss: 0.7219840288162231, ACC:0.484375\n",
      "Training iteration 132 loss: 0.6927552819252014, ACC:0.515625\n",
      "Training iteration 133 loss: 0.6888800859451294, ACC:0.546875\n",
      "Training iteration 134 loss: 0.7378592491149902, ACC:0.46875\n",
      "Training iteration 135 loss: 0.7216597199440002, ACC:0.5\n",
      "Training iteration 136 loss: 0.6984929442405701, ACC:0.515625\n",
      "Training iteration 137 loss: 0.6922228336334229, ACC:0.59375\n",
      "Training iteration 138 loss: 0.682440996170044, ACC:0.640625\n",
      "Training iteration 139 loss: 0.7038242220878601, ACC:0.515625\n",
      "Training iteration 140 loss: 0.7617866396903992, ACC:0.421875\n",
      "Training iteration 141 loss: 0.6943144798278809, ACC:0.53125\n",
      "Training iteration 142 loss: 0.6913406252861023, ACC:0.5625\n",
      "Training iteration 143 loss: 0.6845483779907227, ACC:0.59375\n",
      "Training iteration 144 loss: 0.6863058805465698, ACC:0.5625\n",
      "Training iteration 145 loss: 0.7180138826370239, ACC:0.515625\n",
      "Training iteration 146 loss: 0.7460629940032959, ACC:0.453125\n",
      "Training iteration 147 loss: 0.7039200067520142, ACC:0.46875\n",
      "Training iteration 148 loss: 0.7047440409660339, ACC:0.46875\n",
      "Training iteration 149 loss: 0.7225590944290161, ACC:0.484375\n",
      "Training iteration 150 loss: 0.7026635408401489, ACC:0.53125\n",
      "Training iteration 151 loss: 0.7147363424301147, ACC:0.46875\n",
      "Training iteration 152 loss: 0.6921319365501404, ACC:0.546875\n",
      "Training iteration 153 loss: 0.7070820331573486, ACC:0.4375\n",
      "Training iteration 154 loss: 0.7122688889503479, ACC:0.40625\n",
      "Training iteration 155 loss: 0.6917886734008789, ACC:0.53125\n",
      "Training iteration 156 loss: 0.7077349424362183, ACC:0.484375\n",
      "Training iteration 157 loss: 0.7246748208999634, ACC:0.453125\n",
      "Training iteration 158 loss: 0.6887463331222534, ACC:0.546875\n",
      "Training iteration 159 loss: 0.6940217614173889, ACC:0.484375\n",
      "Training iteration 160 loss: 0.6829429864883423, ACC:0.59375\n",
      "Training iteration 161 loss: 0.6997870206832886, ACC:0.53125\n",
      "Training iteration 162 loss: 0.7578305602073669, ACC:0.4375\n",
      "Training iteration 163 loss: 0.710464358329773, ACC:0.484375\n",
      "Training iteration 164 loss: 0.6929144263267517, ACC:0.515625\n",
      "Training iteration 165 loss: 0.6966726779937744, ACC:0.53125\n",
      "Training iteration 166 loss: 0.7706620097160339, ACC:0.421875\n",
      "Training iteration 167 loss: 0.72794508934021, ACC:0.453125\n",
      "Training iteration 168 loss: 0.6930652856826782, ACC:0.53125\n",
      "Training iteration 169 loss: 0.724411129951477, ACC:0.46875\n",
      "Training iteration 170 loss: 0.7030861377716064, ACC:0.546875\n",
      "Training iteration 171 loss: 0.7834941744804382, ACC:0.390625\n",
      "Training iteration 172 loss: 0.6911935806274414, ACC:0.53125\n",
      "Training iteration 173 loss: 0.6926730275154114, ACC:0.53125\n",
      "Training iteration 174 loss: 0.7414763569831848, ACC:0.46875\n",
      "Training iteration 175 loss: 0.6851937174797058, ACC:0.578125\n",
      "Training iteration 176 loss: 0.7417815923690796, ACC:0.4375\n",
      "Training iteration 177 loss: 0.6950846910476685, ACC:0.484375\n",
      "Training iteration 178 loss: 0.708335816860199, ACC:0.484375\n",
      "Training iteration 179 loss: 0.7815545797348022, ACC:0.375\n",
      "Training iteration 180 loss: 0.6970701217651367, ACC:0.515625\n",
      "Training iteration 181 loss: 0.6952522993087769, ACC:0.4375\n",
      "Training iteration 182 loss: 0.6834487915039062, ACC:0.578125\n",
      "Training iteration 183 loss: 0.7419487237930298, ACC:0.40625\n",
      "Training iteration 184 loss: 0.7166832685470581, ACC:0.421875\n",
      "Training iteration 185 loss: 0.7055190205574036, ACC:0.40625\n",
      "Training iteration 186 loss: 0.6956537365913391, ACC:0.515625\n",
      "Training iteration 187 loss: 0.6737209558486938, ACC:0.609375\n",
      "Training iteration 188 loss: 0.6865677833557129, ACC:0.5625\n",
      "Training iteration 189 loss: 0.7073251008987427, ACC:0.515625\n",
      "Training iteration 190 loss: 0.7155369520187378, ACC:0.46875\n",
      "Training iteration 191 loss: 0.6944151520729065, ACC:0.484375\n",
      "Training iteration 192 loss: 0.7138043642044067, ACC:0.4375\n",
      "Training iteration 193 loss: 0.6764978766441345, ACC:0.59375\n",
      "Training iteration 194 loss: 0.6813691854476929, ACC:0.578125\n",
      "Training iteration 195 loss: 0.6961985230445862, ACC:0.546875\n",
      "Training iteration 196 loss: 0.6875629425048828, ACC:0.5625\n",
      "Training iteration 197 loss: 0.7081329822540283, ACC:0.484375\n",
      "Training iteration 198 loss: 0.6926610469818115, ACC:0.515625\n",
      "Training iteration 199 loss: 0.6866247057914734, ACC:0.5625\n",
      "Training iteration 200 loss: 0.6813472509384155, ACC:0.578125\n",
      "Training iteration 201 loss: 0.7576100826263428, ACC:0.453125\n",
      "Training iteration 202 loss: 0.7805286645889282, ACC:0.34375\n",
      "Training iteration 203 loss: 0.6972557902336121, ACC:0.484375\n",
      "Training iteration 204 loss: 0.7553703188896179, ACC:0.453125\n",
      "Training iteration 205 loss: 0.7410886883735657, ACC:0.5\n",
      "Training iteration 206 loss: 0.718066930770874, ACC:0.5\n",
      "Training iteration 207 loss: 0.6929378509521484, ACC:0.515625\n",
      "Training iteration 208 loss: 0.6821320652961731, ACC:0.578125\n",
      "Training iteration 209 loss: 0.7044410109519958, ACC:0.546875\n",
      "Training iteration 210 loss: 0.7539418935775757, ACC:0.5\n",
      "Training iteration 211 loss: 0.7331430315971375, ACC:0.5\n",
      "Training iteration 212 loss: 0.7061766386032104, ACC:0.46875\n",
      "Training iteration 213 loss: 0.6999295353889465, ACC:0.5\n",
      "Training iteration 214 loss: 0.661720871925354, ACC:0.625\n",
      "Training iteration 215 loss: 0.7487767338752747, ACC:0.53125\n",
      "Training iteration 216 loss: 0.8317931294441223, ACC:0.421875\n",
      "Training iteration 217 loss: 0.7020779848098755, ACC:0.515625\n",
      "Training iteration 218 loss: 0.680179238319397, ACC:0.59375\n",
      "Training iteration 219 loss: 0.8154548406600952, ACC:0.421875\n",
      "Training iteration 220 loss: 0.7857505083084106, ACC:0.484375\n",
      "Training iteration 221 loss: 0.7213615775108337, ACC:0.515625\n",
      "Training iteration 222 loss: 0.6948485970497131, ACC:0.5\n",
      "Training iteration 223 loss: 0.6907867193222046, ACC:0.546875\n",
      "Training iteration 224 loss: 0.7254578471183777, ACC:0.53125\n",
      "Training iteration 225 loss: 0.7509006857872009, ACC:0.515625\n",
      "Training iteration 226 loss: 0.6891112327575684, ACC:0.578125\n",
      "Training iteration 227 loss: 0.7154503464698792, ACC:0.46875\n",
      "Training iteration 228 loss: 0.701844334602356, ACC:0.421875\n",
      "Training iteration 229 loss: 0.7422428131103516, ACC:0.375\n",
      "Training iteration 230 loss: 0.6760833859443665, ACC:0.625\n",
      "Training iteration 231 loss: 0.6887688636779785, ACC:0.546875\n",
      "Training iteration 232 loss: 0.7006386518478394, ACC:0.484375\n",
      "Training iteration 233 loss: 0.6944544911384583, ACC:0.5\n",
      "Training iteration 234 loss: 0.693763792514801, ACC:0.46875\n",
      "Training iteration 235 loss: 0.691587507724762, ACC:0.53125\n",
      "Training iteration 236 loss: 0.6912108063697815, ACC:0.53125\n",
      "Training iteration 237 loss: 0.6948069930076599, ACC:0.515625\n",
      "Training iteration 238 loss: 0.6947382092475891, ACC:0.515625\n",
      "Training iteration 239 loss: 0.6890636086463928, ACC:0.546875\n",
      "Training iteration 240 loss: 0.6861116886138916, ACC:0.578125\n",
      "Training iteration 241 loss: 0.6940048336982727, ACC:0.515625\n",
      "Training iteration 242 loss: 0.7189898490905762, ACC:0.375\n",
      "Training iteration 243 loss: 0.687094509601593, ACC:0.578125\n",
      "Training iteration 244 loss: 0.7348664402961731, ACC:0.453125\n",
      "Training iteration 245 loss: 0.7430091500282288, ACC:0.453125\n",
      "Training iteration 246 loss: 0.7091851830482483, ACC:0.46875\n",
      "Training iteration 247 loss: 0.692795991897583, ACC:0.515625\n",
      "Training iteration 248 loss: 0.7065717577934265, ACC:0.515625\n",
      "Training iteration 249 loss: 0.6783588528633118, ACC:0.59375\n",
      "Training iteration 250 loss: 0.7654060125350952, ACC:0.453125\n",
      "Training iteration 251 loss: 0.659522294998169, ACC:0.640625\n",
      "Training iteration 252 loss: 0.6887771487236023, ACC:0.546875\n",
      "Training iteration 253 loss: 0.6993978023529053, ACC:0.421875\n",
      "Training iteration 254 loss: 0.6728730797767639, ACC:0.625\n",
      "Training iteration 255 loss: 0.7246181964874268, ACC:0.515625\n",
      "Training iteration 256 loss: 0.8085897564888, ACC:0.421875\n",
      "Training iteration 257 loss: 0.7214901447296143, ACC:0.484375\n",
      "Training iteration 258 loss: 0.6918836236000061, ACC:0.546875\n",
      "Training iteration 259 loss: 0.7315064668655396, ACC:0.484375\n",
      "Training iteration 260 loss: 0.7849565744400024, ACC:0.453125\n",
      "Training iteration 261 loss: 0.7109397649765015, ACC:0.53125\n",
      "Training iteration 262 loss: 0.7057837247848511, ACC:0.46875\n",
      "Training iteration 263 loss: 0.7075137495994568, ACC:0.453125\n",
      "Training iteration 264 loss: 0.6952632665634155, ACC:0.546875\n",
      "Training iteration 265 loss: 0.7516536712646484, ACC:0.453125\n",
      "Training iteration 266 loss: 0.6704506874084473, ACC:0.609375\n",
      "Training iteration 267 loss: 0.7213903069496155, ACC:0.390625\n",
      "Training iteration 268 loss: 0.707044243812561, ACC:0.4375\n",
      "Training iteration 269 loss: 0.7124601006507874, ACC:0.484375\n",
      "Training iteration 270 loss: 0.7271214127540588, ACC:0.4375\n",
      "Training iteration 271 loss: 0.6913555264472961, ACC:0.53125\n",
      "Training iteration 272 loss: 0.6929663419723511, ACC:0.515625\n",
      "Training iteration 273 loss: 0.7195360660552979, ACC:0.4375\n",
      "Training iteration 274 loss: 0.7114479541778564, ACC:0.4375\n",
      "Training iteration 275 loss: 0.6898419260978699, ACC:0.59375\n",
      "Training iteration 276 loss: 0.7370122075080872, ACC:0.4375\n",
      "Training iteration 277 loss: 0.7260573506355286, ACC:0.484375\n",
      "Training iteration 278 loss: 0.676124095916748, ACC:0.59375\n",
      "Training iteration 279 loss: 0.7167765498161316, ACC:0.40625\n",
      "Training iteration 280 loss: 0.6911941170692444, ACC:0.53125\n",
      "Training iteration 281 loss: 0.6624893546104431, ACC:0.625\n",
      "Training iteration 282 loss: 0.7679975032806396, ACC:0.484375\n",
      "Training iteration 283 loss: 0.7612924575805664, ACC:0.484375\n",
      "Training iteration 284 loss: 0.7262107729911804, ACC:0.453125\n",
      "Training iteration 285 loss: 0.7030208110809326, ACC:0.453125\n",
      "Training iteration 286 loss: 0.6913530230522156, ACC:0.5625\n",
      "Training iteration 287 loss: 0.7708464860916138, ACC:0.46875\n",
      "Training iteration 288 loss: 0.713146984577179, ACC:0.53125\n",
      "Training iteration 289 loss: 0.7061220407485962, ACC:0.484375\n",
      "Training iteration 290 loss: 0.6911959648132324, ACC:0.53125\n",
      "Training iteration 291 loss: 0.6397333741188049, ACC:0.671875\n",
      "Training iteration 292 loss: 0.8135573267936707, ACC:0.46875\n",
      "Training iteration 293 loss: 0.7155234217643738, ACC:0.578125\n",
      "Training iteration 294 loss: 0.7841220498085022, ACC:0.4375\n",
      "Training iteration 295 loss: 0.6912768483161926, ACC:0.53125\n",
      "Training iteration 296 loss: 0.6992184519767761, ACC:0.53125\n",
      "Training iteration 297 loss: 0.7439218163490295, ACC:0.515625\n",
      "Training iteration 298 loss: 0.6211773753166199, ACC:0.6875\n",
      "Training iteration 299 loss: 0.7482139468193054, ACC:0.53125\n",
      "Training iteration 300 loss: 0.7189791202545166, ACC:0.53125\n",
      "Training iteration 301 loss: 0.7102810740470886, ACC:0.453125\n",
      "Training iteration 302 loss: 0.7026171684265137, ACC:0.5\n",
      "Training iteration 303 loss: 0.7305999994277954, ACC:0.515625\n",
      "Training iteration 304 loss: 0.7996695637702942, ACC:0.4375\n",
      "Training iteration 305 loss: 0.7349629402160645, ACC:0.453125\n",
      "Training iteration 306 loss: 0.6964803338050842, ACC:0.46875\n",
      "Training iteration 307 loss: 0.7323958277702332, ACC:0.46875\n",
      "Training iteration 308 loss: 0.731309175491333, ACC:0.5\n",
      "Training iteration 309 loss: 0.7640016674995422, ACC:0.390625\n",
      "Training iteration 310 loss: 0.6925584673881531, ACC:0.53125\n",
      "Training iteration 311 loss: 0.6985934972763062, ACC:0.546875\n",
      "Training iteration 312 loss: 0.7770904898643494, ACC:0.484375\n",
      "Training iteration 313 loss: 0.7672697305679321, ACC:0.484375\n",
      "Training iteration 314 loss: 0.7278580069541931, ACC:0.453125\n",
      "Training iteration 315 loss: 0.6863786578178406, ACC:0.5625\n",
      "Training iteration 316 loss: 0.7698096632957458, ACC:0.46875\n",
      "Training iteration 317 loss: 0.8024095296859741, ACC:0.46875\n",
      "Training iteration 318 loss: 0.7100321054458618, ACC:0.546875\n",
      "Training iteration 319 loss: 0.7124704718589783, ACC:0.453125\n",
      "Training iteration 320 loss: 0.7007330060005188, ACC:0.5\n",
      "Training iteration 321 loss: 0.7553433775901794, ACC:0.46875\n",
      "Training iteration 322 loss: 0.7615328431129456, ACC:0.46875\n",
      "Training iteration 323 loss: 0.6985801458358765, ACC:0.53125\n",
      "Training iteration 324 loss: 0.6971563696861267, ACC:0.40625\n",
      "Training iteration 325 loss: 0.6953604817390442, ACC:0.546875\n",
      "Training iteration 326 loss: 0.7260958552360535, ACC:0.546875\n",
      "Training iteration 327 loss: 0.7509282231330872, ACC:0.53125\n",
      "Training iteration 328 loss: 0.7240601181983948, ACC:0.53125\n",
      "Training iteration 329 loss: 0.7205196022987366, ACC:0.4375\n",
      "Training iteration 330 loss: 0.7172727584838867, ACC:0.4375\n",
      "Training iteration 331 loss: 0.6869193315505981, ACC:0.578125\n",
      "Training iteration 332 loss: 0.7547953128814697, ACC:0.5\n",
      "Training iteration 333 loss: 0.6809148192405701, ACC:0.59375\n",
      "Training iteration 334 loss: 0.6755737662315369, ACC:0.59375\n",
      "Training iteration 335 loss: 0.7107701301574707, ACC:0.453125\n",
      "Training iteration 336 loss: 0.6898708343505859, ACC:0.546875\n",
      "Training iteration 337 loss: 0.7153781056404114, ACC:0.5\n",
      "Training iteration 338 loss: 0.6878359913825989, ACC:0.578125\n",
      "Training iteration 339 loss: 0.6621834635734558, ACC:0.625\n",
      "Training iteration 340 loss: 0.7349629998207092, ACC:0.5\n",
      "Training iteration 341 loss: 0.714545488357544, ACC:0.484375\n",
      "Training iteration 342 loss: 0.6926462054252625, ACC:0.546875\n",
      "Training iteration 343 loss: 0.7077916860580444, ACC:0.515625\n",
      "Training iteration 344 loss: 0.743015468120575, ACC:0.5\n",
      "Training iteration 345 loss: 0.7832109928131104, ACC:0.421875\n",
      "Training iteration 346 loss: 0.6855775117874146, ACC:0.5625\n",
      "Training iteration 347 loss: 0.6971880197525024, ACC:0.484375\n",
      "Training iteration 348 loss: 0.7361270189285278, ACC:0.421875\n",
      "Training iteration 349 loss: 0.694676399230957, ACC:0.53125\n",
      "Training iteration 350 loss: 0.7077549695968628, ACC:0.4375\n",
      "Training iteration 351 loss: 0.6944270133972168, ACC:0.5\n",
      "Training iteration 352 loss: 0.6857171654701233, ACC:0.5625\n",
      "Training iteration 353 loss: 0.7356603145599365, ACC:0.46875\n",
      "Training iteration 354 loss: 0.7421099543571472, ACC:0.421875\n",
      "Training iteration 355 loss: 0.6916276812553406, ACC:0.546875\n",
      "Training iteration 356 loss: 0.7165076732635498, ACC:0.421875\n",
      "Training iteration 357 loss: 0.6740114688873291, ACC:0.609375\n",
      "Training iteration 358 loss: 0.6915096044540405, ACC:0.546875\n",
      "Training iteration 359 loss: 0.6983839869499207, ACC:0.53125\n",
      "Training iteration 360 loss: 0.7042105197906494, ACC:0.5\n",
      "Training iteration 361 loss: 0.6967867016792297, ACC:0.484375\n",
      "Training iteration 362 loss: 0.703656017780304, ACC:0.421875\n",
      "Training iteration 363 loss: 0.6985424160957336, ACC:0.484375\n",
      "Training iteration 364 loss: 0.7060302495956421, ACC:0.390625\n",
      "Training iteration 365 loss: 0.6961265802383423, ACC:0.5\n",
      "Training iteration 366 loss: 0.6962391138076782, ACC:0.53125\n",
      "Training iteration 367 loss: 0.7511712908744812, ACC:0.40625\n",
      "Training iteration 368 loss: 0.7043222188949585, ACC:0.453125\n",
      "Training iteration 369 loss: 0.6916925311088562, ACC:0.53125\n",
      "Training iteration 370 loss: 0.744570255279541, ACC:0.453125\n",
      "Training iteration 371 loss: 0.6767881512641907, ACC:0.59375\n",
      "Training iteration 372 loss: 0.7458191514015198, ACC:0.4375\n",
      "Training iteration 373 loss: 0.7042985558509827, ACC:0.4375\n",
      "Training iteration 374 loss: 0.67640620470047, ACC:0.59375\n",
      "Training iteration 375 loss: 0.7443121075630188, ACC:0.515625\n",
      "Training iteration 376 loss: 0.6611180305480957, ACC:0.640625\n",
      "Training iteration 377 loss: 0.7172350883483887, ACC:0.578125\n",
      "Training iteration 378 loss: 0.7738707661628723, ACC:0.46875\n",
      "Training iteration 379 loss: 0.6991991996765137, ACC:0.5\n",
      "Training iteration 380 loss: 0.7084360122680664, ACC:0.484375\n",
      "Training iteration 381 loss: 0.7589837312698364, ACC:0.46875\n",
      "Training iteration 382 loss: 0.769412100315094, ACC:0.453125\n",
      "Training iteration 383 loss: 0.7345290780067444, ACC:0.40625\n",
      "Training iteration 384 loss: 0.721969485282898, ACC:0.40625\n",
      "Training iteration 385 loss: 0.7679551839828491, ACC:0.40625\n",
      "Training iteration 386 loss: 0.7461540102958679, ACC:0.40625\n",
      "Training iteration 387 loss: 0.6927148103713989, ACC:0.53125\n",
      "Training iteration 388 loss: 0.7012597322463989, ACC:0.53125\n",
      "Training iteration 389 loss: 0.7153975963592529, ACC:0.546875\n",
      "Training iteration 390 loss: 0.7647287249565125, ACC:0.484375\n",
      "Training iteration 391 loss: 0.7471890449523926, ACC:0.4375\n",
      "Training iteration 392 loss: 0.6946907639503479, ACC:0.46875\n",
      "Training iteration 393 loss: 0.7287181615829468, ACC:0.46875\n",
      "Training iteration 394 loss: 0.7571144104003906, ACC:0.453125\n",
      "Training iteration 395 loss: 0.7298230528831482, ACC:0.453125\n",
      "Training iteration 396 loss: 0.6925902366638184, ACC:0.546875\n",
      "Training iteration 397 loss: 0.6893176436424255, ACC:0.546875\n",
      "Training iteration 398 loss: 0.7206658720970154, ACC:0.5\n",
      "Training iteration 399 loss: 0.646537184715271, ACC:0.65625\n",
      "Training iteration 400 loss: 0.6617783904075623, ACC:0.625\n",
      "Training iteration 401 loss: 0.8128142952919006, ACC:0.375\n",
      "Training iteration 402 loss: 0.6957372426986694, ACC:0.515625\n",
      "Training iteration 403 loss: 0.7032002210617065, ACC:0.46875\n",
      "Training iteration 404 loss: 0.7827969193458557, ACC:0.34375\n",
      "Training iteration 405 loss: 0.6853370070457458, ACC:0.5625\n",
      "Training iteration 406 loss: 0.6937853097915649, ACC:0.5\n",
      "Training iteration 407 loss: 0.6945568919181824, ACC:0.5\n",
      "Training iteration 408 loss: 0.6959355473518372, ACC:0.515625\n",
      "Training iteration 409 loss: 0.6937352418899536, ACC:0.53125\n",
      "Training iteration 410 loss: 0.7207654118537903, ACC:0.421875\n",
      "Training iteration 411 loss: 0.6939814686775208, ACC:0.453125\n",
      "Training iteration 412 loss: 0.6639169454574585, ACC:0.640625\n",
      "Training iteration 413 loss: 0.7553578615188599, ACC:0.484375\n",
      "Training iteration 414 loss: 0.733873188495636, ACC:0.53125\n",
      "Training iteration 415 loss: 0.7316546440124512, ACC:0.5\n",
      "Training iteration 416 loss: 0.69507896900177, ACC:0.515625\n",
      "Training iteration 417 loss: 0.6996230483055115, ACC:0.484375\n",
      "Training iteration 418 loss: 0.6756569743156433, ACC:0.59375\n",
      "Training iteration 419 loss: 0.7706660628318787, ACC:0.453125\n",
      "Training iteration 420 loss: 0.7867966890335083, ACC:0.375\n",
      "Training iteration 421 loss: 0.693072497844696, ACC:0.53125\n",
      "Training iteration 422 loss: 0.7128838300704956, ACC:0.5\n",
      "Training iteration 423 loss: 0.6720893383026123, ACC:0.609375\n",
      "Training iteration 424 loss: 0.8393385410308838, ACC:0.390625\n",
      "Training iteration 425 loss: 0.7452020049095154, ACC:0.421875\n",
      "Training iteration 426 loss: 0.7070731520652771, ACC:0.4375\n",
      "Training iteration 427 loss: 0.6778332591056824, ACC:0.59375\n",
      "Training iteration 428 loss: 0.817680835723877, ACC:0.4375\n",
      "Training iteration 429 loss: 0.7731302380561829, ACC:0.453125\n",
      "Training iteration 430 loss: 0.6796795129776001, ACC:0.59375\n",
      "Training iteration 431 loss: 0.6869454383850098, ACC:0.5625\n",
      "Training iteration 432 loss: 0.7885622382164001, ACC:0.359375\n",
      "Training iteration 433 loss: 0.6875119805335999, ACC:0.5625\n",
      "Training iteration 434 loss: 0.7010512948036194, ACC:0.5\n",
      "Training iteration 435 loss: 0.6932973265647888, ACC:0.5\n",
      "Training iteration 436 loss: 0.6969413161277771, ACC:0.5\n",
      "Training iteration 437 loss: 0.7086114883422852, ACC:0.484375\n",
      "Training iteration 438 loss: 0.697822630405426, ACC:0.515625\n",
      "Training iteration 439 loss: 0.6888312101364136, ACC:0.546875\n",
      "Training iteration 440 loss: 0.6951233744621277, ACC:0.484375\n",
      "Training iteration 441 loss: 0.6975607872009277, ACC:0.421875\n",
      "Training iteration 442 loss: 0.6910780072212219, ACC:0.578125\n",
      "Training iteration 443 loss: 0.705738365650177, ACC:0.40625\n",
      "Training iteration 444 loss: 0.6935670375823975, ACC:0.421875\n",
      "Training iteration 445 loss: 0.6956688165664673, ACC:0.515625\n",
      "Training iteration 446 loss: 0.6755152940750122, ACC:0.59375\n",
      "Training iteration 447 loss: 0.7233108282089233, ACC:0.5\n",
      "Training iteration 448 loss: 0.7557595372200012, ACC:0.40625\n",
      "Training iteration 449 loss: 0.6926614046096802, ACC:0.515625\n",
      "Training iteration 450 loss: 0.7023987174034119, ACC:0.5\n",
      "Validation iteration 451 loss: 0.6981216073036194, ACC: 0.546875\n",
      "Validation iteration 452 loss: 0.7416920065879822, ACC: 0.453125\n",
      "Validation iteration 453 loss: 0.6908599138259888, ACC: 0.5625\n",
      "Validation iteration 454 loss: 0.7562154531478882, ACC: 0.421875\n",
      "Validation iteration 455 loss: 0.7271685600280762, ACC: 0.484375\n",
      "Validation iteration 456 loss: 0.719906747341156, ACC: 0.5\n",
      "Validation iteration 457 loss: 0.7053833603858948, ACC: 0.53125\n",
      "Validation iteration 458 loss: 0.7416920065879822, ACC: 0.453125\n",
      "Validation iteration 459 loss: 0.7344302535057068, ACC: 0.46875\n",
      "Validation iteration 460 loss: 0.756215512752533, ACC: 0.421875\n",
      "Validation iteration 461 loss: 0.7199068665504456, ACC: 0.5\n",
      "Validation iteration 462 loss: 0.7489537000656128, ACC: 0.4375\n",
      "Validation iteration 463 loss: 0.6327660083770752, ACC: 0.6875\n",
      "Validation iteration 464 loss: 0.7562156319618225, ACC: 0.421875\n",
      "Validation iteration 465 loss: 0.756215512752533, ACC: 0.421875\n",
      "Validation iteration 466 loss: 0.7126450538635254, ACC: 0.515625\n",
      "Validation iteration 467 loss: 0.6835981011390686, ACC: 0.578125\n",
      "Validation iteration 468 loss: 0.741692066192627, ACC: 0.453125\n",
      "Validation iteration 469 loss: 0.7489537596702576, ACC: 0.4375\n",
      "Validation iteration 470 loss: 0.7126451134681702, ACC: 0.515625\n",
      "Validation iteration 471 loss: 0.7416920065879822, ACC: 0.453125\n",
      "Validation iteration 472 loss: 0.661812961101532, ACC: 0.625\n",
      "Validation iteration 473 loss: 0.756215512752533, ACC: 0.421875\n",
      "Validation iteration 474 loss: 0.7271685004234314, ACC: 0.484375\n",
      "Validation iteration 475 loss: 0.741692066192627, ACC: 0.453125\n",
      "Validation iteration 476 loss: 0.7053833603858948, ACC: 0.53125\n",
      "Validation iteration 477 loss: 0.785262405872345, ACC: 0.359375\n",
      "Validation iteration 478 loss: 0.7344303131103516, ACC: 0.46875\n",
      "Validation iteration 479 loss: 0.6981215476989746, ACC: 0.546875\n",
      "Validation iteration 480 loss: 0.7707390189170837, ACC: 0.390625\n",
      "Validation iteration 481 loss: 0.7344302535057068, ACC: 0.46875\n",
      "Validation iteration 482 loss: 0.7344302535057068, ACC: 0.46875\n",
      "Validation iteration 483 loss: 0.7344303727149963, ACC: 0.46875\n",
      "Validation iteration 484 loss: 0.6981216073036194, ACC: 0.546875\n",
      "Validation iteration 485 loss: 0.661812961101532, ACC: 0.625\n",
      "Validation iteration 486 loss: 0.7489537596702576, ACC: 0.4375\n",
      "Validation iteration 487 loss: 0.7053833603858948, ACC: 0.53125\n",
      "Validation iteration 488 loss: 0.7562154531478882, ACC: 0.421875\n",
      "Validation iteration 489 loss: 0.6835981011390686, ACC: 0.578125\n",
      "Validation iteration 490 loss: 0.7126450538635254, ACC: 0.515625\n",
      "Validation iteration 491 loss: 0.690859854221344, ACC: 0.5625\n",
      "Validation iteration 492 loss: 0.7852624654769897, ACC: 0.359375\n",
      "Validation iteration 493 loss: 0.6835981011390686, ACC: 0.578125\n",
      "Validation iteration 494 loss: 0.6690746545791626, ACC: 0.609375\n",
      "Validation iteration 495 loss: 0.7271685004234314, ACC: 0.484375\n",
      "Validation iteration 496 loss: 0.7271685004234314, ACC: 0.484375\n",
      "Validation iteration 497 loss: 0.7126451134681702, ACC: 0.515625\n",
      "Validation iteration 498 loss: 0.7199068665504456, ACC: 0.5\n",
      "Validation iteration 499 loss: 0.7126451134681702, ACC: 0.515625\n",
      "Validation iteration 500 loss: 0.676336407661438, ACC: 0.59375\n",
      "-- Epoch 3 done -- Train loss: 0.7123479576905568, train ACC: 0.49972222222222223, val loss: 0.7216496336460113, val ACC: 0.49625\n",
      "<--- 9481.30698132515 seconds --->\n",
      "Training iteration 1 loss: 0.756215512752533, ACC:0.421875\n",
      "Training iteration 2 loss: 0.7074571251869202, ACC:0.5\n",
      "Training iteration 3 loss: 0.6927962303161621, ACC:0.515625\n",
      "Training iteration 4 loss: 0.697603166103363, ACC:0.484375\n",
      "Training iteration 5 loss: 0.6979223489761353, ACC:0.515625\n",
      "Training iteration 6 loss: 0.6953133344650269, ACC:0.53125\n",
      "Training iteration 7 loss: 0.70669025182724, ACC:0.484375\n",
      "Training iteration 8 loss: 0.6877204179763794, ACC:0.5625\n",
      "Training iteration 9 loss: 0.6922335624694824, ACC:0.546875\n",
      "Training iteration 10 loss: 0.693038821220398, ACC:0.53125\n",
      "Training iteration 11 loss: 0.6910879611968994, ACC:0.5625\n",
      "Training iteration 12 loss: 0.6912571787834167, ACC:0.53125\n",
      "Training iteration 13 loss: 0.7153709530830383, ACC:0.4375\n",
      "Training iteration 14 loss: 0.6849828958511353, ACC:0.578125\n",
      "Training iteration 15 loss: 0.7028254866600037, ACC:0.421875\n",
      "Training iteration 16 loss: 0.699065089225769, ACC:0.4375\n",
      "Training iteration 17 loss: 0.6987375617027283, ACC:0.46875\n",
      "Training iteration 18 loss: 0.6973530650138855, ACC:0.4375\n",
      "Training iteration 19 loss: 0.6803596019744873, ACC:0.625\n",
      "Training iteration 20 loss: 0.7022148966789246, ACC:0.53125\n",
      "Training iteration 21 loss: 0.708132266998291, ACC:0.546875\n",
      "Training iteration 22 loss: 0.6987058520317078, ACC:0.5625\n",
      "Training iteration 23 loss: 0.6904643177986145, ACC:0.5625\n",
      "Training iteration 24 loss: 0.685335099697113, ACC:0.5625\n",
      "Training iteration 25 loss: 0.6980957388877869, ACC:0.453125\n",
      "Training iteration 26 loss: 0.7086473107337952, ACC:0.4375\n",
      "Training iteration 27 loss: 0.685314953327179, ACC:0.5625\n",
      "Training iteration 28 loss: 0.7454239726066589, ACC:0.375\n",
      "Training iteration 29 loss: 0.6961622834205627, ACC:0.453125\n",
      "Training iteration 30 loss: 0.6510024070739746, ACC:0.6875\n",
      "Training iteration 31 loss: 0.7635993361473083, ACC:0.5\n",
      "Training iteration 32 loss: 0.7508767247200012, ACC:0.546875\n",
      "Training iteration 33 loss: 0.7688427567481995, ACC:0.5\n",
      "Training iteration 34 loss: 0.7055020332336426, ACC:0.515625\n",
      "Training iteration 35 loss: 0.6928609609603882, ACC:0.515625\n",
      "Training iteration 36 loss: 0.7108880877494812, ACC:0.53125\n",
      "Training iteration 37 loss: 0.8098766803741455, ACC:0.4375\n",
      "Training iteration 38 loss: 0.7257809638977051, ACC:0.515625\n",
      "Training iteration 39 loss: 0.7033448219299316, ACC:0.484375\n",
      "Training iteration 40 loss: 0.6887645125389099, ACC:0.546875\n",
      "Training iteration 41 loss: 0.70771723985672, ACC:0.546875\n",
      "Training iteration 42 loss: 0.8038029670715332, ACC:0.453125\n",
      "Training iteration 43 loss: 0.7459313869476318, ACC:0.484375\n",
      "Training iteration 44 loss: 0.6971344947814941, ACC:0.5\n",
      "Training iteration 45 loss: 0.710158109664917, ACC:0.46875\n",
      "Training iteration 46 loss: 0.7176614999771118, ACC:0.515625\n",
      "Training iteration 47 loss: 0.6795715093612671, ACC:0.59375\n",
      "Training iteration 48 loss: 0.6446927189826965, ACC:0.65625\n",
      "Training iteration 49 loss: 0.7495902180671692, ACC:0.46875\n",
      "Training iteration 50 loss: 0.7203372120857239, ACC:0.453125\n",
      "Training iteration 51 loss: 0.6944073438644409, ACC:0.5\n",
      "Training iteration 52 loss: 0.7303472757339478, ACC:0.46875\n",
      "Training iteration 53 loss: 0.6697371602058411, ACC:0.609375\n",
      "Training iteration 54 loss: 0.7512548565864563, ACC:0.46875\n",
      "Training iteration 55 loss: 0.6700918674468994, ACC:0.609375\n",
      "Training iteration 56 loss: 0.6887463331222534, ACC:0.546875\n",
      "Training iteration 57 loss: 0.6945919394493103, ACC:0.46875\n",
      "Training iteration 58 loss: 0.7133463621139526, ACC:0.390625\n",
      "Training iteration 59 loss: 0.6992338299751282, ACC:0.453125\n",
      "Training iteration 60 loss: 0.6898518800735474, ACC:0.5625\n",
      "Training iteration 61 loss: 0.6766678690910339, ACC:0.59375\n",
      "Training iteration 62 loss: 0.8066433668136597, ACC:0.34375\n",
      "Training iteration 63 loss: 0.7170053720474243, ACC:0.453125\n",
      "Training iteration 64 loss: 0.688118577003479, ACC:0.5625\n",
      "Training iteration 65 loss: 0.8047490119934082, ACC:0.34375\n",
      "Training iteration 66 loss: 0.7652726173400879, ACC:0.390625\n",
      "Training iteration 67 loss: 0.6978663206100464, ACC:0.390625\n",
      "Training iteration 68 loss: 0.6772817969322205, ACC:0.59375\n",
      "Training iteration 69 loss: 0.8013320565223694, ACC:0.5\n",
      "Training iteration 70 loss: 0.8931499719619751, ACC:0.421875\n",
      "Training iteration 71 loss: 0.7166052460670471, ACC:0.53125\n",
      "Training iteration 72 loss: 0.6964892148971558, ACC:0.390625\n",
      "Training iteration 73 loss: 0.754561722278595, ACC:0.484375\n",
      "Training iteration 74 loss: 0.8418960571289062, ACC:0.46875\n",
      "Training iteration 75 loss: 0.7911011576652527, ACC:0.5\n",
      "Training iteration 76 loss: 0.7236174941062927, ACC:0.5\n",
      "Training iteration 77 loss: 0.6934946775436401, ACC:0.5\n",
      "Training iteration 78 loss: 0.7109966278076172, ACC:0.53125\n",
      "Training iteration 79 loss: 0.7971271276473999, ACC:0.46875\n",
      "Training iteration 80 loss: 0.7780244946479797, ACC:0.46875\n",
      "Training iteration 81 loss: 0.72151780128479, ACC:0.453125\n",
      "Training iteration 82 loss: 0.7198137640953064, ACC:0.40625\n",
      "Training iteration 83 loss: 0.7297018766403198, ACC:0.484375\n",
      "Training iteration 84 loss: 0.7503990530967712, ACC:0.453125\n",
      "Training iteration 85 loss: 0.7207786440849304, ACC:0.4375\n",
      "Training iteration 86 loss: 0.6911954879760742, ACC:0.53125\n",
      "Training iteration 87 loss: 0.7474380731582642, ACC:0.453125\n",
      "Training iteration 88 loss: 0.7159134745597839, ACC:0.53125\n",
      "Training iteration 89 loss: 0.7362455725669861, ACC:0.46875\n",
      "Training iteration 90 loss: 0.696500301361084, ACC:0.5\n",
      "Training iteration 91 loss: 0.6859351396560669, ACC:0.5625\n",
      "Training iteration 92 loss: 0.7262023091316223, ACC:0.5\n",
      "Training iteration 93 loss: 0.7317888140678406, ACC:0.515625\n",
      "Training iteration 94 loss: 0.759007453918457, ACC:0.4375\n",
      "Training iteration 95 loss: 0.7038210034370422, ACC:0.4375\n",
      "Training iteration 96 loss: 0.7202794551849365, ACC:0.46875\n",
      "Training iteration 97 loss: 0.7597029209136963, ACC:0.46875\n",
      "Training iteration 98 loss: 0.725433349609375, ACC:0.515625\n",
      "Training iteration 99 loss: 0.6755514740943909, ACC:0.59375\n",
      "Training iteration 100 loss: 0.6987704634666443, ACC:0.484375\n",
      "Training iteration 101 loss: 0.6959545612335205, ACC:0.484375\n",
      "Training iteration 102 loss: 0.7234770059585571, ACC:0.421875\n",
      "Training iteration 103 loss: 0.6855908036231995, ACC:0.5625\n",
      "Training iteration 104 loss: 0.7064765095710754, ACC:0.4375\n",
      "Training iteration 105 loss: 0.6948485374450684, ACC:0.453125\n",
      "Training iteration 106 loss: 0.6950860023498535, ACC:0.5\n",
      "Training iteration 107 loss: 0.6963199377059937, ACC:0.5\n",
      "Training iteration 108 loss: 0.7013664841651917, ACC:0.453125\n",
      "Training iteration 109 loss: 0.6923832297325134, ACC:0.546875\n",
      "Training iteration 110 loss: 0.7123097777366638, ACC:0.4375\n",
      "Training iteration 111 loss: 0.69221031665802, ACC:0.53125\n",
      "Training iteration 112 loss: 0.717237651348114, ACC:0.390625\n",
      "Training iteration 113 loss: 0.691406786441803, ACC:0.53125\n",
      "Training iteration 114 loss: 0.739237368106842, ACC:0.40625\n",
      "Training iteration 115 loss: 0.7088460326194763, ACC:0.484375\n",
      "Training iteration 116 loss: 0.7008951306343079, ACC:0.453125\n",
      "Training iteration 117 loss: 0.6889176368713379, ACC:0.546875\n",
      "Training iteration 118 loss: 0.7336313724517822, ACC:0.453125\n",
      "Training iteration 119 loss: 0.7086108922958374, ACC:0.515625\n",
      "Training iteration 120 loss: 0.6855478882789612, ACC:0.5625\n",
      "Training iteration 121 loss: 0.6939573884010315, ACC:0.515625\n",
      "Training iteration 122 loss: 0.6940675377845764, ACC:0.390625\n",
      "Training iteration 123 loss: 0.7092703580856323, ACC:0.484375\n",
      "Training iteration 124 loss: 0.7106339335441589, ACC:0.515625\n",
      "Training iteration 125 loss: 0.7359442114830017, ACC:0.453125\n",
      "Training iteration 126 loss: 0.6802517771720886, ACC:0.59375\n",
      "Training iteration 127 loss: 0.6912029385566711, ACC:0.5625\n",
      "Training iteration 128 loss: 0.6934515237808228, ACC:0.484375\n",
      "Training iteration 129 loss: 0.6922721862792969, ACC:0.53125\n",
      "Training iteration 130 loss: 0.6876296401023865, ACC:0.578125\n",
      "Training iteration 131 loss: 0.7356197834014893, ACC:0.328125\n",
      "Training iteration 132 loss: 0.6933333873748779, ACC:0.484375\n",
      "Training iteration 133 loss: 0.6974366903305054, ACC:0.5\n",
      "Training iteration 134 loss: 0.7023486495018005, ACC:0.5\n",
      "Training iteration 135 loss: 0.7045467495918274, ACC:0.484375\n",
      "Training iteration 136 loss: 0.6961828470230103, ACC:0.484375\n",
      "Training iteration 137 loss: 0.6913880705833435, ACC:0.53125\n",
      "Training iteration 138 loss: 0.6938926577568054, ACC:0.53125\n",
      "Training iteration 139 loss: 0.6875067949295044, ACC:0.5625\n",
      "Training iteration 140 loss: 0.7304129600524902, ACC:0.46875\n",
      "Training iteration 141 loss: 0.7317308783531189, ACC:0.390625\n",
      "Training iteration 142 loss: 0.7124263644218445, ACC:0.390625\n",
      "Training iteration 143 loss: 0.7283126711845398, ACC:0.421875\n",
      "Training iteration 144 loss: 0.7016695141792297, ACC:0.484375\n",
      "Training iteration 145 loss: 0.6924311518669128, ACC:0.546875\n",
      "Training iteration 146 loss: 0.6857528686523438, ACC:0.59375\n",
      "Training iteration 147 loss: 0.6763506531715393, ACC:0.59375\n",
      "Training iteration 148 loss: 0.693436324596405, ACC:0.5625\n",
      "Training iteration 149 loss: 0.8434212803840637, ACC:0.328125\n",
      "Training iteration 150 loss: 0.6920709609985352, ACC:0.53125\n",
      "Training iteration 151 loss: 0.7172679901123047, ACC:0.390625\n",
      "Training iteration 152 loss: 0.6949064135551453, ACC:0.53125\n",
      "Training iteration 153 loss: 0.7494339346885681, ACC:0.375\n",
      "Training iteration 154 loss: 0.6960379481315613, ACC:0.453125\n",
      "Training iteration 155 loss: 0.7084401249885559, ACC:0.484375\n",
      "Training iteration 156 loss: 0.7205967307090759, ACC:0.5\n",
      "Training iteration 157 loss: 0.725793182849884, ACC:0.484375\n",
      "Training iteration 158 loss: 0.7044428586959839, ACC:0.484375\n",
      "Training iteration 159 loss: 0.690769374370575, ACC:0.546875\n",
      "Training iteration 160 loss: 0.6754668951034546, ACC:0.59375\n",
      "Training iteration 161 loss: 0.801435649394989, ACC:0.421875\n",
      "Training iteration 162 loss: 0.7327866554260254, ACC:0.5\n",
      "Training iteration 163 loss: 0.6893925666809082, ACC:0.546875\n",
      "Training iteration 164 loss: 0.6913670301437378, ACC:0.546875\n",
      "Training iteration 165 loss: 0.7450670003890991, ACC:0.40625\n",
      "Training iteration 166 loss: 0.693438708782196, ACC:0.546875\n",
      "Training iteration 167 loss: 0.7160550951957703, ACC:0.46875\n",
      "Training iteration 168 loss: 0.6958165168762207, ACC:0.484375\n",
      "Training iteration 169 loss: 0.6888020634651184, ACC:0.546875\n",
      "Training iteration 170 loss: 0.7077749967575073, ACC:0.515625\n",
      "Training iteration 171 loss: 0.739905595779419, ACC:0.46875\n",
      "Training iteration 172 loss: 0.713494062423706, ACC:0.484375\n",
      "Training iteration 173 loss: 0.6918551325798035, ACC:0.53125\n",
      "Training iteration 174 loss: 0.6974146962165833, ACC:0.5\n",
      "Training iteration 175 loss: 0.6962846517562866, ACC:0.53125\n",
      "Training iteration 176 loss: 0.6873493790626526, ACC:0.5625\n",
      "Training iteration 177 loss: 0.6873171329498291, ACC:0.5625\n",
      "Training iteration 178 loss: 0.6912497282028198, ACC:0.546875\n",
      "Training iteration 179 loss: 0.6999200582504272, ACC:0.5\n",
      "Training iteration 180 loss: 0.6891460418701172, ACC:0.578125\n",
      "Training iteration 181 loss: 0.6937969923019409, ACC:0.359375\n",
      "Training iteration 182 loss: 0.7012026906013489, ACC:0.46875\n",
      "Training iteration 183 loss: 0.710193395614624, ACC:0.4375\n",
      "Training iteration 184 loss: 0.687088668346405, ACC:0.609375\n",
      "Training iteration 185 loss: 0.70798259973526, ACC:0.328125\n",
      "Training iteration 186 loss: 0.6827111840248108, ACC:0.578125\n",
      "Training iteration 187 loss: 0.7528615593910217, ACC:0.4375\n",
      "Training iteration 188 loss: 0.7797073125839233, ACC:0.375\n",
      "Training iteration 189 loss: 0.6856700778007507, ACC:0.59375\n",
      "Training iteration 190 loss: 0.7002220153808594, ACC:0.46875\n",
      "Training iteration 191 loss: 0.705022931098938, ACC:0.484375\n",
      "Training iteration 192 loss: 0.6889647841453552, ACC:0.546875\n",
      "Training iteration 193 loss: 0.6917853951454163, ACC:0.53125\n",
      "Training iteration 194 loss: 0.6869680881500244, ACC:0.5625\n",
      "Training iteration 195 loss: 0.6832700371742249, ACC:0.59375\n",
      "Training iteration 196 loss: 0.68548583984375, ACC:0.5625\n",
      "Training iteration 197 loss: 0.7000652551651001, ACC:0.515625\n",
      "Training iteration 198 loss: 0.6902203559875488, ACC:0.546875\n",
      "Training iteration 199 loss: 0.7162674069404602, ACC:0.4375\n",
      "Training iteration 200 loss: 0.6922019720077515, ACC:0.578125\n",
      "Training iteration 201 loss: 0.6992974281311035, ACC:0.4375\n",
      "Training iteration 202 loss: 0.6940450668334961, ACC:0.484375\n",
      "Training iteration 203 loss: 0.6938461065292358, ACC:0.484375\n",
      "Training iteration 204 loss: 0.6894973516464233, ACC:0.5625\n",
      "Training iteration 205 loss: 0.704461395740509, ACC:0.453125\n",
      "Training iteration 206 loss: 0.6780313849449158, ACC:0.640625\n",
      "Training iteration 207 loss: 0.6891986131668091, ACC:0.546875\n",
      "Training iteration 208 loss: 0.7182645201683044, ACC:0.46875\n",
      "Training iteration 209 loss: 0.7006916403770447, ACC:0.5\n",
      "Training iteration 210 loss: 0.6978307366371155, ACC:0.4375\n",
      "Training iteration 211 loss: 0.6990496516227722, ACC:0.5\n",
      "Training iteration 212 loss: 0.7180711627006531, ACC:0.484375\n",
      "Training iteration 213 loss: 0.6754717230796814, ACC:0.59375\n",
      "Training iteration 214 loss: 0.709153413772583, ACC:0.5\n",
      "Training iteration 215 loss: 0.6953723430633545, ACC:0.515625\n",
      "Training iteration 216 loss: 0.6931948661804199, ACC:0.5\n",
      "Training iteration 217 loss: 0.6938350796699524, ACC:0.515625\n",
      "Training iteration 218 loss: 0.7075967192649841, ACC:0.484375\n",
      "Training iteration 219 loss: 0.701708197593689, ACC:0.5\n",
      "Training iteration 220 loss: 0.702315628528595, ACC:0.453125\n",
      "Training iteration 221 loss: 0.6971668601036072, ACC:0.46875\n",
      "Training iteration 222 loss: 0.6957900524139404, ACC:0.515625\n",
      "Training iteration 223 loss: 0.6896761059761047, ACC:0.546875\n",
      "Training iteration 224 loss: 0.6809665560722351, ACC:0.578125\n",
      "Training iteration 225 loss: 0.7396206855773926, ACC:0.390625\n",
      "Training iteration 226 loss: 0.6920079588890076, ACC:0.53125\n",
      "Training iteration 227 loss: 0.6966124176979065, ACC:0.5\n",
      "Training iteration 228 loss: 0.6855180859565735, ACC:0.5625\n",
      "Training iteration 229 loss: 0.7485138177871704, ACC:0.40625\n",
      "Training iteration 230 loss: 0.6858938932418823, ACC:0.5625\n",
      "Training iteration 231 loss: 0.6942828893661499, ACC:0.453125\n",
      "Training iteration 232 loss: 0.7046142816543579, ACC:0.46875\n",
      "Training iteration 233 loss: 0.7202773094177246, ACC:0.4375\n",
      "Training iteration 234 loss: 0.7091081738471985, ACC:0.40625\n",
      "Training iteration 235 loss: 0.6774967312812805, ACC:0.609375\n",
      "Training iteration 236 loss: 0.6887568235397339, ACC:0.578125\n",
      "Training iteration 237 loss: 0.8154593706130981, ACC:0.453125\n",
      "Training iteration 238 loss: 0.7397635579109192, ACC:0.515625\n",
      "Training iteration 239 loss: 0.701054573059082, ACC:0.515625\n",
      "Training iteration 240 loss: 0.6912590265274048, ACC:0.53125\n",
      "Training iteration 241 loss: 0.7618218660354614, ACC:0.421875\n",
      "Training iteration 242 loss: 0.7036047577857971, ACC:0.546875\n",
      "Training iteration 243 loss: 0.6969029307365417, ACC:0.546875\n",
      "Training iteration 244 loss: 0.7186856865882874, ACC:0.4375\n",
      "Training iteration 245 loss: 0.691314160823822, ACC:0.53125\n",
      "Training iteration 246 loss: 0.6555865406990051, ACC:0.640625\n",
      "Training iteration 247 loss: 0.8057120442390442, ACC:0.453125\n",
      "Training iteration 248 loss: 0.6562510132789612, ACC:0.640625\n",
      "Training iteration 249 loss: 0.7653988599777222, ACC:0.46875\n",
      "Training iteration 250 loss: 0.715229868888855, ACC:0.453125\n",
      "Training iteration 251 loss: 0.6891292333602905, ACC:0.546875\n",
      "Training iteration 252 loss: 0.6329352855682373, ACC:0.671875\n",
      "Training iteration 253 loss: 0.768127977848053, ACC:0.5625\n",
      "Training iteration 254 loss: 0.8202527165412903, ACC:0.53125\n",
      "Training iteration 255 loss: 0.760880708694458, ACC:0.53125\n",
      "Training iteration 256 loss: 0.6866443753242493, ACC:0.5625\n",
      "Training iteration 257 loss: 0.6964717507362366, ACC:0.5\n",
      "Training iteration 258 loss: 0.6885389089584351, ACC:0.578125\n",
      "Training iteration 259 loss: 0.7786455154418945, ACC:0.5\n",
      "Training iteration 260 loss: 0.7789963483810425, ACC:0.484375\n",
      "Training iteration 261 loss: 0.7207945585250854, ACC:0.484375\n",
      "Training iteration 262 loss: 0.7046975493431091, ACC:0.390625\n",
      "Training iteration 263 loss: 0.7190715074539185, ACC:0.46875\n",
      "Training iteration 264 loss: 0.6878294944763184, ACC:0.5625\n",
      "Training iteration 265 loss: 0.7293596863746643, ACC:0.453125\n",
      "Training iteration 266 loss: 0.6913049221038818, ACC:0.53125\n",
      "Training iteration 267 loss: 0.6945872902870178, ACC:0.484375\n",
      "Training iteration 268 loss: 0.7084139585494995, ACC:0.453125\n",
      "Training iteration 269 loss: 0.6887465119361877, ACC:0.546875\n",
      "Training iteration 270 loss: 0.6914565563201904, ACC:0.53125\n",
      "Training iteration 271 loss: 0.6805007457733154, ACC:0.609375\n",
      "Training iteration 272 loss: 0.6923646330833435, ACC:0.53125\n",
      "Training iteration 273 loss: 0.705784261226654, ACC:0.484375\n",
      "Training iteration 274 loss: 0.7022767066955566, ACC:0.46875\n",
      "Training iteration 275 loss: 0.6949393153190613, ACC:0.421875\n",
      "Training iteration 276 loss: 0.6926631331443787, ACC:0.515625\n",
      "Training iteration 277 loss: 0.6861763000488281, ACC:0.578125\n",
      "Training iteration 278 loss: 0.6993210315704346, ACC:0.5\n",
      "Training iteration 279 loss: 0.7303293943405151, ACC:0.375\n",
      "Training iteration 280 loss: 0.6951161026954651, ACC:0.4375\n",
      "Training iteration 281 loss: 0.6939544677734375, ACC:0.515625\n",
      "Training iteration 282 loss: 0.7161277532577515, ACC:0.4375\n",
      "Training iteration 283 loss: 0.6972313523292542, ACC:0.484375\n",
      "Training iteration 284 loss: 0.6934738755226135, ACC:0.5\n",
      "Training iteration 285 loss: 0.7008353471755981, ACC:0.484375\n",
      "Training iteration 286 loss: 0.6958491206169128, ACC:0.515625\n",
      "Training iteration 287 loss: 0.6945960521697998, ACC:0.515625\n",
      "Training iteration 288 loss: 0.6979329586029053, ACC:0.46875\n",
      "Training iteration 289 loss: 0.6909520030021667, ACC:0.546875\n",
      "Training iteration 290 loss: 0.7089650630950928, ACC:0.46875\n",
      "Training iteration 291 loss: 0.6943223476409912, ACC:0.53125\n",
      "Training iteration 292 loss: 0.7049722671508789, ACC:0.484375\n",
      "Training iteration 293 loss: 0.6959959268569946, ACC:0.484375\n",
      "Training iteration 294 loss: 0.6958904266357422, ACC:0.484375\n",
      "Training iteration 295 loss: 0.6854751110076904, ACC:0.5625\n",
      "Training iteration 296 loss: 0.6966051459312439, ACC:0.53125\n",
      "Training iteration 297 loss: 0.6980838775634766, ACC:0.53125\n",
      "Training iteration 298 loss: 0.6947687864303589, ACC:0.53125\n",
      "Training iteration 299 loss: 0.6861312389373779, ACC:0.5625\n",
      "Training iteration 300 loss: 0.6941299438476562, ACC:0.5\n",
      "Training iteration 301 loss: 0.6943851113319397, ACC:0.4375\n",
      "Training iteration 302 loss: 0.6926435232162476, ACC:0.546875\n",
      "Training iteration 303 loss: 0.693808913230896, ACC:0.5\n",
      "Training iteration 304 loss: 0.6881503462791443, ACC:0.5625\n",
      "Training iteration 305 loss: 0.6887493133544922, ACC:0.546875\n",
      "Training iteration 306 loss: 0.6812626123428345, ACC:0.578125\n",
      "Training iteration 307 loss: 0.719346821308136, ACC:0.46875\n",
      "Training iteration 308 loss: 0.689174473285675, ACC:0.546875\n",
      "Training iteration 309 loss: 0.6993892192840576, ACC:0.46875\n",
      "Training iteration 310 loss: 0.6926738023757935, ACC:0.515625\n",
      "Training iteration 311 loss: 0.7130296230316162, ACC:0.453125\n",
      "Training iteration 312 loss: 0.7081031203269958, ACC:0.46875\n",
      "Training iteration 313 loss: 0.7011697888374329, ACC:0.421875\n",
      "Training iteration 314 loss: 0.692001461982727, ACC:0.53125\n",
      "Training iteration 315 loss: 0.7034527659416199, ACC:0.53125\n",
      "Training iteration 316 loss: 0.7030835151672363, ACC:0.546875\n",
      "Training iteration 317 loss: 0.6691923141479492, ACC:0.609375\n",
      "Training iteration 318 loss: 0.6823321580886841, ACC:0.578125\n",
      "Training iteration 319 loss: 0.6912508606910706, ACC:0.546875\n",
      "Training iteration 320 loss: 0.6827710270881653, ACC:0.578125\n",
      "Training iteration 321 loss: 0.6929857134819031, ACC:0.515625\n",
      "Training iteration 322 loss: 0.6950729489326477, ACC:0.421875\n",
      "Training iteration 323 loss: 0.688746452331543, ACC:0.546875\n",
      "Training iteration 324 loss: 0.6575955748558044, ACC:0.640625\n",
      "Training iteration 325 loss: 0.8002794981002808, ACC:0.40625\n",
      "Training iteration 326 loss: 0.633020281791687, ACC:0.6875\n",
      "Training iteration 327 loss: 0.7360687255859375, ACC:0.453125\n",
      "Training iteration 328 loss: 0.6938414573669434, ACC:0.515625\n",
      "Training iteration 329 loss: 0.6968437433242798, ACC:0.484375\n",
      "Training iteration 330 loss: 0.7279478311538696, ACC:0.421875\n",
      "Training iteration 331 loss: 0.6888968348503113, ACC:0.546875\n",
      "Training iteration 332 loss: 0.6825270056724548, ACC:0.59375\n",
      "Training iteration 333 loss: 0.7059866189956665, ACC:0.4375\n",
      "Training iteration 334 loss: 0.6929743885993958, ACC:0.515625\n",
      "Training iteration 335 loss: 0.6897570490837097, ACC:0.546875\n",
      "Training iteration 336 loss: 0.7035115957260132, ACC:0.484375\n",
      "Training iteration 337 loss: 0.6779125928878784, ACC:0.59375\n",
      "Training iteration 338 loss: 0.6855701804161072, ACC:0.5625\n",
      "Training iteration 339 loss: 0.6705297827720642, ACC:0.609375\n",
      "Training iteration 340 loss: 0.7519747614860535, ACC:0.40625\n",
      "Training iteration 341 loss: 0.6914235353469849, ACC:0.53125\n",
      "Training iteration 342 loss: 0.6916559934616089, ACC:0.53125\n",
      "Training iteration 343 loss: 0.6995236873626709, ACC:0.515625\n",
      "Training iteration 344 loss: 0.7275726199150085, ACC:0.46875\n",
      "Training iteration 345 loss: 0.6905984282493591, ACC:0.546875\n",
      "Training iteration 346 loss: 0.6966855525970459, ACC:0.5\n",
      "Training iteration 347 loss: 0.6911888122558594, ACC:0.578125\n",
      "Training iteration 348 loss: 0.7152948379516602, ACC:0.46875\n",
      "Training iteration 349 loss: 0.7005985975265503, ACC:0.53125\n",
      "Training iteration 350 loss: 0.7045373916625977, ACC:0.515625\n",
      "Training iteration 351 loss: 0.6678292751312256, ACC:0.640625\n",
      "Training iteration 352 loss: 0.6747731566429138, ACC:0.609375\n",
      "Training iteration 353 loss: 0.7296252250671387, ACC:0.421875\n",
      "Training iteration 354 loss: 0.6956838369369507, ACC:0.5\n",
      "Training iteration 355 loss: 0.6885124444961548, ACC:0.578125\n",
      "Training iteration 356 loss: 0.7085544466972351, ACC:0.5\n",
      "Training iteration 357 loss: 0.6691091656684875, ACC:0.609375\n",
      "Training iteration 358 loss: 0.6708276271820068, ACC:0.609375\n",
      "Training iteration 359 loss: 0.7770520448684692, ACC:0.4375\n",
      "Training iteration 360 loss: 0.6857938766479492, ACC:0.5625\n",
      "Training iteration 361 loss: 0.6932131052017212, ACC:0.5\n",
      "Training iteration 362 loss: 0.7046977281570435, ACC:0.484375\n",
      "Training iteration 363 loss: 0.7101078033447266, ACC:0.5\n",
      "Training iteration 364 loss: 0.6809321641921997, ACC:0.578125\n",
      "Training iteration 365 loss: 0.7039349675178528, ACC:0.5\n",
      "Training iteration 366 loss: 0.7001577019691467, ACC:0.46875\n",
      "Training iteration 367 loss: 0.6881511807441711, ACC:0.5625\n",
      "Training iteration 368 loss: 0.7522714138031006, ACC:0.390625\n",
      "Training iteration 369 loss: 0.7191758155822754, ACC:0.453125\n",
      "Training iteration 370 loss: 0.6948721408843994, ACC:0.46875\n",
      "Training iteration 371 loss: 0.7132495641708374, ACC:0.453125\n",
      "Training iteration 372 loss: 0.6865516901016235, ACC:0.5625\n",
      "Training iteration 373 loss: 0.7004923820495605, ACC:0.53125\n",
      "Training iteration 374 loss: 0.7019201517105103, ACC:0.515625\n",
      "Training iteration 375 loss: 0.7127866744995117, ACC:0.40625\n",
      "Training iteration 376 loss: 0.7009010314941406, ACC:0.484375\n",
      "Training iteration 377 loss: 0.6954191327095032, ACC:0.546875\n",
      "Training iteration 378 loss: 0.7702915668487549, ACC:0.421875\n",
      "Training iteration 379 loss: 0.695225715637207, ACC:0.53125\n",
      "Training iteration 380 loss: 0.6954564452171326, ACC:0.421875\n",
      "Training iteration 381 loss: 0.7256950736045837, ACC:0.453125\n",
      "Training iteration 382 loss: 0.7455798387527466, ACC:0.453125\n",
      "Training iteration 383 loss: 0.7451941967010498, ACC:0.390625\n",
      "Training iteration 384 loss: 0.6943114399909973, ACC:0.5\n",
      "Training iteration 385 loss: 0.6619487404823303, ACC:0.625\n",
      "Training iteration 386 loss: 0.7829025387763977, ACC:0.484375\n",
      "Training iteration 387 loss: 0.7430010437965393, ACC:0.53125\n",
      "Training iteration 388 loss: 0.7429627776145935, ACC:0.46875\n",
      "Training iteration 389 loss: 0.6938893795013428, ACC:0.484375\n",
      "Training iteration 390 loss: 0.7032933235168457, ACC:0.53125\n",
      "Training iteration 391 loss: 0.7609806656837463, ACC:0.5\n",
      "Training iteration 392 loss: 0.7945132851600647, ACC:0.453125\n",
      "Training iteration 393 loss: 0.7229938507080078, ACC:0.46875\n",
      "Training iteration 394 loss: 0.6999704837799072, ACC:0.46875\n",
      "Training iteration 395 loss: 0.7088074088096619, ACC:0.53125\n",
      "Training iteration 396 loss: 0.7294072508811951, ACC:0.53125\n",
      "Training iteration 397 loss: 0.7831680774688721, ACC:0.4375\n",
      "Training iteration 398 loss: 0.7210415005683899, ACC:0.421875\n",
      "Training iteration 399 loss: 0.7152717709541321, ACC:0.46875\n",
      "Training iteration 400 loss: 0.6639320850372314, ACC:0.625\n",
      "Training iteration 401 loss: 0.9062961935997009, ACC:0.375\n",
      "Training iteration 402 loss: 0.7951822280883789, ACC:0.40625\n",
      "Training iteration 403 loss: 0.6933629512786865, ACC:0.5\n",
      "Training iteration 404 loss: 0.6911923289299011, ACC:0.578125\n",
      "Training iteration 405 loss: 0.7840431332588196, ACC:0.53125\n",
      "Training iteration 406 loss: 0.8759058713912964, ACC:0.453125\n",
      "Training iteration 407 loss: 0.8152985572814941, ACC:0.390625\n",
      "Training iteration 408 loss: 0.6911996603012085, ACC:0.53125\n",
      "Training iteration 409 loss: 0.7122421264648438, ACC:0.578125\n",
      "Training iteration 410 loss: 0.8843211531639099, ACC:0.5\n",
      "Training iteration 411 loss: 0.7190217971801758, ACC:0.625\n",
      "Training iteration 412 loss: 0.8070213794708252, ACC:0.5\n",
      "Training iteration 413 loss: 0.6995229721069336, ACC:0.53125\n",
      "Training iteration 414 loss: 0.7194846272468567, ACC:0.421875\n",
      "Training iteration 415 loss: 0.6623630523681641, ACC:0.625\n",
      "Training iteration 416 loss: 0.7594658136367798, ACC:0.53125\n",
      "Training iteration 417 loss: 0.8382827043533325, ACC:0.4375\n",
      "Training iteration 418 loss: 0.766079843044281, ACC:0.390625\n",
      "Training iteration 419 loss: 0.702350914478302, ACC:0.5\n",
      "Training iteration 420 loss: 0.7987893223762512, ACC:0.46875\n",
      "Training iteration 421 loss: 0.8326610922813416, ACC:0.46875\n",
      "Training iteration 422 loss: 0.7521676421165466, ACC:0.5\n",
      "Training iteration 423 loss: 0.6791801452636719, ACC:0.59375\n",
      "Training iteration 424 loss: 0.710062563419342, ACC:0.421875\n",
      "Training iteration 425 loss: 0.7263498306274414, ACC:0.4375\n",
      "Training iteration 426 loss: 0.6967014074325562, ACC:0.515625\n",
      "Training iteration 427 loss: 0.7008013129234314, ACC:0.4375\n",
      "Training iteration 428 loss: 0.6963323950767517, ACC:0.5\n",
      "Training iteration 429 loss: 0.7070391774177551, ACC:0.5\n",
      "Training iteration 430 loss: 0.7438143491744995, ACC:0.40625\n",
      "Training iteration 431 loss: 0.6860535144805908, ACC:0.578125\n",
      "Training iteration 432 loss: 0.692072868347168, ACC:0.53125\n",
      "Training iteration 433 loss: 0.7019032835960388, ACC:0.484375\n",
      "Training iteration 434 loss: 0.6531566977500916, ACC:0.6875\n",
      "Training iteration 435 loss: 0.7381004691123962, ACC:0.46875\n",
      "Training iteration 436 loss: 0.6909639835357666, ACC:0.5625\n",
      "Training iteration 437 loss: 0.7270143628120422, ACC:0.453125\n",
      "Training iteration 438 loss: 0.6916349530220032, ACC:0.53125\n",
      "Training iteration 439 loss: 0.6744894981384277, ACC:0.625\n",
      "Training iteration 440 loss: 0.686336874961853, ACC:0.578125\n",
      "Training iteration 441 loss: 0.7875692248344421, ACC:0.46875\n",
      "Training iteration 442 loss: 0.7642737627029419, ACC:0.46875\n",
      "Training iteration 443 loss: 0.7051168084144592, ACC:0.484375\n",
      "Training iteration 444 loss: 0.6956936717033386, ACC:0.515625\n",
      "Training iteration 445 loss: 0.7352293133735657, ACC:0.5\n",
      "Training iteration 446 loss: 0.7742452621459961, ACC:0.46875\n",
      "Training iteration 447 loss: 0.7617016434669495, ACC:0.421875\n",
      "Training iteration 448 loss: 0.6933619379997253, ACC:0.46875\n",
      "Training iteration 449 loss: 0.6767069697380066, ACC:0.59375\n",
      "Training iteration 450 loss: 0.7739242315292358, ACC:0.515625\n",
      "Validation iteration 451 loss: 0.8173572421073914, ACC: 0.484375\n",
      "Validation iteration 452 loss: 0.9066402316093445, ACC: 0.390625\n",
      "Validation iteration 453 loss: 0.7875963449478149, ACC: 0.515625\n",
      "Validation iteration 454 loss: 0.8322377800941467, ACC: 0.46875\n",
      "Validation iteration 455 loss: 0.8917597532272339, ACC: 0.40625\n",
      "Validation iteration 456 loss: 0.8024768233299255, ACC: 0.5\n",
      "Validation iteration 457 loss: 0.8024768233299255, ACC: 0.5\n",
      "Validation iteration 458 loss: 0.8024768233299255, ACC: 0.5\n",
      "Validation iteration 459 loss: 0.8173573017120361, ACC: 0.484375\n",
      "Validation iteration 460 loss: 0.8322377800941467, ACC: 0.46875\n",
      "Validation iteration 461 loss: 0.8024768233299255, ACC: 0.5\n",
      "Validation iteration 462 loss: 0.8619987964630127, ACC: 0.4375\n",
      "Validation iteration 463 loss: 0.8917597532272339, ACC: 0.40625\n",
      "Validation iteration 464 loss: 0.8619987368583679, ACC: 0.4375\n",
      "Validation iteration 465 loss: 0.8322378396987915, ACC: 0.46875\n",
      "Validation iteration 466 loss: 0.8917596936225891, ACC: 0.40625\n",
      "Validation iteration 467 loss: 0.8322378396987915, ACC: 0.46875\n",
      "Validation iteration 468 loss: 0.8619987964630127, ACC: 0.4375\n",
      "Validation iteration 469 loss: 0.8322377800941467, ACC: 0.46875\n",
      "Validation iteration 470 loss: 0.8173573017120361, ACC: 0.484375\n",
      "Validation iteration 471 loss: 0.7727158665657043, ACC: 0.53125\n",
      "Validation iteration 472 loss: 0.683432936668396, ACC: 0.625\n",
      "Validation iteration 473 loss: 0.713193953037262, ACC: 0.59375\n",
      "Validation iteration 474 loss: 0.6387915015220642, ACC: 0.671875\n",
      "Validation iteration 475 loss: 0.8768792748451233, ACC: 0.421875\n",
      "Validation iteration 476 loss: 0.8173573613166809, ACC: 0.484375\n",
      "Validation iteration 477 loss: 0.8471182584762573, ACC: 0.453125\n",
      "Validation iteration 478 loss: 0.8173573613166809, ACC: 0.484375\n",
      "Validation iteration 479 loss: 0.7875963449478149, ACC: 0.515625\n",
      "Validation iteration 480 loss: 0.7727158665657043, ACC: 0.53125\n",
      "Validation iteration 481 loss: 0.7875962853431702, ACC: 0.515625\n",
      "Validation iteration 482 loss: 0.8917596936225891, ACC: 0.40625\n",
      "Validation iteration 483 loss: 0.8322378396987915, ACC: 0.46875\n",
      "Validation iteration 484 loss: 0.8322377800941467, ACC: 0.46875\n",
      "Validation iteration 485 loss: 0.8173573017120361, ACC: 0.484375\n",
      "Validation iteration 486 loss: 0.7727159261703491, ACC: 0.53125\n",
      "Validation iteration 487 loss: 0.8768792748451233, ACC: 0.421875\n",
      "Validation iteration 488 loss: 0.8322378396987915, ACC: 0.46875\n",
      "Validation iteration 489 loss: 0.7875964045524597, ACC: 0.515625\n",
      "Validation iteration 490 loss: 0.7875962853431702, ACC: 0.515625\n",
      "Validation iteration 491 loss: 0.8322378396987915, ACC: 0.46875\n",
      "Validation iteration 492 loss: 0.8024768233299255, ACC: 0.5\n",
      "Validation iteration 493 loss: 0.7131938934326172, ACC: 0.59375\n",
      "Validation iteration 494 loss: 0.9066402316093445, ACC: 0.390625\n",
      "Validation iteration 495 loss: 0.8619987964630127, ACC: 0.4375\n",
      "Validation iteration 496 loss: 0.8768792748451233, ACC: 0.421875\n",
      "Validation iteration 497 loss: 0.8917597532272339, ACC: 0.40625\n",
      "Validation iteration 498 loss: 0.7875963449478149, ACC: 0.515625\n",
      "Validation iteration 499 loss: 0.6685524582862854, ACC: 0.640625\n",
      "Validation iteration 500 loss: 0.8173573017120361, ACC: 0.484375\n",
      "-- Epoch 4 done -- Train loss: 0.7118703027566274, train ACC: 0.5027083333333333, val loss: 0.817654926776886, val ACC: 0.4840625\n",
      "<--- 9756.376902103424 seconds --->\n",
      "Training iteration 1 loss: 0.8917597532272339, ACC:0.40625\n",
      "Training iteration 2 loss: 0.7201100587844849, ACC:0.515625\n",
      "Training iteration 3 loss: 0.6936761140823364, ACC:0.46875\n",
      "Training iteration 4 loss: 0.7048404812812805, ACC:0.53125\n",
      "Training iteration 5 loss: 0.7702268362045288, ACC:0.484375\n",
      "Training iteration 6 loss: 0.7298315763473511, ACC:0.53125\n",
      "Training iteration 7 loss: 0.7177729606628418, ACC:0.5\n",
      "Training iteration 8 loss: 0.6907811760902405, ACC:0.546875\n",
      "Training iteration 9 loss: 0.7151703238487244, ACC:0.4375\n",
      "Training iteration 10 loss: 0.6909701228141785, ACC:0.546875\n",
      "Training iteration 11 loss: 0.7244543433189392, ACC:0.453125\n",
      "Training iteration 12 loss: 0.6993030309677124, ACC:0.484375\n",
      "Training iteration 13 loss: 0.6952295899391174, ACC:0.484375\n",
      "Training iteration 14 loss: 0.693146288394928, ACC:0.53125\n",
      "Training iteration 15 loss: 0.6922243237495422, ACC:0.546875\n",
      "Training iteration 16 loss: 0.6876099109649658, ACC:0.5625\n",
      "Training iteration 17 loss: 0.7098423838615417, ACC:0.5\n",
      "Training iteration 18 loss: 0.695320188999176, ACC:0.515625\n",
      "Training iteration 19 loss: 0.6915961503982544, ACC:0.640625\n",
      "Training iteration 20 loss: 0.6961067318916321, ACC:0.4375\n",
      "Training iteration 21 loss: 0.6933391094207764, ACC:0.5\n",
      "Training iteration 22 loss: 0.7020795345306396, ACC:0.421875\n",
      "Training iteration 23 loss: 0.6937147378921509, ACC:0.375\n",
      "Training iteration 24 loss: 0.692790687084198, ACC:0.515625\n",
      "Training iteration 25 loss: 0.696852445602417, ACC:0.5\n",
      "Training iteration 26 loss: 0.6970235109329224, ACC:0.5\n",
      "Training iteration 27 loss: 0.705287516117096, ACC:0.40625\n",
      "Training iteration 28 loss: 0.7034666538238525, ACC:0.4375\n",
      "Training iteration 29 loss: 0.6823869347572327, ACC:0.578125\n",
      "Training iteration 30 loss: 0.7199512124061584, ACC:0.453125\n",
      "Training iteration 31 loss: 0.682011067867279, ACC:0.578125\n",
      "Training iteration 32 loss: 0.688779890537262, ACC:0.546875\n",
      "Training iteration 33 loss: 0.6848897337913513, ACC:0.578125\n",
      "Training iteration 34 loss: 0.7099987268447876, ACC:0.421875\n",
      "Training iteration 35 loss: 0.6937044262886047, ACC:0.46875\n",
      "Training iteration 36 loss: 0.686026930809021, ACC:0.5625\n",
      "Training iteration 37 loss: 0.7225961685180664, ACC:0.46875\n",
      "Training iteration 38 loss: 0.7214639186859131, ACC:0.46875\n",
      "Training iteration 39 loss: 0.6887768507003784, ACC:0.546875\n",
      "Training iteration 40 loss: 0.6930444240570068, ACC:0.546875\n",
      "Training iteration 41 loss: 0.6852063536643982, ACC:0.609375\n",
      "Training iteration 42 loss: 0.7082434892654419, ACC:0.484375\n",
      "Training iteration 43 loss: 0.6864170432090759, ACC:0.5625\n",
      "Training iteration 44 loss: 0.7430559992790222, ACC:0.40625\n",
      "Training iteration 45 loss: 0.6783414483070374, ACC:0.671875\n",
      "Training iteration 46 loss: 0.6852376461029053, ACC:0.609375\n",
      "Training iteration 47 loss: 0.7016147375106812, ACC:0.484375\n",
      "Training iteration 48 loss: 0.7192292809486389, ACC:0.40625\n",
      "Training iteration 49 loss: 0.6931472420692444, ACC:0.5\n",
      "Training iteration 50 loss: 0.6918065547943115, ACC:0.53125\n",
      "Training iteration 51 loss: 0.7347611784934998, ACC:0.421875\n",
      "Training iteration 52 loss: 0.6782844662666321, ACC:0.59375\n",
      "Training iteration 53 loss: 0.7055445909500122, ACC:0.453125\n",
      "Training iteration 54 loss: 0.6931500434875488, ACC:0.5\n",
      "Training iteration 55 loss: 0.6808913350105286, ACC:0.59375\n",
      "Training iteration 56 loss: 0.7311131954193115, ACC:0.453125\n",
      "Training iteration 57 loss: 0.675464928150177, ACC:0.59375\n",
      "Training iteration 58 loss: 0.7094615697860718, ACC:0.5\n",
      "Training iteration 59 loss: 0.688809335231781, ACC:0.546875\n",
      "Training iteration 60 loss: 0.6926599740982056, ACC:0.515625\n",
      "Training iteration 61 loss: 0.693697452545166, ACC:0.5\n",
      "Training iteration 62 loss: 0.7013110518455505, ACC:0.46875\n",
      "Training iteration 63 loss: 0.6891095042228699, ACC:0.546875\n",
      "Training iteration 64 loss: 0.6973673105239868, ACC:0.484375\n",
      "Training iteration 65 loss: 0.6935214400291443, ACC:0.5\n",
      "Training iteration 66 loss: 0.6955324411392212, ACC:0.4375\n",
      "Training iteration 67 loss: 0.6929430365562439, ACC:0.53125\n",
      "Training iteration 68 loss: 0.6924102902412415, ACC:0.53125\n",
      "Training iteration 69 loss: 0.6901389956474304, ACC:0.546875\n",
      "Training iteration 70 loss: 0.7104213833808899, ACC:0.421875\n",
      "Training iteration 71 loss: 0.6905536651611328, ACC:0.546875\n",
      "Training iteration 72 loss: 0.6923138499259949, ACC:0.546875\n",
      "Training iteration 73 loss: 0.6908397674560547, ACC:0.5625\n",
      "Training iteration 74 loss: 0.6978178024291992, ACC:0.484375\n",
      "Training iteration 75 loss: 0.693670928478241, ACC:0.515625\n",
      "Training iteration 76 loss: 0.7032435536384583, ACC:0.4375\n",
      "Training iteration 77 loss: 0.6958342790603638, ACC:0.4375\n",
      "Training iteration 78 loss: 0.692692756652832, ACC:0.515625\n",
      "Training iteration 79 loss: 0.7047162652015686, ACC:0.40625\n",
      "Training iteration 80 loss: 0.6985114216804504, ACC:0.40625\n",
      "Training iteration 81 loss: 0.6922663450241089, ACC:0.53125\n",
      "Training iteration 82 loss: 0.6926803588867188, ACC:0.515625\n",
      "Training iteration 83 loss: 0.6948471665382385, ACC:0.484375\n",
      "Training iteration 84 loss: 0.6927415728569031, ACC:0.515625\n",
      "Training iteration 85 loss: 0.6931835412979126, ACC:0.5\n",
      "Training iteration 86 loss: 0.6937292814254761, ACC:0.421875\n",
      "Training iteration 87 loss: 0.7002255320549011, ACC:0.4375\n",
      "Training iteration 88 loss: 0.6918401122093201, ACC:0.53125\n",
      "Training iteration 89 loss: 0.6906780004501343, ACC:0.5625\n",
      "Training iteration 90 loss: 0.6947976350784302, ACC:0.5\n",
      "Training iteration 91 loss: 0.6933096051216125, ACC:0.515625\n",
      "Training iteration 92 loss: 0.6950821876525879, ACC:0.5\n",
      "Training iteration 93 loss: 0.7009970545768738, ACC:0.390625\n",
      "Training iteration 94 loss: 0.6773726344108582, ACC:0.609375\n",
      "Training iteration 95 loss: 0.7406172752380371, ACC:0.46875\n",
      "Training iteration 96 loss: 0.7572136521339417, ACC:0.453125\n",
      "Training iteration 97 loss: 0.749879002571106, ACC:0.359375\n",
      "Training iteration 98 loss: 0.7003660202026367, ACC:0.5\n",
      "Training iteration 99 loss: 0.7346158027648926, ACC:0.515625\n",
      "Training iteration 100 loss: 0.7577603459358215, ACC:0.515625\n",
      "Training iteration 101 loss: 0.7668836116790771, ACC:0.46875\n",
      "Training iteration 102 loss: 0.7040451765060425, ACC:0.484375\n",
      "Training iteration 103 loss: 0.6853150725364685, ACC:0.5625\n",
      "Training iteration 104 loss: 0.807265043258667, ACC:0.421875\n",
      "Training iteration 105 loss: 0.7444940209388733, ACC:0.515625\n",
      "Training iteration 106 loss: 0.7837331891059875, ACC:0.375\n",
      "Training iteration 107 loss: 0.7000419497489929, ACC:0.4375\n",
      "Training iteration 108 loss: 0.6994496583938599, ACC:0.546875\n",
      "Training iteration 109 loss: 0.7121099829673767, ACC:0.5625\n",
      "Training iteration 110 loss: 0.7774216532707214, ACC:0.484375\n",
      "Training iteration 111 loss: 0.7400428056716919, ACC:0.46875\n",
      "Training iteration 112 loss: 0.6924775838851929, ACC:0.53125\n",
      "Training iteration 113 loss: 0.7042391896247864, ACC:0.515625\n",
      "Training iteration 114 loss: 0.747564435005188, ACC:0.484375\n",
      "Training iteration 115 loss: 0.7061522603034973, ACC:0.546875\n",
      "Training iteration 116 loss: 0.6754811406135559, ACC:0.59375\n",
      "Training iteration 117 loss: 0.7032899856567383, ACC:0.484375\n",
      "Training iteration 118 loss: 0.6968573927879333, ACC:0.375\n",
      "Training iteration 119 loss: 0.693550169467926, ACC:0.5\n",
      "Training iteration 120 loss: 0.6964090466499329, ACC:0.453125\n",
      "Training iteration 121 loss: 0.6932820081710815, ACC:0.5\n",
      "Training iteration 122 loss: 0.6944918632507324, ACC:0.5\n",
      "Training iteration 123 loss: 0.7051320672035217, ACC:0.421875\n",
      "Training iteration 124 loss: 0.692868709564209, ACC:0.515625\n",
      "Training iteration 125 loss: 0.6887799501419067, ACC:0.546875\n",
      "Training iteration 126 loss: 0.695562481880188, ACC:0.53125\n",
      "Training iteration 127 loss: 0.7330455780029297, ACC:0.4375\n",
      "Training iteration 128 loss: 0.6887540221214294, ACC:0.546875\n",
      "Training iteration 129 loss: 0.6927451491355896, ACC:0.53125\n",
      "Training iteration 130 loss: 0.6898659467697144, ACC:0.546875\n",
      "Training iteration 131 loss: 0.671275794506073, ACC:0.625\n",
      "Training iteration 132 loss: 0.6407659649848938, ACC:0.671875\n",
      "Training iteration 133 loss: 0.7619548439979553, ACC:0.5\n",
      "Training iteration 134 loss: 0.7948126196861267, ACC:0.453125\n",
      "Training iteration 135 loss: 0.7534308433532715, ACC:0.390625\n",
      "Training iteration 136 loss: 0.681369960308075, ACC:0.578125\n",
      "Training iteration 137 loss: 0.8603090047836304, ACC:0.40625\n",
      "Training iteration 138 loss: 0.8121965527534485, ACC:0.484375\n",
      "Training iteration 139 loss: 0.7039463520050049, ACC:0.5625\n",
      "Training iteration 140 loss: 0.7005725502967834, ACC:0.5\n",
      "Training iteration 141 loss: 0.7052453756332397, ACC:0.46875\n",
      "Training iteration 142 loss: 0.7382590174674988, ACC:0.46875\n",
      "Training iteration 143 loss: 0.6854313015937805, ACC:0.578125\n",
      "Training iteration 144 loss: 0.7187275290489197, ACC:0.5\n",
      "Training iteration 145 loss: 0.710821270942688, ACC:0.453125\n",
      "Training iteration 146 loss: 0.6871911883354187, ACC:0.5625\n",
      "Training iteration 147 loss: 0.7240948677062988, ACC:0.5\n",
      "Training iteration 148 loss: 0.7646310925483704, ACC:0.46875\n",
      "Training iteration 149 loss: 0.7227039337158203, ACC:0.5\n",
      "Training iteration 150 loss: 0.6939056515693665, ACC:0.515625\n",
      "Training iteration 151 loss: 0.7046860456466675, ACC:0.453125\n",
      "Training iteration 152 loss: 0.7481721639633179, ACC:0.375\n",
      "Training iteration 153 loss: 0.6839354038238525, ACC:0.578125\n",
      "Training iteration 154 loss: 0.691703200340271, ACC:0.53125\n",
      "Training iteration 155 loss: 0.6932494044303894, ACC:0.40625\n",
      "Training iteration 156 loss: 0.6927383542060852, ACC:0.515625\n",
      "Training iteration 157 loss: 0.7036737203598022, ACC:0.453125\n",
      "Training iteration 158 loss: 0.6927411556243896, ACC:0.515625\n",
      "Training iteration 159 loss: 0.6948668360710144, ACC:0.390625\n",
      "Training iteration 160 loss: 0.7064726948738098, ACC:0.46875\n",
      "Training iteration 161 loss: 0.743140459060669, ACC:0.375\n",
      "Training iteration 162 loss: 0.6871971487998962, ACC:0.59375\n",
      "Training iteration 163 loss: 0.6932270526885986, ACC:0.5\n",
      "Training iteration 164 loss: 0.6896803379058838, ACC:0.546875\n",
      "Training iteration 165 loss: 0.6951209902763367, ACC:0.515625\n",
      "Training iteration 166 loss: 0.7003731727600098, ACC:0.5\n",
      "Training iteration 167 loss: 0.7001302242279053, ACC:0.484375\n",
      "Training iteration 168 loss: 0.6905683279037476, ACC:0.578125\n",
      "Training iteration 169 loss: 0.692547082901001, ACC:0.5625\n",
      "Training iteration 170 loss: 0.6964474320411682, ACC:0.46875\n",
      "Training iteration 171 loss: 0.6946572065353394, ACC:0.484375\n",
      "Training iteration 172 loss: 0.6931132078170776, ACC:0.5625\n",
      "Training iteration 173 loss: 0.6920960545539856, ACC:0.53125\n",
      "Training iteration 174 loss: 0.6963151097297668, ACC:0.484375\n",
      "Training iteration 175 loss: 0.6961218118667603, ACC:0.484375\n",
      "Training iteration 176 loss: 0.6971652507781982, ACC:0.390625\n",
      "Training iteration 177 loss: 0.7347002029418945, ACC:0.328125\n",
      "Training iteration 178 loss: 0.690117597579956, ACC:0.546875\n",
      "Training iteration 179 loss: 0.6928675770759583, ACC:0.53125\n",
      "Training iteration 180 loss: 0.6931659579277039, ACC:0.5\n",
      "Training iteration 181 loss: 0.6898763179779053, ACC:0.625\n",
      "Training iteration 182 loss: 0.6958852410316467, ACC:0.515625\n",
      "Training iteration 183 loss: 0.6809563636779785, ACC:0.578125\n",
      "Training iteration 184 loss: 0.7028791308403015, ACC:0.53125\n",
      "Training iteration 185 loss: 0.7633107900619507, ACC:0.375\n",
      "Training iteration 186 loss: 0.6888346076011658, ACC:0.671875\n",
      "Training iteration 187 loss: 0.6935399174690247, ACC:0.5\n",
      "Training iteration 188 loss: 0.691230058670044, ACC:0.53125\n",
      "Training iteration 189 loss: 0.698909342288971, ACC:0.484375\n",
      "Training iteration 190 loss: 0.6988549828529358, ACC:0.46875\n",
      "Training iteration 191 loss: 0.6932831406593323, ACC:0.484375\n",
      "Training iteration 192 loss: 0.6912813782691956, ACC:0.53125\n",
      "Training iteration 193 loss: 0.6946880221366882, ACC:0.515625\n",
      "Training iteration 194 loss: 0.681922197341919, ACC:0.578125\n",
      "Training iteration 195 loss: 0.7215057015419006, ACC:0.4375\n",
      "Training iteration 196 loss: 0.7020073533058167, ACC:0.453125\n",
      "Training iteration 197 loss: 0.7085531949996948, ACC:0.390625\n",
      "Training iteration 198 loss: 0.6956247091293335, ACC:0.5\n",
      "Training iteration 199 loss: 0.691260576248169, ACC:0.53125\n",
      "Training iteration 200 loss: 0.6938640475273132, ACC:0.5\n",
      "Training iteration 201 loss: 0.6942796111106873, ACC:0.453125\n",
      "Training iteration 202 loss: 0.6912347078323364, ACC:0.53125\n",
      "Training iteration 203 loss: 0.7214034795761108, ACC:0.40625\n",
      "Training iteration 204 loss: 0.6929003000259399, ACC:0.515625\n",
      "Training iteration 205 loss: 0.6935593485832214, ACC:0.484375\n",
      "Training iteration 206 loss: 0.6928876042366028, ACC:0.515625\n",
      "Training iteration 207 loss: 0.6939272284507751, ACC:0.515625\n",
      "Training iteration 208 loss: 0.6887629628181458, ACC:0.546875\n",
      "Training iteration 209 loss: 0.6977785229682922, ACC:0.5\n",
      "Training iteration 210 loss: 0.6829360723495483, ACC:0.59375\n",
      "Training iteration 211 loss: 0.66987544298172, ACC:0.65625\n",
      "Training iteration 212 loss: 0.7237542867660522, ACC:0.46875\n",
      "Training iteration 213 loss: 0.7170686721801758, ACC:0.484375\n",
      "Training iteration 214 loss: 0.6855931282043457, ACC:0.5625\n",
      "Training iteration 215 loss: 0.6897034645080566, ACC:0.5625\n",
      "Training iteration 216 loss: 0.6934556365013123, ACC:0.46875\n",
      "Training iteration 217 loss: 0.6899079084396362, ACC:0.546875\n",
      "Training iteration 218 loss: 0.678565502166748, ACC:0.59375\n",
      "Training iteration 219 loss: 0.762282133102417, ACC:0.375\n",
      "Training iteration 220 loss: 0.6984860301017761, ACC:0.5\n",
      "Training iteration 221 loss: 0.6926981210708618, ACC:0.515625\n",
      "Training iteration 222 loss: 0.7158417701721191, ACC:0.453125\n",
      "Training iteration 223 loss: 0.7000725865364075, ACC:0.515625\n",
      "Training iteration 224 loss: 0.7001930475234985, ACC:0.5\n",
      "Training iteration 225 loss: 0.6927100419998169, ACC:0.515625\n",
      "Training iteration 226 loss: 0.690639317035675, ACC:0.546875\n",
      "Training iteration 227 loss: 0.685318112373352, ACC:0.5625\n",
      "Training iteration 228 loss: 0.6948371529579163, ACC:0.546875\n",
      "Training iteration 229 loss: 0.7357916831970215, ACC:0.46875\n",
      "Training iteration 230 loss: 0.6668921709060669, ACC:0.625\n",
      "Training iteration 231 loss: 0.7096590399742126, ACC:0.453125\n",
      "Training iteration 232 loss: 0.6943677663803101, ACC:0.359375\n",
      "Training iteration 233 loss: 0.6928917765617371, ACC:0.53125\n",
      "Training iteration 234 loss: 0.6959272027015686, ACC:0.46875\n",
      "Training iteration 235 loss: 0.6979565024375916, ACC:0.40625\n",
      "Training iteration 236 loss: 0.6869197487831116, ACC:0.5625\n",
      "Training iteration 237 loss: 0.7028257250785828, ACC:0.515625\n",
      "Training iteration 238 loss: 0.7024920582771301, ACC:0.53125\n",
      "Training iteration 239 loss: 0.7053533792495728, ACC:0.515625\n",
      "Training iteration 240 loss: 0.6888543367385864, ACC:0.546875\n",
      "Training iteration 241 loss: 0.6971566677093506, ACC:0.4375\n",
      "Training iteration 242 loss: 0.7017728686332703, ACC:0.484375\n",
      "Training iteration 243 loss: 0.734138011932373, ACC:0.421875\n",
      "Training iteration 244 loss: 0.6919628381729126, ACC:0.53125\n",
      "Training iteration 245 loss: 0.6934626698493958, ACC:0.5\n",
      "Training iteration 246 loss: 0.7013456225395203, ACC:0.4375\n",
      "Training iteration 247 loss: 0.6980510354042053, ACC:0.46875\n",
      "Training iteration 248 loss: 0.6934709548950195, ACC:0.484375\n",
      "Training iteration 249 loss: 0.6898043751716614, ACC:0.546875\n",
      "Training iteration 250 loss: 0.7110006213188171, ACC:0.453125\n",
      "Training iteration 251 loss: 0.6947885155677795, ACC:0.515625\n",
      "Training iteration 252 loss: 0.6961513161659241, ACC:0.484375\n",
      "Training iteration 253 loss: 0.6952856183052063, ACC:0.453125\n",
      "Training iteration 254 loss: 0.6927061676979065, ACC:0.515625\n",
      "Training iteration 255 loss: 0.7016345262527466, ACC:0.4375\n",
      "Training iteration 256 loss: 0.69383704662323, ACC:0.375\n",
      "Training iteration 257 loss: 0.6927499175071716, ACC:0.515625\n",
      "Training iteration 258 loss: 0.7016606330871582, ACC:0.46875\n",
      "Training iteration 259 loss: 0.7003728747367859, ACC:0.453125\n",
      "Training iteration 260 loss: 0.6983944177627563, ACC:0.390625\n",
      "Training iteration 261 loss: 0.693084180355072, ACC:0.5625\n",
      "Training iteration 262 loss: 0.6954485774040222, ACC:0.46875\n",
      "Training iteration 263 loss: 0.6896774768829346, ACC:0.59375\n",
      "Training iteration 264 loss: 0.6961008906364441, ACC:0.5\n",
      "Training iteration 265 loss: 0.6979542374610901, ACC:0.5\n",
      "Training iteration 266 loss: 0.7084766030311584, ACC:0.421875\n",
      "Training iteration 267 loss: 0.6916494965553284, ACC:0.53125\n",
      "Training iteration 268 loss: 0.6989837288856506, ACC:0.515625\n",
      "Training iteration 269 loss: 0.7568630576133728, ACC:0.390625\n",
      "Training iteration 270 loss: 0.6968123316764832, ACC:0.5\n",
      "Training iteration 271 loss: 0.6928611397743225, ACC:0.515625\n",
      "Training iteration 272 loss: 0.7069997787475586, ACC:0.5\n",
      "Training iteration 273 loss: 0.7396852970123291, ACC:0.4375\n",
      "Training iteration 274 loss: 0.6785786747932434, ACC:0.59375\n",
      "Training iteration 275 loss: 0.7049478888511658, ACC:0.40625\n",
      "Training iteration 276 loss: 0.7057438492774963, ACC:0.453125\n",
      "Training iteration 277 loss: 0.7134239673614502, ACC:0.46875\n",
      "Training iteration 278 loss: 0.6926828622817993, ACC:0.53125\n",
      "Training iteration 279 loss: 0.7017834186553955, ACC:0.453125\n",
      "Training iteration 280 loss: 0.6965587735176086, ACC:0.46875\n",
      "Training iteration 281 loss: 0.6948713660240173, ACC:0.515625\n",
      "Training iteration 282 loss: 0.7083175778388977, ACC:0.46875\n",
      "Training iteration 283 loss: 0.6957073211669922, ACC:0.5\n",
      "Training iteration 284 loss: 0.6922658085823059, ACC:0.65625\n",
      "Training iteration 285 loss: 0.7297163605690002, ACC:0.453125\n",
      "Training iteration 286 loss: 0.7321328520774841, ACC:0.484375\n",
      "Training iteration 287 loss: 0.735791802406311, ACC:0.4375\n",
      "Training iteration 288 loss: 0.6893471479415894, ACC:0.625\n",
      "Training iteration 289 loss: 0.700803279876709, ACC:0.4375\n",
      "Training iteration 290 loss: 0.6914244890213013, ACC:0.53125\n",
      "Training iteration 291 loss: 0.6888901591300964, ACC:0.5625\n",
      "Training iteration 292 loss: 0.6935650110244751, ACC:0.515625\n",
      "Training iteration 293 loss: 0.6914865970611572, ACC:0.53125\n",
      "Training iteration 294 loss: 0.6887587904930115, ACC:0.546875\n",
      "Training iteration 295 loss: 0.7061586380004883, ACC:0.453125\n",
      "Training iteration 296 loss: 0.6920074820518494, ACC:0.53125\n",
      "Training iteration 297 loss: 0.6934705376625061, ACC:0.5\n",
      "Training iteration 298 loss: 0.7057411670684814, ACC:0.40625\n",
      "Training iteration 299 loss: 0.6941620111465454, ACC:0.453125\n",
      "Training iteration 300 loss: 0.6955093145370483, ACC:0.46875\n",
      "Training iteration 301 loss: 0.6917778849601746, ACC:0.5625\n",
      "Training iteration 302 loss: 0.7001467347145081, ACC:0.421875\n",
      "Training iteration 303 loss: 0.6916041374206543, ACC:0.5625\n",
      "Training iteration 304 loss: 0.7085400819778442, ACC:0.453125\n",
      "Training iteration 305 loss: 0.6922608613967896, ACC:0.53125\n",
      "Training iteration 306 loss: 0.7210245728492737, ACC:0.375\n",
      "Training iteration 307 loss: 0.6929835677146912, ACC:0.515625\n",
      "Training iteration 308 loss: 0.7324532270431519, ACC:0.4375\n",
      "Training iteration 309 loss: 0.7243454456329346, ACC:0.453125\n",
      "Training iteration 310 loss: 0.7023387551307678, ACC:0.4375\n",
      "Training iteration 311 loss: 0.718170166015625, ACC:0.421875\n",
      "Training iteration 312 loss: 0.7329465746879578, ACC:0.421875\n",
      "Training iteration 313 loss: 0.6992047429084778, ACC:0.484375\n",
      "Training iteration 314 loss: 0.6880126595497131, ACC:0.578125\n",
      "Training iteration 315 loss: 0.7298292517662048, ACC:0.453125\n",
      "Training iteration 316 loss: 0.7790687084197998, ACC:0.359375\n",
      "Training iteration 317 loss: 0.6942111253738403, ACC:0.5\n",
      "Training iteration 318 loss: 0.6812929511070251, ACC:0.578125\n",
      "Training iteration 319 loss: 0.7925927639007568, ACC:0.40625\n",
      "Training iteration 320 loss: 0.7102863192558289, ACC:0.53125\n",
      "Training iteration 321 loss: 0.7089813351631165, ACC:0.484375\n",
      "Training iteration 322 loss: 0.691403329372406, ACC:0.546875\n",
      "Training iteration 323 loss: 0.7200721502304077, ACC:0.484375\n",
      "Training iteration 324 loss: 0.7564806342124939, ACC:0.453125\n",
      "Training iteration 325 loss: 0.7000349164009094, ACC:0.53125\n",
      "Training iteration 326 loss: 0.693524181842804, ACC:0.515625\n",
      "Training iteration 327 loss: 0.6980726718902588, ACC:0.46875\n",
      "Training iteration 328 loss: 0.6930147409439087, ACC:0.53125\n",
      "Training iteration 329 loss: 0.7060250043869019, ACC:0.5\n",
      "Training iteration 330 loss: 0.7263578176498413, ACC:0.40625\n",
      "Training iteration 331 loss: 0.6954226493835449, ACC:0.453125\n",
      "Training iteration 332 loss: 0.7031347751617432, ACC:0.484375\n",
      "Training iteration 333 loss: 0.7070921659469604, ACC:0.484375\n",
      "Training iteration 334 loss: 0.7005224823951721, ACC:0.484375\n",
      "Training iteration 335 loss: 0.6930249333381653, ACC:0.515625\n",
      "Training iteration 336 loss: 0.7077220678329468, ACC:0.40625\n",
      "Training iteration 337 loss: 0.6979814767837524, ACC:0.421875\n",
      "Training iteration 338 loss: 0.6888562440872192, ACC:0.546875\n",
      "Training iteration 339 loss: 0.7266344428062439, ACC:0.453125\n",
      "Training iteration 340 loss: 0.6860814690589905, ACC:0.5625\n",
      "Training iteration 341 loss: 0.7064187526702881, ACC:0.484375\n",
      "Training iteration 342 loss: 0.6914133429527283, ACC:0.53125\n",
      "Training iteration 343 loss: 0.6915980577468872, ACC:0.53125\n",
      "Training iteration 344 loss: 0.692481279373169, ACC:0.53125\n",
      "Training iteration 345 loss: 0.7375892996788025, ACC:0.40625\n",
      "Training iteration 346 loss: 0.6890085339546204, ACC:0.546875\n",
      "Training iteration 347 loss: 0.6936999559402466, ACC:0.4375\n",
      "Training iteration 348 loss: 0.6933584213256836, ACC:0.5\n",
      "Training iteration 349 loss: 0.6926681995391846, ACC:0.515625\n",
      "Training iteration 350 loss: 0.6948145031929016, ACC:0.484375\n",
      "Training iteration 351 loss: 0.6900026798248291, ACC:0.609375\n",
      "Training iteration 352 loss: 0.6839315295219421, ACC:0.578125\n",
      "Training iteration 353 loss: 0.7280512452125549, ACC:0.4375\n",
      "Training iteration 354 loss: 0.6935061812400818, ACC:0.53125\n",
      "Training iteration 355 loss: 0.6956506371498108, ACC:0.5\n",
      "Training iteration 356 loss: 0.6921268105506897, ACC:0.546875\n",
      "Training iteration 357 loss: 0.696466326713562, ACC:0.515625\n",
      "Training iteration 358 loss: 0.7036802768707275, ACC:0.515625\n",
      "Training iteration 359 loss: 0.6976425051689148, ACC:0.53125\n",
      "Training iteration 360 loss: 0.7053939700126648, ACC:0.484375\n",
      "Training iteration 361 loss: 0.6953604221343994, ACC:0.453125\n",
      "Training iteration 362 loss: 0.6967150568962097, ACC:0.515625\n",
      "Training iteration 363 loss: 0.7493190169334412, ACC:0.421875\n",
      "Training iteration 364 loss: 0.6996619701385498, ACC:0.515625\n",
      "Training iteration 365 loss: 0.6885280609130859, ACC:0.5625\n",
      "Training iteration 366 loss: 0.6900608539581299, ACC:0.59375\n",
      "Training iteration 367 loss: 0.7385886907577515, ACC:0.375\n",
      "Training iteration 368 loss: 0.6917781829833984, ACC:0.53125\n",
      "Training iteration 369 loss: 0.6913867592811584, ACC:0.53125\n",
      "Training iteration 370 loss: 0.6928898692131042, ACC:0.5625\n",
      "Training iteration 371 loss: 0.6937160491943359, ACC:0.484375\n",
      "Training iteration 372 loss: 0.695090115070343, ACC:0.375\n",
      "Training iteration 373 loss: 0.7111385464668274, ACC:0.4375\n",
      "Training iteration 374 loss: 0.6682535409927368, ACC:0.640625\n",
      "Training iteration 375 loss: 0.6696611046791077, ACC:0.609375\n",
      "Training iteration 376 loss: 0.7119635939598083, ACC:0.53125\n",
      "Training iteration 377 loss: 0.7845606803894043, ACC:0.390625\n",
      "Training iteration 378 loss: 0.7098526358604431, ACC:0.390625\n",
      "Training iteration 379 loss: 0.6900533437728882, ACC:0.5625\n",
      "Training iteration 380 loss: 0.776146411895752, ACC:0.515625\n",
      "Training iteration 381 loss: 0.9022372961044312, ACC:0.40625\n",
      "Training iteration 382 loss: 0.7308396100997925, ACC:0.5\n",
      "Training iteration 383 loss: 0.6974999308586121, ACC:0.421875\n",
      "Training iteration 384 loss: 0.7323291301727295, ACC:0.46875\n",
      "Training iteration 385 loss: 0.6894702911376953, ACC:0.578125\n",
      "Training iteration 386 loss: 0.7657103538513184, ACC:0.453125\n",
      "Training iteration 387 loss: 0.6905767321586609, ACC:0.546875\n",
      "Training iteration 388 loss: 0.6931848526000977, ACC:0.40625\n",
      "Training iteration 389 loss: 0.7559369206428528, ACC:0.40625\n",
      "Training iteration 390 loss: 0.7235509157180786, ACC:0.5\n",
      "Training iteration 391 loss: 0.7152704000473022, ACC:0.484375\n",
      "Training iteration 392 loss: 0.6926845908164978, ACC:0.515625\n",
      "Training iteration 393 loss: 0.6975131630897522, ACC:0.5\n",
      "Training iteration 394 loss: 0.6865177154541016, ACC:0.5625\n",
      "Training iteration 395 loss: 0.6977376937866211, ACC:0.546875\n",
      "Training iteration 396 loss: 0.6972623467445374, ACC:0.546875\n",
      "Training iteration 397 loss: 0.6809834241867065, ACC:0.578125\n",
      "Training iteration 398 loss: 0.7182536125183105, ACC:0.421875\n",
      "Training iteration 399 loss: 0.6901600360870361, ACC:0.546875\n",
      "Training iteration 400 loss: 0.6819308996200562, ACC:0.578125\n",
      "Training iteration 401 loss: 0.7787673473358154, ACC:0.453125\n",
      "Training iteration 402 loss: 0.7771053314208984, ACC:0.421875\n",
      "Training iteration 403 loss: 0.6891055107116699, ACC:0.546875\n",
      "Training iteration 404 loss: 0.6999859809875488, ACC:0.5\n",
      "Training iteration 405 loss: 0.7652125954627991, ACC:0.40625\n",
      "Training iteration 406 loss: 0.7253237366676331, ACC:0.453125\n",
      "Training iteration 407 loss: 0.6953545808792114, ACC:0.4375\n",
      "Training iteration 408 loss: 0.7100086808204651, ACC:0.5\n",
      "Training iteration 409 loss: 0.744444727897644, ACC:0.484375\n",
      "Training iteration 410 loss: 0.7029119729995728, ACC:0.546875\n",
      "Training iteration 411 loss: 0.7135894894599915, ACC:0.484375\n",
      "Training iteration 412 loss: 0.6942734718322754, ACC:0.453125\n",
      "Training iteration 413 loss: 0.670026421546936, ACC:0.609375\n",
      "Training iteration 414 loss: 0.7009949684143066, ACC:0.578125\n",
      "Training iteration 415 loss: 0.8075515627861023, ACC:0.484375\n",
      "Training iteration 416 loss: 0.7663055062294006, ACC:0.484375\n",
      "Training iteration 417 loss: 0.7261750102043152, ACC:0.390625\n",
      "Training iteration 418 loss: 0.7117845416069031, ACC:0.515625\n",
      "Training iteration 419 loss: 0.7535207867622375, ACC:0.546875\n",
      "Training iteration 420 loss: 0.8667933940887451, ACC:0.46875\n",
      "Training iteration 421 loss: 0.7433281540870667, ACC:0.53125\n",
      "Training iteration 422 loss: 0.7062772512435913, ACC:0.484375\n",
      "Training iteration 423 loss: 0.7113656401634216, ACC:0.484375\n",
      "Training iteration 424 loss: 0.7653738856315613, ACC:0.484375\n",
      "Training iteration 425 loss: 0.7159156203269958, ACC:0.5625\n",
      "Training iteration 426 loss: 0.7978684902191162, ACC:0.40625\n",
      "Training iteration 427 loss: 0.686166524887085, ACC:0.578125\n",
      "Training iteration 428 loss: 0.6895735859870911, ACC:0.546875\n",
      "Training iteration 429 loss: 0.752467691898346, ACC:0.46875\n",
      "Training iteration 430 loss: 0.6987883448600769, ACC:0.5625\n",
      "Training iteration 431 loss: 0.7273218631744385, ACC:0.484375\n",
      "Training iteration 432 loss: 0.7081339955329895, ACC:0.421875\n",
      "Training iteration 433 loss: 0.6954305768013, ACC:0.53125\n",
      "Training iteration 434 loss: 0.7562741637229919, ACC:0.484375\n",
      "Training iteration 435 loss: 0.784629762172699, ACC:0.453125\n",
      "Training iteration 436 loss: 0.7367159724235535, ACC:0.4375\n",
      "Training iteration 437 loss: 0.6871538758277893, ACC:0.5625\n",
      "Training iteration 438 loss: 0.7040601968765259, ACC:0.5625\n",
      "Training iteration 439 loss: 0.8653877973556519, ACC:0.4375\n",
      "Training iteration 440 loss: 0.733799934387207, ACC:0.546875\n",
      "Training iteration 441 loss: 0.7565200328826904, ACC:0.40625\n",
      "Training iteration 442 loss: 0.6888337731361389, ACC:0.546875\n",
      "Training iteration 443 loss: 0.7806325554847717, ACC:0.484375\n",
      "Training iteration 444 loss: 0.9473943710327148, ACC:0.359375\n",
      "Training iteration 445 loss: 0.7264895439147949, ACC:0.515625\n",
      "Training iteration 446 loss: 0.692755937576294, ACC:0.515625\n",
      "Training iteration 447 loss: 0.6973015666007996, ACC:0.546875\n",
      "Training iteration 448 loss: 0.8064361810684204, ACC:0.453125\n",
      "Training iteration 449 loss: 0.689967155456543, ACC:0.59375\n",
      "Training iteration 450 loss: 0.7392386794090271, ACC:0.484375\n",
      "Validation iteration 451 loss: 0.6986676454544067, ACC: 0.484375\n",
      "Validation iteration 452 loss: 0.6790667772293091, ACC: 0.609375\n",
      "Validation iteration 453 loss: 0.6962175369262695, ACC: 0.5\n",
      "Validation iteration 454 loss: 0.6986676454544067, ACC: 0.484375\n",
      "Validation iteration 455 loss: 0.6913173198699951, ACC: 0.53125\n",
      "Validation iteration 456 loss: 0.71336829662323, ACC: 0.390625\n",
      "Validation iteration 457 loss: 0.6864171028137207, ACC: 0.5625\n",
      "Validation iteration 458 loss: 0.6937674283981323, ACC: 0.515625\n",
      "Validation iteration 459 loss: 0.6790667772293091, ACC: 0.609375\n",
      "Validation iteration 460 loss: 0.7060179710388184, ACC: 0.4375\n",
      "Validation iteration 461 loss: 0.7035678625106812, ACC: 0.453125\n",
      "Validation iteration 462 loss: 0.7084680795669556, ACC: 0.421875\n",
      "Validation iteration 463 loss: 0.701117753982544, ACC: 0.46875\n",
      "Validation iteration 464 loss: 0.701117753982544, ACC: 0.46875\n",
      "Validation iteration 465 loss: 0.6962175369262695, ACC: 0.5\n",
      "Validation iteration 466 loss: 0.6766166687011719, ACC: 0.625\n",
      "Validation iteration 467 loss: 0.7084680795669556, ACC: 0.421875\n",
      "Validation iteration 468 loss: 0.6937674283981323, ACC: 0.515625\n",
      "Validation iteration 469 loss: 0.6913172602653503, ACC: 0.53125\n",
      "Validation iteration 470 loss: 0.7207186222076416, ACC: 0.34375\n",
      "Validation iteration 471 loss: 0.6790667772293091, ACC: 0.609375\n",
      "Validation iteration 472 loss: 0.6986676454544067, ACC: 0.484375\n",
      "Validation iteration 473 loss: 0.6888672113418579, ACC: 0.546875\n",
      "Validation iteration 474 loss: 0.6766166687011719, ACC: 0.625\n",
      "Validation iteration 475 loss: 0.7035678625106812, ACC: 0.453125\n",
      "Validation iteration 476 loss: 0.6937674283981323, ACC: 0.515625\n",
      "Validation iteration 477 loss: 0.6962175369262695, ACC: 0.5\n",
      "Validation iteration 478 loss: 0.6864171028137207, ACC: 0.5625\n",
      "Validation iteration 479 loss: 0.6790667772293091, ACC: 0.609375\n",
      "Validation iteration 480 loss: 0.7060179710388184, ACC: 0.4375\n",
      "Validation iteration 481 loss: 0.6839669942855835, ACC: 0.578125\n",
      "Validation iteration 482 loss: 0.6986676454544067, ACC: 0.484375\n",
      "Validation iteration 483 loss: 0.698667585849762, ACC: 0.484375\n",
      "Validation iteration 484 loss: 0.7060179114341736, ACC: 0.4375\n",
      "Validation iteration 485 loss: 0.6913173198699951, ACC: 0.53125\n",
      "Validation iteration 486 loss: 0.6986676454544067, ACC: 0.484375\n",
      "Validation iteration 487 loss: 0.6937674283981323, ACC: 0.515625\n",
      "Validation iteration 488 loss: 0.6839669942855835, ACC: 0.578125\n",
      "Validation iteration 489 loss: 0.701117753982544, ACC: 0.46875\n",
      "Validation iteration 490 loss: 0.6913173198699951, ACC: 0.53125\n",
      "Validation iteration 491 loss: 0.6864171028137207, ACC: 0.5625\n",
      "Validation iteration 492 loss: 0.7109181880950928, ACC: 0.40625\n",
      "Validation iteration 493 loss: 0.6962175369262695, ACC: 0.5\n",
      "Validation iteration 494 loss: 0.701117753982544, ACC: 0.46875\n",
      "Validation iteration 495 loss: 0.6937673091888428, ACC: 0.515625\n",
      "Validation iteration 496 loss: 0.6815168857574463, ACC: 0.59375\n",
      "Validation iteration 497 loss: 0.6962175369262695, ACC: 0.5\n",
      "Validation iteration 498 loss: 0.7207186222076416, ACC: 0.34375\n",
      "Validation iteration 499 loss: 0.6741665601730347, ACC: 0.640625\n",
      "Validation iteration 500 loss: 0.6913173198699951, ACC: 0.53125\n",
      "-- Epoch 5 done -- Train loss: 0.7086905399958293, train ACC: 0.4973958333333333, val loss: 0.6950414788722992, val ACC: 0.5075\n",
      "<--- 9984.11250114441 seconds --->\n",
      "Training iteration 1 loss: 0.6937674283981323, ACC:0.515625\n",
      "Training iteration 2 loss: 0.7110074758529663, ACC:0.453125\n",
      "Training iteration 3 loss: 0.7220422625541687, ACC:0.484375\n",
      "Training iteration 4 loss: 0.7201744914054871, ACC:0.484375\n",
      "Training iteration 5 loss: 0.7055097818374634, ACC:0.46875\n",
      "Training iteration 6 loss: 0.6884428262710571, ACC:0.5625\n",
      "Training iteration 7 loss: 0.6891598701477051, ACC:0.5625\n",
      "Training iteration 8 loss: 0.7162683606147766, ACC:0.546875\n",
      "Training iteration 9 loss: 0.7315084338188171, ACC:0.53125\n",
      "Training iteration 10 loss: 0.7430251240730286, ACC:0.46875\n",
      "Training iteration 11 loss: 0.6929793357849121, ACC:0.515625\n",
      "Training iteration 12 loss: 0.7110522985458374, ACC:0.46875\n",
      "Training iteration 13 loss: 0.6910419464111328, ACC:0.5625\n",
      "Training iteration 14 loss: 0.7759893536567688, ACC:0.421875\n",
      "Training iteration 15 loss: 0.7151018381118774, ACC:0.46875\n",
      "Training iteration 16 loss: 0.6951956152915955, ACC:0.484375\n",
      "Training iteration 17 loss: 0.7286799550056458, ACC:0.453125\n",
      "Training iteration 18 loss: 0.7227303981781006, ACC:0.484375\n",
      "Training iteration 19 loss: 0.7079138159751892, ACC:0.484375\n",
      "Training iteration 20 loss: 0.6893912553787231, ACC:0.65625\n",
      "Training iteration 21 loss: 0.6932008862495422, ACC:0.5\n",
      "Training iteration 22 loss: 0.6930128931999207, ACC:0.515625\n",
      "Training iteration 23 loss: 0.694114625453949, ACC:0.453125\n",
      "Training iteration 24 loss: 0.7015126347541809, ACC:0.34375\n",
      "Training iteration 25 loss: 0.6889568567276001, ACC:0.546875\n",
      "Training iteration 26 loss: 0.686305582523346, ACC:0.5625\n",
      "Training iteration 27 loss: 0.7596033811569214, ACC:0.421875\n",
      "Training iteration 28 loss: 0.6904178261756897, ACC:0.546875\n",
      "Training iteration 29 loss: 0.7068912982940674, ACC:0.375\n",
      "Training iteration 30 loss: 0.6759456396102905, ACC:0.59375\n",
      "Training iteration 31 loss: 0.6775397658348083, ACC:0.609375\n",
      "Training iteration 32 loss: 0.8845287561416626, ACC:0.421875\n",
      "Training iteration 33 loss: 0.7488421201705933, ACC:0.515625\n",
      "Training iteration 34 loss: 0.7247910499572754, ACC:0.421875\n",
      "Training iteration 35 loss: 0.6982183456420898, ACC:0.53125\n",
      "Training iteration 36 loss: 0.7680315971374512, ACC:0.515625\n",
      "Training iteration 37 loss: 0.7831666469573975, ACC:0.53125\n",
      "Training iteration 38 loss: 0.8428195118904114, ACC:0.421875\n",
      "Training iteration 39 loss: 0.6853177547454834, ACC:0.5625\n",
      "Training iteration 40 loss: 0.7060546278953552, ACC:0.484375\n",
      "Training iteration 41 loss: 0.7579408884048462, ACC:0.46875\n",
      "Training iteration 42 loss: 0.7240006923675537, ACC:0.53125\n",
      "Training iteration 43 loss: 0.744829535484314, ACC:0.453125\n",
      "Training iteration 44 loss: 0.6860228776931763, ACC:0.59375\n",
      "Training iteration 45 loss: 0.6913579702377319, ACC:0.53125\n",
      "Training iteration 46 loss: 0.7102058529853821, ACC:0.5\n",
      "Training iteration 47 loss: 0.7148047685623169, ACC:0.5\n",
      "Training iteration 48 loss: 0.7139513492584229, ACC:0.46875\n",
      "Training iteration 49 loss: 0.6916691660881042, ACC:0.546875\n",
      "Training iteration 50 loss: 0.6843514442443848, ACC:0.578125\n",
      "Training iteration 51 loss: 0.7223148941993713, ACC:0.46875\n",
      "Training iteration 52 loss: 0.675475537776947, ACC:0.59375\n",
      "Training iteration 53 loss: 0.6941762566566467, ACC:0.546875\n",
      "Training iteration 54 loss: 0.6655578017234802, ACC:0.625\n",
      "Training iteration 55 loss: 0.6964516639709473, ACC:0.53125\n",
      "Training iteration 56 loss: 0.7093551158905029, ACC:0.46875\n",
      "Training iteration 57 loss: 0.6893846988677979, ACC:0.59375\n",
      "Training iteration 58 loss: 0.6925248503684998, ACC:0.53125\n",
      "Training iteration 59 loss: 0.6799864172935486, ACC:0.625\n",
      "Training iteration 60 loss: 0.7198350429534912, ACC:0.46875\n",
      "Training iteration 61 loss: 0.6993512511253357, ACC:0.53125\n",
      "Training iteration 62 loss: 0.6857336163520813, ACC:0.5625\n",
      "Training iteration 63 loss: 0.6888926029205322, ACC:0.546875\n",
      "Training iteration 64 loss: 0.6871132254600525, ACC:0.5625\n",
      "Training iteration 65 loss: 0.692784309387207, ACC:0.515625\n",
      "Training iteration 66 loss: 0.697288453578949, ACC:0.421875\n",
      "Training iteration 67 loss: 0.6891463994979858, ACC:0.546875\n",
      "Training iteration 68 loss: 0.7301963567733765, ACC:0.421875\n",
      "Training iteration 69 loss: 0.7002974152565002, ACC:0.5\n",
      "Training iteration 70 loss: 0.6926916837692261, ACC:0.515625\n",
      "Training iteration 71 loss: 0.6904125809669495, ACC:0.546875\n",
      "Training iteration 72 loss: 0.6853145360946655, ACC:0.5625\n",
      "Training iteration 73 loss: 0.7209964394569397, ACC:0.484375\n",
      "Training iteration 74 loss: 0.7109053134918213, ACC:0.5\n",
      "Training iteration 75 loss: 0.7043940424919128, ACC:0.46875\n",
      "Training iteration 76 loss: 0.6940690875053406, ACC:0.5\n",
      "Training iteration 77 loss: 0.7390358448028564, ACC:0.390625\n",
      "Training iteration 78 loss: 0.7167718410491943, ACC:0.421875\n",
      "Training iteration 79 loss: 0.6985105872154236, ACC:0.421875\n",
      "Training iteration 80 loss: 0.7050928473472595, ACC:0.453125\n",
      "Training iteration 81 loss: 0.7017287611961365, ACC:0.453125\n",
      "Training iteration 82 loss: 0.69402676820755, ACC:0.46875\n",
      "Training iteration 83 loss: 0.7034079432487488, ACC:0.421875\n",
      "Training iteration 84 loss: 0.6932220458984375, ACC:0.5\n",
      "Training iteration 85 loss: 0.6956295967102051, ACC:0.46875\n",
      "Training iteration 86 loss: 0.6969994902610779, ACC:0.453125\n",
      "Training iteration 87 loss: 0.6940054893493652, ACC:0.46875\n",
      "Training iteration 88 loss: 0.6910595297813416, ACC:0.546875\n",
      "Training iteration 89 loss: 0.6891866326332092, ACC:0.546875\n",
      "Training iteration 90 loss: 0.6681891679763794, ACC:0.640625\n",
      "Training iteration 91 loss: 0.73048335313797, ACC:0.46875\n",
      "Training iteration 92 loss: 0.7016051411628723, ACC:0.53125\n",
      "Training iteration 93 loss: 0.6899260878562927, ACC:0.546875\n",
      "Training iteration 94 loss: 0.6911975741386414, ACC:0.53125\n",
      "Training iteration 95 loss: 0.6934786438941956, ACC:0.46875\n",
      "Training iteration 96 loss: 0.687412679195404, ACC:0.578125\n",
      "Training iteration 97 loss: 0.7116814255714417, ACC:0.453125\n",
      "Training iteration 98 loss: 0.685478150844574, ACC:0.5625\n",
      "Training iteration 99 loss: 0.6759487390518188, ACC:0.609375\n",
      "Training iteration 100 loss: 0.6764872670173645, ACC:0.59375\n",
      "Training iteration 101 loss: 0.7002886533737183, ACC:0.53125\n",
      "Training iteration 102 loss: 0.6510624885559082, ACC:0.65625\n",
      "Training iteration 103 loss: 0.7738929986953735, ACC:0.390625\n",
      "Training iteration 104 loss: 0.7142981290817261, ACC:0.421875\n",
      "Training iteration 105 loss: 0.7310766577720642, ACC:0.375\n",
      "Training iteration 106 loss: 0.6978809237480164, ACC:0.53125\n",
      "Training iteration 107 loss: 0.7392288446426392, ACC:0.421875\n",
      "Training iteration 108 loss: 0.6893479228019714, ACC:0.546875\n",
      "Training iteration 109 loss: 0.692706286907196, ACC:0.515625\n",
      "Training iteration 110 loss: 0.6853158473968506, ACC:0.5625\n",
      "Training iteration 111 loss: 0.7011477947235107, ACC:0.53125\n",
      "Training iteration 112 loss: 0.7789082527160645, ACC:0.359375\n",
      "Training iteration 113 loss: 0.6926949620246887, ACC:0.515625\n",
      "Training iteration 114 loss: 0.7363447546958923, ACC:0.359375\n",
      "Training iteration 115 loss: 0.6853569746017456, ACC:0.5625\n",
      "Training iteration 116 loss: 0.7028432488441467, ACC:0.484375\n",
      "Training iteration 117 loss: 0.6993542909622192, ACC:0.453125\n",
      "Training iteration 118 loss: 0.693030059337616, ACC:0.515625\n",
      "Training iteration 119 loss: 0.7177771925926208, ACC:0.453125\n",
      "Training iteration 120 loss: 0.6937094330787659, ACC:0.53125\n",
      "Training iteration 121 loss: 0.7005131244659424, ACC:0.484375\n",
      "Training iteration 122 loss: 0.6928918361663818, ACC:0.515625\n",
      "Training iteration 123 loss: 0.7075108289718628, ACC:0.390625\n",
      "Training iteration 124 loss: 0.6918329000473022, ACC:0.5625\n",
      "Training iteration 125 loss: 0.6956994533538818, ACC:0.40625\n",
      "Training iteration 126 loss: 0.6754021048545837, ACC:0.65625\n",
      "Training iteration 127 loss: 0.7263215780258179, ACC:0.484375\n",
      "Training iteration 128 loss: 0.6792128682136536, ACC:0.59375\n",
      "Training iteration 129 loss: 0.7709174156188965, ACC:0.4375\n",
      "Training iteration 130 loss: 0.6898826956748962, ACC:0.546875\n",
      "Training iteration 131 loss: 0.6925818920135498, ACC:0.53125\n",
      "Training iteration 132 loss: 0.7122572064399719, ACC:0.484375\n",
      "Training iteration 133 loss: 0.7125436067581177, ACC:0.515625\n",
      "Training iteration 134 loss: 0.7291218638420105, ACC:0.46875\n",
      "Training iteration 135 loss: 0.6944193243980408, ACC:0.515625\n",
      "Training iteration 136 loss: 0.6927280426025391, ACC:0.515625\n",
      "Training iteration 137 loss: 0.7156398296356201, ACC:0.46875\n",
      "Training iteration 138 loss: 0.7143071889877319, ACC:0.484375\n",
      "Training iteration 139 loss: 0.6924606561660767, ACC:0.53125\n",
      "Training iteration 140 loss: 0.6848395466804504, ACC:0.625\n",
      "Training iteration 141 loss: 0.7037402391433716, ACC:0.40625\n",
      "Training iteration 142 loss: 0.693601131439209, ACC:0.5\n",
      "Training iteration 143 loss: 0.6944028735160828, ACC:0.515625\n",
      "Training iteration 144 loss: 0.7157344222068787, ACC:0.4375\n",
      "Training iteration 145 loss: 0.6912329792976379, ACC:0.53125\n",
      "Training iteration 146 loss: 0.6936769485473633, ACC:0.4375\n",
      "Training iteration 147 loss: 0.6924557685852051, ACC:0.5625\n",
      "Training iteration 148 loss: 0.6878759264945984, ACC:0.5625\n",
      "Training iteration 149 loss: 0.7165378332138062, ACC:0.4375\n",
      "Training iteration 150 loss: 0.7087413668632507, ACC:0.4375\n",
      "Training iteration 151 loss: 0.6926900744438171, ACC:0.515625\n",
      "Training iteration 152 loss: 0.6853227019309998, ACC:0.5625\n",
      "Training iteration 153 loss: 0.6760293245315552, ACC:0.59375\n",
      "Training iteration 154 loss: 0.6715810894966125, ACC:0.609375\n",
      "Training iteration 155 loss: 0.7582833766937256, ACC:0.484375\n",
      "Training iteration 156 loss: 0.7048859000205994, ACC:0.53125\n",
      "Training iteration 157 loss: 0.6979315280914307, ACC:0.484375\n",
      "Training iteration 158 loss: 0.6992876529693604, ACC:0.5\n",
      "Training iteration 159 loss: 0.6980454325675964, ACC:0.546875\n",
      "Training iteration 160 loss: 0.7430717349052429, ACC:0.484375\n",
      "Training iteration 161 loss: 0.6825690865516663, ACC:0.578125\n",
      "Training iteration 162 loss: 0.673172116279602, ACC:0.609375\n",
      "Training iteration 163 loss: 0.6887725591659546, ACC:0.546875\n",
      "Training iteration 164 loss: 0.6880756616592407, ACC:0.5625\n",
      "Training iteration 165 loss: 0.6898046731948853, ACC:0.546875\n",
      "Training iteration 166 loss: 0.6911929845809937, ACC:0.53125\n",
      "Training iteration 167 loss: 0.6936880946159363, ACC:0.515625\n",
      "Training iteration 168 loss: 0.6934486031532288, ACC:0.515625\n",
      "Training iteration 169 loss: 0.6944139003753662, ACC:0.5\n",
      "Training iteration 170 loss: 0.690990686416626, ACC:0.59375\n",
      "Training iteration 171 loss: 0.6985834836959839, ACC:0.453125\n",
      "Training iteration 172 loss: 0.6975867748260498, ACC:0.40625\n",
      "Training iteration 173 loss: 0.6939482688903809, ACC:0.515625\n",
      "Training iteration 174 loss: 0.7052927613258362, ACC:0.5\n",
      "Training iteration 175 loss: 0.7011135220527649, ACC:0.515625\n",
      "Training iteration 176 loss: 0.6745092272758484, ACC:0.609375\n",
      "Training iteration 177 loss: 0.7069790363311768, ACC:0.46875\n",
      "Training iteration 178 loss: 0.6927503347396851, ACC:0.515625\n",
      "Training iteration 179 loss: 0.7001842260360718, ACC:0.359375\n",
      "Training iteration 180 loss: 0.6959160566329956, ACC:0.46875\n",
      "Training iteration 181 loss: 0.6866779923439026, ACC:0.578125\n",
      "Training iteration 182 loss: 0.7018610835075378, ACC:0.484375\n",
      "Training iteration 183 loss: 0.6761682033538818, ACC:0.609375\n",
      "Training iteration 184 loss: 0.7035202383995056, ACC:0.5\n",
      "Training iteration 185 loss: 0.713036060333252, ACC:0.453125\n",
      "Training iteration 186 loss: 0.6926853656768799, ACC:0.515625\n",
      "Training iteration 187 loss: 0.7035175561904907, ACC:0.4375\n",
      "Training iteration 188 loss: 0.6912274360656738, ACC:0.53125\n",
      "Training iteration 189 loss: 0.6935011148452759, ACC:0.515625\n",
      "Training iteration 190 loss: 0.6965856552124023, ACC:0.484375\n",
      "Training iteration 191 loss: 0.6929113864898682, ACC:0.515625\n",
      "Training iteration 192 loss: 0.693486750125885, ACC:0.5\n",
      "Training iteration 193 loss: 0.6986716389656067, ACC:0.453125\n",
      "Training iteration 194 loss: 0.6925660967826843, ACC:0.53125\n",
      "Training iteration 195 loss: 0.6930017471313477, ACC:0.53125\n",
      "Training iteration 196 loss: 0.6938506960868835, ACC:0.5\n",
      "Training iteration 197 loss: 0.7013999819755554, ACC:0.4375\n",
      "Training iteration 198 loss: 0.693333625793457, ACC:0.453125\n",
      "Training iteration 199 loss: 0.6941583156585693, ACC:0.46875\n",
      "Training iteration 200 loss: 0.6931591033935547, ACC:0.5\n",
      "Training iteration 201 loss: 0.6933549642562866, ACC:0.5\n",
      "Training iteration 202 loss: 0.694310188293457, ACC:0.484375\n",
      "Training iteration 203 loss: 0.6953240633010864, ACC:0.375\n",
      "Training iteration 204 loss: 0.702680766582489, ACC:0.484375\n",
      "Training iteration 205 loss: 0.7060847282409668, ACC:0.5\n",
      "Training iteration 206 loss: 0.730206310749054, ACC:0.40625\n",
      "Training iteration 207 loss: 0.6924597024917603, ACC:0.578125\n",
      "Training iteration 208 loss: 0.7002330422401428, ACC:0.53125\n",
      "Training iteration 209 loss: 0.6833807826042175, ACC:0.59375\n",
      "Training iteration 210 loss: 0.7635992169380188, ACC:0.5\n",
      "Training iteration 211 loss: 0.6802374124526978, ACC:0.59375\n",
      "Training iteration 212 loss: 0.7023917436599731, ACC:0.515625\n",
      "Training iteration 213 loss: 0.6943503022193909, ACC:0.4375\n",
      "Training iteration 214 loss: 0.7289430499076843, ACC:0.453125\n",
      "Training iteration 215 loss: 0.7097944617271423, ACC:0.53125\n",
      "Training iteration 216 loss: 0.7221793532371521, ACC:0.5\n",
      "Training iteration 217 loss: 0.7020749449729919, ACC:0.5\n",
      "Training iteration 218 loss: 0.6964309811592102, ACC:0.40625\n",
      "Training iteration 219 loss: 0.7067134380340576, ACC:0.421875\n",
      "Training iteration 220 loss: 0.6881476044654846, ACC:0.59375\n",
      "Training iteration 221 loss: 0.7133504152297974, ACC:0.328125\n",
      "Training iteration 222 loss: 0.6939042210578918, ACC:0.515625\n",
      "Training iteration 223 loss: 0.7213723659515381, ACC:0.46875\n",
      "Training iteration 224 loss: 0.6978214979171753, ACC:0.53125\n",
      "Training iteration 225 loss: 0.7213769555091858, ACC:0.421875\n",
      "Training iteration 226 loss: 0.6907833218574524, ACC:0.546875\n",
      "Training iteration 227 loss: 0.681681752204895, ACC:0.578125\n",
      "Training iteration 228 loss: 0.7060688734054565, ACC:0.5625\n",
      "Training iteration 229 loss: 0.7272570133209229, ACC:0.546875\n",
      "Training iteration 230 loss: 0.7888582944869995, ACC:0.421875\n",
      "Training iteration 231 loss: 0.6975672841072083, ACC:0.484375\n",
      "Training iteration 232 loss: 0.7110083103179932, ACC:0.5\n",
      "Training iteration 233 loss: 0.7195622324943542, ACC:0.546875\n",
      "Training iteration 234 loss: 0.6456199884414673, ACC:0.65625\n",
      "Training iteration 235 loss: 0.6851725578308105, ACC:0.609375\n",
      "Training iteration 236 loss: 0.7668372988700867, ACC:0.484375\n",
      "Training iteration 237 loss: 0.716212272644043, ACC:0.46875\n",
      "Training iteration 238 loss: 0.7114206552505493, ACC:0.4375\n",
      "Training iteration 239 loss: 0.716485321521759, ACC:0.515625\n",
      "Training iteration 240 loss: 0.7183727025985718, ACC:0.53125\n",
      "Training iteration 241 loss: 0.7007182240486145, ACC:0.546875\n",
      "Training iteration 242 loss: 0.7085117101669312, ACC:0.484375\n",
      "Training iteration 243 loss: 0.6948539018630981, ACC:0.453125\n",
      "Training iteration 244 loss: 0.7116079330444336, ACC:0.453125\n",
      "Training iteration 245 loss: 0.7213786244392395, ACC:0.421875\n",
      "Training iteration 246 loss: 0.6933998465538025, ACC:0.5\n",
      "Training iteration 247 loss: 0.6760570406913757, ACC:0.625\n",
      "Training iteration 248 loss: 0.7298771739006042, ACC:0.484375\n",
      "Training iteration 249 loss: 0.7406388521194458, ACC:0.484375\n",
      "Training iteration 250 loss: 0.7236500978469849, ACC:0.46875\n",
      "Training iteration 251 loss: 0.6923999190330505, ACC:0.53125\n",
      "Training iteration 252 loss: 0.7134336233139038, ACC:0.453125\n",
      "Training iteration 253 loss: 0.7124427556991577, ACC:0.484375\n",
      "Training iteration 254 loss: 0.7009117603302002, ACC:0.5\n",
      "Training iteration 255 loss: 0.6960844993591309, ACC:0.46875\n",
      "Training iteration 256 loss: 0.7146213054656982, ACC:0.390625\n",
      "Training iteration 257 loss: 0.7129794955253601, ACC:0.375\n",
      "Training iteration 258 loss: 0.7020331621170044, ACC:0.453125\n",
      "Training iteration 259 loss: 0.7095810770988464, ACC:0.46875\n",
      "Training iteration 260 loss: 0.7156808972358704, ACC:0.421875\n",
      "Training iteration 261 loss: 0.6943719387054443, ACC:0.46875\n",
      "Training iteration 262 loss: 0.6979088187217712, ACC:0.5\n",
      "Training iteration 263 loss: 0.6894327402114868, ACC:0.546875\n",
      "Training iteration 264 loss: 0.7038986682891846, ACC:0.5\n",
      "Training iteration 265 loss: 0.6887840032577515, ACC:0.546875\n",
      "Training iteration 266 loss: 0.7042314410209656, ACC:0.421875\n",
      "Training iteration 267 loss: 0.7014148235321045, ACC:0.453125\n",
      "Training iteration 268 loss: 0.6961027979850769, ACC:0.515625\n",
      "Training iteration 269 loss: 0.704929530620575, ACC:0.484375\n",
      "Training iteration 270 loss: 0.6866747140884399, ACC:0.5625\n",
      "Training iteration 271 loss: 0.7044795155525208, ACC:0.390625\n",
      "Training iteration 272 loss: 0.7069674134254456, ACC:0.4375\n",
      "Training iteration 273 loss: 0.7072573304176331, ACC:0.46875\n",
      "Training iteration 274 loss: 0.6794357895851135, ACC:0.609375\n",
      "Training iteration 275 loss: 0.6797417402267456, ACC:0.59375\n",
      "Training iteration 276 loss: 0.695067286491394, ACC:0.53125\n",
      "Training iteration 277 loss: 0.7067815065383911, ACC:0.5\n",
      "Training iteration 278 loss: 0.6995104551315308, ACC:0.5\n",
      "Training iteration 279 loss: 0.692072868347168, ACC:0.53125\n",
      "Training iteration 280 loss: 0.6984886527061462, ACC:0.453125\n",
      "Training iteration 281 loss: 0.692827582359314, ACC:0.515625\n",
      "Training iteration 282 loss: 0.6884740591049194, ACC:0.5625\n",
      "Training iteration 283 loss: 0.6958863735198975, ACC:0.5\n",
      "Training iteration 284 loss: 0.6868526339530945, ACC:0.5625\n",
      "Training iteration 285 loss: 0.7025558948516846, ACC:0.46875\n",
      "Training iteration 286 loss: 0.6982944011688232, ACC:0.453125\n",
      "Training iteration 287 loss: 0.6865214109420776, ACC:0.578125\n",
      "Training iteration 288 loss: 0.6922678351402283, ACC:0.546875\n",
      "Training iteration 289 loss: 0.7348381876945496, ACC:0.484375\n",
      "Training iteration 290 loss: 0.6690313816070557, ACC:0.609375\n",
      "Training iteration 291 loss: 0.6926659345626831, ACC:0.546875\n",
      "Training iteration 292 loss: 0.7179005146026611, ACC:0.421875\n",
      "Training iteration 293 loss: 0.6980093121528625, ACC:0.46875\n",
      "Training iteration 294 loss: 0.7159630656242371, ACC:0.46875\n",
      "Training iteration 295 loss: 0.7642723321914673, ACC:0.328125\n",
      "Training iteration 296 loss: 0.6996115446090698, ACC:0.40625\n",
      "Training iteration 297 loss: 0.7120997309684753, ACC:0.453125\n",
      "Training iteration 298 loss: 0.7000362873077393, ACC:0.5\n",
      "Training iteration 299 loss: 0.699467658996582, ACC:0.46875\n",
      "Training iteration 300 loss: 0.6986044049263, ACC:0.421875\n",
      "Training iteration 301 loss: 0.6955142617225647, ACC:0.484375\n",
      "Training iteration 302 loss: 0.6974269151687622, ACC:0.421875\n",
      "Training iteration 303 loss: 0.6952393651008606, ACC:0.5\n",
      "Training iteration 304 loss: 0.7227739691734314, ACC:0.40625\n",
      "Training iteration 305 loss: 0.6913273334503174, ACC:0.53125\n",
      "Training iteration 306 loss: 0.6928145885467529, ACC:0.515625\n",
      "Training iteration 307 loss: 0.7029041647911072, ACC:0.453125\n",
      "Training iteration 308 loss: 0.6834803819656372, ACC:0.59375\n",
      "Training iteration 309 loss: 0.6982220411300659, ACC:0.5\n",
      "Training iteration 310 loss: 0.7036862373352051, ACC:0.46875\n",
      "Training iteration 311 loss: 0.6909562945365906, ACC:0.546875\n",
      "Training iteration 312 loss: 0.6929446458816528, ACC:0.515625\n",
      "Training iteration 313 loss: 0.7018458247184753, ACC:0.421875\n",
      "Training iteration 314 loss: 0.6930685639381409, ACC:0.515625\n",
      "Training iteration 315 loss: 0.6943495869636536, ACC:0.484375\n",
      "Training iteration 316 loss: 0.6907830834388733, ACC:0.546875\n",
      "Training iteration 317 loss: 0.6838800311088562, ACC:0.59375\n",
      "Training iteration 318 loss: 0.7151985168457031, ACC:0.453125\n",
      "Training iteration 319 loss: 0.7102744579315186, ACC:0.453125\n",
      "Training iteration 320 loss: 0.6922708749771118, ACC:0.546875\n",
      "Training iteration 321 loss: 0.7037277817726135, ACC:0.40625\n",
      "Training iteration 322 loss: 0.6938968896865845, ACC:0.453125\n",
      "Training iteration 323 loss: 0.6912301182746887, ACC:0.53125\n",
      "Training iteration 324 loss: 0.6897376179695129, ACC:0.546875\n",
      "Training iteration 325 loss: 0.6982207298278809, ACC:0.53125\n",
      "Training iteration 326 loss: 0.7134487628936768, ACC:0.484375\n",
      "Training iteration 327 loss: 0.6862065196037292, ACC:0.5625\n",
      "Training iteration 328 loss: 0.691528856754303, ACC:0.546875\n",
      "Training iteration 329 loss: 0.6931801438331604, ACC:0.5\n",
      "Training iteration 330 loss: 0.6926602721214294, ACC:0.515625\n",
      "Training iteration 331 loss: 0.6820136308670044, ACC:0.625\n",
      "Training iteration 332 loss: 0.6855757236480713, ACC:0.5625\n",
      "Training iteration 333 loss: 0.7191354632377625, ACC:0.5\n",
      "Training iteration 334 loss: 0.7084527611732483, ACC:0.515625\n",
      "Training iteration 335 loss: 0.6777335405349731, ACC:0.59375\n",
      "Training iteration 336 loss: 0.7084261178970337, ACC:0.40625\n",
      "Training iteration 337 loss: 0.6888125538825989, ACC:0.546875\n",
      "Training iteration 338 loss: 0.6760076284408569, ACC:0.59375\n",
      "Training iteration 339 loss: 0.8332332968711853, ACC:0.375\n",
      "Training iteration 340 loss: 0.7014036178588867, ACC:0.53125\n",
      "Training iteration 341 loss: 0.6956235766410828, ACC:0.46875\n",
      "Training iteration 342 loss: 0.6757875084877014, ACC:0.59375\n",
      "Training iteration 343 loss: 0.7641741633415222, ACC:0.484375\n",
      "Training iteration 344 loss: 0.7711291313171387, ACC:0.484375\n",
      "Training iteration 345 loss: 0.7265421152114868, ACC:0.484375\n",
      "Training iteration 346 loss: 0.6931478381156921, ACC:0.5\n",
      "Training iteration 347 loss: 0.7397465705871582, ACC:0.4375\n",
      "Training iteration 348 loss: 0.6615681052207947, ACC:0.625\n",
      "Training iteration 349 loss: 0.7581750750541687, ACC:0.46875\n",
      "Training iteration 350 loss: 0.7578285336494446, ACC:0.390625\n",
      "Training iteration 351 loss: 0.683856725692749, ACC:0.59375\n",
      "Training iteration 352 loss: 0.719403862953186, ACC:0.546875\n",
      "Training iteration 353 loss: 0.753480851650238, ACC:0.5625\n",
      "Training iteration 354 loss: 0.7398969531059265, ACC:0.578125\n",
      "Training iteration 355 loss: 0.8672095537185669, ACC:0.375\n",
      "Training iteration 356 loss: 0.6948117613792419, ACC:0.484375\n",
      "Training iteration 357 loss: 0.7322818636894226, ACC:0.515625\n",
      "Training iteration 358 loss: 0.6927577257156372, ACC:0.625\n",
      "Training iteration 359 loss: 0.8129817843437195, ACC:0.546875\n",
      "Training iteration 360 loss: 1.013779878616333, ACC:0.3125\n",
      "Training iteration 361 loss: 0.6972651481628418, ACC:0.5\n",
      "Training iteration 362 loss: 0.7867835760116577, ACC:0.4375\n",
      "Training iteration 363 loss: 0.8926872611045837, ACC:0.4375\n",
      "Training iteration 364 loss: 0.901020884513855, ACC:0.390625\n",
      "Training iteration 365 loss: 0.7003418207168579, ACC:0.515625\n",
      "Training iteration 366 loss: 0.6918803453445435, ACC:0.546875\n",
      "Training iteration 367 loss: 0.8005236983299255, ACC:0.484375\n",
      "Training iteration 368 loss: 0.7834525108337402, ACC:0.53125\n",
      "Training iteration 369 loss: 0.7471186518669128, ACC:0.53125\n",
      "Training iteration 370 loss: 0.7398499250411987, ACC:0.421875\n",
      "Training iteration 371 loss: 0.7030084133148193, ACC:0.5\n",
      "Training iteration 372 loss: 0.7164440155029297, ACC:0.5625\n",
      "Training iteration 373 loss: 0.7521007061004639, ACC:0.5625\n",
      "Training iteration 374 loss: 0.9099290370941162, ACC:0.390625\n",
      "Training iteration 375 loss: 0.7320578694343567, ACC:0.453125\n",
      "Training iteration 376 loss: 0.7015233039855957, ACC:0.515625\n",
      "Training iteration 377 loss: 0.7604352831840515, ACC:0.53125\n",
      "Training iteration 378 loss: 0.8397982120513916, ACC:0.5\n",
      "Training iteration 379 loss: 0.7234464883804321, ACC:0.578125\n",
      "Training iteration 380 loss: 0.7118231654167175, ACC:0.53125\n",
      "Training iteration 381 loss: 0.6933013796806335, ACC:0.5\n",
      "Training iteration 382 loss: 0.715976893901825, ACC:0.5\n",
      "Training iteration 383 loss: 0.7822506427764893, ACC:0.453125\n",
      "Training iteration 384 loss: 0.7804250717163086, ACC:0.421875\n",
      "Training iteration 385 loss: 0.6865426898002625, ACC:0.5625\n",
      "Training iteration 386 loss: 0.707314133644104, ACC:0.453125\n",
      "Training iteration 387 loss: 0.7436120510101318, ACC:0.40625\n",
      "Training iteration 388 loss: 0.6676180958747864, ACC:0.640625\n",
      "Training iteration 389 loss: 0.729473352432251, ACC:0.375\n",
      "Training iteration 390 loss: 0.6953262686729431, ACC:0.453125\n",
      "Training iteration 391 loss: 0.7077609896659851, ACC:0.453125\n",
      "Training iteration 392 loss: 0.7010286450386047, ACC:0.484375\n",
      "Training iteration 393 loss: 0.6842504143714905, ACC:0.609375\n",
      "Training iteration 394 loss: 0.6871305704116821, ACC:0.5625\n",
      "Training iteration 395 loss: 0.6755623817443848, ACC:0.609375\n",
      "Training iteration 396 loss: 0.6813072562217712, ACC:0.578125\n",
      "Training iteration 397 loss: 0.7081875801086426, ACC:0.53125\n",
      "Training iteration 398 loss: 0.7421540021896362, ACC:0.453125\n",
      "Training iteration 399 loss: 0.6887467503547668, ACC:0.546875\n",
      "Training iteration 400 loss: 0.6954946517944336, ACC:0.46875\n",
      "Training iteration 401 loss: 0.6888303756713867, ACC:0.546875\n",
      "Training iteration 402 loss: 0.6916837692260742, ACC:0.546875\n",
      "Training iteration 403 loss: 0.719158411026001, ACC:0.484375\n",
      "Training iteration 404 loss: 0.681110680103302, ACC:0.578125\n",
      "Training iteration 405 loss: 0.7051631212234497, ACC:0.453125\n",
      "Training iteration 406 loss: 0.6876181960105896, ACC:0.609375\n",
      "Training iteration 407 loss: 0.7075251340866089, ACC:0.515625\n",
      "Training iteration 408 loss: 0.7462319135665894, ACC:0.484375\n",
      "Training iteration 409 loss: 0.7259184122085571, ACC:0.5\n",
      "Training iteration 410 loss: 0.7045244574546814, ACC:0.484375\n",
      "Training iteration 411 loss: 0.6948211193084717, ACC:0.5\n",
      "Training iteration 412 loss: 0.7188509702682495, ACC:0.484375\n",
      "Training iteration 413 loss: 0.7355400919914246, ACC:0.46875\n",
      "Training iteration 414 loss: 0.6808914542198181, ACC:0.578125\n",
      "Training iteration 415 loss: 0.6914244890213013, ACC:0.53125\n",
      "Training iteration 416 loss: 0.6925070285797119, ACC:0.53125\n",
      "Training iteration 417 loss: 0.6969678401947021, ACC:0.453125\n",
      "Training iteration 418 loss: 0.6950766444206238, ACC:0.46875\n",
      "Training iteration 419 loss: 0.6901611089706421, ACC:0.59375\n",
      "Training iteration 420 loss: 0.704284131526947, ACC:0.484375\n",
      "Training iteration 421 loss: 0.7004280686378479, ACC:0.515625\n",
      "Training iteration 422 loss: 0.6810497045516968, ACC:0.578125\n",
      "Training iteration 423 loss: 0.7222310900688171, ACC:0.421875\n",
      "Training iteration 424 loss: 0.6917315721511841, ACC:0.5625\n",
      "Training iteration 425 loss: 0.7073108553886414, ACC:0.375\n",
      "Training iteration 426 loss: 0.6928998827934265, ACC:0.515625\n",
      "Training iteration 427 loss: 0.6867107152938843, ACC:0.5625\n",
      "Training iteration 428 loss: 0.6996642351150513, ACC:0.515625\n",
      "Training iteration 429 loss: 0.6701578497886658, ACC:0.609375\n",
      "Training iteration 430 loss: 0.7024165391921997, ACC:0.53125\n",
      "Training iteration 431 loss: 0.7532199621200562, ACC:0.390625\n",
      "Training iteration 432 loss: 0.6947583556175232, ACC:0.4375\n",
      "Training iteration 433 loss: 0.7135315537452698, ACC:0.5\n",
      "Training iteration 434 loss: 0.6931796073913574, ACC:0.578125\n",
      "Training iteration 435 loss: 0.7234948873519897, ACC:0.546875\n",
      "Training iteration 436 loss: 0.7480353713035583, ACC:0.484375\n",
      "Training iteration 437 loss: 0.7200065851211548, ACC:0.421875\n",
      "Training iteration 438 loss: 0.6994050145149231, ACC:0.515625\n",
      "Training iteration 439 loss: 0.7629305720329285, ACC:0.484375\n",
      "Training iteration 440 loss: 0.7888641953468323, ACC:0.46875\n",
      "Training iteration 441 loss: 0.723961353302002, ACC:0.5\n",
      "Training iteration 442 loss: 0.6990117430686951, ACC:0.421875\n",
      "Training iteration 443 loss: 0.7203491926193237, ACC:0.5\n",
      "Training iteration 444 loss: 0.8141239881515503, ACC:0.4375\n",
      "Training iteration 445 loss: 0.7778577208518982, ACC:0.453125\n",
      "Training iteration 446 loss: 0.7155045866966248, ACC:0.4375\n",
      "Training iteration 447 loss: 0.686011552810669, ACC:0.5625\n",
      "Training iteration 448 loss: 0.8488501310348511, ACC:0.40625\n",
      "Training iteration 449 loss: 0.7461775541305542, ACC:0.53125\n",
      "Training iteration 450 loss: 0.7044053673744202, ACC:0.546875\n",
      "Validation iteration 451 loss: 0.6858725547790527, ACC: 0.5625\n",
      "Validation iteration 452 loss: 0.6714941263198853, ACC: 0.640625\n",
      "Validation iteration 453 loss: 0.6916239261627197, ACC: 0.53125\n",
      "Validation iteration 454 loss: 0.6973752975463867, ACC: 0.5\n",
      "Validation iteration 455 loss: 0.7031267285346985, ACC: 0.46875\n",
      "Validation iteration 456 loss: 0.7203808426856995, ACC: 0.375\n",
      "Validation iteration 457 loss: 0.7060024738311768, ACC: 0.453125\n",
      "Validation iteration 458 loss: 0.711753785610199, ACC: 0.421875\n",
      "Validation iteration 459 loss: 0.7002509832382202, ACC: 0.484375\n",
      "Validation iteration 460 loss: 0.7031266689300537, ACC: 0.46875\n",
      "Validation iteration 461 loss: 0.6743698120117188, ACC: 0.625\n",
      "Validation iteration 462 loss: 0.694499671459198, ACC: 0.515625\n",
      "Validation iteration 463 loss: 0.700251042842865, ACC: 0.484375\n",
      "Validation iteration 464 loss: 0.6801211833953857, ACC: 0.59375\n",
      "Validation iteration 465 loss: 0.694499671459198, ACC: 0.515625\n",
      "Validation iteration 466 loss: 0.6944996118545532, ACC: 0.515625\n",
      "Validation iteration 467 loss: 0.706002414226532, ACC: 0.453125\n",
      "Validation iteration 468 loss: 0.6858725547790527, ACC: 0.5625\n",
      "Validation iteration 469 loss: 0.6973753571510315, ACC: 0.5\n",
      "Validation iteration 470 loss: 0.6772454977035522, ACC: 0.609375\n",
      "Validation iteration 471 loss: 0.7261322736740112, ACC: 0.34375\n",
      "Validation iteration 472 loss: 0.7117538452148438, ACC: 0.421875\n",
      "Validation iteration 473 loss: 0.6887481808662415, ACC: 0.546875\n",
      "Validation iteration 474 loss: 0.6944996118545532, ACC: 0.515625\n",
      "Validation iteration 475 loss: 0.6916239261627197, ACC: 0.53125\n",
      "Validation iteration 476 loss: 0.7060023546218872, ACC: 0.453125\n",
      "Validation iteration 477 loss: 0.7175052165985107, ACC: 0.390625\n",
      "Validation iteration 478 loss: 0.7117538452148438, ACC: 0.421875\n",
      "Validation iteration 479 loss: 0.6916239857673645, ACC: 0.53125\n",
      "Validation iteration 480 loss: 0.711753785610199, ACC: 0.421875\n",
      "Validation iteration 481 loss: 0.6772454380989075, ACC: 0.609375\n",
      "Validation iteration 482 loss: 0.694499671459198, ACC: 0.515625\n",
      "Validation iteration 483 loss: 0.6829968690872192, ACC: 0.578125\n",
      "Validation iteration 484 loss: 0.7002509832382202, ACC: 0.484375\n",
      "Validation iteration 485 loss: 0.700251042842865, ACC: 0.484375\n",
      "Validation iteration 486 loss: 0.6858725547790527, ACC: 0.5625\n",
      "Validation iteration 487 loss: 0.6858725547790527, ACC: 0.5625\n",
      "Validation iteration 488 loss: 0.6973753571510315, ACC: 0.5\n",
      "Validation iteration 489 loss: 0.6973752975463867, ACC: 0.5\n",
      "Validation iteration 490 loss: 0.7146295309066772, ACC: 0.40625\n",
      "Validation iteration 491 loss: 0.6887482404708862, ACC: 0.546875\n",
      "Validation iteration 492 loss: 0.6973752975463867, ACC: 0.5\n",
      "Validation iteration 493 loss: 0.6944996118545532, ACC: 0.515625\n",
      "Validation iteration 494 loss: 0.6973753571510315, ACC: 0.5\n",
      "Validation iteration 495 loss: 0.6858725547790527, ACC: 0.5625\n",
      "Validation iteration 496 loss: 0.7031266689300537, ACC: 0.46875\n",
      "Validation iteration 497 loss: 0.7088780403137207, ACC: 0.4375\n",
      "Validation iteration 498 loss: 0.7031267285346985, ACC: 0.46875\n",
      "Validation iteration 499 loss: 0.6973753571510315, ACC: 0.5\n",
      "Validation iteration 500 loss: 0.7175052165985107, ACC: 0.390625\n",
      "-- Epoch 6 done -- Train loss: 0.7097398187054528, train ACC: 0.5020486111111111, val loss: 0.6975478720664978, val ACC: 0.4990625\n",
      "<--- 10204.752908945084 seconds --->\n"
     ]
    }
   ],
   "source": [
    "# for timing model training purposes\n",
    "start_time = time.time()\n",
    "\n",
    "# input variables\n",
    "epoch_num = 6\n",
    "learning_rate = 0.01\n",
    "\n",
    "# train model\n",
    "model = LeNet5()\n",
    "cost_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# to store loss for training & validation set\n",
    "jw_train_epoch = []  ## to store loss for training set by epoch (average loss of iterations)\n",
    "jw_val_epoch = []    ## to store loss for validation set by epoch (average loss of iterations)\n",
    "\n",
    "# to store accuracy of training & validation set\n",
    "acc_train_epoch = []\n",
    "acc_val_epoch = []\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "\n",
    "    # track train & validation loss & accuracy by iteration for each epoch\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    test_counter = 1\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ''' include below IF statement if want to limit number of batches '''\n",
    "        if i+1 == 501:  # should have total of 500 batches after train & val sets\n",
    "            break\n",
    "        \n",
    "        # use 80% for training, 20% for testing, and 10% for validation of the 40k training samples\n",
    "        ## batch size=64 so 625 batches total: 450 train batches, 50 val batches, and 125 test batches\n",
    "\n",
    "        if i+1 > 450:  # validate model with validation set (10% of total train data)\n",
    "            inputs, labels = data\n",
    "            \n",
    "            logits, outputs = model(inputs)\n",
    "            cost = cost_fn(logits, labels)\n",
    "            \n",
    "            jw_val = float(cost)\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "            correct = pred == labels\n",
    "            acc = sum(np.array(correct))/len(np.array(correct))\n",
    "\n",
    "            acc_val = acc\n",
    "\n",
    "            val_loss.append(jw_val)\n",
    "            val_acc.append(acc_val)\n",
    "            \n",
    "            print(f'Validation iteration {i+1} loss: {jw_val}, ACC: {acc_val}')\n",
    "            \n",
    "        else:  # train model with training set\n",
    "\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()            # zero the parameter gradients\n",
    "            logits, outputs = model(inputs)  # forward\n",
    "            cost = cost_fn(logits, labels)   # input logits prior to softmax activation into cost function\n",
    "            cost.backward()                  # backward\n",
    "            optimizer.step()                 # optimize\n",
    "\n",
    "            jw_train = float(cost)\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred = torch.Tensor.argmax(outputs, dim=1)            # get labels of prediction with highest probability\n",
    "            correct = pred == labels                              # compare to actual labels and see which was predicted correctly\n",
    "\n",
    "            acc = sum(np.array(correct))/len(np.array(correct))   # calculate accuracy\n",
    "\n",
    "            acc_train = acc\n",
    "\n",
    "            train_loss.append(jw_train)\n",
    "            train_acc.append(acc_train)\n",
    "            \n",
    "            print(f'Training iteration {i+1} loss: {jw_train}, ACC:{acc_train}')\n",
    "\n",
    "    # to save time, epoch loss = the lowest loss, epoch acc = highest acc in training\n",
    "    epoch_jw = np.mean(np.array(train_loss))\n",
    "    epoch_acc = np.mean(np.array(train_acc))\n",
    "\n",
    "    jw_train_epoch.append(epoch_jw)\n",
    "    jw_val_epoch.append(np.mean(val_loss))\n",
    "    acc_train_epoch.append(epoch_acc)\n",
    "    acc_val_epoch.append(np.mean(val_acc))\n",
    "    \n",
    "    print(f'-- Epoch {epoch+1} done -- Train loss: {epoch_jw}, train ACC: {epoch_acc}, val loss: {np.mean(val_loss)}, val ACC: {np.mean(val_acc)}')\n",
    "    \n",
    "    print(\"<--- %s seconds --->\" % (time.time() - start_time))\n",
    "\n",
    "    # save model at every epoch\n",
    "    path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.01/lenet5_lr0.01_cpu_epoch{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwdS7oqJweUo"
   },
   "source": [
    "Plot loss and accuracy over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1636915078917,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "hZPUGYuZYBLH",
    "outputId": "0b43887a-c559-4887-9f85-c1fdd55e9cf0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss over epoch\n",
    "plt.plot([i+1 for i in range(len(jw_train_epoch))],jw_train_epoch)\n",
    "plt.plot([i+1 for i in range(len(jw_val_epoch))],jw_val_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Loss with learning rate {learning_rate}')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1636915080090,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "j8coUlN-YBLI",
    "outputId": "9d1da1a0-419f-419e-a791-f2fd9a91b6d6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracy over epoch\n",
    "plt.plot([i+1 for i in range(len(acc_train_epoch))],acc_train_epoch)\n",
    "plt.plot([i+1 for i in range(len(acc_val_epoch))],acc_val_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Accuracy with learning rate {learning_rate}')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJQvqXWnYBLI"
   },
   "source": [
    "**Calculate test accuracy for model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 108959,
     "status": "ok",
     "timestamp": 1636915777774,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "EoO1l_KoBvaf",
    "outputId": "a373c7ed-b6eb-469e-d5b4-fe18d5b42290"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing accuracy = 50.2%\n"
     ]
    }
   ],
   "source": [
    "# load saved model \n",
    "# path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.01/lenet5_lr0.01_cpu_epoch{6}.pth\"\n",
    "\n",
    "# model = LeNet5()\n",
    "# model.load_state_dict(torch.load(path))\n",
    "# print(model.eval())\n",
    "\n",
    "# or use model in current session\n",
    "model = model\n",
    "\n",
    "# calculate test accuracy of model at a given epoch\n",
    "\n",
    "testset_path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_testset.pth\"\n",
    "inputs, labels = torch.load(testset_path)\n",
    "\n",
    "logits, outputs = model(inputs)\n",
    "\n",
    "# calculate overall accuracy across all classes\n",
    "pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "correct = pred == labels\n",
    "\n",
    "correct_cnt = sum(np.array(correct))\n",
    "data_cnt = len(np.array(correct))\n",
    "\n",
    "print(f'\\n Testing accuracy = {(correct_cnt/data_cnt)*100:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LeNet5_model_noAugmentation_lr0.01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
