{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLEh-LthYBKv"
   },
   "source": [
    "# Surface Crack Images - LeNet5 model\n",
    "This notebook contains the code training the LeNet5 model (without augmented data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEb3XLxjYBK4"
   },
   "source": [
    "**Load packages/modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24798,
     "status": "ok",
     "timestamp": 1636923250786,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "H-oXyKDyYEED",
    "outputId": "34cf12ea-405b-4842-cad2-ce90c0844348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27656,
     "status": "ok",
     "timestamp": 1636923289976,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "9AKkpfhtYBK5",
    "outputId": "fb77b3de-34d5-4434-d50f-18c753fa5dff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.10.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print(f'Torch version: {torch .__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4310,
     "status": "ok",
     "timestamp": 1636923294783,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "i_OfDGEmYBK8",
    "outputId": "7f09adbe-4342-47df-e703-daaa300b9425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "%pip install torchsummary\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysKmpc9cYBK8"
   },
   "source": [
    "**Load data into tensors first**<br>\n",
    "Then concatenate the tensors with original and augmented data into one for ease of model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syPC_CMNYBK_"
   },
   "outputs": [],
   "source": [
    "# load data into tensors first\n",
    "data_dir = '/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/archive/original'\n",
    "# aug_data_dir = '/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/archive/augmented'\n",
    "batch_size = 64 \n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "data = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(data, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True, \n",
    "                                         pin_memory=True)\n",
    "\n",
    "# aug_dataloader = torch.utils.data.DataLoader(aug_data,\n",
    "#                                              batch_size=batch_size*9,  # multiply by 9 since augmented x9 images for each original\n",
    "#                                              shuffle=True,\n",
    "#                                              pin_memory=True)\n",
    "\n",
    "# images, labels = next(iter(dataloader))\n",
    "# aug_images, aug_labels = next(iter(aug_dataloader))\n",
    "\n",
    "# print(f'Original images shape: {images.shape}')\n",
    "# print(f'Augmented images shape: {aug_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9C4r55Yj-Pf"
   },
   "outputs": [],
   "source": [
    "# generate and save test set\n",
    "test_counter = 1\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    if i+1 > 500:\n",
    "        if test_counter == 1:\n",
    "            test_inputs, test_labels = data \n",
    "            test_counter += 1\n",
    "        else:\n",
    "            new_inputs, new_labels = data\n",
    "            test_inputs = torch.concat((test_inputs, new_inputs), 0)\n",
    "            test_labels = torch.concat((test_labels, new_labels), 0)\n",
    "            test_counter += 1\n",
    "\n",
    "        # print(f'Batch {i+1} for test set complete.')\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# print(f'Saving test inputs {test_inputs.shape}, test labels {test_labels.shape}')\n",
    "\n",
    "# save test set at the end of training\n",
    "testset = [test_inputs, test_labels]\n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_testset.pth\"\n",
    "torch.save(testset, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZaeGsiNYBLC"
   },
   "source": [
    "## Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0gG2OoGYBLD"
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, padding=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=2, stride=2)       \n",
    "        self.conv3 = nn.Conv2d(16, 120, 5, padding=2, stride=2)\n",
    "\n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(120*49, 84)\n",
    "        self.fc2 = nn.Linear(84, 2)   # have two classes (has crack/no crack)\n",
    "        \n",
    "        # softmax layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # pooling layer\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2)  # Average pool 2x2\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = F.tanh(self.conv1(x)) # layer 1\n",
    "        x = self.pool(x)          # layer 2\n",
    "        \n",
    "        x = F.tanh(self.conv2(x)) # layer 3\n",
    "        x = self.pool(x)          # layer 4\n",
    "        \n",
    "        x = F.tanh(self.conv3(x)) # layer 5\n",
    "        \n",
    "        x = torch.flatten(x, 1)   # flatten\n",
    "        \n",
    "        x = F.tanh(self.fc1(x))   # reshape layer\n",
    "        logit = self.fc2(x)       # layer 6\n",
    "        \n",
    "        output = self.softmax(logit) # layer 7\n",
    "\n",
    "        return logit, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1636923328722,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "0gbcU53BYBLE",
    "outputId": "92af5e50-4952-4032-f7a1-996c66656c1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 114, 114]             456\n",
      "         AvgPool2d-2            [-1, 6, 57, 57]               0\n",
      "            Conv2d-3           [-1, 16, 29, 29]           2,416\n",
      "         AvgPool2d-4           [-1, 16, 14, 14]               0\n",
      "            Conv2d-5            [-1, 120, 7, 7]          48,120\n",
      "            Linear-6                   [-1, 84]         494,004\n",
      "            Linear-7                    [-1, 2]             170\n",
      "           Softmax-8                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 545,166\n",
      "Trainable params: 545,166\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 0.92\n",
      "Params size (MB): 2.08\n",
      "Estimated Total Size (MB): 3.59\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "# print model summary\n",
    "\n",
    "model = LeNet5()\n",
    "\n",
    "channels = 3\n",
    "H = 227\n",
    "W = 227\n",
    "\n",
    "summary(model, (channels, H, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp89_6KnjowJ"
   },
   "source": [
    "### Find optimal learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiN5vfU2U_-j"
   },
   "source": [
    "**Train model with learning rates by starting with small number of batches**, so can confirm that architecture is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2491309,
     "status": "ok",
     "timestamp": 1636883674062,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "0acPeyXXsDdk",
    "outputId": "e9505549-7212-47fa-f3b1-12b766c3cd7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Training iteration 21 loss: 0.6363949179649353, ACC:0.609375\n",
      "Training iteration 22 loss: 0.6330579519271851, ACC:0.625\n",
      "Training iteration 23 loss: 0.6292668581008911, ACC:0.703125\n",
      "Training iteration 24 loss: 0.6243601441383362, ACC:0.765625\n",
      "Training iteration 25 loss: 0.6624336242675781, ACC:0.5625\n",
      "Training iteration 26 loss: 0.6218750476837158, ACC:0.6875\n",
      "Training iteration 27 loss: 0.6770228743553162, ACC:0.515625\n",
      "Training iteration 28 loss: 0.6539794206619263, ACC:0.640625\n",
      "Training iteration 29 loss: 0.658585250377655, ACC:0.53125\n",
      "Training iteration 30 loss: 0.6136534810066223, ACC:0.734375\n",
      "Training iteration 31 loss: 0.6299338340759277, ACC:0.5625\n",
      "Training iteration 32 loss: 0.7123885154724121, ACC:0.5625\n",
      "Training iteration 33 loss: 0.5690518617630005, ACC:0.8125\n",
      "Training iteration 34 loss: 0.7544519901275635, ACC:0.453125\n",
      "Training iteration 35 loss: 0.648278534412384, ACC:0.484375\n",
      "Training iteration 36 loss: 0.5622966289520264, ACC:0.828125\n",
      "Training iteration 37 loss: 0.5901831388473511, ACC:0.703125\n",
      "Training iteration 38 loss: 0.6177345514297485, ACC:0.65625\n",
      "Training iteration 39 loss: 0.6323819160461426, ACC:0.65625\n",
      "Training iteration 40 loss: 0.6610361933708191, ACC:0.578125\n",
      "Training iteration 41 loss: 0.5597429275512695, ACC:0.75\n",
      "Training iteration 42 loss: 0.5843333005905151, ACC:0.734375\n",
      "Training iteration 43 loss: 0.5927895903587341, ACC:0.609375\n",
      "Training iteration 44 loss: 0.6748760938644409, ACC:0.5625\n",
      "Training iteration 45 loss: 0.6013807058334351, ACC:0.671875\n",
      "Training iteration 46 loss: 0.5369476675987244, ACC:0.828125\n",
      "Training iteration 47 loss: 0.5998142957687378, ACC:0.609375\n",
      "Training iteration 48 loss: 0.6848523020744324, ACC:0.53125\n",
      "Training iteration 49 loss: 0.5690309405326843, ACC:0.78125\n",
      "Training iteration 50 loss: 0.5981141328811646, ACC:0.734375\n",
      "Training iteration 51 loss: 0.5666489601135254, ACC:0.6875\n",
      "Training iteration 52 loss: 0.5423009395599365, ACC:0.75\n",
      "Training iteration 53 loss: 0.5214836001396179, ACC:0.796875\n",
      "Training iteration 54 loss: 0.5650273561477661, ACC:0.78125\n",
      "Training iteration 55 loss: 0.5004315376281738, ACC:0.859375\n",
      "Training iteration 56 loss: 0.5524545907974243, ACC:0.734375\n",
      "Training iteration 57 loss: 0.46672242879867554, ACC:0.875\n",
      "Training iteration 58 loss: 0.5441062450408936, ACC:0.703125\n",
      "Training iteration 59 loss: 0.4654214382171631, ACC:0.796875\n",
      "Training iteration 60 loss: 0.5061203241348267, ACC:0.734375\n",
      "Training iteration 61 loss: 0.4506922662258148, ACC:0.84375\n",
      "Training iteration 62 loss: 0.538102924823761, ACC:0.75\n",
      "Training iteration 63 loss: 0.4579445421695709, ACC:0.734375\n",
      "Training iteration 64 loss: 0.46790486574172974, ACC:0.765625\n",
      "Training iteration 65 loss: 0.5385696887969971, ACC:0.78125\n",
      "Training iteration 66 loss: 0.42186370491981506, ACC:0.859375\n",
      "Training iteration 67 loss: 0.4357839822769165, ACC:0.828125\n",
      "Training iteration 68 loss: 0.48091328144073486, ACC:0.734375\n",
      "Training iteration 69 loss: 0.4296278655529022, ACC:0.859375\n",
      "Training iteration 70 loss: 0.5322946310043335, ACC:0.75\n",
      "Training iteration 71 loss: 0.29535719752311707, ACC:0.90625\n",
      "Training iteration 72 loss: 0.6535091400146484, ACC:0.75\n",
      "Training iteration 73 loss: 0.3817934989929199, ACC:0.8125\n",
      "Training iteration 74 loss: 0.5003481507301331, ACC:0.6875\n",
      "Training iteration 75 loss: 0.5463381409645081, ACC:0.71875\n",
      "Training iteration 76 loss: 0.40755799412727356, ACC:0.84375\n",
      "Training iteration 77 loss: 0.5003548264503479, ACC:0.765625\n",
      "Training iteration 78 loss: 0.6849744319915771, ACC:0.671875\n",
      "Training iteration 79 loss: 0.552378237247467, ACC:0.734375\n",
      "Training iteration 80 loss: 0.7099534273147583, ACC:0.421875\n",
      "Training iteration 81 loss: 0.5623351335525513, ACC:0.765625\n",
      "Training iteration 82 loss: 0.5788716077804565, ACC:0.703125\n",
      "Training iteration 83 loss: 0.6540057063102722, ACC:0.640625\n",
      "Training iteration 84 loss: 0.5113968849182129, ACC:0.8125\n",
      "Training iteration 85 loss: 0.5003337264060974, ACC:0.78125\n",
      "Training iteration 86 loss: 0.44954559206962585, ACC:0.890625\n",
      "Training iteration 87 loss: 0.4844328761100769, ACC:0.796875\n",
      "Training iteration 88 loss: 0.6306062936782837, ACC:0.71875\n",
      "Training iteration 89 loss: 0.5418782234191895, ACC:0.78125\n",
      "Training iteration 90 loss: 0.46230795979499817, ACC:0.734375\n",
      "Training iteration 91 loss: 0.6369669437408447, ACC:0.53125\n",
      "Training iteration 92 loss: 0.5692062377929688, ACC:0.59375\n",
      "Training iteration 93 loss: 0.4499598741531372, ACC:0.875\n",
      "Training iteration 94 loss: 0.5022439360618591, ACC:0.75\n",
      "Training iteration 95 loss: 0.5075659155845642, ACC:0.78125\n",
      "Training iteration 96 loss: 0.5259708762168884, ACC:0.78125\n",
      "Training iteration 97 loss: 0.43349510431289673, ACC:0.78125\n",
      "Training iteration 98 loss: 0.42736509442329407, ACC:0.84375\n",
      "Training iteration 99 loss: 0.4628753662109375, ACC:0.75\n",
      "Training iteration 100 loss: 0.4804232120513916, ACC:0.796875\n",
      "Training iteration 101 loss: 0.38114601373672485, ACC:0.890625\n",
      "Training iteration 102 loss: 0.3610207736492157, ACC:0.90625\n",
      "Training iteration 103 loss: 0.4731132984161377, ACC:0.8125\n",
      "Training iteration 104 loss: 0.47090989351272583, ACC:0.8125\n",
      "Training iteration 105 loss: 0.4898715019226074, ACC:0.734375\n",
      "Training iteration 106 loss: 0.37717118859291077, ACC:0.890625\n",
      "Training iteration 107 loss: 0.43012428283691406, ACC:0.796875\n",
      "Training iteration 108 loss: 0.4439648985862732, ACC:0.75\n",
      "Training iteration 109 loss: 0.40602701902389526, ACC:0.84375\n",
      "Training iteration 110 loss: 0.4001624286174774, ACC:0.859375\n",
      "Training iteration 111 loss: 0.46128979325294495, ACC:0.828125\n",
      "Training iteration 112 loss: 0.3853624165058136, ACC:0.859375\n",
      "Training iteration 113 loss: 0.2956070899963379, ACC:0.90625\n",
      "Training iteration 114 loss: 0.38347575068473816, ACC:0.859375\n",
      "Training iteration 115 loss: 0.42734816670417786, ACC:0.859375\n",
      "Training iteration 116 loss: 0.36628201603889465, ACC:0.828125\n",
      "Training iteration 117 loss: 0.3763737976551056, ACC:0.875\n",
      "Training iteration 118 loss: 0.3551194667816162, ACC:0.828125\n",
      "Training iteration 119 loss: 0.4107808768749237, ACC:0.875\n",
      "Training iteration 120 loss: 0.3937929570674896, ACC:0.84375\n",
      "Training iteration 121 loss: 0.3880443871021271, ACC:0.875\n",
      "Training iteration 122 loss: 0.36428916454315186, ACC:0.875\n",
      "Training iteration 123 loss: 0.3637402653694153, ACC:0.828125\n",
      "Training iteration 124 loss: 0.27617543935775757, ACC:0.921875\n",
      "Training iteration 125 loss: 0.5965543389320374, ACC:0.75\n",
      "Training iteration 126 loss: 0.3185884654521942, ACC:0.859375\n",
      "Training iteration 127 loss: 0.21595416963100433, ACC:0.953125\n",
      "Training iteration 128 loss: 0.38272616267204285, ACC:0.875\n",
      "Training iteration 129 loss: 0.33904603123664856, ACC:0.890625\n",
      "Training iteration 130 loss: 0.5011501312255859, ACC:0.84375\n",
      "Training iteration 131 loss: 0.48085254430770874, ACC:0.828125\n",
      "Training iteration 132 loss: 0.3951435983181, ACC:0.921875\n",
      "Training iteration 133 loss: 0.5244343876838684, ACC:0.8125\n",
      "Training iteration 134 loss: 0.3829592168331146, ACC:0.796875\n",
      "Training iteration 135 loss: 0.27238279581069946, ACC:0.90625\n",
      "Training iteration 136 loss: 0.5806385278701782, ACC:0.8125\n",
      "Training iteration 137 loss: 0.2714191675186157, ACC:0.9375\n",
      "Training iteration 138 loss: 0.44671469926834106, ACC:0.875\n",
      "Training iteration 139 loss: 0.358624666929245, ACC:0.859375\n",
      "Training iteration 140 loss: 0.468979150056839, ACC:0.84375\n",
      "Training iteration 141 loss: 0.46707168221473694, ACC:0.828125\n",
      "Training iteration 142 loss: 0.4392208158969879, ACC:0.875\n",
      "Training iteration 143 loss: 0.3644794821739197, ACC:0.921875\n",
      "Training iteration 144 loss: 0.540192186832428, ACC:0.8125\n",
      "Training iteration 145 loss: 0.37561124563217163, ACC:0.90625\n",
      "Training iteration 146 loss: 0.6511706113815308, ACC:0.78125\n",
      "Training iteration 147 loss: 0.45897945761680603, ACC:0.84375\n",
      "Training iteration 148 loss: 0.37436652183532715, ACC:0.84375\n",
      "Training iteration 149 loss: 0.5020999908447266, ACC:0.84375\n",
      "Training iteration 150 loss: 0.3606334328651428, ACC:0.828125\n",
      "Training iteration 151 loss: 0.5101548433303833, ACC:0.8125\n",
      "Training iteration 152 loss: 0.42897865176200867, ACC:0.828125\n",
      "Training iteration 153 loss: 0.3305629789829254, ACC:0.890625\n",
      "Training iteration 154 loss: 0.3964778184890747, ACC:0.859375\n",
      "Training iteration 155 loss: 0.44918084144592285, ACC:0.765625\n",
      "Training iteration 156 loss: 0.47370612621307373, ACC:0.8125\n",
      "Training iteration 157 loss: 0.3398102819919586, ACC:0.875\n",
      "Training iteration 158 loss: 0.358247309923172, ACC:0.828125\n",
      "Training iteration 159 loss: 0.3506219685077667, ACC:0.859375\n",
      "Training iteration 160 loss: 0.3969365060329437, ACC:0.875\n",
      "Training iteration 161 loss: 0.5210729837417603, ACC:0.8125\n",
      "Training iteration 162 loss: 0.27138498425483704, ACC:0.921875\n",
      "Training iteration 163 loss: 0.4522474408149719, ACC:0.8125\n",
      "Training iteration 164 loss: 0.2742580771446228, ACC:0.921875\n",
      "Training iteration 165 loss: 0.29625341296195984, ACC:0.921875\n",
      "Training iteration 166 loss: 0.3615230619907379, ACC:0.921875\n",
      "Training iteration 167 loss: 0.3771727681159973, ACC:0.859375\n",
      "Training iteration 168 loss: 0.27577051520347595, ACC:0.921875\n",
      "Training iteration 169 loss: 0.4504685401916504, ACC:0.828125\n",
      "Training iteration 170 loss: 0.3649802505970001, ACC:0.84375\n",
      "Training iteration 171 loss: 0.31685444712638855, ACC:0.890625\n",
      "Training iteration 172 loss: 0.201157346367836, ACC:0.953125\n",
      "Training iteration 173 loss: 0.49097740650177, ACC:0.765625\n",
      "Training iteration 174 loss: 0.4569803774356842, ACC:0.828125\n",
      "Training iteration 175 loss: 0.4122340679168701, ACC:0.890625\n",
      "Training iteration 176 loss: 0.35540688037872314, ACC:0.890625\n",
      "Training iteration 177 loss: 0.40137478709220886, ACC:0.84375\n",
      "Training iteration 178 loss: 0.5253143906593323, ACC:0.84375\n",
      "Training iteration 179 loss: 0.36624038219451904, ACC:0.859375\n",
      "Training iteration 180 loss: 0.2878163754940033, ACC:0.890625\n",
      "Training iteration 181 loss: 0.2764713168144226, ACC:0.90625\n",
      "Training iteration 182 loss: 0.42095208168029785, ACC:0.796875\n",
      "Training iteration 183 loss: 0.3977190852165222, ACC:0.78125\n",
      "Training iteration 184 loss: 0.45605403184890747, ACC:0.78125\n",
      "Training iteration 185 loss: 0.4810461103916168, ACC:0.8125\n",
      "Training iteration 186 loss: 0.24677793681621552, ACC:0.875\n",
      "Training iteration 187 loss: 0.2594553530216217, ACC:0.9375\n",
      "Training iteration 188 loss: 0.3374612033367157, ACC:0.875\n",
      "Training iteration 189 loss: 0.41424560546875, ACC:0.78125\n",
      "Training iteration 190 loss: 0.3440324366092682, ACC:0.890625\n",
      "Training iteration 191 loss: 0.3231103718280792, ACC:0.90625\n",
      "Training iteration 192 loss: 0.2694820761680603, ACC:0.90625\n",
      "Training iteration 193 loss: 0.27495506405830383, ACC:0.90625\n",
      "Training iteration 194 loss: 0.44457074999809265, ACC:0.84375\n",
      "Training iteration 195 loss: 0.4428224563598633, ACC:0.90625\n",
      "Training iteration 196 loss: 0.45477113127708435, ACC:0.8125\n",
      "Training iteration 197 loss: 0.33960476517677307, ACC:0.859375\n",
      "Training iteration 198 loss: 0.34253403544425964, ACC:0.875\n",
      "Training iteration 199 loss: 0.3414837718009949, ACC:0.828125\n",
      "Training iteration 200 loss: 0.4467277228832245, ACC:0.84375\n",
      "Training iteration 201 loss: 0.2998475134372711, ACC:0.921875\n",
      "Training iteration 202 loss: 0.29201337695121765, ACC:0.90625\n",
      "Training iteration 203 loss: 0.3583095669746399, ACC:0.8125\n",
      "Training iteration 204 loss: 0.31623420119285583, ACC:0.890625\n",
      "Training iteration 205 loss: 0.2871295213699341, ACC:0.84375\n",
      "Training iteration 206 loss: 0.42453569173812866, ACC:0.875\n",
      "Training iteration 207 loss: 0.591224193572998, ACC:0.796875\n",
      "Training iteration 208 loss: 0.32522931694984436, ACC:0.875\n",
      "Training iteration 209 loss: 0.4329911172389984, ACC:0.796875\n",
      "Training iteration 210 loss: 0.4418240785598755, ACC:0.890625\n",
      "Training iteration 211 loss: 0.4752498269081116, ACC:0.859375\n",
      "Training iteration 212 loss: 0.4089036285877228, ACC:0.921875\n",
      "Training iteration 213 loss: 0.3875298500061035, ACC:0.921875\n",
      "Training iteration 214 loss: 0.44008633494377136, ACC:0.875\n",
      "Training iteration 215 loss: 0.38319769501686096, ACC:0.875\n",
      "Training iteration 216 loss: 0.46769726276397705, ACC:0.828125\n",
      "Training iteration 217 loss: 0.4592364430427551, ACC:0.875\n",
      "Training iteration 218 loss: 0.39075395464897156, ACC:0.890625\n",
      "Training iteration 219 loss: 0.49592819809913635, ACC:0.796875\n",
      "Training iteration 220 loss: 0.37040475010871887, ACC:0.875\n",
      "Training iteration 221 loss: 0.5268471240997314, ACC:0.828125\n",
      "Training iteration 222 loss: 0.423026978969574, ACC:0.796875\n",
      "Training iteration 223 loss: 0.2888590097427368, ACC:0.90625\n",
      "Training iteration 224 loss: 0.2536260485649109, ACC:0.953125\n",
      "Training iteration 225 loss: 0.22148144245147705, ACC:0.9375\n",
      "Training iteration 226 loss: 0.2921578884124756, ACC:0.875\n",
      "Training iteration 227 loss: 0.5043887495994568, ACC:0.796875\n",
      "Training iteration 228 loss: 0.6190285682678223, ACC:0.8125\n",
      "Training iteration 229 loss: 0.3886086046695709, ACC:0.828125\n",
      "Training iteration 230 loss: 0.30173468589782715, ACC:0.890625\n",
      "Training iteration 231 loss: 0.38297873735427856, ACC:0.84375\n",
      "Training iteration 232 loss: 0.5347704291343689, ACC:0.71875\n",
      "Training iteration 233 loss: 0.36113300919532776, ACC:0.890625\n",
      "Training iteration 234 loss: 0.3789317309856415, ACC:0.859375\n",
      "Training iteration 235 loss: 0.4074974060058594, ACC:0.84375\n",
      "Training iteration 236 loss: 0.2586928904056549, ACC:0.90625\n",
      "Training iteration 237 loss: 0.38468125462532043, ACC:0.859375\n",
      "Training iteration 238 loss: 0.41446956992149353, ACC:0.84375\n",
      "Training iteration 239 loss: 0.32022297382354736, ACC:0.875\n",
      "Training iteration 240 loss: 0.39466023445129395, ACC:0.890625\n",
      "Training iteration 241 loss: 0.33503347635269165, ACC:0.921875\n",
      "Training iteration 242 loss: 0.48856741189956665, ACC:0.890625\n",
      "Training iteration 243 loss: 0.4578135907649994, ACC:0.8125\n",
      "Training iteration 244 loss: 0.45052364468574524, ACC:0.859375\n",
      "Training iteration 245 loss: 0.42232629656791687, ACC:0.8125\n",
      "Training iteration 246 loss: 0.4117472171783447, ACC:0.84375\n",
      "Training iteration 247 loss: 0.3306146264076233, ACC:0.875\n",
      "Training iteration 248 loss: 0.35456037521362305, ACC:0.859375\n",
      "Training iteration 249 loss: 0.39012718200683594, ACC:0.890625\n",
      "Training iteration 250 loss: 0.3702530264854431, ACC:0.84375\n",
      "Training iteration 251 loss: 0.37192386388778687, ACC:0.859375\n",
      "Training iteration 252 loss: 0.3515301048755646, ACC:0.890625\n",
      "Training iteration 253 loss: 0.2692331373691559, ACC:0.921875\n",
      "Training iteration 254 loss: 0.23861688375473022, ACC:0.921875\n",
      "Training iteration 255 loss: 0.33018431067466736, ACC:0.859375\n",
      "Training iteration 256 loss: 0.3624674379825592, ACC:0.90625\n",
      "Training iteration 257 loss: 0.3038399815559387, ACC:0.875\n",
      "Training iteration 258 loss: 0.19249679148197174, ACC:0.9375\n",
      "Training iteration 259 loss: 0.25652220845222473, ACC:0.90625\n",
      "Training iteration 260 loss: 0.23031142354011536, ACC:0.9375\n",
      "Training iteration 261 loss: 0.20672237873077393, ACC:0.9375\n",
      "Training iteration 262 loss: 0.4731411635875702, ACC:0.84375\n",
      "Training iteration 263 loss: 0.26570960879325867, ACC:0.921875\n",
      "Training iteration 264 loss: 0.3618990182876587, ACC:0.8125\n",
      "Training iteration 265 loss: 0.49644455313682556, ACC:0.828125\n",
      "Training iteration 266 loss: 0.37086257338523865, ACC:0.890625\n",
      "Training iteration 267 loss: 0.1772189736366272, ACC:0.921875\n",
      "Training iteration 268 loss: 0.5537347197532654, ACC:0.8125\n",
      "Training iteration 269 loss: 0.20950333774089813, ACC:0.921875\n",
      "Training iteration 270 loss: 0.5410017371177673, ACC:0.78125\n",
      "Training iteration 271 loss: 0.2912769019603729, ACC:0.828125\n",
      "Training iteration 272 loss: 0.3382164537906647, ACC:0.859375\n",
      "Training iteration 273 loss: 0.34204238653182983, ACC:0.875\n",
      "Training iteration 274 loss: 0.17637565732002258, ACC:0.953125\n",
      "Training iteration 275 loss: 0.24432674050331116, ACC:0.921875\n",
      "Training iteration 276 loss: 0.3254968225955963, ACC:0.84375\n",
      "Training iteration 277 loss: 0.48522844910621643, ACC:0.828125\n",
      "Training iteration 278 loss: 0.3323848247528076, ACC:0.875\n",
      "Training iteration 279 loss: 0.3760499358177185, ACC:0.859375\n",
      "Training iteration 280 loss: 0.37119147181510925, ACC:0.84375\n",
      "Training iteration 281 loss: 0.35618501901626587, ACC:0.890625\n",
      "Training iteration 282 loss: 0.39061239361763, ACC:0.84375\n",
      "Training iteration 283 loss: 0.3650376796722412, ACC:0.890625\n",
      "Training iteration 284 loss: 0.32637926936149597, ACC:0.890625\n",
      "Training iteration 285 loss: 0.238128200173378, ACC:0.953125\n",
      "Training iteration 286 loss: 0.29019421339035034, ACC:0.921875\n",
      "Training iteration 287 loss: 0.42655742168426514, ACC:0.875\n",
      "Training iteration 288 loss: 0.3244704008102417, ACC:0.875\n",
      "Training iteration 289 loss: 0.3913063406944275, ACC:0.859375\n",
      "Training iteration 290 loss: 0.15691271424293518, ACC:0.953125\n",
      "Training iteration 291 loss: 0.21533524990081787, ACC:0.953125\n",
      "Training iteration 292 loss: 0.29340723156929016, ACC:0.921875\n",
      "Training iteration 293 loss: 0.39072734117507935, ACC:0.890625\n",
      "Training iteration 294 loss: 0.2354673445224762, ACC:0.9375\n",
      "Training iteration 295 loss: 0.4485301375389099, ACC:0.84375\n",
      "Training iteration 296 loss: 0.2190053015947342, ACC:0.9375\n",
      "Training iteration 297 loss: 0.3748103380203247, ACC:0.890625\n",
      "Training iteration 298 loss: 0.337240606546402, ACC:0.875\n",
      "Training iteration 299 loss: 0.2981080412864685, ACC:0.90625\n",
      "Training iteration 300 loss: 0.2750575542449951, ACC:0.90625\n",
      "Training iteration 301 loss: 0.2556456923484802, ACC:0.90625\n",
      "Training iteration 302 loss: 0.23453643918037415, ACC:0.890625\n",
      "Training iteration 303 loss: 0.36118772625923157, ACC:0.875\n",
      "Training iteration 304 loss: 0.2939426600933075, ACC:0.859375\n",
      "Training iteration 305 loss: 0.30210039019584656, ACC:0.890625\n",
      "Training iteration 306 loss: 0.342893123626709, ACC:0.859375\n",
      "Training iteration 307 loss: 0.4322282373905182, ACC:0.859375\n",
      "Training iteration 308 loss: 0.3938963711261749, ACC:0.859375\n",
      "Training iteration 309 loss: 0.38938796520233154, ACC:0.828125\n",
      "Training iteration 310 loss: 0.323042094707489, ACC:0.890625\n",
      "Training iteration 311 loss: 0.36680498719215393, ACC:0.84375\n",
      "Training iteration 312 loss: 0.2327001541852951, ACC:0.921875\n",
      "Training iteration 313 loss: 0.29746413230895996, ACC:0.890625\n",
      "Training iteration 314 loss: 0.26936668157577515, ACC:0.921875\n",
      "Training iteration 315 loss: 0.2906945049762726, ACC:0.90625\n",
      "Training iteration 316 loss: 0.2220895141363144, ACC:0.9375\n",
      "Training iteration 317 loss: 0.49582308530807495, ACC:0.828125\n",
      "Training iteration 318 loss: 0.23550383746623993, ACC:0.890625\n",
      "Training iteration 319 loss: 0.32381755113601685, ACC:0.875\n",
      "Training iteration 320 loss: 0.31993767619132996, ACC:0.921875\n",
      "Training iteration 321 loss: 0.2847645580768585, ACC:0.9375\n",
      "Training iteration 322 loss: 0.4142058491706848, ACC:0.875\n",
      "Training iteration 323 loss: 0.41567862033843994, ACC:0.859375\n",
      "Training iteration 324 loss: 0.25110840797424316, ACC:0.921875\n",
      "Training iteration 325 loss: 0.28631654381752014, ACC:0.890625\n",
      "Training iteration 326 loss: 0.13698983192443848, ACC:0.96875\n",
      "Training iteration 327 loss: 0.23749597370624542, ACC:0.890625\n",
      "Training iteration 328 loss: 0.3028344511985779, ACC:0.921875\n",
      "Training iteration 329 loss: 0.406122624874115, ACC:0.875\n",
      "Training iteration 330 loss: 0.5701643824577332, ACC:0.78125\n",
      "Training iteration 331 loss: 0.31156104803085327, ACC:0.90625\n",
      "Training iteration 332 loss: 0.21770025789737701, ACC:0.953125\n",
      "Training iteration 333 loss: 0.3358137011528015, ACC:0.890625\n",
      "Training iteration 334 loss: 0.32870835065841675, ACC:0.890625\n",
      "Training iteration 335 loss: 0.3365989625453949, ACC:0.890625\n",
      "Training iteration 336 loss: 0.24136191606521606, ACC:0.96875\n",
      "Training iteration 337 loss: 0.2758030891418457, ACC:0.921875\n",
      "Training iteration 338 loss: 0.20850151777267456, ACC:0.953125\n",
      "Training iteration 339 loss: 0.45597341656684875, ACC:0.828125\n",
      "Training iteration 340 loss: 0.23192891478538513, ACC:0.953125\n",
      "Training iteration 341 loss: 0.14981462061405182, ACC:0.96875\n",
      "Training iteration 342 loss: 0.23744723200798035, ACC:0.921875\n",
      "Training iteration 343 loss: 0.34618332982063293, ACC:0.859375\n",
      "Training iteration 344 loss: 0.23423922061920166, ACC:0.9375\n",
      "Training iteration 345 loss: 0.1578950136899948, ACC:0.9375\n",
      "Training iteration 346 loss: 0.5324895977973938, ACC:0.796875\n",
      "Training iteration 347 loss: 0.23756957054138184, ACC:0.90625\n",
      "Training iteration 348 loss: 0.3436100482940674, ACC:0.859375\n",
      "Training iteration 349 loss: 0.3029332756996155, ACC:0.890625\n",
      "Training iteration 350 loss: 0.27098819613456726, ACC:0.921875\n",
      "Training iteration 351 loss: 0.255839467048645, ACC:0.9375\n",
      "Training iteration 352 loss: 0.36281630396842957, ACC:0.890625\n",
      "Training iteration 353 loss: 0.21911592781543732, ACC:0.9375\n",
      "Training iteration 354 loss: 0.23877698183059692, ACC:0.9375\n",
      "Training iteration 355 loss: 0.30030280351638794, ACC:0.875\n",
      "Training iteration 356 loss: 0.4627993404865265, ACC:0.796875\n",
      "Training iteration 357 loss: 0.24319127202033997, ACC:0.890625\n",
      "Training iteration 358 loss: 0.19559459388256073, ACC:0.9375\n",
      "Training iteration 359 loss: 0.14456512033939362, ACC:0.96875\n",
      "Training iteration 360 loss: 0.281708687543869, ACC:0.890625\n",
      "Training iteration 361 loss: 0.292768657207489, ACC:0.890625\n",
      "Training iteration 362 loss: 0.2703806161880493, ACC:0.921875\n",
      "Training iteration 363 loss: 0.3783572018146515, ACC:0.828125\n",
      "Training iteration 364 loss: 0.42417582869529724, ACC:0.875\n",
      "Training iteration 365 loss: 0.48150408267974854, ACC:0.84375\n",
      "Training iteration 366 loss: 0.24832268059253693, ACC:0.90625\n",
      "Training iteration 367 loss: 0.24947905540466309, ACC:0.9375\n",
      "Training iteration 368 loss: 0.27788329124450684, ACC:0.890625\n",
      "Training iteration 369 loss: 0.23599088191986084, ACC:0.890625\n",
      "Training iteration 370 loss: 0.21626132726669312, ACC:0.921875\n",
      "Training iteration 371 loss: 0.30060768127441406, ACC:0.921875\n",
      "Training iteration 372 loss: 0.23208709061145782, ACC:0.9375\n",
      "Training iteration 373 loss: 0.2754240930080414, ACC:0.90625\n",
      "Training iteration 374 loss: 0.3187958896160126, ACC:0.890625\n",
      "Training iteration 375 loss: 0.32096153497695923, ACC:0.90625\n",
      "Training iteration 376 loss: 0.38812026381492615, ACC:0.875\n",
      "Training iteration 377 loss: 0.225759357213974, ACC:0.9375\n",
      "Training iteration 378 loss: 0.3001052141189575, ACC:0.921875\n",
      "Training iteration 379 loss: 0.34834280610084534, ACC:0.890625\n",
      "Training iteration 380 loss: 0.2287086695432663, ACC:0.921875\n",
      "Training iteration 381 loss: 0.2978363633155823, ACC:0.890625\n",
      "Training iteration 382 loss: 0.26998913288116455, ACC:0.90625\n",
      "Training iteration 383 loss: 0.21100199222564697, ACC:0.90625\n",
      "Training iteration 384 loss: 0.4396055340766907, ACC:0.859375\n",
      "Training iteration 385 loss: 0.2824734151363373, ACC:0.921875\n",
      "Training iteration 386 loss: 0.27239763736724854, ACC:0.9375\n",
      "Training iteration 387 loss: 0.23220568895339966, ACC:0.9375\n",
      "Training iteration 388 loss: 0.29854077100753784, ACC:0.921875\n",
      "Training iteration 389 loss: 0.4043877124786377, ACC:0.84375\n",
      "Training iteration 390 loss: 0.2456589788198471, ACC:0.921875\n",
      "Training iteration 391 loss: 0.244516983628273, ACC:0.9375\n",
      "Training iteration 392 loss: 0.25899121165275574, ACC:0.90625\n",
      "Training iteration 393 loss: 0.18355317413806915, ACC:0.96875\n",
      "Training iteration 394 loss: 0.24910007417201996, ACC:0.921875\n",
      "Training iteration 395 loss: 0.20660613477230072, ACC:0.953125\n",
      "Training iteration 396 loss: 0.2826415002346039, ACC:0.90625\n",
      "Training iteration 397 loss: 0.403507262468338, ACC:0.875\n",
      "Training iteration 398 loss: 0.2559158504009247, ACC:0.921875\n",
      "Training iteration 399 loss: 0.21906739473342896, ACC:0.953125\n",
      "Training iteration 400 loss: 0.5797567963600159, ACC:0.75\n",
      "Training iteration 401 loss: 0.30189239978790283, ACC:0.890625\n",
      "Training iteration 402 loss: 0.2889541983604431, ACC:0.90625\n",
      "Training iteration 403 loss: 0.2570376694202423, ACC:0.921875\n",
      "Training iteration 404 loss: 0.4252065122127533, ACC:0.875\n",
      "Training iteration 405 loss: 0.2876575291156769, ACC:0.921875\n",
      "Training iteration 406 loss: 0.235849529504776, ACC:0.921875\n",
      "Training iteration 407 loss: 0.40202417969703674, ACC:0.84375\n",
      "Training iteration 408 loss: 0.24237892031669617, ACC:0.890625\n",
      "Training iteration 409 loss: 0.15784423053264618, ACC:0.953125\n",
      "Training iteration 410 loss: 0.38349124789237976, ACC:0.828125\n",
      "Training iteration 411 loss: 0.37090107798576355, ACC:0.890625\n",
      "Training iteration 412 loss: 0.20370903611183167, ACC:0.921875\n",
      "Training iteration 413 loss: 0.33536437153816223, ACC:0.875\n",
      "Training iteration 414 loss: 0.2842501997947693, ACC:0.90625\n",
      "Training iteration 415 loss: 0.2787978947162628, ACC:0.890625\n",
      "Training iteration 416 loss: 0.24382123351097107, ACC:0.90625\n",
      "Training iteration 417 loss: 0.3045608401298523, ACC:0.90625\n",
      "Training iteration 418 loss: 0.2816193103790283, ACC:0.90625\n",
      "Training iteration 419 loss: 0.2492726445198059, ACC:0.921875\n",
      "Training iteration 420 loss: 0.41822579503059387, ACC:0.828125\n",
      "Training iteration 421 loss: 0.39111000299453735, ACC:0.84375\n",
      "Training iteration 422 loss: 0.2956259846687317, ACC:0.890625\n",
      "Training iteration 423 loss: 0.37392181158065796, ACC:0.8125\n",
      "Training iteration 424 loss: 0.28679952025413513, ACC:0.921875\n",
      "Training iteration 425 loss: 0.32635679841041565, ACC:0.875\n",
      "Training iteration 426 loss: 0.29562515020370483, ACC:0.859375\n",
      "Training iteration 427 loss: 0.36216020584106445, ACC:0.859375\n",
      "Training iteration 428 loss: 0.25654155015945435, ACC:0.9375\n",
      "Training iteration 429 loss: 0.14630936086177826, ACC:0.96875\n",
      "Training iteration 430 loss: 0.2669885456562042, ACC:0.859375\n",
      "Training iteration 431 loss: 0.2909417152404785, ACC:0.890625\n",
      "Training iteration 432 loss: 0.3443332612514496, ACC:0.828125\n",
      "Training iteration 433 loss: 0.23856836557388306, ACC:0.90625\n",
      "Training iteration 434 loss: 0.20450513064861298, ACC:0.921875\n",
      "Training iteration 435 loss: 0.2772912383079529, ACC:0.90625\n",
      "Training iteration 436 loss: 0.3078354001045227, ACC:0.875\n",
      "Training iteration 437 loss: 0.20116440951824188, ACC:0.90625\n",
      "Training iteration 438 loss: 0.26386046409606934, ACC:0.9375\n",
      "Training iteration 439 loss: 0.27184543013572693, ACC:0.890625\n",
      "Training iteration 440 loss: 0.2781525254249573, ACC:0.90625\n",
      "Training iteration 441 loss: 0.39247894287109375, ACC:0.84375\n",
      "Training iteration 442 loss: 0.29633140563964844, ACC:0.875\n",
      "Training iteration 443 loss: 0.2313799262046814, ACC:0.9375\n",
      "Training iteration 444 loss: 0.14848043024539948, ACC:0.953125\n",
      "Training iteration 445 loss: 0.2279099076986313, ACC:0.9375\n",
      "Training iteration 446 loss: 0.21635867655277252, ACC:0.921875\n",
      "Training iteration 447 loss: 0.2675570547580719, ACC:0.90625\n",
      "Training iteration 448 loss: 0.22554630041122437, ACC:0.921875\n",
      "Training iteration 449 loss: 0.33382076025009155, ACC:0.890625\n",
      "Training iteration 450 loss: 0.2873258590698242, ACC:0.921875\n",
      "Validation iteration 451 loss: 0.1797606647014618, ACC: 0.953125\n",
      "Validation iteration 452 loss: 0.19621625542640686, ACC: 0.9375\n",
      "Validation iteration 453 loss: 0.11866607517004013, ACC: 0.984375\n",
      "Validation iteration 454 loss: 0.3013496398925781, ACC: 0.875\n",
      "Validation iteration 455 loss: 0.21762874722480774, ACC: 0.9375\n",
      "Validation iteration 456 loss: 0.29554739594459534, ACC: 0.921875\n",
      "Validation iteration 457 loss: 0.2181655764579773, ACC: 0.9375\n",
      "Validation iteration 458 loss: 0.35724329948425293, ACC: 0.890625\n",
      "Validation iteration 459 loss: 0.34040921926498413, ACC: 0.84375\n",
      "Validation iteration 460 loss: 0.37465783953666687, ACC: 0.828125\n",
      "Validation iteration 461 loss: 0.3612496554851532, ACC: 0.875\n",
      "Validation iteration 462 loss: 0.3572913110256195, ACC: 0.84375\n",
      "Validation iteration 463 loss: 0.18653640151023865, ACC: 0.9375\n",
      "Validation iteration 464 loss: 0.1963006556034088, ACC: 0.9375\n",
      "Validation iteration 465 loss: 0.19807371497154236, ACC: 0.9375\n",
      "Validation iteration 466 loss: 0.27240189909935, ACC: 0.84375\n",
      "Validation iteration 467 loss: 0.2081279307603836, ACC: 0.9375\n",
      "Validation iteration 468 loss: 0.21559804677963257, ACC: 0.921875\n",
      "Validation iteration 469 loss: 0.2847433388233185, ACC: 0.890625\n",
      "Validation iteration 470 loss: 0.22673723101615906, ACC: 0.9375\n",
      "Validation iteration 471 loss: 0.24239446222782135, ACC: 0.921875\n",
      "Validation iteration 472 loss: 0.15531155467033386, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.14959576725959778, ACC: 0.96875\n",
      "Validation iteration 474 loss: 0.3253452777862549, ACC: 0.859375\n",
      "Validation iteration 475 loss: 0.30861055850982666, ACC: 0.90625\n",
      "Validation iteration 476 loss: 0.2967243194580078, ACC: 0.90625\n",
      "Validation iteration 477 loss: 0.34995922446250916, ACC: 0.875\n",
      "Validation iteration 478 loss: 0.3150632977485657, ACC: 0.859375\n",
      "Validation iteration 479 loss: 0.244452103972435, ACC: 0.9375\n",
      "Validation iteration 480 loss: 0.3437177538871765, ACC: 0.890625\n",
      "Validation iteration 481 loss: 0.16518890857696533, ACC: 0.953125\n",
      "Validation iteration 482 loss: 0.18906071782112122, ACC: 0.9375\n",
      "Validation iteration 483 loss: 0.19063521921634674, ACC: 0.921875\n",
      "Validation iteration 484 loss: 0.35980790853500366, ACC: 0.828125\n",
      "Validation iteration 485 loss: 0.2873419225215912, ACC: 0.90625\n",
      "Validation iteration 486 loss: 0.24100308120250702, ACC: 0.921875\n",
      "Validation iteration 487 loss: 0.25837311148643494, ACC: 0.921875\n",
      "Validation iteration 488 loss: 0.22439038753509521, ACC: 0.859375\n",
      "Validation iteration 489 loss: 0.2880335748195648, ACC: 0.890625\n",
      "Validation iteration 490 loss: 0.34127286076545715, ACC: 0.859375\n",
      "Validation iteration 491 loss: 0.39006155729293823, ACC: 0.859375\n",
      "Validation iteration 492 loss: 0.38763245940208435, ACC: 0.890625\n",
      "Validation iteration 493 loss: 0.309678316116333, ACC: 0.890625\n",
      "Validation iteration 494 loss: 0.17245960235595703, ACC: 0.9375\n",
      "Validation iteration 495 loss: 0.23391689360141754, ACC: 0.90625\n",
      "Validation iteration 496 loss: 0.3390296697616577, ACC: 0.8125\n",
      "Validation iteration 497 loss: 0.2971643805503845, ACC: 0.890625\n",
      "Validation iteration 498 loss: 0.3491959869861603, ACC: 0.875\n",
      "Validation iteration 499 loss: 0.25590789318084717, ACC: 0.890625\n",
      "Validation iteration 500 loss: 0.40367749333381653, ACC: 0.859375\n",
      "-- Epoch 1 done -- Train loss: 0.3953290178378423, train ACC: 0.8362847222222223, val loss: 0.2704342232644558, val ACC: 0.901875\n",
      "<--- 262.3801076412201 seconds --->\n",
      "Training iteration 1 loss: 0.3544989824295044, ACC:0.875\n",
      "Training iteration 2 loss: 0.3912416398525238, ACC:0.84375\n",
      "Training iteration 3 loss: 0.32594430446624756, ACC:0.875\n",
      "Training iteration 4 loss: 0.18375051021575928, ACC:0.953125\n",
      "Training iteration 5 loss: 0.2133892923593521, ACC:0.90625\n",
      "Training iteration 6 loss: 0.23732000589370728, ACC:0.90625\n",
      "Training iteration 7 loss: 0.23120911419391632, ACC:0.921875\n",
      "Training iteration 8 loss: 0.1797400414943695, ACC:0.9375\n",
      "Training iteration 9 loss: 0.3825058043003082, ACC:0.859375\n",
      "Training iteration 10 loss: 0.21558979153633118, ACC:0.953125\n",
      "Training iteration 11 loss: 0.3895259499549866, ACC:0.859375\n",
      "Training iteration 12 loss: 0.29654785990715027, ACC:0.90625\n",
      "Training iteration 13 loss: 0.2580490708351135, ACC:0.859375\n",
      "Training iteration 14 loss: 0.13921603560447693, ACC:0.953125\n",
      "Training iteration 15 loss: 0.22589091956615448, ACC:0.921875\n",
      "Training iteration 16 loss: 0.2796249985694885, ACC:0.921875\n",
      "Training iteration 17 loss: 0.260356605052948, ACC:0.90625\n",
      "Training iteration 18 loss: 0.29883047938346863, ACC:0.890625\n",
      "Training iteration 19 loss: 0.14551687240600586, ACC:1.0\n",
      "Training iteration 20 loss: 0.2939379811286926, ACC:0.890625\n",
      "Training iteration 21 loss: 0.22334983944892883, ACC:0.9375\n",
      "Training iteration 22 loss: 0.25526902079582214, ACC:0.90625\n",
      "Training iteration 23 loss: 0.17389699816703796, ACC:0.921875\n",
      "Training iteration 24 loss: 0.28464850783348083, ACC:0.90625\n",
      "Training iteration 25 loss: 0.20691916346549988, ACC:0.9375\n",
      "Training iteration 26 loss: 0.29637038707733154, ACC:0.890625\n",
      "Training iteration 27 loss: 0.19456544518470764, ACC:0.921875\n",
      "Training iteration 28 loss: 0.2844233512878418, ACC:0.921875\n",
      "Training iteration 29 loss: 0.23032964766025543, ACC:0.90625\n",
      "Training iteration 30 loss: 0.2463836818933487, ACC:0.90625\n",
      "Training iteration 31 loss: 0.35990995168685913, ACC:0.859375\n",
      "Training iteration 32 loss: 0.2588573694229126, ACC:0.90625\n",
      "Training iteration 33 loss: 0.15139628946781158, ACC:0.96875\n",
      "Training iteration 34 loss: 0.3890320658683777, ACC:0.84375\n",
      "Training iteration 35 loss: 0.27481982111930847, ACC:0.90625\n",
      "Training iteration 36 loss: 0.24408288300037384, ACC:0.90625\n",
      "Training iteration 37 loss: 0.2603912353515625, ACC:0.921875\n",
      "Training iteration 38 loss: 0.20752917230129242, ACC:0.9375\n",
      "Training iteration 39 loss: 0.17191585898399353, ACC:0.921875\n",
      "Training iteration 40 loss: 0.2731981873512268, ACC:0.90625\n",
      "Training iteration 41 loss: 0.1382376104593277, ACC:0.953125\n",
      "Training iteration 42 loss: 0.24276171624660492, ACC:0.890625\n",
      "Training iteration 43 loss: 0.21739409863948822, ACC:0.890625\n",
      "Training iteration 44 loss: 0.25704991817474365, ACC:0.90625\n",
      "Training iteration 45 loss: 0.37102028727531433, ACC:0.84375\n",
      "Training iteration 46 loss: 0.22460632026195526, ACC:0.953125\n",
      "Training iteration 47 loss: 0.2603166103363037, ACC:0.875\n",
      "Training iteration 48 loss: 0.23184606432914734, ACC:0.9375\n",
      "Training iteration 49 loss: 0.3099767565727234, ACC:0.84375\n",
      "Training iteration 50 loss: 0.2254805862903595, ACC:0.875\n",
      "Training iteration 51 loss: 0.2368391752243042, ACC:0.9375\n",
      "Training iteration 52 loss: 0.2399715781211853, ACC:0.921875\n",
      "Training iteration 53 loss: 0.24197334051132202, ACC:0.921875\n",
      "Training iteration 54 loss: 0.14750206470489502, ACC:0.96875\n",
      "Training iteration 55 loss: 0.2923930585384369, ACC:0.890625\n",
      "Training iteration 56 loss: 0.21355363726615906, ACC:0.921875\n",
      "Training iteration 57 loss: 0.2200610488653183, ACC:0.890625\n",
      "Training iteration 58 loss: 0.1549491435289383, ACC:0.953125\n",
      "Training iteration 59 loss: 0.12992067635059357, ACC:0.9375\n",
      "Training iteration 60 loss: 0.2683139145374298, ACC:0.875\n",
      "Training iteration 61 loss: 0.18876159191131592, ACC:0.9375\n",
      "Training iteration 62 loss: 0.2643078863620758, ACC:0.890625\n",
      "Training iteration 63 loss: 0.28828883171081543, ACC:0.875\n",
      "Training iteration 64 loss: 0.2179378718137741, ACC:0.921875\n",
      "Training iteration 65 loss: 0.3356882929801941, ACC:0.890625\n",
      "Training iteration 66 loss: 0.1775716096162796, ACC:0.9375\n",
      "Training iteration 67 loss: 0.2331010103225708, ACC:0.9375\n",
      "Training iteration 68 loss: 0.3071238398551941, ACC:0.84375\n",
      "Training iteration 69 loss: 0.37915557622909546, ACC:0.8125\n",
      "Training iteration 70 loss: 0.13028472661972046, ACC:0.96875\n",
      "Training iteration 71 loss: 0.2883020341396332, ACC:0.859375\n",
      "Training iteration 72 loss: 0.24563263356685638, ACC:0.90625\n",
      "Training iteration 73 loss: 0.16171187162399292, ACC:0.953125\n",
      "Training iteration 74 loss: 0.1956552416086197, ACC:0.9375\n",
      "Training iteration 75 loss: 0.26621705293655396, ACC:0.90625\n",
      "Training iteration 76 loss: 0.23363368213176727, ACC:0.9375\n",
      "Training iteration 77 loss: 0.22135303914546967, ACC:0.890625\n",
      "Training iteration 78 loss: 0.2357109636068344, ACC:0.90625\n",
      "Training iteration 79 loss: 0.2972858250141144, ACC:0.890625\n",
      "Training iteration 80 loss: 0.23931004106998444, ACC:0.921875\n",
      "Training iteration 81 loss: 0.2363586723804474, ACC:0.890625\n",
      "Training iteration 82 loss: 0.32455193996429443, ACC:0.84375\n",
      "Training iteration 83 loss: 0.1920923888683319, ACC:0.953125\n",
      "Training iteration 84 loss: 0.19702297449111938, ACC:0.890625\n",
      "Training iteration 85 loss: 0.2565401792526245, ACC:0.921875\n",
      "Training iteration 86 loss: 0.1833466738462448, ACC:0.9375\n",
      "Training iteration 87 loss: 0.2562219500541687, ACC:0.890625\n",
      "Training iteration 88 loss: 0.2857310175895691, ACC:0.90625\n",
      "Training iteration 89 loss: 0.2165195345878601, ACC:0.90625\n",
      "Training iteration 90 loss: 0.18207453191280365, ACC:0.953125\n",
      "Training iteration 91 loss: 0.3745037913322449, ACC:0.859375\n",
      "Training iteration 92 loss: 0.20024490356445312, ACC:0.9375\n",
      "Training iteration 93 loss: 0.2097417712211609, ACC:0.90625\n",
      "Training iteration 94 loss: 0.2009020298719406, ACC:0.921875\n",
      "Training iteration 95 loss: 0.2428056299686432, ACC:0.890625\n",
      "Training iteration 96 loss: 0.2527502775192261, ACC:0.921875\n",
      "Training iteration 97 loss: 0.22146174311637878, ACC:0.921875\n",
      "Training iteration 98 loss: 0.22896140813827515, ACC:0.921875\n",
      "Training iteration 99 loss: 0.2701772451400757, ACC:0.90625\n",
      "Training iteration 100 loss: 0.16407319903373718, ACC:0.9375\n",
      "Training iteration 101 loss: 0.21357537806034088, ACC:0.9375\n",
      "Training iteration 102 loss: 0.2745956778526306, ACC:0.90625\n",
      "Training iteration 103 loss: 0.2503272294998169, ACC:0.890625\n",
      "Training iteration 104 loss: 0.16933146119117737, ACC:0.9375\n",
      "Training iteration 105 loss: 0.3188089430332184, ACC:0.875\n",
      "Training iteration 106 loss: 0.16074256598949432, ACC:0.96875\n",
      "Training iteration 107 loss: 0.3258792757987976, ACC:0.90625\n",
      "Training iteration 108 loss: 0.3135063350200653, ACC:0.859375\n",
      "Training iteration 109 loss: 0.23678410053253174, ACC:0.90625\n",
      "Training iteration 110 loss: 0.20307862758636475, ACC:0.921875\n",
      "Training iteration 111 loss: 0.19799955189228058, ACC:0.90625\n",
      "Training iteration 112 loss: 0.2949102222919464, ACC:0.90625\n",
      "Training iteration 113 loss: 0.3412973880767822, ACC:0.859375\n",
      "Training iteration 114 loss: 0.32153400778770447, ACC:0.875\n",
      "Training iteration 115 loss: 0.16444750130176544, ACC:0.96875\n",
      "Training iteration 116 loss: 0.25641125440597534, ACC:0.890625\n",
      "Training iteration 117 loss: 0.20778581500053406, ACC:0.921875\n",
      "Training iteration 118 loss: 0.2350292205810547, ACC:0.921875\n",
      "Training iteration 119 loss: 0.2316872477531433, ACC:0.9375\n",
      "Training iteration 120 loss: 0.28082045912742615, ACC:0.90625\n",
      "Training iteration 121 loss: 0.16040000319480896, ACC:0.953125\n",
      "Training iteration 122 loss: 0.09789824485778809, ACC:0.984375\n",
      "Training iteration 123 loss: 0.17228946089744568, ACC:0.953125\n",
      "Training iteration 124 loss: 0.2673916518688202, ACC:0.890625\n",
      "Training iteration 125 loss: 0.0795508325099945, ACC:1.0\n",
      "Training iteration 126 loss: 0.13968855142593384, ACC:0.96875\n",
      "Training iteration 127 loss: 0.23803597688674927, ACC:0.921875\n",
      "Training iteration 128 loss: 0.31893226504325867, ACC:0.875\n",
      "Training iteration 129 loss: 0.17217184603214264, ACC:0.9375\n",
      "Training iteration 130 loss: 0.23640835285186768, ACC:0.953125\n",
      "Training iteration 131 loss: 0.35672661662101746, ACC:0.875\n",
      "Training iteration 132 loss: 0.1248718723654747, ACC:0.953125\n",
      "Training iteration 133 loss: 0.15927095711231232, ACC:0.953125\n",
      "Training iteration 134 loss: 0.20664608478546143, ACC:0.953125\n",
      "Training iteration 135 loss: 0.2166459858417511, ACC:0.921875\n",
      "Training iteration 136 loss: 0.21862074732780457, ACC:0.890625\n",
      "Training iteration 137 loss: 0.1770366132259369, ACC:0.953125\n",
      "Training iteration 138 loss: 0.1864968240261078, ACC:0.953125\n",
      "Training iteration 139 loss: 0.14647752046585083, ACC:0.9375\n",
      "Training iteration 140 loss: 0.2573428153991699, ACC:0.9375\n",
      "Training iteration 141 loss: 0.07430665194988251, ACC:0.984375\n",
      "Training iteration 142 loss: 0.24311234056949615, ACC:0.890625\n",
      "Training iteration 143 loss: 0.16173431277275085, ACC:0.921875\n",
      "Training iteration 144 loss: 0.1981605589389801, ACC:0.9375\n",
      "Training iteration 145 loss: 0.2959529757499695, ACC:0.890625\n",
      "Training iteration 146 loss: 0.2549671530723572, ACC:0.875\n",
      "Training iteration 147 loss: 0.330802321434021, ACC:0.875\n",
      "Training iteration 148 loss: 0.1604728251695633, ACC:0.953125\n",
      "Training iteration 149 loss: 0.21697208285331726, ACC:0.921875\n",
      "Training iteration 150 loss: 0.223915234208107, ACC:0.90625\n",
      "Training iteration 151 loss: 0.2643052637577057, ACC:0.921875\n",
      "Training iteration 152 loss: 0.3338959515094757, ACC:0.90625\n",
      "Training iteration 153 loss: 0.30341988801956177, ACC:0.921875\n",
      "Training iteration 154 loss: 0.28981390595436096, ACC:0.859375\n",
      "Training iteration 155 loss: 0.10692588984966278, ACC:0.984375\n",
      "Training iteration 156 loss: 0.18527428805828094, ACC:0.9375\n",
      "Training iteration 157 loss: 0.16556893289089203, ACC:0.921875\n",
      "Training iteration 158 loss: 0.19390985369682312, ACC:0.9375\n",
      "Training iteration 159 loss: 0.1383110135793686, ACC:0.9375\n",
      "Training iteration 160 loss: 0.19315610826015472, ACC:0.921875\n",
      "Training iteration 161 loss: 0.2364017814397812, ACC:0.921875\n",
      "Training iteration 162 loss: 0.14164221286773682, ACC:0.96875\n",
      "Training iteration 163 loss: 0.1486450582742691, ACC:0.96875\n",
      "Training iteration 164 loss: 0.2212740182876587, ACC:0.953125\n",
      "Training iteration 165 loss: 0.34370410442352295, ACC:0.875\n",
      "Training iteration 166 loss: 0.13409952819347382, ACC:0.96875\n",
      "Training iteration 167 loss: 0.16125814616680145, ACC:0.953125\n",
      "Training iteration 168 loss: 0.21720057725906372, ACC:0.90625\n",
      "Training iteration 169 loss: 0.25224927067756653, ACC:0.890625\n",
      "Training iteration 170 loss: 0.22544962167739868, ACC:0.9375\n",
      "Training iteration 171 loss: 0.21100367605686188, ACC:0.9375\n",
      "Training iteration 172 loss: 0.25515127182006836, ACC:0.890625\n",
      "Training iteration 173 loss: 0.3025026321411133, ACC:0.90625\n",
      "Training iteration 174 loss: 0.2552798092365265, ACC:0.890625\n",
      "Training iteration 175 loss: 0.25410258769989014, ACC:0.921875\n",
      "Training iteration 176 loss: 0.2211112678050995, ACC:0.9375\n",
      "Training iteration 177 loss: 0.304151326417923, ACC:0.90625\n",
      "Training iteration 178 loss: 0.12654060125350952, ACC:0.96875\n",
      "Training iteration 179 loss: 0.37529152631759644, ACC:0.796875\n",
      "Training iteration 180 loss: 0.24341659247875214, ACC:0.921875\n",
      "Training iteration 181 loss: 0.19131770730018616, ACC:0.953125\n",
      "Training iteration 182 loss: 0.15900592505931854, ACC:0.96875\n",
      "Training iteration 183 loss: 0.24721506237983704, ACC:0.890625\n",
      "Training iteration 184 loss: 0.3743530809879303, ACC:0.828125\n",
      "Training iteration 185 loss: 0.2589842677116394, ACC:0.953125\n",
      "Training iteration 186 loss: 0.2782251536846161, ACC:0.890625\n",
      "Training iteration 187 loss: 0.16624587774276733, ACC:0.96875\n",
      "Training iteration 188 loss: 0.24998760223388672, ACC:0.890625\n",
      "Training iteration 189 loss: 0.2105000615119934, ACC:0.9375\n",
      "Training iteration 190 loss: 0.25762903690338135, ACC:0.890625\n",
      "Training iteration 191 loss: 0.20789597928524017, ACC:0.90625\n",
      "Training iteration 192 loss: 0.17719154059886932, ACC:0.90625\n",
      "Training iteration 193 loss: 0.29420605301856995, ACC:0.90625\n",
      "Training iteration 194 loss: 0.28023165464401245, ACC:0.875\n",
      "Training iteration 195 loss: 0.11755623668432236, ACC:0.984375\n",
      "Training iteration 196 loss: 0.258322149515152, ACC:0.875\n",
      "Training iteration 197 loss: 0.17257115244865417, ACC:0.953125\n",
      "Training iteration 198 loss: 0.2888103425502777, ACC:0.859375\n",
      "Training iteration 199 loss: 0.22240890562534332, ACC:0.90625\n",
      "Training iteration 200 loss: 0.17894378304481506, ACC:0.9375\n",
      "Training iteration 201 loss: 0.13336311280727386, ACC:0.96875\n",
      "Training iteration 202 loss: 0.2864196300506592, ACC:0.875\n",
      "Training iteration 203 loss: 0.1798436939716339, ACC:0.921875\n",
      "Training iteration 204 loss: 0.23298709094524384, ACC:0.921875\n",
      "Training iteration 205 loss: 0.15453483164310455, ACC:0.953125\n",
      "Training iteration 206 loss: 0.19908155500888824, ACC:0.90625\n",
      "Training iteration 207 loss: 0.1481589823961258, ACC:0.96875\n",
      "Training iteration 208 loss: 0.06918225437402725, ACC:1.0\n",
      "Training iteration 209 loss: 0.10346201807260513, ACC:0.984375\n",
      "Training iteration 210 loss: 0.14910916984081268, ACC:0.953125\n",
      "Training iteration 211 loss: 0.1885690540075302, ACC:0.921875\n",
      "Training iteration 212 loss: 0.42728376388549805, ACC:0.84375\n",
      "Training iteration 213 loss: 0.2707008421421051, ACC:0.90625\n",
      "Training iteration 214 loss: 0.2370496392250061, ACC:0.90625\n",
      "Training iteration 215 loss: 0.16355279088020325, ACC:0.953125\n",
      "Training iteration 216 loss: 0.11395222693681717, ACC:0.96875\n",
      "Training iteration 217 loss: 0.2634960412979126, ACC:0.90625\n",
      "Training iteration 218 loss: 0.29367828369140625, ACC:0.90625\n",
      "Training iteration 219 loss: 0.2035483568906784, ACC:0.921875\n",
      "Training iteration 220 loss: 0.18955974280834198, ACC:0.9375\n",
      "Training iteration 221 loss: 0.07736603915691376, ACC:0.984375\n",
      "Training iteration 222 loss: 0.15666763484477997, ACC:0.96875\n",
      "Training iteration 223 loss: 0.17102251946926117, ACC:0.953125\n",
      "Training iteration 224 loss: 0.165553480386734, ACC:0.953125\n",
      "Training iteration 225 loss: 0.2672075629234314, ACC:0.890625\n",
      "Training iteration 226 loss: 0.3246771991252899, ACC:0.875\n",
      "Training iteration 227 loss: 0.13057352602481842, ACC:0.953125\n",
      "Training iteration 228 loss: 0.10974179208278656, ACC:0.984375\n",
      "Training iteration 229 loss: 0.2651958167552948, ACC:0.90625\n",
      "Training iteration 230 loss: 0.2032046616077423, ACC:0.90625\n",
      "Training iteration 231 loss: 0.13022635877132416, ACC:0.96875\n",
      "Training iteration 232 loss: 0.12642468512058258, ACC:0.96875\n",
      "Training iteration 233 loss: 0.18700894713401794, ACC:0.9375\n",
      "Training iteration 234 loss: 0.17552034556865692, ACC:0.90625\n",
      "Training iteration 235 loss: 0.16010016202926636, ACC:0.96875\n",
      "Training iteration 236 loss: 0.26080140471458435, ACC:0.921875\n",
      "Training iteration 237 loss: 0.14157381653785706, ACC:0.96875\n",
      "Training iteration 238 loss: 0.1852816492319107, ACC:0.921875\n",
      "Training iteration 239 loss: 0.20463678240776062, ACC:0.9375\n",
      "Training iteration 240 loss: 0.16516084969043732, ACC:0.96875\n",
      "Training iteration 241 loss: 0.11371420323848724, ACC:0.96875\n",
      "Training iteration 242 loss: 0.2077847123146057, ACC:0.9375\n",
      "Training iteration 243 loss: 0.06290606409311295, ACC:1.0\n",
      "Training iteration 244 loss: 0.3678322434425354, ACC:0.859375\n",
      "Training iteration 245 loss: 0.17533822357654572, ACC:0.9375\n",
      "Training iteration 246 loss: 0.20952224731445312, ACC:0.921875\n",
      "Training iteration 247 loss: 0.15696874260902405, ACC:0.984375\n",
      "Training iteration 248 loss: 0.2587073743343353, ACC:0.875\n",
      "Training iteration 249 loss: 0.1335880160331726, ACC:0.96875\n",
      "Training iteration 250 loss: 0.31458863615989685, ACC:0.84375\n",
      "Training iteration 251 loss: 0.1385473608970642, ACC:0.953125\n",
      "Training iteration 252 loss: 0.22121921181678772, ACC:0.90625\n",
      "Training iteration 253 loss: 0.274260938167572, ACC:0.90625\n",
      "Training iteration 254 loss: 0.19600185751914978, ACC:0.9375\n",
      "Training iteration 255 loss: 0.25544577836990356, ACC:0.921875\n",
      "Training iteration 256 loss: 0.19087347388267517, ACC:0.9375\n",
      "Training iteration 257 loss: 0.2668841779232025, ACC:0.875\n",
      "Training iteration 258 loss: 0.14763705432415009, ACC:0.984375\n",
      "Training iteration 259 loss: 0.17272454500198364, ACC:0.9375\n",
      "Training iteration 260 loss: 0.14990326762199402, ACC:0.953125\n",
      "Training iteration 261 loss: 0.250473290681839, ACC:0.890625\n",
      "Training iteration 262 loss: 0.15254628658294678, ACC:0.921875\n",
      "Training iteration 263 loss: 0.24550792574882507, ACC:0.9375\n",
      "Training iteration 264 loss: 0.3468717932701111, ACC:0.859375\n",
      "Training iteration 265 loss: 0.19753925502300262, ACC:0.90625\n",
      "Training iteration 266 loss: 0.21853013336658478, ACC:0.921875\n",
      "Training iteration 267 loss: 0.33894407749176025, ACC:0.875\n",
      "Training iteration 268 loss: 0.14848408102989197, ACC:0.96875\n",
      "Training iteration 269 loss: 0.23088324069976807, ACC:0.890625\n",
      "Training iteration 270 loss: 0.17513854801654816, ACC:0.96875\n",
      "Training iteration 271 loss: 0.24746733903884888, ACC:0.9375\n",
      "Training iteration 272 loss: 0.1598762422800064, ACC:0.953125\n",
      "Training iteration 273 loss: 0.15996991097927094, ACC:0.9375\n",
      "Training iteration 274 loss: 0.24407103657722473, ACC:0.90625\n",
      "Training iteration 275 loss: 0.2169906198978424, ACC:0.921875\n",
      "Training iteration 276 loss: 0.23858559131622314, ACC:0.90625\n",
      "Training iteration 277 loss: 0.18522807955741882, ACC:0.96875\n",
      "Training iteration 278 loss: 0.1379757523536682, ACC:0.953125\n",
      "Training iteration 279 loss: 0.11644837260246277, ACC:0.953125\n",
      "Training iteration 280 loss: 0.14113642275333405, ACC:0.953125\n",
      "Training iteration 281 loss: 0.25363677740097046, ACC:0.859375\n",
      "Training iteration 282 loss: 0.17910386621952057, ACC:0.953125\n",
      "Training iteration 283 loss: 0.20192687213420868, ACC:0.9375\n",
      "Training iteration 284 loss: 0.19517596065998077, ACC:0.953125\n",
      "Training iteration 285 loss: 0.16556388139724731, ACC:0.9375\n",
      "Training iteration 286 loss: 0.1738128811120987, ACC:0.921875\n",
      "Training iteration 287 loss: 0.19219423830509186, ACC:0.953125\n",
      "Training iteration 288 loss: 0.13728322088718414, ACC:0.96875\n",
      "Training iteration 289 loss: 0.21857529878616333, ACC:0.9375\n",
      "Training iteration 290 loss: 0.09846870601177216, ACC:0.984375\n",
      "Training iteration 291 loss: 0.18610858917236328, ACC:0.9375\n",
      "Training iteration 292 loss: 0.24662010371685028, ACC:0.90625\n",
      "Training iteration 293 loss: 0.14230787754058838, ACC:0.953125\n",
      "Training iteration 294 loss: 0.22087223827838898, ACC:0.921875\n",
      "Training iteration 295 loss: 0.1887526661157608, ACC:0.921875\n",
      "Training iteration 296 loss: 0.30541980266571045, ACC:0.875\n",
      "Training iteration 297 loss: 0.1012759655714035, ACC:0.984375\n",
      "Training iteration 298 loss: 0.18284326791763306, ACC:0.90625\n",
      "Training iteration 299 loss: 0.09648176282644272, ACC:0.953125\n",
      "Training iteration 300 loss: 0.19970086216926575, ACC:0.90625\n",
      "Training iteration 301 loss: 0.25201451778411865, ACC:0.921875\n",
      "Training iteration 302 loss: 0.25554487109184265, ACC:0.921875\n",
      "Training iteration 303 loss: 0.18802790343761444, ACC:0.953125\n",
      "Training iteration 304 loss: 0.27998650074005127, ACC:0.890625\n",
      "Training iteration 305 loss: 0.35485538840293884, ACC:0.859375\n",
      "Training iteration 306 loss: 0.09564340859651566, ACC:0.984375\n",
      "Training iteration 307 loss: 0.20674261450767517, ACC:0.921875\n",
      "Training iteration 308 loss: 0.23526537418365479, ACC:0.9375\n",
      "Training iteration 309 loss: 0.10432060807943344, ACC:0.96875\n",
      "Training iteration 310 loss: 0.2202841341495514, ACC:0.890625\n",
      "Training iteration 311 loss: 0.13059139251708984, ACC:0.953125\n",
      "Training iteration 312 loss: 0.16631609201431274, ACC:0.953125\n",
      "Training iteration 313 loss: 0.1360805332660675, ACC:0.96875\n",
      "Training iteration 314 loss: 0.16380278766155243, ACC:0.953125\n",
      "Training iteration 315 loss: 0.1058541014790535, ACC:0.96875\n",
      "Training iteration 316 loss: 0.18902507424354553, ACC:0.90625\n",
      "Training iteration 317 loss: 0.17606154084205627, ACC:0.9375\n",
      "Training iteration 318 loss: 0.2398340106010437, ACC:0.9375\n",
      "Training iteration 319 loss: 0.12389408051967621, ACC:0.953125\n",
      "Training iteration 320 loss: 0.24022525548934937, ACC:0.890625\n",
      "Training iteration 321 loss: 0.09470560401678085, ACC:0.953125\n",
      "Training iteration 322 loss: 0.13712731003761292, ACC:0.9375\n",
      "Training iteration 323 loss: 0.2655197083950043, ACC:0.90625\n",
      "Training iteration 324 loss: 0.2060130536556244, ACC:0.921875\n",
      "Training iteration 325 loss: 0.19024914503097534, ACC:0.9375\n",
      "Training iteration 326 loss: 0.1486915498971939, ACC:0.96875\n",
      "Training iteration 327 loss: 0.19958268105983734, ACC:0.9375\n",
      "Training iteration 328 loss: 0.11146848648786545, ACC:0.953125\n",
      "Training iteration 329 loss: 0.1315302699804306, ACC:0.96875\n",
      "Training iteration 330 loss: 0.24994461238384247, ACC:0.90625\n",
      "Training iteration 331 loss: 0.17565131187438965, ACC:0.9375\n",
      "Training iteration 332 loss: 0.17029820382595062, ACC:0.9375\n",
      "Training iteration 333 loss: 0.18422073125839233, ACC:0.953125\n",
      "Training iteration 334 loss: 0.18932540714740753, ACC:0.9375\n",
      "Training iteration 335 loss: 0.20199370384216309, ACC:0.953125\n",
      "Training iteration 336 loss: 0.08053794503211975, ACC:0.953125\n",
      "Training iteration 337 loss: 0.09380730241537094, ACC:0.953125\n",
      "Training iteration 338 loss: 0.2848134934902191, ACC:0.953125\n",
      "Training iteration 339 loss: 0.10322585701942444, ACC:0.984375\n",
      "Training iteration 340 loss: 0.16600722074508667, ACC:0.9375\n",
      "Training iteration 341 loss: 0.15638433396816254, ACC:0.953125\n",
      "Training iteration 342 loss: 0.1553085446357727, ACC:0.9375\n",
      "Training iteration 343 loss: 0.10989003628492355, ACC:0.953125\n",
      "Training iteration 344 loss: 0.17275820672512054, ACC:0.953125\n",
      "Training iteration 345 loss: 0.09681785106658936, ACC:0.96875\n",
      "Training iteration 346 loss: 0.13818857073783875, ACC:0.984375\n",
      "Training iteration 347 loss: 0.21962891519069672, ACC:0.921875\n",
      "Training iteration 348 loss: 0.10192625969648361, ACC:0.96875\n",
      "Training iteration 349 loss: 0.08483971655368805, ACC:0.984375\n",
      "Training iteration 350 loss: 0.0778818130493164, ACC:0.984375\n",
      "Training iteration 351 loss: 0.11084874719381332, ACC:0.96875\n",
      "Training iteration 352 loss: 0.14951933920383453, ACC:0.953125\n",
      "Training iteration 353 loss: 0.08439064025878906, ACC:0.96875\n",
      "Training iteration 354 loss: 0.17386524379253387, ACC:0.9375\n",
      "Training iteration 355 loss: 0.12849698960781097, ACC:0.953125\n",
      "Training iteration 356 loss: 0.16548876464366913, ACC:0.90625\n",
      "Training iteration 357 loss: 0.1839493364095688, ACC:0.953125\n",
      "Training iteration 358 loss: 0.17396168410778046, ACC:0.921875\n",
      "Training iteration 359 loss: 0.26284968852996826, ACC:0.890625\n",
      "Training iteration 360 loss: 0.280044823884964, ACC:0.890625\n",
      "Training iteration 361 loss: 0.15702033042907715, ACC:0.953125\n",
      "Training iteration 362 loss: 0.1196250170469284, ACC:0.984375\n",
      "Training iteration 363 loss: 0.16566117107868195, ACC:0.96875\n",
      "Training iteration 364 loss: 0.15378977358341217, ACC:0.9375\n",
      "Training iteration 365 loss: 0.13947677612304688, ACC:0.96875\n",
      "Training iteration 366 loss: 0.13469965755939484, ACC:0.921875\n",
      "Training iteration 367 loss: 0.1348513662815094, ACC:0.921875\n",
      "Training iteration 368 loss: 0.10758169740438461, ACC:0.953125\n",
      "Training iteration 369 loss: 0.05833834409713745, ACC:0.984375\n",
      "Training iteration 370 loss: 0.07047753036022186, ACC:0.984375\n",
      "Training iteration 371 loss: 0.05293501168489456, ACC:1.0\n",
      "Training iteration 372 loss: 0.12787021696567535, ACC:0.9375\n",
      "Training iteration 373 loss: 0.12098335474729538, ACC:0.96875\n",
      "Training iteration 374 loss: 0.16865931451320648, ACC:0.921875\n",
      "Training iteration 375 loss: 0.11398821324110031, ACC:0.984375\n",
      "Training iteration 376 loss: 0.2187568098306656, ACC:0.9375\n",
      "Training iteration 377 loss: 0.13728754222393036, ACC:0.921875\n",
      "Training iteration 378 loss: 0.20273125171661377, ACC:0.890625\n",
      "Training iteration 379 loss: 0.26210057735443115, ACC:0.921875\n",
      "Training iteration 380 loss: 0.11249765753746033, ACC:0.953125\n",
      "Training iteration 381 loss: 0.1427493840456009, ACC:0.953125\n",
      "Training iteration 382 loss: 0.11603617668151855, ACC:0.96875\n",
      "Training iteration 383 loss: 0.0727260634303093, ACC:0.984375\n",
      "Training iteration 384 loss: 0.15614540874958038, ACC:0.9375\n",
      "Training iteration 385 loss: 0.10357186943292618, ACC:0.96875\n",
      "Training iteration 386 loss: 0.12902268767356873, ACC:0.9375\n",
      "Training iteration 387 loss: 0.2911585569381714, ACC:0.90625\n",
      "Training iteration 388 loss: 0.13816140592098236, ACC:0.9375\n",
      "Training iteration 389 loss: 0.16917181015014648, ACC:0.953125\n",
      "Training iteration 390 loss: 0.22119443118572235, ACC:0.90625\n",
      "Training iteration 391 loss: 0.16416186094284058, ACC:0.9375\n",
      "Training iteration 392 loss: 0.10219824314117432, ACC:0.953125\n",
      "Training iteration 393 loss: 0.12640056014060974, ACC:0.984375\n",
      "Training iteration 394 loss: 0.0916864424943924, ACC:0.984375\n",
      "Training iteration 395 loss: 0.13780194520950317, ACC:0.953125\n",
      "Training iteration 396 loss: 0.07466619461774826, ACC:0.984375\n",
      "Training iteration 397 loss: 0.11339682340621948, ACC:0.96875\n",
      "Training iteration 398 loss: 0.15959696471691132, ACC:0.953125\n",
      "Training iteration 399 loss: 0.13168182969093323, ACC:0.96875\n",
      "Training iteration 400 loss: 0.13674670457839966, ACC:0.96875\n",
      "Training iteration 401 loss: 0.16187435388565063, ACC:0.96875\n",
      "Training iteration 402 loss: 0.19657716155052185, ACC:0.921875\n",
      "Training iteration 403 loss: 0.05943916365504265, ACC:0.984375\n",
      "Training iteration 404 loss: 0.10584517568349838, ACC:0.953125\n",
      "Training iteration 405 loss: 0.11572225391864777, ACC:0.984375\n",
      "Training iteration 406 loss: 0.09971723705530167, ACC:0.96875\n",
      "Training iteration 407 loss: 0.08475003391504288, ACC:0.96875\n",
      "Training iteration 408 loss: 0.23074515163898468, ACC:0.921875\n",
      "Training iteration 409 loss: 0.15454621613025665, ACC:0.90625\n",
      "Training iteration 410 loss: 0.04891231283545494, ACC:0.984375\n",
      "Training iteration 411 loss: 0.10279940813779831, ACC:0.96875\n",
      "Training iteration 412 loss: 0.1995004415512085, ACC:0.921875\n",
      "Training iteration 413 loss: 0.10104696452617645, ACC:0.953125\n",
      "Training iteration 414 loss: 0.07774510234594345, ACC:0.984375\n",
      "Training iteration 415 loss: 0.1439739614725113, ACC:0.953125\n",
      "Training iteration 416 loss: 0.19666090607643127, ACC:0.953125\n",
      "Training iteration 417 loss: 0.0876995250582695, ACC:0.984375\n",
      "Training iteration 418 loss: 0.12579600512981415, ACC:0.96875\n",
      "Training iteration 419 loss: 0.21269594132900238, ACC:0.921875\n",
      "Training iteration 420 loss: 0.1311626136302948, ACC:0.9375\n",
      "Training iteration 421 loss: 0.13015809655189514, ACC:0.96875\n",
      "Training iteration 422 loss: 0.18751932680606842, ACC:0.9375\n",
      "Training iteration 423 loss: 0.06936921179294586, ACC:0.984375\n",
      "Training iteration 424 loss: 0.19139210879802704, ACC:0.921875\n",
      "Training iteration 425 loss: 0.1717883050441742, ACC:0.921875\n",
      "Training iteration 426 loss: 0.24844276905059814, ACC:0.90625\n",
      "Training iteration 427 loss: 0.08474432677030563, ACC:0.96875\n",
      "Training iteration 428 loss: 0.20112088322639465, ACC:0.921875\n",
      "Training iteration 429 loss: 0.18969669938087463, ACC:0.921875\n",
      "Training iteration 430 loss: 0.11407148092985153, ACC:0.984375\n",
      "Training iteration 431 loss: 0.07268023490905762, ACC:0.984375\n",
      "Training iteration 432 loss: 0.2060905247926712, ACC:0.9375\n",
      "Training iteration 433 loss: 0.19006149470806122, ACC:0.921875\n",
      "Training iteration 434 loss: 0.14830008149147034, ACC:0.9375\n",
      "Training iteration 435 loss: 0.13153445720672607, ACC:0.9375\n",
      "Training iteration 436 loss: 0.1652604639530182, ACC:0.9375\n",
      "Training iteration 437 loss: 0.11342661082744598, ACC:0.984375\n",
      "Training iteration 438 loss: 0.08564241230487823, ACC:0.984375\n",
      "Training iteration 439 loss: 0.23281732201576233, ACC:0.9375\n",
      "Training iteration 440 loss: 0.14084607362747192, ACC:0.96875\n",
      "Training iteration 441 loss: 0.18488985300064087, ACC:0.921875\n",
      "Training iteration 442 loss: 0.10275177657604218, ACC:0.96875\n",
      "Training iteration 443 loss: 0.16665874421596527, ACC:0.9375\n",
      "Training iteration 444 loss: 0.13631363213062286, ACC:0.953125\n",
      "Training iteration 445 loss: 0.125808447599411, ACC:0.953125\n",
      "Training iteration 446 loss: 0.1734618842601776, ACC:0.96875\n",
      "Training iteration 447 loss: 0.3066238462924957, ACC:0.875\n",
      "Training iteration 448 loss: 0.046509917825460434, ACC:1.0\n",
      "Training iteration 449 loss: 0.1954629272222519, ACC:0.9375\n",
      "Training iteration 450 loss: 0.14750966429710388, ACC:0.96875\n",
      "Validation iteration 451 loss: 0.21465900540351868, ACC: 0.9375\n",
      "Validation iteration 452 loss: 0.17022164165973663, ACC: 0.921875\n",
      "Validation iteration 453 loss: 0.21213945746421814, ACC: 0.9375\n",
      "Validation iteration 454 loss: 0.2700873613357544, ACC: 0.890625\n",
      "Validation iteration 455 loss: 0.148968905210495, ACC: 0.9375\n",
      "Validation iteration 456 loss: 0.1787416785955429, ACC: 0.921875\n",
      "Validation iteration 457 loss: 0.1144157350063324, ACC: 0.953125\n",
      "Validation iteration 458 loss: 0.11012085527181625, ACC: 0.96875\n",
      "Validation iteration 459 loss: 0.1129172071814537, ACC: 0.96875\n",
      "Validation iteration 460 loss: 0.1532125622034073, ACC: 0.9375\n",
      "Validation iteration 461 loss: 0.258100688457489, ACC: 0.90625\n",
      "Validation iteration 462 loss: 0.2615376114845276, ACC: 0.921875\n",
      "Validation iteration 463 loss: 0.1079215481877327, ACC: 0.984375\n",
      "Validation iteration 464 loss: 0.12223053723573685, ACC: 0.96875\n",
      "Validation iteration 465 loss: 0.25367119908332825, ACC: 0.890625\n",
      "Validation iteration 466 loss: 0.07281938195228577, ACC: 0.984375\n",
      "Validation iteration 467 loss: 0.16999778151512146, ACC: 0.9375\n",
      "Validation iteration 468 loss: 0.0958922803401947, ACC: 0.984375\n",
      "Validation iteration 469 loss: 0.3258730471134186, ACC: 0.875\n",
      "Validation iteration 470 loss: 0.1837509572505951, ACC: 0.921875\n",
      "Validation iteration 471 loss: 0.1850733757019043, ACC: 0.953125\n",
      "Validation iteration 472 loss: 0.14313772320747375, ACC: 0.953125\n",
      "Validation iteration 473 loss: 0.1150064617395401, ACC: 0.953125\n",
      "Validation iteration 474 loss: 0.13754911720752716, ACC: 0.9375\n",
      "Validation iteration 475 loss: 0.11054862290620804, ACC: 0.953125\n",
      "Validation iteration 476 loss: 0.09235632419586182, ACC: 0.96875\n",
      "Validation iteration 477 loss: 0.07431197166442871, ACC: 0.96875\n",
      "Validation iteration 478 loss: 0.12792809307575226, ACC: 0.9375\n",
      "Validation iteration 479 loss: 0.11535578966140747, ACC: 0.953125\n",
      "Validation iteration 480 loss: 0.2374599128961563, ACC: 0.921875\n",
      "Validation iteration 481 loss: 0.18919944763183594, ACC: 0.9375\n",
      "Validation iteration 482 loss: 0.08300045132637024, ACC: 0.96875\n",
      "Validation iteration 483 loss: 0.11346964538097382, ACC: 0.984375\n",
      "Validation iteration 484 loss: 0.23701727390289307, ACC: 0.921875\n",
      "Validation iteration 485 loss: 0.17095501720905304, ACC: 0.96875\n",
      "Validation iteration 486 loss: 0.14745940268039703, ACC: 0.90625\n",
      "Validation iteration 487 loss: 0.1688014268875122, ACC: 0.9375\n",
      "Validation iteration 488 loss: 0.24841046333312988, ACC: 0.875\n",
      "Validation iteration 489 loss: 0.09038883447647095, ACC: 0.96875\n",
      "Validation iteration 490 loss: 0.26632991433143616, ACC: 0.90625\n",
      "Validation iteration 491 loss: 0.10011205077171326, ACC: 0.96875\n",
      "Validation iteration 492 loss: 0.0667807087302208, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.16131195425987244, ACC: 0.953125\n",
      "Validation iteration 494 loss: 0.13210885226726532, ACC: 0.9375\n",
      "Validation iteration 495 loss: 0.08522626757621765, ACC: 0.984375\n",
      "Validation iteration 496 loss: 0.20941108465194702, ACC: 0.921875\n",
      "Validation iteration 497 loss: 0.14756840467453003, ACC: 0.96875\n",
      "Validation iteration 498 loss: 0.14138910174369812, ACC: 0.953125\n",
      "Validation iteration 499 loss: 0.09511715918779373, ACC: 0.984375\n",
      "Validation iteration 500 loss: 0.11429961770772934, ACC: 0.96875\n",
      "-- Epoch 2 done -- Train loss: 0.19908084779149957, train ACC: 0.9299305555555556, val loss: 0.1568872782588005, val ACC: 0.9453125\n",
      "<--- 507.82623958587646 seconds --->\n",
      "Training iteration 1 loss: 0.08523810654878616, ACC:0.96875\n",
      "Training iteration 2 loss: 0.0901716873049736, ACC:0.96875\n",
      "Training iteration 3 loss: 0.16070736944675446, ACC:0.953125\n",
      "Training iteration 4 loss: 0.16134808957576752, ACC:0.953125\n",
      "Training iteration 5 loss: 0.05011173337697983, ACC:1.0\n",
      "Training iteration 6 loss: 0.17069478332996368, ACC:0.9375\n",
      "Training iteration 7 loss: 0.15922614932060242, ACC:0.9375\n",
      "Training iteration 8 loss: 0.21183569729328156, ACC:0.9375\n",
      "Training iteration 9 loss: 0.22541368007659912, ACC:0.90625\n",
      "Training iteration 10 loss: 0.09252680838108063, ACC:0.96875\n",
      "Training iteration 11 loss: 0.09874793887138367, ACC:0.96875\n",
      "Training iteration 12 loss: 0.07060928642749786, ACC:0.953125\n",
      "Training iteration 13 loss: 0.16617673635482788, ACC:0.921875\n",
      "Training iteration 14 loss: 0.16024447977542877, ACC:0.953125\n",
      "Training iteration 15 loss: 0.18043766915798187, ACC:0.953125\n",
      "Training iteration 16 loss: 0.09248974174261093, ACC:0.96875\n",
      "Training iteration 17 loss: 0.20765374600887299, ACC:0.90625\n",
      "Training iteration 18 loss: 0.15604710578918457, ACC:0.953125\n",
      "Training iteration 19 loss: 0.10132209211587906, ACC:0.953125\n",
      "Training iteration 20 loss: 0.0659063383936882, ACC:0.96875\n",
      "Training iteration 21 loss: 0.13856562972068787, ACC:0.96875\n",
      "Training iteration 22 loss: 0.10366886854171753, ACC:0.96875\n",
      "Training iteration 23 loss: 0.054658591747283936, ACC:0.984375\n",
      "Training iteration 24 loss: 0.19279906153678894, ACC:0.921875\n",
      "Training iteration 25 loss: 0.09701094031333923, ACC:0.984375\n",
      "Training iteration 26 loss: 0.2703886926174164, ACC:0.90625\n",
      "Training iteration 27 loss: 0.07430076599121094, ACC:0.96875\n",
      "Training iteration 28 loss: 0.1375463604927063, ACC:0.953125\n",
      "Training iteration 29 loss: 0.2487412542104721, ACC:0.921875\n",
      "Training iteration 30 loss: 0.21788926422595978, ACC:0.921875\n",
      "Training iteration 31 loss: 0.10792412608861923, ACC:0.96875\n",
      "Training iteration 32 loss: 0.24677400290966034, ACC:0.921875\n",
      "Training iteration 33 loss: 0.14075368642807007, ACC:0.953125\n",
      "Training iteration 34 loss: 0.22741985321044922, ACC:0.921875\n",
      "Training iteration 35 loss: 0.20078648626804352, ACC:0.9375\n",
      "Training iteration 36 loss: 0.18422220647335052, ACC:0.921875\n",
      "Training iteration 37 loss: 0.18027901649475098, ACC:0.921875\n",
      "Training iteration 38 loss: 0.10314326733350754, ACC:0.953125\n",
      "Training iteration 39 loss: 0.09491360932588577, ACC:0.96875\n",
      "Training iteration 40 loss: 0.22232861816883087, ACC:0.890625\n",
      "Training iteration 41 loss: 0.18600605428218842, ACC:0.96875\n",
      "Training iteration 42 loss: 0.11011459678411484, ACC:0.96875\n",
      "Training iteration 43 loss: 0.07883430272340775, ACC:0.96875\n",
      "Training iteration 44 loss: 0.21811354160308838, ACC:0.921875\n",
      "Training iteration 45 loss: 0.10229215770959854, ACC:0.953125\n",
      "Training iteration 46 loss: 0.08035258948802948, ACC:0.984375\n",
      "Training iteration 47 loss: 0.08321157842874527, ACC:1.0\n",
      "Training iteration 48 loss: 0.12527108192443848, ACC:0.953125\n",
      "Training iteration 49 loss: 0.1585930436849594, ACC:0.953125\n",
      "Training iteration 50 loss: 0.17583909630775452, ACC:0.9375\n",
      "Training iteration 51 loss: 0.12206084281206131, ACC:0.96875\n",
      "Training iteration 52 loss: 0.15454208850860596, ACC:0.953125\n",
      "Training iteration 53 loss: 0.17414790391921997, ACC:0.953125\n",
      "Training iteration 54 loss: 0.05675685405731201, ACC:1.0\n",
      "Training iteration 55 loss: 0.06972181797027588, ACC:0.96875\n",
      "Training iteration 56 loss: 0.05891117453575134, ACC:0.984375\n",
      "Training iteration 57 loss: 0.06402657926082611, ACC:0.984375\n",
      "Training iteration 58 loss: 0.0873936116695404, ACC:0.984375\n",
      "Training iteration 59 loss: 0.07869293540716171, ACC:0.984375\n",
      "Training iteration 60 loss: 0.03969285637140274, ACC:0.984375\n",
      "Training iteration 61 loss: 0.09414992481470108, ACC:0.953125\n",
      "Training iteration 62 loss: 0.10082674771547318, ACC:0.953125\n",
      "Training iteration 63 loss: 0.027367684990167618, ACC:0.984375\n",
      "Training iteration 64 loss: 0.13224130868911743, ACC:0.9375\n",
      "Training iteration 65 loss: 0.09428664296865463, ACC:0.96875\n",
      "Training iteration 66 loss: 0.10704728960990906, ACC:0.96875\n",
      "Training iteration 67 loss: 0.1597358137369156, ACC:0.953125\n",
      "Training iteration 68 loss: 0.09812919795513153, ACC:0.96875\n",
      "Training iteration 69 loss: 0.07448197901248932, ACC:0.984375\n",
      "Training iteration 70 loss: 0.15102273225784302, ACC:0.96875\n",
      "Training iteration 71 loss: 0.18607372045516968, ACC:0.953125\n",
      "Training iteration 72 loss: 0.08252369612455368, ACC:0.96875\n",
      "Training iteration 73 loss: 0.06451624631881714, ACC:0.984375\n",
      "Training iteration 74 loss: 0.17012090981006622, ACC:0.9375\n",
      "Training iteration 75 loss: 0.20415841042995453, ACC:0.921875\n",
      "Training iteration 76 loss: 0.10501520335674286, ACC:0.96875\n",
      "Training iteration 77 loss: 0.1136319637298584, ACC:0.953125\n",
      "Training iteration 78 loss: 0.0684916228055954, ACC:0.984375\n",
      "Training iteration 79 loss: 0.10947316139936447, ACC:0.9375\n",
      "Training iteration 80 loss: 0.05901075527071953, ACC:1.0\n",
      "Training iteration 81 loss: 0.13011689484119415, ACC:0.9375\n",
      "Training iteration 82 loss: 0.18691664934158325, ACC:0.9375\n",
      "Training iteration 83 loss: 0.1883697807788849, ACC:0.921875\n",
      "Training iteration 84 loss: 0.11343704909086227, ACC:0.9375\n",
      "Training iteration 85 loss: 0.1932583898305893, ACC:0.90625\n",
      "Training iteration 86 loss: 0.21832136809825897, ACC:0.9375\n",
      "Training iteration 87 loss: 0.25048166513442993, ACC:0.921875\n",
      "Training iteration 88 loss: 0.11670271307229996, ACC:0.984375\n",
      "Training iteration 89 loss: 0.1338883936405182, ACC:0.953125\n",
      "Training iteration 90 loss: 0.21876142919063568, ACC:0.921875\n",
      "Training iteration 91 loss: 0.17993229627609253, ACC:0.9375\n",
      "Training iteration 92 loss: 0.11253867298364639, ACC:0.96875\n",
      "Training iteration 93 loss: 0.10170013457536697, ACC:0.96875\n",
      "Training iteration 94 loss: 0.10851840674877167, ACC:0.953125\n",
      "Training iteration 95 loss: 0.1879449039697647, ACC:0.921875\n",
      "Training iteration 96 loss: 0.08461829274892807, ACC:0.96875\n",
      "Training iteration 97 loss: 0.13812734186649323, ACC:0.96875\n",
      "Training iteration 98 loss: 0.07745403051376343, ACC:0.984375\n",
      "Training iteration 99 loss: 0.1300055980682373, ACC:0.96875\n",
      "Training iteration 100 loss: 0.12062583863735199, ACC:0.96875\n",
      "Training iteration 101 loss: 0.2118883728981018, ACC:0.890625\n",
      "Training iteration 102 loss: 0.15302139520645142, ACC:0.9375\n",
      "Training iteration 103 loss: 0.20021724700927734, ACC:0.90625\n",
      "Training iteration 104 loss: 0.1062329113483429, ACC:0.96875\n",
      "Training iteration 105 loss: 0.04838784039020538, ACC:1.0\n",
      "Training iteration 106 loss: 0.12884369492530823, ACC:0.953125\n",
      "Training iteration 107 loss: 0.11688457429409027, ACC:0.96875\n",
      "Training iteration 108 loss: 0.09313714504241943, ACC:0.953125\n",
      "Training iteration 109 loss: 0.05667625367641449, ACC:0.96875\n",
      "Training iteration 110 loss: 0.08836402744054794, ACC:0.953125\n",
      "Training iteration 111 loss: 0.24379847943782806, ACC:0.90625\n",
      "Training iteration 112 loss: 0.0584920234978199, ACC:0.984375\n",
      "Training iteration 113 loss: 0.15786480903625488, ACC:0.921875\n",
      "Training iteration 114 loss: 0.2716427743434906, ACC:0.9375\n",
      "Training iteration 115 loss: 0.14463216066360474, ACC:0.953125\n",
      "Training iteration 116 loss: 0.0943915843963623, ACC:0.984375\n",
      "Training iteration 117 loss: 0.09236849099397659, ACC:0.96875\n",
      "Training iteration 118 loss: 0.14887022972106934, ACC:0.953125\n",
      "Training iteration 119 loss: 0.144197016954422, ACC:0.953125\n",
      "Training iteration 120 loss: 0.28746309876441956, ACC:0.84375\n",
      "Training iteration 121 loss: 0.11369243264198303, ACC:0.96875\n",
      "Training iteration 122 loss: 0.13374081254005432, ACC:0.953125\n",
      "Training iteration 123 loss: 0.09311952441930771, ACC:0.96875\n",
      "Training iteration 124 loss: 0.12616485357284546, ACC:0.921875\n",
      "Training iteration 125 loss: 0.14845766127109528, ACC:0.953125\n",
      "Training iteration 126 loss: 0.17338009178638458, ACC:0.953125\n",
      "Training iteration 127 loss: 0.14000378549098969, ACC:0.9375\n",
      "Training iteration 128 loss: 0.12508909404277802, ACC:0.953125\n",
      "Training iteration 129 loss: 0.1589682400226593, ACC:0.9375\n",
      "Training iteration 130 loss: 0.08896850794553757, ACC:0.96875\n",
      "Training iteration 131 loss: 0.12225989997386932, ACC:0.9375\n",
      "Training iteration 132 loss: 0.056014787405729294, ACC:1.0\n",
      "Training iteration 133 loss: 0.10437512397766113, ACC:0.96875\n",
      "Training iteration 134 loss: 0.03613715246319771, ACC:1.0\n",
      "Training iteration 135 loss: 0.12175602465867996, ACC:0.96875\n",
      "Training iteration 136 loss: 0.11200869828462601, ACC:0.953125\n",
      "Training iteration 137 loss: 0.103263720870018, ACC:0.953125\n",
      "Training iteration 138 loss: 0.1068975180387497, ACC:0.96875\n",
      "Training iteration 139 loss: 0.15169884264469147, ACC:0.96875\n",
      "Training iteration 140 loss: 0.05758024379611015, ACC:0.984375\n",
      "Training iteration 141 loss: 0.11577969044446945, ACC:0.96875\n",
      "Training iteration 142 loss: 0.2024117410182953, ACC:0.9375\n",
      "Training iteration 143 loss: 0.08239424228668213, ACC:0.984375\n",
      "Training iteration 144 loss: 0.11357026547193527, ACC:0.96875\n",
      "Training iteration 145 loss: 0.19867180287837982, ACC:0.9375\n",
      "Training iteration 146 loss: 0.09408446401357651, ACC:0.96875\n",
      "Training iteration 147 loss: 0.08448442071676254, ACC:0.953125\n",
      "Training iteration 148 loss: 0.099354088306427, ACC:0.96875\n",
      "Training iteration 149 loss: 0.159971222281456, ACC:0.953125\n",
      "Training iteration 150 loss: 0.14927339553833008, ACC:0.9375\n",
      "Training iteration 151 loss: 0.1819644570350647, ACC:0.90625\n",
      "Training iteration 152 loss: 0.14096875488758087, ACC:0.96875\n",
      "Training iteration 153 loss: 0.07559867948293686, ACC:0.96875\n",
      "Training iteration 154 loss: 0.04681330546736717, ACC:0.984375\n",
      "Training iteration 155 loss: 0.10729586333036423, ACC:0.96875\n",
      "Training iteration 156 loss: 0.19628874957561493, ACC:0.9375\n",
      "Training iteration 157 loss: 0.10619711875915527, ACC:0.96875\n",
      "Training iteration 158 loss: 0.047342196106910706, ACC:1.0\n",
      "Training iteration 159 loss: 0.12025116384029388, ACC:0.953125\n",
      "Training iteration 160 loss: 0.13406018912792206, ACC:0.96875\n",
      "Training iteration 161 loss: 0.1349593997001648, ACC:0.96875\n",
      "Training iteration 162 loss: 0.18345214426517487, ACC:0.9375\n",
      "Training iteration 163 loss: 0.08503195643424988, ACC:0.96875\n",
      "Training iteration 164 loss: 0.12285371124744415, ACC:0.9375\n",
      "Training iteration 165 loss: 0.12100780755281448, ACC:0.953125\n",
      "Training iteration 166 loss: 0.14497295022010803, ACC:0.96875\n",
      "Training iteration 167 loss: 0.09980642795562744, ACC:0.96875\n",
      "Training iteration 168 loss: 0.11207246035337448, ACC:0.96875\n",
      "Training iteration 169 loss: 0.0786493793129921, ACC:0.984375\n",
      "Training iteration 170 loss: 0.16457663476467133, ACC:0.9375\n",
      "Training iteration 171 loss: 0.11597814410924911, ACC:0.96875\n",
      "Training iteration 172 loss: 0.09168139100074768, ACC:0.984375\n",
      "Training iteration 173 loss: 0.12055422365665436, ACC:0.953125\n",
      "Training iteration 174 loss: 0.14786016941070557, ACC:0.953125\n",
      "Training iteration 175 loss: 0.12260895222425461, ACC:0.96875\n",
      "Training iteration 176 loss: 0.13444599509239197, ACC:0.953125\n",
      "Training iteration 177 loss: 0.14192844927310944, ACC:0.953125\n",
      "Training iteration 178 loss: 0.07948402315378189, ACC:0.96875\n",
      "Training iteration 179 loss: 0.18122118711471558, ACC:0.9375\n",
      "Training iteration 180 loss: 0.21124108135700226, ACC:0.921875\n",
      "Training iteration 181 loss: 0.23663677275180817, ACC:0.890625\n",
      "Training iteration 182 loss: 0.2714169919490814, ACC:0.875\n",
      "Training iteration 183 loss: 0.0839938148856163, ACC:0.96875\n",
      "Training iteration 184 loss: 0.09801487624645233, ACC:0.953125\n",
      "Training iteration 185 loss: 0.0705396756529808, ACC:0.96875\n",
      "Training iteration 186 loss: 0.22825613617897034, ACC:0.890625\n",
      "Training iteration 187 loss: 0.23097731173038483, ACC:0.890625\n",
      "Training iteration 188 loss: 0.07234543561935425, ACC:0.984375\n",
      "Training iteration 189 loss: 0.21375057101249695, ACC:0.9375\n",
      "Training iteration 190 loss: 0.18353816866874695, ACC:0.921875\n",
      "Training iteration 191 loss: 0.11042022705078125, ACC:0.96875\n",
      "Training iteration 192 loss: 0.0736536905169487, ACC:0.984375\n",
      "Training iteration 193 loss: 0.05696319416165352, ACC:1.0\n",
      "Training iteration 194 loss: 0.09596429765224457, ACC:0.984375\n",
      "Training iteration 195 loss: 0.1469249725341797, ACC:0.953125\n",
      "Training iteration 196 loss: 0.18662017583847046, ACC:0.921875\n",
      "Training iteration 197 loss: 0.10200919210910797, ACC:0.96875\n",
      "Training iteration 198 loss: 0.14488640427589417, ACC:0.953125\n",
      "Training iteration 199 loss: 0.10946651548147202, ACC:0.96875\n",
      "Training iteration 200 loss: 0.10910513252019882, ACC:0.9375\n",
      "Training iteration 201 loss: 0.10959994047880173, ACC:0.96875\n",
      "Training iteration 202 loss: 0.1426183134317398, ACC:0.9375\n",
      "Training iteration 203 loss: 0.1352623850107193, ACC:0.953125\n",
      "Training iteration 204 loss: 0.0777958333492279, ACC:0.984375\n",
      "Training iteration 205 loss: 0.1255820244550705, ACC:0.953125\n",
      "Training iteration 206 loss: 0.07738431543111801, ACC:0.96875\n",
      "Training iteration 207 loss: 0.18074564635753632, ACC:0.921875\n",
      "Training iteration 208 loss: 0.14624884724617004, ACC:0.96875\n",
      "Training iteration 209 loss: 0.1070074662566185, ACC:0.953125\n",
      "Training iteration 210 loss: 0.14345091581344604, ACC:0.953125\n",
      "Training iteration 211 loss: 0.0719265565276146, ACC:0.984375\n",
      "Training iteration 212 loss: 0.15930208563804626, ACC:0.953125\n",
      "Training iteration 213 loss: 0.1681107133626938, ACC:0.953125\n",
      "Training iteration 214 loss: 0.09449177235364914, ACC:0.96875\n",
      "Training iteration 215 loss: 0.14134657382965088, ACC:0.953125\n",
      "Training iteration 216 loss: 0.1458718478679657, ACC:0.953125\n",
      "Training iteration 217 loss: 0.12161925435066223, ACC:0.953125\n",
      "Training iteration 218 loss: 0.05697018653154373, ACC:0.984375\n",
      "Training iteration 219 loss: 0.16794107854366302, ACC:0.921875\n",
      "Training iteration 220 loss: 0.20537090301513672, ACC:0.9375\n",
      "Training iteration 221 loss: 0.04408831149339676, ACC:1.0\n",
      "Training iteration 222 loss: 0.15806609392166138, ACC:0.9375\n",
      "Training iteration 223 loss: 0.10168266296386719, ACC:0.96875\n",
      "Training iteration 224 loss: 0.07915559411048889, ACC:0.984375\n",
      "Training iteration 225 loss: 0.1668950468301773, ACC:0.90625\n",
      "Training iteration 226 loss: 0.08321789652109146, ACC:0.953125\n",
      "Training iteration 227 loss: 0.1422022134065628, ACC:0.96875\n",
      "Training iteration 228 loss: 0.20807123184204102, ACC:0.921875\n",
      "Training iteration 229 loss: 0.20040740072727203, ACC:0.96875\n",
      "Training iteration 230 loss: 0.1610855609178543, ACC:0.9375\n",
      "Training iteration 231 loss: 0.14534279704093933, ACC:0.953125\n",
      "Training iteration 232 loss: 0.10608842968940735, ACC:0.953125\n",
      "Training iteration 233 loss: 0.06282095611095428, ACC:0.984375\n",
      "Training iteration 234 loss: 0.06513085961341858, ACC:0.984375\n",
      "Training iteration 235 loss: 0.21197956800460815, ACC:0.90625\n",
      "Training iteration 236 loss: 0.1071859672665596, ACC:0.953125\n",
      "Training iteration 237 loss: 0.11459460109472275, ACC:0.984375\n",
      "Training iteration 238 loss: 0.1785132735967636, ACC:0.953125\n",
      "Training iteration 239 loss: 0.04721491038799286, ACC:1.0\n",
      "Training iteration 240 loss: 0.15832018852233887, ACC:0.9375\n",
      "Training iteration 241 loss: 0.025549639016389847, ACC:1.0\n",
      "Training iteration 242 loss: 0.10229826718568802, ACC:0.96875\n",
      "Training iteration 243 loss: 0.21429963409900665, ACC:0.90625\n",
      "Training iteration 244 loss: 0.04243939369916916, ACC:1.0\n",
      "Training iteration 245 loss: 0.11875632405281067, ACC:0.953125\n",
      "Training iteration 246 loss: 0.15423478186130524, ACC:0.96875\n",
      "Training iteration 247 loss: 0.14664901793003082, ACC:0.953125\n",
      "Training iteration 248 loss: 0.08994199335575104, ACC:0.96875\n",
      "Training iteration 249 loss: 0.05506545677781105, ACC:0.984375\n",
      "Training iteration 250 loss: 0.06775762140750885, ACC:0.984375\n",
      "Training iteration 251 loss: 0.14267653226852417, ACC:0.96875\n",
      "Training iteration 252 loss: 0.13999120891094208, ACC:0.953125\n",
      "Training iteration 253 loss: 0.07466085255146027, ACC:0.953125\n",
      "Training iteration 254 loss: 0.16191117465496063, ACC:0.9375\n",
      "Training iteration 255 loss: 0.15310314297676086, ACC:0.9375\n",
      "Training iteration 256 loss: 0.13209232687950134, ACC:0.953125\n",
      "Training iteration 257 loss: 0.14253126084804535, ACC:0.96875\n",
      "Training iteration 258 loss: 0.07994473725557327, ACC:0.96875\n",
      "Training iteration 259 loss: 0.20852769911289215, ACC:0.9375\n",
      "Training iteration 260 loss: 0.08955056965351105, ACC:0.953125\n",
      "Training iteration 261 loss: 0.08668871223926544, ACC:0.96875\n",
      "Training iteration 262 loss: 0.11670169979333878, ACC:0.953125\n",
      "Training iteration 263 loss: 0.12327848374843597, ACC:0.96875\n",
      "Training iteration 264 loss: 0.06167874485254288, ACC:0.96875\n",
      "Training iteration 265 loss: 0.1264592707157135, ACC:0.96875\n",
      "Training iteration 266 loss: 0.06283621490001678, ACC:0.984375\n",
      "Training iteration 267 loss: 0.07217183709144592, ACC:0.984375\n",
      "Training iteration 268 loss: 0.06907859444618225, ACC:0.984375\n",
      "Training iteration 269 loss: 0.16261011362075806, ACC:0.953125\n",
      "Training iteration 270 loss: 0.02027479000389576, ACC:1.0\n",
      "Training iteration 271 loss: 0.18222323060035706, ACC:0.953125\n",
      "Training iteration 272 loss: 0.07880029827356339, ACC:0.984375\n",
      "Training iteration 273 loss: 0.2323555052280426, ACC:0.9375\n",
      "Training iteration 274 loss: 0.080424964427948, ACC:0.984375\n",
      "Training iteration 275 loss: 0.12939319014549255, ACC:0.953125\n",
      "Training iteration 276 loss: 0.19372838735580444, ACC:0.9375\n",
      "Training iteration 277 loss: 0.0996524766087532, ACC:0.953125\n",
      "Training iteration 278 loss: 0.09905840456485748, ACC:0.96875\n",
      "Training iteration 279 loss: 0.09808103740215302, ACC:0.96875\n",
      "Training iteration 280 loss: 0.06169476732611656, ACC:0.984375\n",
      "Training iteration 281 loss: 0.0790877565741539, ACC:0.984375\n",
      "Training iteration 282 loss: 0.08832783252000809, ACC:0.953125\n",
      "Training iteration 283 loss: 0.15248095989227295, ACC:0.9375\n",
      "Training iteration 284 loss: 0.1435362547636032, ACC:0.9375\n",
      "Training iteration 285 loss: 0.030317986384034157, ACC:1.0\n",
      "Training iteration 286 loss: 0.06123051419854164, ACC:0.96875\n",
      "Training iteration 287 loss: 0.06322456896305084, ACC:0.96875\n",
      "Training iteration 288 loss: 0.11100097745656967, ACC:0.96875\n",
      "Training iteration 289 loss: 0.10068626701831818, ACC:0.984375\n",
      "Training iteration 290 loss: 0.10559311509132385, ACC:0.953125\n",
      "Training iteration 291 loss: 0.12310955673456192, ACC:0.953125\n",
      "Training iteration 292 loss: 0.12797911465168, ACC:0.96875\n",
      "Training iteration 293 loss: 0.0552641823887825, ACC:0.984375\n",
      "Training iteration 294 loss: 0.06653092056512833, ACC:0.984375\n",
      "Training iteration 295 loss: 0.021889183670282364, ACC:1.0\n",
      "Training iteration 296 loss: 0.13199324905872345, ACC:0.953125\n",
      "Training iteration 297 loss: 0.08911621570587158, ACC:0.953125\n",
      "Training iteration 298 loss: 0.08358334004878998, ACC:0.953125\n",
      "Training iteration 299 loss: 0.12179385870695114, ACC:0.96875\n",
      "Training iteration 300 loss: 0.10398195683956146, ACC:0.96875\n",
      "Training iteration 301 loss: 0.1447094827890396, ACC:0.953125\n",
      "Training iteration 302 loss: 0.09264446794986725, ACC:0.984375\n",
      "Training iteration 303 loss: 0.12450277805328369, ACC:0.9375\n",
      "Training iteration 304 loss: 0.1000421792268753, ACC:0.953125\n",
      "Training iteration 305 loss: 0.06944995373487473, ACC:0.984375\n",
      "Training iteration 306 loss: 0.18108558654785156, ACC:0.921875\n",
      "Training iteration 307 loss: 0.09462164342403412, ACC:0.96875\n",
      "Training iteration 308 loss: 0.1257692128419876, ACC:0.90625\n",
      "Training iteration 309 loss: 0.03414947912096977, ACC:1.0\n",
      "Training iteration 310 loss: 0.020338207483291626, ACC:1.0\n",
      "Training iteration 311 loss: 0.07294859737157822, ACC:0.96875\n",
      "Training iteration 312 loss: 0.041023485362529755, ACC:0.984375\n",
      "Training iteration 313 loss: 0.16127872467041016, ACC:0.9375\n",
      "Training iteration 314 loss: 0.1044704020023346, ACC:0.9375\n",
      "Training iteration 315 loss: 0.13110671937465668, ACC:0.9375\n",
      "Training iteration 316 loss: 0.04907401278614998, ACC:0.984375\n",
      "Training iteration 317 loss: 0.09948424249887466, ACC:0.96875\n",
      "Training iteration 318 loss: 0.11300057172775269, ACC:0.9375\n",
      "Training iteration 319 loss: 0.18381015956401825, ACC:0.953125\n",
      "Training iteration 320 loss: 0.06875529140233994, ACC:0.96875\n",
      "Training iteration 321 loss: 0.13946905732154846, ACC:0.953125\n",
      "Training iteration 322 loss: 0.11903297156095505, ACC:0.953125\n",
      "Training iteration 323 loss: 0.11449293047189713, ACC:0.953125\n",
      "Training iteration 324 loss: 0.039748869836330414, ACC:1.0\n",
      "Training iteration 325 loss: 0.10972543060779572, ACC:0.953125\n",
      "Training iteration 326 loss: 0.1076328232884407, ACC:0.953125\n",
      "Training iteration 327 loss: 0.08175139129161835, ACC:0.953125\n",
      "Training iteration 328 loss: 0.05960657820105553, ACC:0.984375\n",
      "Training iteration 329 loss: 0.08922775089740753, ACC:0.953125\n",
      "Training iteration 330 loss: 0.1130077913403511, ACC:0.96875\n",
      "Training iteration 331 loss: 0.08511215448379517, ACC:0.953125\n",
      "Training iteration 332 loss: 0.07024676352739334, ACC:0.96875\n",
      "Training iteration 333 loss: 0.12648849189281464, ACC:0.96875\n",
      "Training iteration 334 loss: 0.03392433747649193, ACC:1.0\n",
      "Training iteration 335 loss: 0.024524353444576263, ACC:1.0\n",
      "Training iteration 336 loss: 0.08779454976320267, ACC:0.96875\n",
      "Training iteration 337 loss: 0.06259103864431381, ACC:0.984375\n",
      "Training iteration 338 loss: 0.20175616443157196, ACC:0.953125\n",
      "Training iteration 339 loss: 0.1852688044309616, ACC:0.953125\n",
      "Training iteration 340 loss: 0.04642457887530327, ACC:0.984375\n",
      "Training iteration 341 loss: 0.03632964938879013, ACC:0.984375\n",
      "Training iteration 342 loss: 0.01766977831721306, ACC:1.0\n",
      "Training iteration 343 loss: 0.06812410056591034, ACC:0.96875\n",
      "Training iteration 344 loss: 0.03402526676654816, ACC:0.96875\n",
      "Training iteration 345 loss: 0.13892610371112823, ACC:0.921875\n",
      "Training iteration 346 loss: 0.14438360929489136, ACC:0.96875\n",
      "Training iteration 347 loss: 0.009392060339450836, ACC:1.0\n",
      "Training iteration 348 loss: 0.1149691492319107, ACC:0.953125\n",
      "Training iteration 349 loss: 0.04158288612961769, ACC:0.984375\n",
      "Training iteration 350 loss: 0.062098000198602676, ACC:0.984375\n",
      "Training iteration 351 loss: 0.03257153555750847, ACC:1.0\n",
      "Training iteration 352 loss: 0.21831823885440826, ACC:0.921875\n",
      "Training iteration 353 loss: 0.12019126862287521, ACC:0.953125\n",
      "Training iteration 354 loss: 0.022101962938904762, ACC:1.0\n",
      "Training iteration 355 loss: 0.08316739648580551, ACC:0.984375\n",
      "Training iteration 356 loss: 0.12359406054019928, ACC:0.953125\n",
      "Training iteration 357 loss: 0.08008590340614319, ACC:0.953125\n",
      "Training iteration 358 loss: 0.06576833873987198, ACC:0.96875\n",
      "Training iteration 359 loss: 0.047971904277801514, ACC:0.984375\n",
      "Training iteration 360 loss: 0.0362992063164711, ACC:0.984375\n",
      "Training iteration 361 loss: 0.058098386973142624, ACC:0.96875\n",
      "Training iteration 362 loss: 0.06260830163955688, ACC:0.984375\n",
      "Training iteration 363 loss: 0.14208681881427765, ACC:0.953125\n",
      "Training iteration 364 loss: 0.15635325014591217, ACC:0.953125\n",
      "Training iteration 365 loss: 0.03797028213739395, ACC:0.984375\n",
      "Training iteration 366 loss: 0.2131473869085312, ACC:0.921875\n",
      "Training iteration 367 loss: 0.1443551778793335, ACC:0.953125\n",
      "Training iteration 368 loss: 0.14656049013137817, ACC:0.953125\n",
      "Training iteration 369 loss: 0.12981584668159485, ACC:0.953125\n",
      "Training iteration 370 loss: 0.18144191801548004, ACC:0.921875\n",
      "Training iteration 371 loss: 0.16629037261009216, ACC:0.9375\n",
      "Training iteration 372 loss: 0.11351989954710007, ACC:0.96875\n",
      "Training iteration 373 loss: 0.16978980600833893, ACC:0.953125\n",
      "Training iteration 374 loss: 0.09406296163797379, ACC:0.984375\n",
      "Training iteration 375 loss: 0.044529929757118225, ACC:0.96875\n",
      "Training iteration 376 loss: 0.15329338610172272, ACC:0.921875\n",
      "Training iteration 377 loss: 0.03257876634597778, ACC:1.0\n",
      "Training iteration 378 loss: 0.07501519471406937, ACC:0.96875\n",
      "Training iteration 379 loss: 0.12068220227956772, ACC:0.96875\n",
      "Training iteration 380 loss: 0.18747057020664215, ACC:0.9375\n",
      "Training iteration 381 loss: 0.07164224237203598, ACC:0.984375\n",
      "Training iteration 382 loss: 0.08448441326618195, ACC:0.96875\n",
      "Training iteration 383 loss: 0.19210025668144226, ACC:0.9375\n",
      "Training iteration 384 loss: 0.07965581119060516, ACC:0.96875\n",
      "Training iteration 385 loss: 0.12659239768981934, ACC:0.921875\n",
      "Training iteration 386 loss: 0.11854534596204758, ACC:0.9375\n",
      "Training iteration 387 loss: 0.06575164943933487, ACC:0.984375\n",
      "Training iteration 388 loss: 0.12922967970371246, ACC:0.96875\n",
      "Training iteration 389 loss: 0.1490814983844757, ACC:0.96875\n",
      "Training iteration 390 loss: 0.08170150220394135, ACC:0.984375\n",
      "Training iteration 391 loss: 0.05223476141691208, ACC:1.0\n",
      "Training iteration 392 loss: 0.04198630154132843, ACC:1.0\n",
      "Training iteration 393 loss: 0.10217690467834473, ACC:0.96875\n",
      "Training iteration 394 loss: 0.17294657230377197, ACC:0.9375\n",
      "Training iteration 395 loss: 0.02619263529777527, ACC:1.0\n",
      "Training iteration 396 loss: 0.04838711395859718, ACC:1.0\n",
      "Training iteration 397 loss: 0.1178903803229332, ACC:0.96875\n",
      "Training iteration 398 loss: 0.09913932532072067, ACC:0.984375\n",
      "Training iteration 399 loss: 0.06181931868195534, ACC:0.984375\n",
      "Training iteration 400 loss: 0.05166032165288925, ACC:0.984375\n",
      "Training iteration 401 loss: 0.042919836938381195, ACC:0.984375\n",
      "Training iteration 402 loss: 0.0331045500934124, ACC:0.984375\n",
      "Training iteration 403 loss: 0.02768825925886631, ACC:1.0\n",
      "Training iteration 404 loss: 0.1501832753419876, ACC:0.9375\n",
      "Training iteration 405 loss: 0.04552005976438522, ACC:0.96875\n",
      "Training iteration 406 loss: 0.08038199692964554, ACC:0.96875\n",
      "Training iteration 407 loss: 0.16322162747383118, ACC:0.9375\n",
      "Training iteration 408 loss: 0.0222762543708086, ACC:1.0\n",
      "Training iteration 409 loss: 0.07601719349622726, ACC:0.96875\n",
      "Training iteration 410 loss: 0.04434383660554886, ACC:0.984375\n",
      "Training iteration 411 loss: 0.1563112437725067, ACC:0.9375\n",
      "Training iteration 412 loss: 0.07759793847799301, ACC:0.96875\n",
      "Training iteration 413 loss: 0.04942391812801361, ACC:0.984375\n",
      "Training iteration 414 loss: 0.1221168264746666, ACC:0.921875\n",
      "Training iteration 415 loss: 0.11328057199716568, ACC:0.953125\n",
      "Training iteration 416 loss: 0.0197407528758049, ACC:1.0\n",
      "Training iteration 417 loss: 0.12779058516025543, ACC:0.9375\n",
      "Training iteration 418 loss: 0.12651871144771576, ACC:0.9375\n",
      "Training iteration 419 loss: 0.06989160925149918, ACC:0.984375\n",
      "Training iteration 420 loss: 0.10738737881183624, ACC:0.96875\n",
      "Training iteration 421 loss: 0.10670774430036545, ACC:0.953125\n",
      "Training iteration 422 loss: 0.18352536857128143, ACC:0.9375\n",
      "Training iteration 423 loss: 0.17230172455310822, ACC:0.9375\n",
      "Training iteration 424 loss: 0.11101634055376053, ACC:0.953125\n",
      "Training iteration 425 loss: 0.08457449823617935, ACC:0.96875\n",
      "Training iteration 426 loss: 0.06720306724309921, ACC:0.984375\n",
      "Training iteration 427 loss: 0.04607555642724037, ACC:0.984375\n",
      "Training iteration 428 loss: 0.04738520830869675, ACC:0.984375\n",
      "Training iteration 429 loss: 0.056047070771455765, ACC:0.984375\n",
      "Training iteration 430 loss: 0.10124030709266663, ACC:0.96875\n",
      "Training iteration 431 loss: 0.10147657245397568, ACC:0.953125\n",
      "Training iteration 432 loss: 0.12444110214710236, ACC:0.953125\n",
      "Training iteration 433 loss: 0.07877448201179504, ACC:0.96875\n",
      "Training iteration 434 loss: 0.04923674836754799, ACC:0.984375\n",
      "Training iteration 435 loss: 0.049668386578559875, ACC:1.0\n",
      "Training iteration 436 loss: 0.1552766114473343, ACC:0.953125\n",
      "Training iteration 437 loss: 0.09939560294151306, ACC:0.953125\n",
      "Training iteration 438 loss: 0.06227777153253555, ACC:0.96875\n",
      "Training iteration 439 loss: 0.12410593777894974, ACC:0.9375\n",
      "Training iteration 440 loss: 0.15162411332130432, ACC:0.953125\n",
      "Training iteration 441 loss: 0.09744954854249954, ACC:0.953125\n",
      "Training iteration 442 loss: 0.058871492743492126, ACC:0.96875\n",
      "Training iteration 443 loss: 0.1444847136735916, ACC:0.953125\n",
      "Training iteration 444 loss: 0.09810266643762589, ACC:0.953125\n",
      "Training iteration 445 loss: 0.12954303622245789, ACC:0.9375\n",
      "Training iteration 446 loss: 0.12337207049131393, ACC:0.9375\n",
      "Training iteration 447 loss: 0.08361976593732834, ACC:0.96875\n",
      "Training iteration 448 loss: 0.07563817501068115, ACC:0.984375\n",
      "Training iteration 449 loss: 0.06339418143033981, ACC:0.984375\n",
      "Training iteration 450 loss: 0.1550765335559845, ACC:0.953125\n",
      "Validation iteration 451 loss: 0.10120488703250885, ACC: 0.953125\n",
      "Validation iteration 452 loss: 0.12584185600280762, ACC: 0.953125\n",
      "Validation iteration 453 loss: 0.18850012123584747, ACC: 0.9375\n",
      "Validation iteration 454 loss: 0.12244437634944916, ACC: 0.96875\n",
      "Validation iteration 455 loss: 0.1033596619963646, ACC: 0.96875\n",
      "Validation iteration 456 loss: 0.04145932197570801, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.11482500284910202, ACC: 0.96875\n",
      "Validation iteration 458 loss: 0.09449019283056259, ACC: 0.96875\n",
      "Validation iteration 459 loss: 0.247774139046669, ACC: 0.90625\n",
      "Validation iteration 460 loss: 0.17301921546459198, ACC: 0.953125\n",
      "Validation iteration 461 loss: 0.12062692642211914, ACC: 0.96875\n",
      "Validation iteration 462 loss: 0.06823866069316864, ACC: 0.96875\n",
      "Validation iteration 463 loss: 0.1668427586555481, ACC: 0.9375\n",
      "Validation iteration 464 loss: 0.07592543214559555, ACC: 0.96875\n",
      "Validation iteration 465 loss: 0.0744650736451149, ACC: 0.96875\n",
      "Validation iteration 466 loss: 0.1370478719472885, ACC: 0.96875\n",
      "Validation iteration 467 loss: 0.12167835235595703, ACC: 0.9375\n",
      "Validation iteration 468 loss: 0.16123038530349731, ACC: 0.9375\n",
      "Validation iteration 469 loss: 0.05097585916519165, ACC: 0.984375\n",
      "Validation iteration 470 loss: 0.08685536682605743, ACC: 0.953125\n",
      "Validation iteration 471 loss: 0.029921820387244225, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.029122060164809227, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.13364416360855103, ACC: 0.953125\n",
      "Validation iteration 474 loss: 0.12859956920146942, ACC: 0.9375\n",
      "Validation iteration 475 loss: 0.07862843573093414, ACC: 0.96875\n",
      "Validation iteration 476 loss: 0.030981922522187233, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.14400948584079742, ACC: 0.953125\n",
      "Validation iteration 478 loss: 0.08380945026874542, ACC: 0.96875\n",
      "Validation iteration 479 loss: 0.2371157556772232, ACC: 0.921875\n",
      "Validation iteration 480 loss: 0.13022391498088837, ACC: 0.953125\n",
      "Validation iteration 481 loss: 0.08102916926145554, ACC: 0.984375\n",
      "Validation iteration 482 loss: 0.1036149114370346, ACC: 0.96875\n",
      "Validation iteration 483 loss: 0.08869460225105286, ACC: 0.96875\n",
      "Validation iteration 484 loss: 0.14163383841514587, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.2391611784696579, ACC: 0.9375\n",
      "Validation iteration 486 loss: 0.09732648730278015, ACC: 0.96875\n",
      "Validation iteration 487 loss: 0.11743921041488647, ACC: 0.953125\n",
      "Validation iteration 488 loss: 0.04459090903401375, ACC: 0.984375\n",
      "Validation iteration 489 loss: 0.06777463108301163, ACC: 0.984375\n",
      "Validation iteration 490 loss: 0.054282382130622864, ACC: 0.984375\n",
      "Validation iteration 491 loss: 0.05655067041516304, ACC: 0.984375\n",
      "Validation iteration 492 loss: 0.07410784810781479, ACC: 0.984375\n",
      "Validation iteration 493 loss: 0.032291166484355927, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.060699257999658585, ACC: 0.984375\n",
      "Validation iteration 495 loss: 0.08374020457267761, ACC: 0.96875\n",
      "Validation iteration 496 loss: 0.07375364005565643, ACC: 0.984375\n",
      "Validation iteration 497 loss: 0.1376911699771881, ACC: 0.96875\n",
      "Validation iteration 498 loss: 0.2577822208404541, ACC: 0.890625\n",
      "Validation iteration 499 loss: 0.09757018089294434, ACC: 0.96875\n",
      "Validation iteration 500 loss: 0.12126094847917557, ACC: 0.953125\n",
      "-- Epoch 3 done -- Train loss: 0.11603556702120436, train ACC: 0.9597569444444445, val loss: 0.108677133359015, val ACC: 0.9646875\n",
      "<--- 751.390222787857 seconds --->\n",
      "Training iteration 1 loss: 0.06640269607305527, ACC:0.984375\n",
      "Training iteration 2 loss: 0.0965479239821434, ACC:0.96875\n",
      "Training iteration 3 loss: 0.09825904667377472, ACC:0.984375\n",
      "Training iteration 4 loss: 0.028430309146642685, ACC:1.0\n",
      "Training iteration 5 loss: 0.20311038196086884, ACC:0.90625\n",
      "Training iteration 6 loss: 0.13043168187141418, ACC:0.953125\n",
      "Training iteration 7 loss: 0.13452470302581787, ACC:0.96875\n",
      "Training iteration 8 loss: 0.06993661820888519, ACC:0.984375\n",
      "Training iteration 9 loss: 0.07437126338481903, ACC:0.96875\n",
      "Training iteration 10 loss: 0.04985572025179863, ACC:0.984375\n",
      "Training iteration 11 loss: 0.08129420131444931, ACC:0.96875\n",
      "Training iteration 12 loss: 0.03067670948803425, ACC:1.0\n",
      "Training iteration 13 loss: 0.0400908887386322, ACC:1.0\n",
      "Training iteration 14 loss: 0.1445389986038208, ACC:0.96875\n",
      "Training iteration 15 loss: 0.11832473427057266, ACC:0.9375\n",
      "Training iteration 16 loss: 0.1062878742814064, ACC:0.953125\n",
      "Training iteration 17 loss: 0.061558011919260025, ACC:0.96875\n",
      "Training iteration 18 loss: 0.17997118830680847, ACC:0.9375\n",
      "Training iteration 19 loss: 0.04724578186869621, ACC:1.0\n",
      "Training iteration 20 loss: 0.057890620082616806, ACC:1.0\n",
      "Training iteration 21 loss: 0.061433836817741394, ACC:0.984375\n",
      "Training iteration 22 loss: 0.1413918286561966, ACC:0.953125\n",
      "Training iteration 23 loss: 0.026953577995300293, ACC:1.0\n",
      "Training iteration 24 loss: 0.05486254021525383, ACC:0.984375\n",
      "Training iteration 25 loss: 0.030683746561408043, ACC:1.0\n",
      "Training iteration 26 loss: 0.07187914848327637, ACC:0.96875\n",
      "Training iteration 27 loss: 0.09838224202394485, ACC:0.953125\n",
      "Training iteration 28 loss: 0.03748596832156181, ACC:1.0\n",
      "Training iteration 29 loss: 0.10482446849346161, ACC:0.96875\n",
      "Training iteration 30 loss: 0.051089320331811905, ACC:1.0\n",
      "Training iteration 31 loss: 0.02303137071430683, ACC:1.0\n",
      "Training iteration 32 loss: 0.019255556166172028, ACC:1.0\n",
      "Training iteration 33 loss: 0.15333130955696106, ACC:0.96875\n",
      "Training iteration 34 loss: 0.14671817421913147, ACC:0.953125\n",
      "Training iteration 35 loss: 0.08452728390693665, ACC:0.96875\n",
      "Training iteration 36 loss: 0.09468168765306473, ACC:0.96875\n",
      "Training iteration 37 loss: 0.2006685733795166, ACC:0.953125\n",
      "Training iteration 38 loss: 0.19062940776348114, ACC:0.90625\n",
      "Training iteration 39 loss: 0.06233866140246391, ACC:0.96875\n",
      "Training iteration 40 loss: 0.015759212896227837, ACC:1.0\n",
      "Training iteration 41 loss: 0.0674312487244606, ACC:0.96875\n",
      "Training iteration 42 loss: 0.1230357438325882, ACC:0.96875\n",
      "Training iteration 43 loss: 0.1546718031167984, ACC:0.9375\n",
      "Training iteration 44 loss: 0.18389947712421417, ACC:0.9375\n",
      "Training iteration 45 loss: 0.14600372314453125, ACC:0.9375\n",
      "Training iteration 46 loss: 0.20271040499210358, ACC:0.90625\n",
      "Training iteration 47 loss: 0.04832207038998604, ACC:0.984375\n",
      "Training iteration 48 loss: 0.04970422387123108, ACC:1.0\n",
      "Training iteration 49 loss: 0.022659556940197945, ACC:1.0\n",
      "Training iteration 50 loss: 0.10276150703430176, ACC:0.96875\n",
      "Training iteration 51 loss: 0.074189692735672, ACC:0.96875\n",
      "Training iteration 52 loss: 0.10599005967378616, ACC:0.9375\n",
      "Training iteration 53 loss: 0.12857763469219208, ACC:0.9375\n",
      "Training iteration 54 loss: 0.09781449288129807, ACC:0.953125\n",
      "Training iteration 55 loss: 0.05801468342542648, ACC:0.96875\n",
      "Training iteration 56 loss: 0.19152027368545532, ACC:0.921875\n",
      "Training iteration 57 loss: 0.1392231434583664, ACC:0.953125\n",
      "Training iteration 58 loss: 0.056226056069135666, ACC:1.0\n",
      "Training iteration 59 loss: 0.12415800243616104, ACC:0.96875\n",
      "Training iteration 60 loss: 0.11427279561758041, ACC:0.921875\n",
      "Training iteration 61 loss: 0.05031514912843704, ACC:1.0\n",
      "Training iteration 62 loss: 0.12869839370250702, ACC:0.96875\n",
      "Training iteration 63 loss: 0.11866315454244614, ACC:0.953125\n",
      "Training iteration 64 loss: 0.06835635006427765, ACC:0.96875\n",
      "Training iteration 65 loss: 0.061576105654239655, ACC:0.984375\n",
      "Training iteration 66 loss: 0.06672678887844086, ACC:0.984375\n",
      "Training iteration 67 loss: 0.05452898517251015, ACC:1.0\n",
      "Training iteration 68 loss: 0.09727127104997635, ACC:0.984375\n",
      "Training iteration 69 loss: 0.028550000861287117, ACC:1.0\n",
      "Training iteration 70 loss: 0.02123981900513172, ACC:1.0\n",
      "Training iteration 71 loss: 0.3319334089756012, ACC:0.875\n",
      "Training iteration 72 loss: 0.11363155394792557, ACC:0.96875\n",
      "Training iteration 73 loss: 0.05968266725540161, ACC:0.96875\n",
      "Training iteration 74 loss: 0.04748285189270973, ACC:0.984375\n",
      "Training iteration 75 loss: 0.04114260524511337, ACC:0.984375\n",
      "Training iteration 76 loss: 0.1486411988735199, ACC:0.953125\n",
      "Training iteration 77 loss: 0.21022069454193115, ACC:0.90625\n",
      "Training iteration 78 loss: 0.08047618716955185, ACC:0.96875\n",
      "Training iteration 79 loss: 0.16280217468738556, ACC:0.9375\n",
      "Training iteration 80 loss: 0.10993293672800064, ACC:0.953125\n",
      "Training iteration 81 loss: 0.09848559647798538, ACC:0.96875\n",
      "Training iteration 82 loss: 0.0909103974699974, ACC:0.96875\n",
      "Training iteration 83 loss: 0.052019961178302765, ACC:1.0\n",
      "Training iteration 84 loss: 0.10239408165216446, ACC:0.953125\n",
      "Training iteration 85 loss: 0.1608881652355194, ACC:0.9375\n",
      "Training iteration 86 loss: 0.06924083828926086, ACC:0.96875\n",
      "Training iteration 87 loss: 0.07653555274009705, ACC:0.984375\n",
      "Training iteration 88 loss: 0.12464164197444916, ACC:0.9375\n",
      "Training iteration 89 loss: 0.04888230189681053, ACC:1.0\n",
      "Training iteration 90 loss: 0.06532683223485947, ACC:0.984375\n",
      "Training iteration 91 loss: 0.03388838842511177, ACC:0.984375\n",
      "Training iteration 92 loss: 0.07347129285335541, ACC:0.984375\n",
      "Training iteration 93 loss: 0.115513376891613, ACC:0.96875\n",
      "Training iteration 94 loss: 0.08367538452148438, ACC:0.96875\n",
      "Training iteration 95 loss: 0.2001125067472458, ACC:0.921875\n",
      "Training iteration 96 loss: 0.06637155264616013, ACC:0.984375\n",
      "Training iteration 97 loss: 0.14845523238182068, ACC:0.953125\n",
      "Training iteration 98 loss: 0.10742063075304031, ACC:0.96875\n",
      "Training iteration 99 loss: 0.16716985404491425, ACC:0.9375\n",
      "Training iteration 100 loss: 0.05424611270427704, ACC:0.984375\n",
      "Training iteration 101 loss: 0.09464386850595474, ACC:0.984375\n",
      "Training iteration 102 loss: 0.10772033780813217, ACC:0.9375\n",
      "Training iteration 103 loss: 0.08160045742988586, ACC:0.953125\n",
      "Training iteration 104 loss: 0.11068161576986313, ACC:0.953125\n",
      "Training iteration 105 loss: 0.15715356171131134, ACC:0.953125\n",
      "Training iteration 106 loss: 0.16616037487983704, ACC:0.953125\n",
      "Training iteration 107 loss: 0.017927611246705055, ACC:1.0\n",
      "Training iteration 108 loss: 0.10973411798477173, ACC:0.953125\n",
      "Training iteration 109 loss: 0.12118399143218994, ACC:0.96875\n",
      "Training iteration 110 loss: 0.09626724570989609, ACC:0.96875\n",
      "Training iteration 111 loss: 0.09158175438642502, ACC:0.984375\n",
      "Training iteration 112 loss: 0.06549760699272156, ACC:0.984375\n",
      "Training iteration 113 loss: 0.0517704114317894, ACC:0.984375\n",
      "Training iteration 114 loss: 0.09058783948421478, ACC:0.953125\n",
      "Training iteration 115 loss: 0.026575831696391106, ACC:1.0\n",
      "Training iteration 116 loss: 0.14898405969142914, ACC:0.953125\n",
      "Training iteration 117 loss: 0.037355247884988785, ACC:0.984375\n",
      "Training iteration 118 loss: 0.08973575383424759, ACC:0.984375\n",
      "Training iteration 119 loss: 0.19186365604400635, ACC:0.921875\n",
      "Training iteration 120 loss: 0.04917158931493759, ACC:0.96875\n",
      "Training iteration 121 loss: 0.06426355987787247, ACC:0.96875\n",
      "Training iteration 122 loss: 0.15722902119159698, ACC:0.921875\n",
      "Training iteration 123 loss: 0.1418013721704483, ACC:0.9375\n",
      "Training iteration 124 loss: 0.13030454516410828, ACC:0.953125\n",
      "Training iteration 125 loss: 0.08050090819597244, ACC:0.96875\n",
      "Training iteration 126 loss: 0.12018702924251556, ACC:0.96875\n",
      "Training iteration 127 loss: 0.16852450370788574, ACC:0.953125\n",
      "Training iteration 128 loss: 0.07182576507329941, ACC:0.96875\n",
      "Training iteration 129 loss: 0.09352394193410873, ACC:0.96875\n",
      "Training iteration 130 loss: 0.10490762442350388, ACC:0.953125\n",
      "Training iteration 131 loss: 0.15623781085014343, ACC:0.90625\n",
      "Training iteration 132 loss: 0.04184515029191971, ACC:1.0\n",
      "Training iteration 133 loss: 0.16294053196907043, ACC:0.953125\n",
      "Training iteration 134 loss: 0.14396807551383972, ACC:0.9375\n",
      "Training iteration 135 loss: 0.12850388884544373, ACC:0.953125\n",
      "Training iteration 136 loss: 0.13362257182598114, ACC:0.9375\n",
      "Training iteration 137 loss: 0.04836332052946091, ACC:1.0\n",
      "Training iteration 138 loss: 0.036583926528692245, ACC:1.0\n",
      "Training iteration 139 loss: 0.11337701976299286, ACC:0.96875\n",
      "Training iteration 140 loss: 0.1395837962627411, ACC:0.953125\n",
      "Training iteration 141 loss: 0.07448988407850266, ACC:1.0\n",
      "Training iteration 142 loss: 0.1471550613641739, ACC:0.953125\n",
      "Training iteration 143 loss: 0.08144025504589081, ACC:0.984375\n",
      "Training iteration 144 loss: 0.18117475509643555, ACC:0.90625\n",
      "Training iteration 145 loss: 0.13187476992607117, ACC:0.953125\n",
      "Training iteration 146 loss: 0.16134360432624817, ACC:0.953125\n",
      "Training iteration 147 loss: 0.1627117395401001, ACC:0.953125\n",
      "Training iteration 148 loss: 0.06863310188055038, ACC:0.984375\n",
      "Training iteration 149 loss: 0.1179751381278038, ACC:0.96875\n",
      "Training iteration 150 loss: 0.140691876411438, ACC:0.953125\n",
      "Training iteration 151 loss: 0.0760926678776741, ACC:0.984375\n",
      "Training iteration 152 loss: 0.062199853360652924, ACC:0.984375\n",
      "Training iteration 153 loss: 0.08537469804286957, ACC:0.96875\n",
      "Training iteration 154 loss: 0.03803011402487755, ACC:1.0\n",
      "Training iteration 155 loss: 0.08421154320240021, ACC:0.96875\n",
      "Training iteration 156 loss: 0.1956745833158493, ACC:0.921875\n",
      "Training iteration 157 loss: 0.04609314352273941, ACC:0.984375\n",
      "Training iteration 158 loss: 0.047089625149965286, ACC:1.0\n",
      "Training iteration 159 loss: 0.20551982522010803, ACC:0.96875\n",
      "Training iteration 160 loss: 0.09850183874368668, ACC:0.96875\n",
      "Training iteration 161 loss: 0.05131010711193085, ACC:0.984375\n",
      "Training iteration 162 loss: 0.1243370920419693, ACC:0.9375\n",
      "Training iteration 163 loss: 0.0932374820113182, ACC:0.96875\n",
      "Training iteration 164 loss: 0.026239413768053055, ACC:1.0\n",
      "Training iteration 165 loss: 0.13983093202114105, ACC:0.96875\n",
      "Training iteration 166 loss: 0.044199176132678986, ACC:0.984375\n",
      "Training iteration 167 loss: 0.03610378876328468, ACC:0.984375\n",
      "Training iteration 168 loss: 0.07849223911762238, ACC:0.96875\n",
      "Training iteration 169 loss: 0.08993569016456604, ACC:0.984375\n",
      "Training iteration 170 loss: 0.08084716647863388, ACC:0.96875\n",
      "Training iteration 171 loss: 0.038393955677747726, ACC:0.984375\n",
      "Training iteration 172 loss: 0.03530816361308098, ACC:1.0\n",
      "Training iteration 173 loss: 0.040845341980457306, ACC:1.0\n",
      "Training iteration 174 loss: 0.04547003284096718, ACC:0.984375\n",
      "Training iteration 175 loss: 0.08871380984783173, ACC:0.96875\n",
      "Training iteration 176 loss: 0.03243490308523178, ACC:1.0\n",
      "Training iteration 177 loss: 0.08162762224674225, ACC:0.96875\n",
      "Training iteration 178 loss: 0.18642503023147583, ACC:0.921875\n",
      "Training iteration 179 loss: 0.03831709176301956, ACC:1.0\n",
      "Training iteration 180 loss: 0.09703194350004196, ACC:0.953125\n",
      "Training iteration 181 loss: 0.15838898718357086, ACC:0.953125\n",
      "Training iteration 182 loss: 0.11249272525310516, ACC:0.953125\n",
      "Training iteration 183 loss: 0.15981484949588776, ACC:0.921875\n",
      "Training iteration 184 loss: 0.06386885792016983, ACC:0.984375\n",
      "Training iteration 185 loss: 0.1494162380695343, ACC:0.953125\n",
      "Training iteration 186 loss: 0.035105638206005096, ACC:1.0\n",
      "Training iteration 187 loss: 0.17147240042686462, ACC:0.953125\n",
      "Training iteration 188 loss: 0.09258674830198288, ACC:0.984375\n",
      "Training iteration 189 loss: 0.09718062728643417, ACC:0.96875\n",
      "Training iteration 190 loss: 0.02787899412214756, ACC:1.0\n",
      "Training iteration 191 loss: 0.10342447459697723, ACC:0.984375\n",
      "Training iteration 192 loss: 0.07925009727478027, ACC:0.96875\n",
      "Training iteration 193 loss: 0.10767288506031036, ACC:0.953125\n",
      "Training iteration 194 loss: 0.09853304177522659, ACC:0.953125\n",
      "Training iteration 195 loss: 0.03970618546009064, ACC:1.0\n",
      "Training iteration 196 loss: 0.06399602442979813, ACC:0.984375\n",
      "Training iteration 197 loss: 0.0622129924595356, ACC:0.984375\n",
      "Training iteration 198 loss: 0.08812753856182098, ACC:0.96875\n",
      "Training iteration 199 loss: 0.050980936735868454, ACC:1.0\n",
      "Training iteration 200 loss: 0.04425573721528053, ACC:1.0\n",
      "Training iteration 201 loss: 0.15188820660114288, ACC:0.9375\n",
      "Training iteration 202 loss: 0.027752580121159554, ACC:1.0\n",
      "Training iteration 203 loss: 0.10149218887090683, ACC:0.96875\n",
      "Training iteration 204 loss: 0.121602363884449, ACC:0.921875\n",
      "Training iteration 205 loss: 0.15010395646095276, ACC:0.96875\n",
      "Training iteration 206 loss: 0.12962742149829865, ACC:0.953125\n",
      "Training iteration 207 loss: 0.11871909350156784, ACC:0.984375\n",
      "Training iteration 208 loss: 0.20934344828128815, ACC:0.9375\n",
      "Training iteration 209 loss: 0.2807886302471161, ACC:0.84375\n",
      "Training iteration 210 loss: 0.07853005081415176, ACC:0.984375\n",
      "Training iteration 211 loss: 0.10466714948415756, ACC:0.953125\n",
      "Training iteration 212 loss: 0.1418878138065338, ACC:0.953125\n",
      "Training iteration 213 loss: 0.09136749804019928, ACC:0.96875\n",
      "Training iteration 214 loss: 0.09533361345529556, ACC:0.953125\n",
      "Training iteration 215 loss: 0.08529748022556305, ACC:0.984375\n",
      "Training iteration 216 loss: 0.15602749586105347, ACC:0.953125\n",
      "Training iteration 217 loss: 0.052880726754665375, ACC:0.984375\n",
      "Training iteration 218 loss: 0.060672253370285034, ACC:1.0\n",
      "Training iteration 219 loss: 0.15551607310771942, ACC:0.9375\n",
      "Training iteration 220 loss: 0.11430931091308594, ACC:0.953125\n",
      "Training iteration 221 loss: 0.10527896881103516, ACC:0.953125\n",
      "Training iteration 222 loss: 0.08187510818243027, ACC:0.953125\n",
      "Training iteration 223 loss: 0.11216145008802414, ACC:0.984375\n",
      "Training iteration 224 loss: 0.045662425458431244, ACC:0.984375\n",
      "Training iteration 225 loss: 0.0882076546549797, ACC:0.984375\n",
      "Training iteration 226 loss: 0.12341710180044174, ACC:0.96875\n",
      "Training iteration 227 loss: 0.12978127598762512, ACC:0.953125\n",
      "Training iteration 228 loss: 0.10112843662500381, ACC:0.96875\n",
      "Training iteration 229 loss: 0.0505782812833786, ACC:0.984375\n",
      "Training iteration 230 loss: 0.04802825674414635, ACC:0.984375\n",
      "Training iteration 231 loss: 0.15842701494693756, ACC:0.9375\n",
      "Training iteration 232 loss: 0.020026277750730515, ACC:1.0\n",
      "Training iteration 233 loss: 0.07554690539836884, ACC:0.96875\n",
      "Training iteration 234 loss: 0.06795359402894974, ACC:0.953125\n",
      "Training iteration 235 loss: 0.05141565948724747, ACC:0.96875\n",
      "Training iteration 236 loss: 0.1127832680940628, ACC:0.953125\n",
      "Training iteration 237 loss: 0.042744219303131104, ACC:0.984375\n",
      "Training iteration 238 loss: 0.11248967051506042, ACC:0.96875\n",
      "Training iteration 239 loss: 0.23072749376296997, ACC:0.9375\n",
      "Training iteration 240 loss: 0.049120090901851654, ACC:0.984375\n",
      "Training iteration 241 loss: 0.0916995033621788, ACC:0.96875\n",
      "Training iteration 242 loss: 0.053481265902519226, ACC:0.96875\n",
      "Training iteration 243 loss: 0.0314025916159153, ACC:1.0\n",
      "Training iteration 244 loss: 0.08533336967229843, ACC:0.984375\n",
      "Training iteration 245 loss: 0.08318668603897095, ACC:0.96875\n",
      "Training iteration 246 loss: 0.018985748291015625, ACC:1.0\n",
      "Training iteration 247 loss: 0.13198153674602509, ACC:0.953125\n",
      "Training iteration 248 loss: 0.06985615193843842, ACC:0.984375\n",
      "Training iteration 249 loss: 0.04000123590230942, ACC:0.984375\n",
      "Training iteration 250 loss: 0.04011625051498413, ACC:0.984375\n",
      "Training iteration 251 loss: 0.015529054217040539, ACC:1.0\n",
      "Training iteration 252 loss: 0.0363844595849514, ACC:1.0\n",
      "Training iteration 253 loss: 0.06324294209480286, ACC:0.96875\n",
      "Training iteration 254 loss: 0.09697917848825455, ACC:0.953125\n",
      "Training iteration 255 loss: 0.03765669837594032, ACC:1.0\n",
      "Training iteration 256 loss: 0.09023573994636536, ACC:0.953125\n",
      "Training iteration 257 loss: 0.047252580523490906, ACC:0.96875\n",
      "Training iteration 258 loss: 0.10147150605916977, ACC:0.96875\n",
      "Training iteration 259 loss: 0.06137879192829132, ACC:0.96875\n",
      "Training iteration 260 loss: 0.060834892094135284, ACC:0.984375\n",
      "Training iteration 261 loss: 0.10293370485305786, ACC:0.96875\n",
      "Training iteration 262 loss: 0.05541451275348663, ACC:0.96875\n",
      "Training iteration 263 loss: 0.036515139043331146, ACC:0.984375\n",
      "Training iteration 264 loss: 0.05589352175593376, ACC:0.984375\n",
      "Training iteration 265 loss: 0.029356759041547775, ACC:0.984375\n",
      "Training iteration 266 loss: 0.0902688056230545, ACC:0.96875\n",
      "Training iteration 267 loss: 0.06309917569160461, ACC:0.96875\n",
      "Training iteration 268 loss: 0.14570742845535278, ACC:0.953125\n",
      "Training iteration 269 loss: 0.10221852362155914, ACC:0.96875\n",
      "Training iteration 270 loss: 0.09379633516073227, ACC:0.984375\n",
      "Training iteration 271 loss: 0.14299514889717102, ACC:0.953125\n",
      "Training iteration 272 loss: 0.10428310185670853, ACC:0.953125\n",
      "Training iteration 273 loss: 0.11418722569942474, ACC:0.96875\n",
      "Training iteration 274 loss: 0.13833032548427582, ACC:0.953125\n",
      "Training iteration 275 loss: 0.13775169849395752, ACC:0.953125\n",
      "Training iteration 276 loss: 0.09044474363327026, ACC:0.96875\n",
      "Training iteration 277 loss: 0.06273307651281357, ACC:0.953125\n",
      "Training iteration 278 loss: 0.017893105745315552, ACC:1.0\n",
      "Training iteration 279 loss: 0.02418990433216095, ACC:1.0\n",
      "Training iteration 280 loss: 0.07896340638399124, ACC:0.96875\n",
      "Training iteration 281 loss: 0.035635579377412796, ACC:1.0\n",
      "Training iteration 282 loss: 0.04436616599559784, ACC:0.984375\n",
      "Training iteration 283 loss: 0.09613577276468277, ACC:0.953125\n",
      "Training iteration 284 loss: 0.11094353348016739, ACC:0.96875\n",
      "Training iteration 285 loss: 0.058196768164634705, ACC:0.984375\n",
      "Training iteration 286 loss: 0.02926400490105152, ACC:1.0\n",
      "Training iteration 287 loss: 0.08432918041944504, ACC:0.984375\n",
      "Training iteration 288 loss: 0.08744527399539948, ACC:0.96875\n",
      "Training iteration 289 loss: 0.2476862519979477, ACC:0.9375\n",
      "Training iteration 290 loss: 0.04317142814397812, ACC:0.984375\n",
      "Training iteration 291 loss: 0.20059537887573242, ACC:0.96875\n",
      "Training iteration 292 loss: 0.1039365604519844, ACC:0.953125\n",
      "Training iteration 293 loss: 0.16826920211315155, ACC:0.9375\n",
      "Training iteration 294 loss: 0.06068301200866699, ACC:0.984375\n",
      "Training iteration 295 loss: 0.05746803432703018, ACC:0.984375\n",
      "Training iteration 296 loss: 0.05975090339779854, ACC:0.96875\n",
      "Training iteration 297 loss: 0.1356111764907837, ACC:0.953125\n",
      "Training iteration 298 loss: 0.058334533125162125, ACC:0.96875\n",
      "Training iteration 299 loss: 0.06896939873695374, ACC:0.96875\n",
      "Training iteration 300 loss: 0.08141924440860748, ACC:0.96875\n",
      "Training iteration 301 loss: 0.17546969652175903, ACC:0.9375\n",
      "Training iteration 302 loss: 0.17745116353034973, ACC:0.9375\n",
      "Training iteration 303 loss: 0.23498612642288208, ACC:0.921875\n",
      "Training iteration 304 loss: 0.03595377877354622, ACC:1.0\n",
      "Training iteration 305 loss: 0.05739223584532738, ACC:0.984375\n",
      "Training iteration 306 loss: 0.136858269572258, ACC:0.984375\n",
      "Training iteration 307 loss: 0.14042705297470093, ACC:0.953125\n",
      "Training iteration 308 loss: 0.15026400983333588, ACC:0.953125\n",
      "Training iteration 309 loss: 0.11905203759670258, ACC:0.953125\n",
      "Training iteration 310 loss: 0.1494123488664627, ACC:0.953125\n",
      "Training iteration 311 loss: 0.14459651708602905, ACC:0.953125\n",
      "Training iteration 312 loss: 0.16612029075622559, ACC:0.9375\n",
      "Training iteration 313 loss: 0.15947194397449493, ACC:0.921875\n",
      "Training iteration 314 loss: 0.15843847393989563, ACC:0.921875\n",
      "Training iteration 315 loss: 0.07354868203401566, ACC:0.984375\n",
      "Training iteration 316 loss: 0.046423230320215225, ACC:1.0\n",
      "Training iteration 317 loss: 0.07191888988018036, ACC:0.984375\n",
      "Training iteration 318 loss: 0.024998875334858894, ACC:1.0\n",
      "Training iteration 319 loss: 0.09511003643274307, ACC:0.953125\n",
      "Training iteration 320 loss: 0.07940950244665146, ACC:0.96875\n",
      "Training iteration 321 loss: 0.12586328387260437, ACC:0.921875\n",
      "Training iteration 322 loss: 0.027935773134231567, ACC:1.0\n",
      "Training iteration 323 loss: 0.06310912221670151, ACC:0.96875\n",
      "Training iteration 324 loss: 0.035922691226005554, ACC:0.984375\n",
      "Training iteration 325 loss: 0.14071440696716309, ACC:0.953125\n",
      "Training iteration 326 loss: 0.015718398615717888, ACC:1.0\n",
      "Training iteration 327 loss: 0.13713732361793518, ACC:0.953125\n",
      "Training iteration 328 loss: 0.1377655267715454, ACC:0.96875\n",
      "Training iteration 329 loss: 0.052728503942489624, ACC:0.984375\n",
      "Training iteration 330 loss: 0.1129210889339447, ACC:0.96875\n",
      "Training iteration 331 loss: 0.03881319612264633, ACC:1.0\n",
      "Training iteration 332 loss: 0.05268654599785805, ACC:0.984375\n",
      "Training iteration 333 loss: 0.11221014708280563, ACC:0.953125\n",
      "Training iteration 334 loss: 0.05061275139451027, ACC:0.984375\n",
      "Training iteration 335 loss: 0.11374347656965256, ACC:0.96875\n",
      "Training iteration 336 loss: 0.020310895517468452, ACC:1.0\n",
      "Training iteration 337 loss: 0.03382876515388489, ACC:1.0\n",
      "Training iteration 338 loss: 0.10114652663469315, ACC:0.984375\n",
      "Training iteration 339 loss: 0.05242052674293518, ACC:1.0\n",
      "Training iteration 340 loss: 0.059126824140548706, ACC:0.96875\n",
      "Training iteration 341 loss: 0.14024336636066437, ACC:0.9375\n",
      "Training iteration 342 loss: 0.16128389537334442, ACC:0.96875\n",
      "Training iteration 343 loss: 0.05220111832022667, ACC:1.0\n",
      "Training iteration 344 loss: 0.1168040856719017, ACC:0.953125\n",
      "Training iteration 345 loss: 0.07268190383911133, ACC:0.96875\n",
      "Training iteration 346 loss: 0.23788096010684967, ACC:0.921875\n",
      "Training iteration 347 loss: 0.131668359041214, ACC:0.953125\n",
      "Training iteration 348 loss: 0.033037543296813965, ACC:1.0\n",
      "Training iteration 349 loss: 0.13151204586029053, ACC:0.96875\n",
      "Training iteration 350 loss: 0.12761521339416504, ACC:0.953125\n",
      "Training iteration 351 loss: 0.08653754740953445, ACC:0.984375\n",
      "Training iteration 352 loss: 0.17998671531677246, ACC:0.921875\n",
      "Training iteration 353 loss: 0.05716439336538315, ACC:0.984375\n",
      "Training iteration 354 loss: 0.11264155060052872, ACC:0.9375\n",
      "Training iteration 355 loss: 0.14066748321056366, ACC:0.9375\n",
      "Training iteration 356 loss: 0.12922146916389465, ACC:0.96875\n",
      "Training iteration 357 loss: 0.04823155328631401, ACC:1.0\n",
      "Training iteration 358 loss: 0.03623443469405174, ACC:1.0\n",
      "Training iteration 359 loss: 0.03551294282078743, ACC:1.0\n",
      "Training iteration 360 loss: 0.039339564740657806, ACC:1.0\n",
      "Training iteration 361 loss: 0.11830463260412216, ACC:0.953125\n",
      "Training iteration 362 loss: 0.2002982795238495, ACC:0.90625\n",
      "Training iteration 363 loss: 0.06511187553405762, ACC:0.984375\n",
      "Training iteration 364 loss: 0.10347731411457062, ACC:0.953125\n",
      "Training iteration 365 loss: 0.05714701861143112, ACC:0.984375\n",
      "Training iteration 366 loss: 0.13487420976161957, ACC:0.96875\n",
      "Training iteration 367 loss: 0.10614372789859772, ACC:0.9375\n",
      "Training iteration 368 loss: 0.14969845116138458, ACC:0.90625\n",
      "Training iteration 369 loss: 0.17693862318992615, ACC:0.90625\n",
      "Training iteration 370 loss: 0.05797753483057022, ACC:0.96875\n",
      "Training iteration 371 loss: 0.08248492330312729, ACC:0.96875\n",
      "Training iteration 372 loss: 0.039363615214824677, ACC:0.984375\n",
      "Training iteration 373 loss: 0.06470620632171631, ACC:0.984375\n",
      "Training iteration 374 loss: 0.08862028270959854, ACC:0.96875\n",
      "Training iteration 375 loss: 0.10072703659534454, ACC:0.96875\n",
      "Training iteration 376 loss: 0.06356696039438248, ACC:0.984375\n",
      "Training iteration 377 loss: 0.036303773522377014, ACC:1.0\n",
      "Training iteration 378 loss: 0.11285598576068878, ACC:0.96875\n",
      "Training iteration 379 loss: 0.17921115458011627, ACC:0.921875\n",
      "Training iteration 380 loss: 0.06941522657871246, ACC:0.96875\n",
      "Training iteration 381 loss: 0.05036791414022446, ACC:0.984375\n",
      "Training iteration 382 loss: 0.08328978717327118, ACC:0.984375\n",
      "Training iteration 383 loss: 0.07501229643821716, ACC:0.96875\n",
      "Training iteration 384 loss: 0.20564499497413635, ACC:0.921875\n",
      "Training iteration 385 loss: 0.1288832277059555, ACC:0.953125\n",
      "Training iteration 386 loss: 0.09687372297048569, ACC:0.96875\n",
      "Training iteration 387 loss: 0.2062796652317047, ACC:0.9375\n",
      "Training iteration 388 loss: 0.14814393222332, ACC:0.953125\n",
      "Training iteration 389 loss: 0.13974158465862274, ACC:0.921875\n",
      "Training iteration 390 loss: 0.11088262498378754, ACC:0.96875\n",
      "Training iteration 391 loss: 0.15421700477600098, ACC:0.96875\n",
      "Training iteration 392 loss: 0.08654014021158218, ACC:0.984375\n",
      "Training iteration 393 loss: 0.13330692052841187, ACC:0.953125\n",
      "Training iteration 394 loss: 0.04230910912156105, ACC:0.984375\n",
      "Training iteration 395 loss: 0.08736682683229446, ACC:0.984375\n",
      "Training iteration 396 loss: 0.1697157919406891, ACC:0.953125\n",
      "Training iteration 397 loss: 0.07958140969276428, ACC:0.984375\n",
      "Training iteration 398 loss: 0.04939710348844528, ACC:1.0\n",
      "Training iteration 399 loss: 0.14830757677555084, ACC:0.9375\n",
      "Training iteration 400 loss: 0.03580950200557709, ACC:1.0\n",
      "Training iteration 401 loss: 0.06292788684368134, ACC:0.984375\n",
      "Training iteration 402 loss: 0.10787663608789444, ACC:0.953125\n",
      "Training iteration 403 loss: 0.08994290232658386, ACC:0.984375\n",
      "Training iteration 404 loss: 0.03493473678827286, ACC:1.0\n",
      "Training iteration 405 loss: 0.07434860616922379, ACC:0.96875\n",
      "Training iteration 406 loss: 0.10534943640232086, ACC:0.9375\n",
      "Training iteration 407 loss: 0.0386308878660202, ACC:1.0\n",
      "Training iteration 408 loss: 0.08818823844194412, ACC:0.984375\n",
      "Training iteration 409 loss: 0.06181332468986511, ACC:0.96875\n",
      "Training iteration 410 loss: 0.09271412342786789, ACC:0.953125\n",
      "Training iteration 411 loss: 0.06505593657493591, ACC:0.96875\n",
      "Training iteration 412 loss: 0.19868481159210205, ACC:0.953125\n",
      "Training iteration 413 loss: 0.09305743873119354, ACC:0.984375\n",
      "Training iteration 414 loss: 0.27287253737449646, ACC:0.9375\n",
      "Training iteration 415 loss: 0.05189301818609238, ACC:0.984375\n",
      "Training iteration 416 loss: 0.1295941025018692, ACC:0.953125\n",
      "Training iteration 417 loss: 0.08873428404331207, ACC:0.984375\n",
      "Training iteration 418 loss: 0.21823056042194366, ACC:0.921875\n",
      "Training iteration 419 loss: 0.06718078255653381, ACC:0.984375\n",
      "Training iteration 420 loss: 0.08191186934709549, ACC:0.984375\n",
      "Training iteration 421 loss: 0.1853836178779602, ACC:0.953125\n",
      "Training iteration 422 loss: 0.09760412573814392, ACC:0.984375\n",
      "Training iteration 423 loss: 0.05838848277926445, ACC:0.984375\n",
      "Training iteration 424 loss: 0.10894015431404114, ACC:0.953125\n",
      "Training iteration 425 loss: 0.04756409302353859, ACC:1.0\n",
      "Training iteration 426 loss: 0.19608129560947418, ACC:0.9375\n",
      "Training iteration 427 loss: 0.11020109057426453, ACC:0.96875\n",
      "Training iteration 428 loss: 0.09849034249782562, ACC:1.0\n",
      "Training iteration 429 loss: 0.033465344458818436, ACC:1.0\n",
      "Training iteration 430 loss: 0.06848042458295822, ACC:0.984375\n",
      "Training iteration 431 loss: 0.0462917722761631, ACC:0.984375\n",
      "Training iteration 432 loss: 0.14074555039405823, ACC:0.953125\n",
      "Training iteration 433 loss: 0.09411922097206116, ACC:0.984375\n",
      "Training iteration 434 loss: 0.1556883454322815, ACC:0.953125\n",
      "Training iteration 435 loss: 0.10543026775121689, ACC:0.953125\n",
      "Training iteration 436 loss: 0.056312162429094315, ACC:0.96875\n",
      "Training iteration 437 loss: 0.10384692996740341, ACC:0.984375\n",
      "Training iteration 438 loss: 0.04973125830292702, ACC:1.0\n",
      "Training iteration 439 loss: 0.07555955648422241, ACC:0.984375\n",
      "Training iteration 440 loss: 0.05878174677491188, ACC:0.984375\n",
      "Training iteration 441 loss: 0.06079995632171631, ACC:0.984375\n",
      "Training iteration 442 loss: 0.08595018088817596, ACC:0.984375\n",
      "Training iteration 443 loss: 0.061149366199970245, ACC:0.984375\n",
      "Training iteration 444 loss: 0.06916850060224533, ACC:0.984375\n",
      "Training iteration 445 loss: 0.11638449132442474, ACC:0.953125\n",
      "Training iteration 446 loss: 0.121839240193367, ACC:0.984375\n",
      "Training iteration 447 loss: 0.14848384261131287, ACC:0.90625\n",
      "Training iteration 448 loss: 0.1383870393037796, ACC:0.953125\n",
      "Training iteration 449 loss: 0.12181779742240906, ACC:0.96875\n",
      "Training iteration 450 loss: 0.12651100754737854, ACC:0.953125\n",
      "Validation iteration 451 loss: 0.1057036891579628, ACC: 0.984375\n",
      "Validation iteration 452 loss: 0.1261008381843567, ACC: 0.984375\n",
      "Validation iteration 453 loss: 0.12241429090499878, ACC: 0.96875\n",
      "Validation iteration 454 loss: 0.12074720114469528, ACC: 0.953125\n",
      "Validation iteration 455 loss: 0.06966397911310196, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.10614581406116486, ACC: 0.984375\n",
      "Validation iteration 457 loss: 0.12125690281391144, ACC: 0.96875\n",
      "Validation iteration 458 loss: 0.1014624536037445, ACC: 0.984375\n",
      "Validation iteration 459 loss: 0.09086054563522339, ACC: 0.984375\n",
      "Validation iteration 460 loss: 0.08419294655323029, ACC: 1.0\n",
      "Validation iteration 461 loss: 0.10611532628536224, ACC: 0.984375\n",
      "Validation iteration 462 loss: 0.11882413923740387, ACC: 0.953125\n",
      "Validation iteration 463 loss: 0.12167471647262573, ACC: 0.953125\n",
      "Validation iteration 464 loss: 0.1193527951836586, ACC: 0.96875\n",
      "Validation iteration 465 loss: 0.14661122858524323, ACC: 0.953125\n",
      "Validation iteration 466 loss: 0.19923707842826843, ACC: 0.9375\n",
      "Validation iteration 467 loss: 0.057914022356271744, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.08320837467908859, ACC: 0.96875\n",
      "Validation iteration 469 loss: 0.16693075001239777, ACC: 0.9375\n",
      "Validation iteration 470 loss: 0.12923003733158112, ACC: 0.953125\n",
      "Validation iteration 471 loss: 0.20522695779800415, ACC: 0.9375\n",
      "Validation iteration 472 loss: 0.09339072555303574, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.09389249235391617, ACC: 0.984375\n",
      "Validation iteration 474 loss: 0.19078131020069122, ACC: 0.90625\n",
      "Validation iteration 475 loss: 0.14846201241016388, ACC: 0.9375\n",
      "Validation iteration 476 loss: 0.08729095011949539, ACC: 0.984375\n",
      "Validation iteration 477 loss: 0.1123487576842308, ACC: 0.96875\n",
      "Validation iteration 478 loss: 0.10001925379037857, ACC: 0.953125\n",
      "Validation iteration 479 loss: 0.08362257480621338, ACC: 0.96875\n",
      "Validation iteration 480 loss: 0.22401301562786102, ACC: 0.953125\n",
      "Validation iteration 481 loss: 0.21941150724887848, ACC: 0.953125\n",
      "Validation iteration 482 loss: 0.14453764259815216, ACC: 0.984375\n",
      "Validation iteration 483 loss: 0.16762615740299225, ACC: 0.9375\n",
      "Validation iteration 484 loss: 0.1412263959646225, ACC: 0.953125\n",
      "Validation iteration 485 loss: 0.06565164774656296, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.19092711806297302, ACC: 0.921875\n",
      "Validation iteration 487 loss: 0.14609789848327637, ACC: 0.9375\n",
      "Validation iteration 488 loss: 0.1316784918308258, ACC: 0.953125\n",
      "Validation iteration 489 loss: 0.11598783731460571, ACC: 0.984375\n",
      "Validation iteration 490 loss: 0.13157011568546295, ACC: 0.96875\n",
      "Validation iteration 491 loss: 0.13006818294525146, ACC: 0.96875\n",
      "Validation iteration 492 loss: 0.18218962848186493, ACC: 0.90625\n",
      "Validation iteration 493 loss: 0.07145470380783081, ACC: 0.984375\n",
      "Validation iteration 494 loss: 0.11980266124010086, ACC: 0.96875\n",
      "Validation iteration 495 loss: 0.13102522492408752, ACC: 0.953125\n",
      "Validation iteration 496 loss: 0.12505973875522614, ACC: 0.953125\n",
      "Validation iteration 497 loss: 0.2258020043373108, ACC: 0.90625\n",
      "Validation iteration 498 loss: 0.13767842948436737, ACC: 0.96875\n",
      "Validation iteration 499 loss: 0.07102624326944351, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.2077731490135193, ACC: 0.9375\n",
      "-- Epoch 4 done -- Train loss: 0.0964371162508097, train ACC: 0.9682986111111112, val loss: 0.12986579917371274, val ACC: 0.963125\n",
      "<--- 997.0955414772034 seconds --->\n",
      "Training iteration 1 loss: 0.10115481913089752, ACC:0.96875\n",
      "Training iteration 2 loss: 0.11645518243312836, ACC:0.96875\n",
      "Training iteration 3 loss: 0.08699517697095871, ACC:0.96875\n",
      "Training iteration 4 loss: 0.20777730643749237, ACC:0.953125\n",
      "Training iteration 5 loss: 0.08654014766216278, ACC:0.96875\n",
      "Training iteration 6 loss: 0.17211106419563293, ACC:0.96875\n",
      "Training iteration 7 loss: 0.07020580023527145, ACC:0.984375\n",
      "Training iteration 8 loss: 0.12608444690704346, ACC:0.953125\n",
      "Training iteration 9 loss: 0.08730832487344742, ACC:0.96875\n",
      "Training iteration 10 loss: 0.1253337264060974, ACC:0.96875\n",
      "Training iteration 11 loss: 0.13156870007514954, ACC:0.921875\n",
      "Training iteration 12 loss: 0.08441293984651566, ACC:0.984375\n",
      "Training iteration 13 loss: 0.079481340944767, ACC:0.984375\n",
      "Training iteration 14 loss: 0.04777578264474869, ACC:0.984375\n",
      "Training iteration 15 loss: 0.027378294616937637, ACC:1.0\n",
      "Training iteration 16 loss: 0.1542046070098877, ACC:0.96875\n",
      "Training iteration 17 loss: 0.028555836528539658, ACC:1.0\n",
      "Training iteration 18 loss: 0.16238534450531006, ACC:0.921875\n",
      "Training iteration 19 loss: 0.06668473035097122, ACC:0.953125\n",
      "Training iteration 20 loss: 0.11994166672229767, ACC:0.984375\n",
      "Training iteration 21 loss: 0.3930548131465912, ACC:0.921875\n",
      "Training iteration 22 loss: 0.03353312239050865, ACC:1.0\n",
      "Training iteration 23 loss: 0.10928011685609818, ACC:0.953125\n",
      "Training iteration 24 loss: 0.039030030369758606, ACC:1.0\n",
      "Training iteration 25 loss: 0.07137545198202133, ACC:0.96875\n",
      "Training iteration 26 loss: 0.06749755889177322, ACC:0.984375\n",
      "Training iteration 27 loss: 0.09044811129570007, ACC:0.96875\n",
      "Training iteration 28 loss: 0.09515153616666794, ACC:0.96875\n",
      "Training iteration 29 loss: 0.158501997590065, ACC:0.9375\n",
      "Training iteration 30 loss: 0.07331506907939911, ACC:0.984375\n",
      "Training iteration 31 loss: 0.09865855425596237, ACC:0.984375\n",
      "Training iteration 32 loss: 0.12465330958366394, ACC:0.984375\n",
      "Training iteration 33 loss: 0.11200667917728424, ACC:0.984375\n",
      "Training iteration 34 loss: 0.07282768189907074, ACC:0.984375\n",
      "Training iteration 35 loss: 0.10098057240247726, ACC:0.96875\n",
      "Training iteration 36 loss: 0.07594504952430725, ACC:0.984375\n",
      "Training iteration 37 loss: 0.1361573487520218, ACC:0.953125\n",
      "Training iteration 38 loss: 0.05057664215564728, ACC:1.0\n",
      "Training iteration 39 loss: 0.1283123940229416, ACC:0.953125\n",
      "Training iteration 40 loss: 0.0784345269203186, ACC:0.96875\n",
      "Training iteration 41 loss: 0.07406251132488251, ACC:0.984375\n",
      "Training iteration 42 loss: 0.05825294554233551, ACC:1.0\n",
      "Training iteration 43 loss: 0.1095171868801117, ACC:0.953125\n",
      "Training iteration 44 loss: 0.06244247406721115, ACC:0.984375\n",
      "Training iteration 45 loss: 0.10104493796825409, ACC:0.96875\n",
      "Training iteration 46 loss: 0.03051232174038887, ACC:1.0\n",
      "Training iteration 47 loss: 0.09660450369119644, ACC:0.96875\n",
      "Training iteration 48 loss: 0.06091464310884476, ACC:0.984375\n",
      "Training iteration 49 loss: 0.07001914083957672, ACC:0.984375\n",
      "Training iteration 50 loss: 0.0583282969892025, ACC:0.984375\n",
      "Training iteration 51 loss: 0.0663183182477951, ACC:0.96875\n",
      "Training iteration 52 loss: 0.04285658150911331, ACC:1.0\n",
      "Training iteration 53 loss: 0.03108099102973938, ACC:1.0\n",
      "Training iteration 54 loss: 0.19113032519817352, ACC:0.9375\n",
      "Training iteration 55 loss: 0.1130092665553093, ACC:0.9375\n",
      "Training iteration 56 loss: 0.028884833678603172, ACC:1.0\n",
      "Training iteration 57 loss: 0.06444189697504044, ACC:0.984375\n",
      "Training iteration 58 loss: 0.09105231612920761, ACC:0.96875\n",
      "Training iteration 59 loss: 0.05101160332560539, ACC:0.984375\n",
      "Training iteration 60 loss: 0.031617362052202225, ACC:1.0\n",
      "Training iteration 61 loss: 0.050120748579502106, ACC:0.96875\n",
      "Training iteration 62 loss: 0.014816708862781525, ACC:1.0\n",
      "Training iteration 63 loss: 0.09492363035678864, ACC:0.984375\n",
      "Training iteration 64 loss: 0.06627307087182999, ACC:0.984375\n",
      "Training iteration 65 loss: 0.05888531357049942, ACC:0.984375\n",
      "Training iteration 66 loss: 0.0317288339138031, ACC:0.984375\n",
      "Training iteration 67 loss: 0.08723022043704987, ACC:0.984375\n",
      "Training iteration 68 loss: 0.1025053933262825, ACC:0.953125\n",
      "Training iteration 69 loss: 0.10555677860975266, ACC:0.953125\n",
      "Training iteration 70 loss: 0.0796036422252655, ACC:0.96875\n",
      "Training iteration 71 loss: 0.02686786837875843, ACC:0.984375\n",
      "Training iteration 72 loss: 0.04198693856596947, ACC:0.984375\n",
      "Training iteration 73 loss: 0.14621028304100037, ACC:0.9375\n",
      "Training iteration 74 loss: 0.07141979783773422, ACC:0.96875\n",
      "Training iteration 75 loss: 0.0926256999373436, ACC:0.96875\n",
      "Training iteration 76 loss: 0.06519930064678192, ACC:0.984375\n",
      "Training iteration 77 loss: 0.03419770300388336, ACC:1.0\n",
      "Training iteration 78 loss: 0.02677268721163273, ACC:1.0\n",
      "Training iteration 79 loss: 0.10363961011171341, ACC:0.96875\n",
      "Training iteration 80 loss: 0.04966025799512863, ACC:0.984375\n",
      "Training iteration 81 loss: 0.04611387103796005, ACC:0.984375\n",
      "Training iteration 82 loss: 0.03896573930978775, ACC:1.0\n",
      "Training iteration 83 loss: 0.06378766894340515, ACC:0.984375\n",
      "Training iteration 84 loss: 0.028138719499111176, ACC:1.0\n",
      "Training iteration 85 loss: 0.09393037855625153, ACC:0.96875\n",
      "Training iteration 86 loss: 0.09753403812646866, ACC:0.96875\n",
      "Training iteration 87 loss: 0.07176223397254944, ACC:0.984375\n",
      "Training iteration 88 loss: 0.11962846666574478, ACC:0.953125\n",
      "Training iteration 89 loss: 0.03983333334326744, ACC:1.0\n",
      "Training iteration 90 loss: 0.09713945537805557, ACC:0.96875\n",
      "Training iteration 91 loss: 0.07035446166992188, ACC:0.96875\n",
      "Training iteration 92 loss: 0.13681961596012115, ACC:0.953125\n",
      "Training iteration 93 loss: 0.034863028675317764, ACC:0.984375\n",
      "Training iteration 94 loss: 0.027372688055038452, ACC:0.984375\n",
      "Training iteration 95 loss: 0.03298668935894966, ACC:0.984375\n",
      "Training iteration 96 loss: 0.05310067534446716, ACC:0.984375\n",
      "Training iteration 97 loss: 0.12801015377044678, ACC:0.96875\n",
      "Training iteration 98 loss: 0.06368158012628555, ACC:0.96875\n",
      "Training iteration 99 loss: 0.06199226900935173, ACC:0.984375\n",
      "Training iteration 100 loss: 0.09289804846048355, ACC:0.96875\n",
      "Training iteration 101 loss: 0.10417652875185013, ACC:0.96875\n",
      "Training iteration 102 loss: 0.14903602004051208, ACC:0.953125\n",
      "Training iteration 103 loss: 0.13133622705936432, ACC:0.9375\n",
      "Training iteration 104 loss: 0.03606937453150749, ACC:1.0\n",
      "Training iteration 105 loss: 0.10804496705532074, ACC:0.96875\n",
      "Training iteration 106 loss: 0.08156289160251617, ACC:0.953125\n",
      "Training iteration 107 loss: 0.0753309354186058, ACC:0.96875\n",
      "Training iteration 108 loss: 0.18422925472259521, ACC:0.9375\n",
      "Training iteration 109 loss: 0.07002395391464233, ACC:0.96875\n",
      "Training iteration 110 loss: 0.11046640574932098, ACC:0.984375\n",
      "Training iteration 111 loss: 0.1271875500679016, ACC:0.953125\n",
      "Training iteration 112 loss: 0.1532580405473709, ACC:0.90625\n",
      "Training iteration 113 loss: 0.022950515151023865, ACC:1.0\n",
      "Training iteration 114 loss: 0.03008364886045456, ACC:0.984375\n",
      "Training iteration 115 loss: 0.1559220552444458, ACC:0.953125\n",
      "Training iteration 116 loss: 0.12999548017978668, ACC:0.9375\n",
      "Training iteration 117 loss: 0.1613280475139618, ACC:0.953125\n",
      "Training iteration 118 loss: 0.010888759978115559, ACC:1.0\n",
      "Training iteration 119 loss: 0.04230847209692001, ACC:0.984375\n",
      "Training iteration 120 loss: 0.08079931139945984, ACC:0.984375\n",
      "Training iteration 121 loss: 0.14497093856334686, ACC:0.96875\n",
      "Training iteration 122 loss: 0.13782964646816254, ACC:0.953125\n",
      "Training iteration 123 loss: 0.06502821296453476, ACC:0.96875\n",
      "Training iteration 124 loss: 0.020067637786269188, ACC:1.0\n",
      "Training iteration 125 loss: 0.09096472710371017, ACC:0.96875\n",
      "Training iteration 126 loss: 0.043926533311605453, ACC:0.96875\n",
      "Training iteration 127 loss: 0.07669419050216675, ACC:0.96875\n",
      "Training iteration 128 loss: 0.09206093847751617, ACC:0.96875\n",
      "Training iteration 129 loss: 0.06388531625270844, ACC:0.984375\n",
      "Training iteration 130 loss: 0.023114319890737534, ACC:1.0\n",
      "Training iteration 131 loss: 0.0840078741312027, ACC:0.96875\n",
      "Training iteration 132 loss: 0.06929492950439453, ACC:0.953125\n",
      "Training iteration 133 loss: 0.013360574841499329, ACC:1.0\n",
      "Training iteration 134 loss: 0.1253402978181839, ACC:0.9375\n",
      "Training iteration 135 loss: 0.06026686355471611, ACC:0.96875\n",
      "Training iteration 136 loss: 0.08060301095247269, ACC:0.96875\n",
      "Training iteration 137 loss: 0.009780796244740486, ACC:1.0\n",
      "Training iteration 138 loss: 0.18603263795375824, ACC:0.953125\n",
      "Training iteration 139 loss: 0.13221903145313263, ACC:0.96875\n",
      "Training iteration 140 loss: 0.03955310583114624, ACC:0.984375\n",
      "Training iteration 141 loss: 0.03499908000230789, ACC:1.0\n",
      "Training iteration 142 loss: 0.10165993869304657, ACC:0.96875\n",
      "Training iteration 143 loss: 0.11494416743516922, ACC:0.9375\n",
      "Training iteration 144 loss: 0.14249950647354126, ACC:0.9375\n",
      "Training iteration 145 loss: 0.1281433403491974, ACC:0.96875\n",
      "Training iteration 146 loss: 0.029113894328475, ACC:0.984375\n",
      "Training iteration 147 loss: 0.05945517122745514, ACC:0.96875\n",
      "Training iteration 148 loss: 0.0881233811378479, ACC:0.96875\n",
      "Training iteration 149 loss: 0.062006399035453796, ACC:0.984375\n",
      "Training iteration 150 loss: 0.20210087299346924, ACC:0.9375\n",
      "Training iteration 151 loss: 0.08578722923994064, ACC:0.984375\n",
      "Training iteration 152 loss: 0.10800736397504807, ACC:0.953125\n",
      "Training iteration 153 loss: 0.04575473070144653, ACC:0.984375\n",
      "Training iteration 154 loss: 0.09807311743497849, ACC:0.953125\n",
      "Training iteration 155 loss: 0.0517115518450737, ACC:0.984375\n",
      "Training iteration 156 loss: 0.03824097290635109, ACC:1.0\n",
      "Training iteration 157 loss: 0.036338962614536285, ACC:0.984375\n",
      "Training iteration 158 loss: 0.016482271254062653, ACC:1.0\n",
      "Training iteration 159 loss: 0.038361210376024246, ACC:0.984375\n",
      "Training iteration 160 loss: 0.14357371628284454, ACC:0.96875\n",
      "Training iteration 161 loss: 0.15962140262126923, ACC:0.96875\n",
      "Training iteration 162 loss: 0.10533595830202103, ACC:0.953125\n",
      "Training iteration 163 loss: 0.12111787497997284, ACC:0.953125\n",
      "Training iteration 164 loss: 0.09138758480548859, ACC:0.96875\n",
      "Training iteration 165 loss: 0.1291608214378357, ACC:0.96875\n",
      "Training iteration 166 loss: 0.04879596829414368, ACC:0.984375\n",
      "Training iteration 167 loss: 0.11415686458349228, ACC:0.9375\n",
      "Training iteration 168 loss: 0.09112415462732315, ACC:0.9375\n",
      "Training iteration 169 loss: 0.07372241467237473, ACC:0.96875\n",
      "Training iteration 170 loss: 0.03468732535839081, ACC:0.984375\n",
      "Training iteration 171 loss: 0.258268266916275, ACC:0.921875\n",
      "Training iteration 172 loss: 0.07469810545444489, ACC:0.984375\n",
      "Training iteration 173 loss: 0.17853060364723206, ACC:0.953125\n",
      "Training iteration 174 loss: 0.08525524288415909, ACC:0.96875\n",
      "Training iteration 175 loss: 0.1887350231409073, ACC:0.9375\n",
      "Training iteration 176 loss: 0.07565151900053024, ACC:0.984375\n",
      "Training iteration 177 loss: 0.2123199999332428, ACC:0.9375\n",
      "Training iteration 178 loss: 0.1001443862915039, ACC:0.96875\n",
      "Training iteration 179 loss: 0.07573296129703522, ACC:0.984375\n",
      "Training iteration 180 loss: 0.07433823496103287, ACC:0.984375\n",
      "Training iteration 181 loss: 0.06300844252109528, ACC:0.96875\n",
      "Training iteration 182 loss: 0.06180606782436371, ACC:1.0\n",
      "Training iteration 183 loss: 0.06597699970006943, ACC:0.96875\n",
      "Training iteration 184 loss: 0.0559241883456707, ACC:0.96875\n",
      "Training iteration 185 loss: 0.09724658727645874, ACC:0.984375\n",
      "Training iteration 186 loss: 0.13093455135822296, ACC:0.953125\n",
      "Training iteration 187 loss: 0.027182210236787796, ACC:1.0\n",
      "Training iteration 188 loss: 0.08577653020620346, ACC:0.96875\n",
      "Training iteration 189 loss: 0.107704758644104, ACC:0.96875\n",
      "Training iteration 190 loss: 0.08598487079143524, ACC:0.96875\n",
      "Training iteration 191 loss: 0.07414621114730835, ACC:0.96875\n",
      "Training iteration 192 loss: 0.09467865526676178, ACC:0.96875\n",
      "Training iteration 193 loss: 0.03697308152914047, ACC:1.0\n",
      "Training iteration 194 loss: 0.09920009970664978, ACC:0.96875\n",
      "Training iteration 195 loss: 0.05935866758227348, ACC:0.96875\n",
      "Training iteration 196 loss: 0.09967349469661713, ACC:0.96875\n",
      "Training iteration 197 loss: 0.06824418902397156, ACC:0.96875\n",
      "Training iteration 198 loss: 0.045102767646312714, ACC:0.984375\n",
      "Training iteration 199 loss: 0.03550250828266144, ACC:1.0\n",
      "Training iteration 200 loss: 0.07976622134447098, ACC:0.96875\n",
      "Training iteration 201 loss: 0.06744424253702164, ACC:0.984375\n",
      "Training iteration 202 loss: 0.06015056371688843, ACC:0.984375\n",
      "Training iteration 203 loss: 0.11153770238161087, ACC:0.953125\n",
      "Training iteration 204 loss: 0.102127306163311, ACC:0.953125\n",
      "Training iteration 205 loss: 0.03358568623661995, ACC:1.0\n",
      "Training iteration 206 loss: 0.07372351735830307, ACC:0.96875\n",
      "Training iteration 207 loss: 0.1083204597234726, ACC:0.96875\n",
      "Training iteration 208 loss: 0.10774870961904526, ACC:0.984375\n",
      "Training iteration 209 loss: 0.07277306914329529, ACC:0.984375\n",
      "Training iteration 210 loss: 0.10133201628923416, ACC:0.953125\n",
      "Training iteration 211 loss: 0.08585584908723831, ACC:0.953125\n",
      "Training iteration 212 loss: 0.06988906115293503, ACC:0.984375\n",
      "Training iteration 213 loss: 0.22025513648986816, ACC:0.921875\n",
      "Training iteration 214 loss: 0.16149026155471802, ACC:0.953125\n",
      "Training iteration 215 loss: 0.1500319242477417, ACC:0.96875\n",
      "Training iteration 216 loss: 0.09772562980651855, ACC:0.96875\n",
      "Training iteration 217 loss: 0.05853824317455292, ACC:0.96875\n",
      "Training iteration 218 loss: 0.14613625407218933, ACC:0.953125\n",
      "Training iteration 219 loss: 0.032544054090976715, ACC:1.0\n",
      "Training iteration 220 loss: 0.056321319192647934, ACC:0.96875\n",
      "Training iteration 221 loss: 0.040873683989048004, ACC:0.984375\n",
      "Training iteration 222 loss: 0.10875394195318222, ACC:0.9375\n",
      "Training iteration 223 loss: 0.08042939752340317, ACC:0.96875\n",
      "Training iteration 224 loss: 0.0281645730137825, ACC:1.0\n",
      "Training iteration 225 loss: 0.1158190667629242, ACC:0.953125\n",
      "Training iteration 226 loss: 0.03736354410648346, ACC:1.0\n",
      "Training iteration 227 loss: 0.04478232190012932, ACC:0.984375\n",
      "Training iteration 228 loss: 0.05561836063861847, ACC:0.984375\n",
      "Training iteration 229 loss: 0.020951928570866585, ACC:1.0\n",
      "Training iteration 230 loss: 0.07954803854227066, ACC:0.984375\n",
      "Training iteration 231 loss: 0.068415068089962, ACC:0.984375\n",
      "Training iteration 232 loss: 0.08265775442123413, ACC:0.953125\n",
      "Training iteration 233 loss: 0.07184766978025436, ACC:0.953125\n",
      "Training iteration 234 loss: 0.08392333984375, ACC:0.984375\n",
      "Training iteration 235 loss: 0.05620408430695534, ACC:0.96875\n",
      "Training iteration 236 loss: 0.0887691080570221, ACC:0.96875\n",
      "Training iteration 237 loss: 0.019178520888090134, ACC:1.0\n",
      "Training iteration 238 loss: 0.1444331556558609, ACC:0.9375\n",
      "Training iteration 239 loss: 0.04535752907395363, ACC:1.0\n",
      "Training iteration 240 loss: 0.06939654797315598, ACC:0.96875\n",
      "Training iteration 241 loss: 0.10246981680393219, ACC:0.96875\n",
      "Training iteration 242 loss: 0.02366057224571705, ACC:1.0\n",
      "Training iteration 243 loss: 0.09621424227952957, ACC:0.96875\n",
      "Training iteration 244 loss: 0.09206455200910568, ACC:0.953125\n",
      "Training iteration 245 loss: 0.06307791918516159, ACC:0.96875\n",
      "Training iteration 246 loss: 0.07989762723445892, ACC:0.984375\n",
      "Training iteration 247 loss: 0.0882931500673294, ACC:0.984375\n",
      "Training iteration 248 loss: 0.11641862988471985, ACC:0.953125\n",
      "Training iteration 249 loss: 0.09108176827430725, ACC:0.96875\n",
      "Training iteration 250 loss: 0.026418792083859444, ACC:1.0\n",
      "Training iteration 251 loss: 0.13288144767284393, ACC:0.9375\n",
      "Training iteration 252 loss: 0.03036353923380375, ACC:1.0\n",
      "Training iteration 253 loss: 0.09103064239025116, ACC:0.953125\n",
      "Training iteration 254 loss: 0.05576006695628166, ACC:0.984375\n",
      "Training iteration 255 loss: 0.08368415385484695, ACC:0.96875\n",
      "Training iteration 256 loss: 0.03336060419678688, ACC:1.0\n",
      "Training iteration 257 loss: 0.14796015620231628, ACC:0.953125\n",
      "Training iteration 258 loss: 0.032331936061382294, ACC:1.0\n",
      "Training iteration 259 loss: 0.026292813941836357, ACC:1.0\n",
      "Training iteration 260 loss: 0.10454321652650833, ACC:0.96875\n",
      "Training iteration 261 loss: 0.06179888918995857, ACC:0.984375\n",
      "Training iteration 262 loss: 0.13407579064369202, ACC:0.953125\n",
      "Training iteration 263 loss: 0.03187105804681778, ACC:1.0\n",
      "Training iteration 264 loss: 0.05867421627044678, ACC:0.984375\n",
      "Training iteration 265 loss: 0.1434200257062912, ACC:0.96875\n",
      "Training iteration 266 loss: 0.08765748143196106, ACC:0.953125\n",
      "Training iteration 267 loss: 0.050191812217235565, ACC:1.0\n",
      "Training iteration 268 loss: 0.11620403081178665, ACC:0.96875\n",
      "Training iteration 269 loss: 0.0510389544069767, ACC:0.984375\n",
      "Training iteration 270 loss: 0.05833899974822998, ACC:0.984375\n",
      "Training iteration 271 loss: 0.03741786628961563, ACC:1.0\n",
      "Training iteration 272 loss: 0.13156402111053467, ACC:0.96875\n",
      "Training iteration 273 loss: 0.11609160155057907, ACC:0.953125\n",
      "Training iteration 274 loss: 0.08527512103319168, ACC:0.96875\n",
      "Training iteration 275 loss: 0.08639345318078995, ACC:0.953125\n",
      "Training iteration 276 loss: 0.05640222132205963, ACC:0.984375\n",
      "Training iteration 277 loss: 0.042975228279829025, ACC:0.984375\n",
      "Training iteration 278 loss: 0.015750357881188393, ACC:1.0\n",
      "Training iteration 279 loss: 0.12959693372249603, ACC:0.96875\n",
      "Training iteration 280 loss: 0.11563752591609955, ACC:0.953125\n",
      "Training iteration 281 loss: 0.05501371994614601, ACC:0.984375\n",
      "Training iteration 282 loss: 0.08234773576259613, ACC:0.96875\n",
      "Training iteration 283 loss: 0.06636367738246918, ACC:0.984375\n",
      "Training iteration 284 loss: 0.05329436808824539, ACC:0.984375\n",
      "Training iteration 285 loss: 0.11138082295656204, ACC:0.953125\n",
      "Training iteration 286 loss: 0.09830862283706665, ACC:0.984375\n",
      "Training iteration 287 loss: 0.05715823918581009, ACC:0.984375\n",
      "Training iteration 288 loss: 0.09319905191659927, ACC:0.984375\n",
      "Training iteration 289 loss: 0.06304238736629486, ACC:0.984375\n",
      "Training iteration 290 loss: 0.06316836178302765, ACC:0.984375\n",
      "Training iteration 291 loss: 0.1877608597278595, ACC:0.953125\n",
      "Training iteration 292 loss: 0.05289113521575928, ACC:0.96875\n",
      "Training iteration 293 loss: 0.039754994213581085, ACC:0.984375\n",
      "Training iteration 294 loss: 0.05076098069548607, ACC:0.984375\n",
      "Training iteration 295 loss: 0.03855930268764496, ACC:1.0\n",
      "Training iteration 296 loss: 0.02588866837322712, ACC:1.0\n",
      "Training iteration 297 loss: 0.11378230899572372, ACC:0.96875\n",
      "Training iteration 298 loss: 0.07990650832653046, ACC:0.96875\n",
      "Training iteration 299 loss: 0.10268809646368027, ACC:0.96875\n",
      "Training iteration 300 loss: 0.08815554529428482, ACC:0.953125\n",
      "Training iteration 301 loss: 0.1219414472579956, ACC:0.96875\n",
      "Training iteration 302 loss: 0.007628840859979391, ACC:1.0\n",
      "Training iteration 303 loss: 0.030086617916822433, ACC:1.0\n",
      "Training iteration 304 loss: 0.11294743418693542, ACC:0.96875\n",
      "Training iteration 305 loss: 0.02749594859778881, ACC:0.984375\n",
      "Training iteration 306 loss: 0.056894391775131226, ACC:0.96875\n",
      "Training iteration 307 loss: 0.1390736997127533, ACC:0.96875\n",
      "Training iteration 308 loss: 0.12834127247333527, ACC:0.96875\n",
      "Training iteration 309 loss: 0.07696670293807983, ACC:0.96875\n",
      "Training iteration 310 loss: 0.013269878923892975, ACC:1.0\n",
      "Training iteration 311 loss: 0.10623171180486679, ACC:0.96875\n",
      "Training iteration 312 loss: 0.011979812756180763, ACC:1.0\n",
      "Training iteration 313 loss: 0.08909354358911514, ACC:0.953125\n",
      "Training iteration 314 loss: 0.06642508506774902, ACC:0.984375\n",
      "Training iteration 315 loss: 0.11030498892068863, ACC:0.96875\n",
      "Training iteration 316 loss: 0.07971170544624329, ACC:0.96875\n",
      "Training iteration 317 loss: 0.059140291064977646, ACC:0.984375\n",
      "Training iteration 318 loss: 0.02261597476899624, ACC:1.0\n",
      "Training iteration 319 loss: 0.02785828337073326, ACC:0.984375\n",
      "Training iteration 320 loss: 0.20700743794441223, ACC:0.9375\n",
      "Training iteration 321 loss: 0.019518457353115082, ACC:1.0\n",
      "Training iteration 322 loss: 0.07244452089071274, ACC:0.984375\n",
      "Training iteration 323 loss: 0.12384578585624695, ACC:0.96875\n",
      "Training iteration 324 loss: 0.11254321038722992, ACC:0.984375\n",
      "Training iteration 325 loss: 0.04604717344045639, ACC:0.984375\n",
      "Training iteration 326 loss: 0.07528936117887497, ACC:0.984375\n",
      "Training iteration 327 loss: 0.03961704671382904, ACC:0.984375\n",
      "Training iteration 328 loss: 0.05987510457634926, ACC:0.984375\n",
      "Training iteration 329 loss: 0.06980389356613159, ACC:0.984375\n",
      "Training iteration 330 loss: 0.08673083782196045, ACC:0.96875\n",
      "Training iteration 331 loss: 0.08935826271772385, ACC:0.953125\n",
      "Training iteration 332 loss: 0.11904136836528778, ACC:0.96875\n",
      "Training iteration 333 loss: 0.13019120693206787, ACC:0.96875\n",
      "Training iteration 334 loss: 0.24277018010616302, ACC:0.921875\n",
      "Training iteration 335 loss: 0.060615211725234985, ACC:0.984375\n",
      "Training iteration 336 loss: 0.07140613347291946, ACC:0.96875\n",
      "Training iteration 337 loss: 0.05971021205186844, ACC:0.984375\n",
      "Training iteration 338 loss: 0.06613974273204803, ACC:0.984375\n",
      "Training iteration 339 loss: 0.06951794028282166, ACC:0.96875\n",
      "Training iteration 340 loss: 0.12721388041973114, ACC:0.953125\n",
      "Training iteration 341 loss: 0.08569028228521347, ACC:0.96875\n",
      "Training iteration 342 loss: 0.1260833442211151, ACC:0.953125\n",
      "Training iteration 343 loss: 0.06606936454772949, ACC:0.984375\n",
      "Training iteration 344 loss: 0.1169402152299881, ACC:0.96875\n",
      "Training iteration 345 loss: 0.1189877912402153, ACC:0.953125\n",
      "Training iteration 346 loss: 0.152630016207695, ACC:0.9375\n",
      "Training iteration 347 loss: 0.08088281005620956, ACC:0.96875\n",
      "Training iteration 348 loss: 0.12537571787834167, ACC:0.953125\n",
      "Training iteration 349 loss: 0.0586235411465168, ACC:0.96875\n",
      "Training iteration 350 loss: 0.1768270581960678, ACC:0.9375\n",
      "Training iteration 351 loss: 0.03591335564851761, ACC:1.0\n",
      "Training iteration 352 loss: 0.08767472952604294, ACC:0.984375\n",
      "Training iteration 353 loss: 0.04404538869857788, ACC:0.984375\n",
      "Training iteration 354 loss: 0.1595437079668045, ACC:0.953125\n",
      "Training iteration 355 loss: 0.10738053172826767, ACC:0.96875\n",
      "Training iteration 356 loss: 0.09023445844650269, ACC:0.96875\n",
      "Training iteration 357 loss: 0.07360328733921051, ACC:0.953125\n",
      "Training iteration 358 loss: 0.02220434695482254, ACC:1.0\n",
      "Training iteration 359 loss: 0.1848326027393341, ACC:0.921875\n",
      "Training iteration 360 loss: 0.12746073305606842, ACC:0.96875\n",
      "Training iteration 361 loss: 0.08386161178350449, ACC:0.984375\n",
      "Training iteration 362 loss: 0.073134645819664, ACC:0.984375\n",
      "Training iteration 363 loss: 0.146813303232193, ACC:0.953125\n",
      "Training iteration 364 loss: 0.05625981464982033, ACC:0.984375\n",
      "Training iteration 365 loss: 0.06459606438875198, ACC:0.96875\n",
      "Training iteration 366 loss: 0.0227703507989645, ACC:1.0\n",
      "Training iteration 367 loss: 0.03637121245265007, ACC:0.984375\n",
      "Training iteration 368 loss: 0.04860677570104599, ACC:1.0\n",
      "Training iteration 369 loss: 0.05505993589758873, ACC:1.0\n",
      "Training iteration 370 loss: 0.1257275938987732, ACC:0.9375\n",
      "Training iteration 371 loss: 0.08691640198230743, ACC:0.984375\n",
      "Training iteration 372 loss: 0.07731305807828903, ACC:0.96875\n",
      "Training iteration 373 loss: 0.04583616182208061, ACC:0.984375\n",
      "Training iteration 374 loss: 0.1962180882692337, ACC:0.9375\n",
      "Training iteration 375 loss: 0.027928324416279793, ACC:0.984375\n",
      "Training iteration 376 loss: 0.06502220779657364, ACC:0.96875\n",
      "Training iteration 377 loss: 0.03945000842213631, ACC:0.984375\n",
      "Training iteration 378 loss: 0.09037913382053375, ACC:0.96875\n",
      "Training iteration 379 loss: 0.03711511567234993, ACC:1.0\n",
      "Training iteration 380 loss: 0.05839464068412781, ACC:0.984375\n",
      "Training iteration 381 loss: 0.10156891494989395, ACC:0.953125\n",
      "Training iteration 382 loss: 0.06838466972112656, ACC:0.984375\n",
      "Training iteration 383 loss: 0.034544024616479874, ACC:1.0\n",
      "Training iteration 384 loss: 0.08399640023708344, ACC:0.984375\n",
      "Training iteration 385 loss: 0.11285703629255295, ACC:0.9375\n",
      "Training iteration 386 loss: 0.18014998733997345, ACC:0.90625\n",
      "Training iteration 387 loss: 0.05780893564224243, ACC:0.96875\n",
      "Training iteration 388 loss: 0.150875523686409, ACC:0.953125\n",
      "Training iteration 389 loss: 0.10355129837989807, ACC:0.96875\n",
      "Training iteration 390 loss: 0.13116922974586487, ACC:0.96875\n",
      "Training iteration 391 loss: 0.18475382030010223, ACC:0.96875\n",
      "Training iteration 392 loss: 0.058426037430763245, ACC:0.984375\n",
      "Training iteration 393 loss: 0.07856884598731995, ACC:0.96875\n",
      "Training iteration 394 loss: 0.06730276346206665, ACC:0.984375\n",
      "Training iteration 395 loss: 0.05413100868463516, ACC:0.96875\n",
      "Training iteration 396 loss: 0.0826365053653717, ACC:0.96875\n",
      "Training iteration 397 loss: 0.06975773721933365, ACC:0.96875\n",
      "Training iteration 398 loss: 0.050715118646621704, ACC:0.96875\n",
      "Training iteration 399 loss: 0.03381331264972687, ACC:0.984375\n",
      "Training iteration 400 loss: 0.09443851560354233, ACC:0.953125\n",
      "Training iteration 401 loss: 0.07578950375318527, ACC:0.96875\n",
      "Training iteration 402 loss: 0.024707350879907608, ACC:1.0\n",
      "Training iteration 403 loss: 0.08588850498199463, ACC:0.96875\n",
      "Training iteration 404 loss: 0.06128614768385887, ACC:1.0\n",
      "Training iteration 405 loss: 0.07503761351108551, ACC:0.96875\n",
      "Training iteration 406 loss: 0.007627772632986307, ACC:1.0\n",
      "Training iteration 407 loss: 0.034738317131996155, ACC:1.0\n",
      "Training iteration 408 loss: 0.01762014627456665, ACC:1.0\n",
      "Training iteration 409 loss: 0.07024320214986801, ACC:0.96875\n",
      "Training iteration 410 loss: 0.0851721316576004, ACC:0.984375\n",
      "Training iteration 411 loss: 0.03650866821408272, ACC:1.0\n",
      "Training iteration 412 loss: 0.03258238360285759, ACC:0.984375\n",
      "Training iteration 413 loss: 0.038025397807359695, ACC:1.0\n",
      "Training iteration 414 loss: 0.08738840371370316, ACC:0.984375\n",
      "Training iteration 415 loss: 0.09864833205938339, ACC:0.984375\n",
      "Training iteration 416 loss: 0.03271428868174553, ACC:1.0\n",
      "Training iteration 417 loss: 0.04496988281607628, ACC:0.984375\n",
      "Training iteration 418 loss: 0.07524841278791428, ACC:0.96875\n",
      "Training iteration 419 loss: 0.04655032977461815, ACC:0.984375\n",
      "Training iteration 420 loss: 0.06757445633411407, ACC:0.96875\n",
      "Training iteration 421 loss: 0.19228008389472961, ACC:0.9375\n",
      "Training iteration 422 loss: 0.08772671222686768, ACC:0.96875\n",
      "Training iteration 423 loss: 0.08301820605993271, ACC:0.953125\n",
      "Training iteration 424 loss: 0.06719659268856049, ACC:0.96875\n",
      "Training iteration 425 loss: 0.10837335139513016, ACC:0.953125\n",
      "Training iteration 426 loss: 0.09922826290130615, ACC:0.96875\n",
      "Training iteration 427 loss: 0.0650574266910553, ACC:0.984375\n",
      "Training iteration 428 loss: 0.115380197763443, ACC:0.96875\n",
      "Training iteration 429 loss: 0.03201597183942795, ACC:0.984375\n",
      "Training iteration 430 loss: 0.053569573909044266, ACC:0.984375\n",
      "Training iteration 431 loss: 0.024093594402074814, ACC:0.984375\n",
      "Training iteration 432 loss: 0.011424893513321877, ACC:1.0\n",
      "Training iteration 433 loss: 0.03701968118548393, ACC:0.984375\n",
      "Training iteration 434 loss: 0.028633955866098404, ACC:0.984375\n",
      "Training iteration 435 loss: 0.010719237849116325, ACC:1.0\n",
      "Training iteration 436 loss: 0.0058230082504451275, ACC:1.0\n",
      "Training iteration 437 loss: 0.1222805604338646, ACC:0.96875\n",
      "Training iteration 438 loss: 0.041955605149269104, ACC:0.984375\n",
      "Training iteration 439 loss: 0.1566164493560791, ACC:0.953125\n",
      "Training iteration 440 loss: 0.07132932543754578, ACC:0.984375\n",
      "Training iteration 441 loss: 0.11805229634046555, ACC:0.9375\n",
      "Training iteration 442 loss: 0.03238170966506004, ACC:1.0\n",
      "Training iteration 443 loss: 0.12845781445503235, ACC:0.96875\n",
      "Training iteration 444 loss: 0.10565952956676483, ACC:0.953125\n",
      "Training iteration 445 loss: 0.10235899686813354, ACC:0.953125\n",
      "Training iteration 446 loss: 0.15438470244407654, ACC:0.953125\n",
      "Training iteration 447 loss: 0.03412582725286484, ACC:1.0\n",
      "Training iteration 448 loss: 0.07146739959716797, ACC:0.984375\n",
      "Training iteration 449 loss: 0.05158890783786774, ACC:0.984375\n",
      "Training iteration 450 loss: 0.0875963643193245, ACC:0.984375\n",
      "Validation iteration 451 loss: 0.04264409840106964, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.16183798015117645, ACC: 0.953125\n",
      "Validation iteration 453 loss: 0.17499658465385437, ACC: 0.9375\n",
      "Validation iteration 454 loss: 0.16489127278327942, ACC: 0.953125\n",
      "Validation iteration 455 loss: 0.04981127008795738, ACC: 0.984375\n",
      "Validation iteration 456 loss: 0.08896935731172562, ACC: 0.953125\n",
      "Validation iteration 457 loss: 0.0671503022313118, ACC: 0.984375\n",
      "Validation iteration 458 loss: 0.030568161979317665, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.03945267200469971, ACC: 0.984375\n",
      "Validation iteration 460 loss: 0.07975296676158905, ACC: 0.96875\n",
      "Validation iteration 461 loss: 0.08021131157875061, ACC: 0.96875\n",
      "Validation iteration 462 loss: 0.09346503764390945, ACC: 0.96875\n",
      "Validation iteration 463 loss: 0.034404851496219635, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.05760793387889862, ACC: 0.96875\n",
      "Validation iteration 465 loss: 0.14172890782356262, ACC: 0.953125\n",
      "Validation iteration 466 loss: 0.04138367623090744, ACC: 0.984375\n",
      "Validation iteration 467 loss: 0.05645841732621193, ACC: 0.984375\n",
      "Validation iteration 468 loss: 0.04663741588592529, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.036084793508052826, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.07131336629390717, ACC: 0.96875\n",
      "Validation iteration 471 loss: 0.10348574817180634, ACC: 0.96875\n",
      "Validation iteration 472 loss: 0.11089002341032028, ACC: 0.9375\n",
      "Validation iteration 473 loss: 0.04963761568069458, ACC: 0.984375\n",
      "Validation iteration 474 loss: 0.03476113826036453, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.04590889811515808, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.18214590847492218, ACC: 0.96875\n",
      "Validation iteration 477 loss: 0.14080052077770233, ACC: 0.96875\n",
      "Validation iteration 478 loss: 0.08866617828607559, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.09965664148330688, ACC: 0.96875\n",
      "Validation iteration 480 loss: 0.053045060485601425, ACC: 0.984375\n",
      "Validation iteration 481 loss: 0.05423682928085327, ACC: 0.984375\n",
      "Validation iteration 482 loss: 0.10733962804079056, ACC: 0.953125\n",
      "Validation iteration 483 loss: 0.247918963432312, ACC: 0.953125\n",
      "Validation iteration 484 loss: 0.03184453025460243, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.11493874341249466, ACC: 0.96875\n",
      "Validation iteration 486 loss: 0.16219517588615417, ACC: 0.90625\n",
      "Validation iteration 487 loss: 0.0221847053617239, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.041175659745931625, ACC: 0.984375\n",
      "Validation iteration 489 loss: 0.02435406483709812, ACC: 1.0\n",
      "Validation iteration 490 loss: 0.05037504434585571, ACC: 0.96875\n",
      "Validation iteration 491 loss: 0.1017584353685379, ACC: 0.984375\n",
      "Validation iteration 492 loss: 0.19879457354545593, ACC: 0.9375\n",
      "Validation iteration 493 loss: 0.041262172162532806, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.042184993624687195, ACC: 0.984375\n",
      "Validation iteration 495 loss: 0.032112542539834976, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.1603965014219284, ACC: 0.953125\n",
      "Validation iteration 497 loss: 0.024522406980395317, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.1258729100227356, ACC: 0.96875\n",
      "Validation iteration 499 loss: 0.19184701144695282, ACC: 0.953125\n",
      "Validation iteration 500 loss: 0.23074950277805328, ACC: 0.921875\n",
      "-- Epoch 5 done -- Train loss: 0.08247860744699008, train ACC: 0.9736805555555555, val loss: 0.0894886501133442, val ACC: 0.9734375\n",
      "<--- 1247.2402815818787 seconds --->\n",
      "Training iteration 1 loss: 0.10786516964435577, ACC:0.953125\n",
      "Training iteration 2 loss: 0.025268344208598137, ACC:0.984375\n",
      "Training iteration 3 loss: 0.012516727671027184, ACC:1.0\n",
      "Training iteration 4 loss: 0.010352815501391888, ACC:1.0\n",
      "Training iteration 5 loss: 0.034859027713537216, ACC:1.0\n",
      "Training iteration 6 loss: 0.11883179098367691, ACC:0.984375\n",
      "Training iteration 7 loss: 0.0693446546792984, ACC:0.96875\n",
      "Training iteration 8 loss: 0.0152932433411479, ACC:1.0\n",
      "Training iteration 9 loss: 0.010206494480371475, ACC:1.0\n",
      "Training iteration 10 loss: 0.01200362853705883, ACC:1.0\n",
      "Training iteration 11 loss: 0.024009281769394875, ACC:1.0\n",
      "Training iteration 12 loss: 0.12234590947628021, ACC:0.96875\n",
      "Training iteration 13 loss: 0.0668763518333435, ACC:0.953125\n",
      "Training iteration 14 loss: 0.05738506093621254, ACC:0.96875\n",
      "Training iteration 15 loss: 0.06934373825788498, ACC:0.984375\n",
      "Training iteration 16 loss: 0.08301494270563126, ACC:0.96875\n",
      "Training iteration 17 loss: 0.05842217057943344, ACC:0.984375\n",
      "Training iteration 18 loss: 0.044709257781505585, ACC:0.984375\n",
      "Training iteration 19 loss: 0.06659017503261566, ACC:0.984375\n",
      "Training iteration 20 loss: 0.07264133542776108, ACC:0.96875\n",
      "Training iteration 21 loss: 0.08035876601934433, ACC:0.953125\n",
      "Training iteration 22 loss: 0.07836580276489258, ACC:0.96875\n",
      "Training iteration 23 loss: 0.03390464186668396, ACC:0.984375\n",
      "Training iteration 24 loss: 0.06871508061885834, ACC:0.96875\n",
      "Training iteration 25 loss: 0.1758168786764145, ACC:0.953125\n",
      "Training iteration 26 loss: 0.04367106035351753, ACC:0.984375\n",
      "Training iteration 27 loss: 0.06952845305204391, ACC:0.984375\n",
      "Training iteration 28 loss: 0.17227604985237122, ACC:0.9375\n",
      "Training iteration 29 loss: 0.0994432121515274, ACC:0.984375\n",
      "Training iteration 30 loss: 0.10769422352313995, ACC:0.953125\n",
      "Training iteration 31 loss: 0.08252591639757156, ACC:0.953125\n",
      "Training iteration 32 loss: 0.07064837962388992, ACC:0.96875\n",
      "Training iteration 33 loss: 0.08251763135194778, ACC:0.984375\n",
      "Training iteration 34 loss: 0.05734929442405701, ACC:0.984375\n",
      "Training iteration 35 loss: 0.07283273339271545, ACC:0.984375\n",
      "Training iteration 36 loss: 0.11233292520046234, ACC:0.984375\n",
      "Training iteration 37 loss: 0.025738555938005447, ACC:1.0\n",
      "Training iteration 38 loss: 0.01607162319123745, ACC:1.0\n",
      "Training iteration 39 loss: 0.044640205800533295, ACC:0.984375\n",
      "Training iteration 40 loss: 0.03749296814203262, ACC:1.0\n",
      "Training iteration 41 loss: 0.06768906116485596, ACC:0.96875\n",
      "Training iteration 42 loss: 0.04074127599596977, ACC:0.96875\n",
      "Training iteration 43 loss: 0.011886529624462128, ACC:1.0\n",
      "Training iteration 44 loss: 0.25962522625923157, ACC:0.921875\n",
      "Training iteration 45 loss: 0.06965427100658417, ACC:0.984375\n",
      "Training iteration 46 loss: 0.011309015564620495, ACC:1.0\n",
      "Training iteration 47 loss: 0.03940761461853981, ACC:1.0\n",
      "Training iteration 48 loss: 0.13218802213668823, ACC:0.96875\n",
      "Training iteration 49 loss: 0.03303198888897896, ACC:1.0\n",
      "Training iteration 50 loss: 0.01647363044321537, ACC:1.0\n",
      "Training iteration 51 loss: 0.038359012454748154, ACC:0.984375\n",
      "Training iteration 52 loss: 0.1529623121023178, ACC:0.953125\n",
      "Training iteration 53 loss: 0.05107376351952553, ACC:0.984375\n",
      "Training iteration 54 loss: 0.13072699308395386, ACC:0.96875\n",
      "Training iteration 55 loss: 0.1381329596042633, ACC:0.953125\n",
      "Training iteration 56 loss: 0.13545404374599457, ACC:0.953125\n",
      "Training iteration 57 loss: 0.12866802513599396, ACC:0.953125\n",
      "Training iteration 58 loss: 0.19505804777145386, ACC:0.9375\n",
      "Training iteration 59 loss: 0.07357403635978699, ACC:0.984375\n",
      "Training iteration 60 loss: 0.11649595201015472, ACC:0.9375\n",
      "Training iteration 61 loss: 0.22863246500492096, ACC:0.890625\n",
      "Training iteration 62 loss: 0.14101918041706085, ACC:0.90625\n",
      "Training iteration 63 loss: 0.026906268671154976, ACC:1.0\n",
      "Training iteration 64 loss: 0.022781843319535255, ACC:1.0\n",
      "Training iteration 65 loss: 0.1699777990579605, ACC:0.953125\n",
      "Training iteration 66 loss: 0.023387789726257324, ACC:1.0\n",
      "Training iteration 67 loss: 0.1320447474718094, ACC:0.953125\n",
      "Training iteration 68 loss: 0.03185630589723587, ACC:1.0\n",
      "Training iteration 69 loss: 0.08404634147882462, ACC:0.984375\n",
      "Training iteration 70 loss: 0.062048040330410004, ACC:0.984375\n",
      "Training iteration 71 loss: 0.08244425803422928, ACC:0.953125\n",
      "Training iteration 72 loss: 0.1568853110074997, ACC:0.953125\n",
      "Training iteration 73 loss: 0.07464419305324554, ACC:0.96875\n",
      "Training iteration 74 loss: 0.09915219247341156, ACC:0.984375\n",
      "Training iteration 75 loss: 0.09792295098304749, ACC:0.96875\n",
      "Training iteration 76 loss: 0.0740632712841034, ACC:0.953125\n",
      "Training iteration 77 loss: 0.0773172602057457, ACC:0.96875\n",
      "Training iteration 78 loss: 0.06925904750823975, ACC:0.984375\n",
      "Training iteration 79 loss: 0.12793947756290436, ACC:0.9375\n",
      "Training iteration 80 loss: 0.10291717946529388, ACC:0.984375\n",
      "Training iteration 81 loss: 0.06267528235912323, ACC:0.984375\n",
      "Training iteration 82 loss: 0.06273392587900162, ACC:0.984375\n",
      "Training iteration 83 loss: 0.02124861627817154, ACC:1.0\n",
      "Training iteration 84 loss: 0.2047504186630249, ACC:0.921875\n",
      "Training iteration 85 loss: 0.07517455518245697, ACC:0.96875\n",
      "Training iteration 86 loss: 0.0928088054060936, ACC:0.953125\n",
      "Training iteration 87 loss: 0.09607971459627151, ACC:0.984375\n",
      "Training iteration 88 loss: 0.06831393390893936, ACC:1.0\n",
      "Training iteration 89 loss: 0.12431623786687851, ACC:0.953125\n",
      "Training iteration 90 loss: 0.018402136862277985, ACC:1.0\n",
      "Training iteration 91 loss: 0.08549480885267258, ACC:0.984375\n",
      "Training iteration 92 loss: 0.0791640654206276, ACC:0.984375\n",
      "Training iteration 93 loss: 0.09818646311759949, ACC:0.984375\n",
      "Training iteration 94 loss: 0.17441348731517792, ACC:0.953125\n",
      "Training iteration 95 loss: 0.1667480766773224, ACC:0.953125\n",
      "Training iteration 96 loss: 0.09493406116962433, ACC:0.9375\n",
      "Training iteration 97 loss: 0.15273955464363098, ACC:0.9375\n",
      "Training iteration 98 loss: 0.17860503494739532, ACC:0.921875\n",
      "Training iteration 99 loss: 0.10692936927080154, ACC:0.96875\n",
      "Training iteration 100 loss: 0.04457757994532585, ACC:0.984375\n",
      "Training iteration 101 loss: 0.10598699003458023, ACC:0.953125\n",
      "Training iteration 102 loss: 0.07539959996938705, ACC:0.984375\n",
      "Training iteration 103 loss: 0.1681857705116272, ACC:0.9375\n",
      "Training iteration 104 loss: 0.08100999146699905, ACC:0.96875\n",
      "Training iteration 105 loss: 0.07861625403165817, ACC:0.984375\n",
      "Training iteration 106 loss: 0.17725712060928345, ACC:0.9375\n",
      "Training iteration 107 loss: 0.11523832380771637, ACC:0.9375\n",
      "Training iteration 108 loss: 0.04000407084822655, ACC:1.0\n",
      "Training iteration 109 loss: 0.02552681788802147, ACC:1.0\n",
      "Training iteration 110 loss: 0.03382943943142891, ACC:0.984375\n",
      "Training iteration 111 loss: 0.19487664103507996, ACC:0.9375\n",
      "Training iteration 112 loss: 0.02705090120434761, ACC:1.0\n",
      "Training iteration 113 loss: 0.09913479536771774, ACC:0.984375\n",
      "Training iteration 114 loss: 0.04478832334280014, ACC:1.0\n",
      "Training iteration 115 loss: 0.02784794382750988, ACC:1.0\n",
      "Training iteration 116 loss: 0.2507183849811554, ACC:0.90625\n",
      "Training iteration 117 loss: 0.054180629551410675, ACC:0.984375\n",
      "Training iteration 118 loss: 0.065283864736557, ACC:0.984375\n",
      "Training iteration 119 loss: 0.02210751175880432, ACC:1.0\n",
      "Training iteration 120 loss: 0.08316852152347565, ACC:0.984375\n",
      "Training iteration 121 loss: 0.11255653947591782, ACC:0.953125\n",
      "Training iteration 122 loss: 0.1942715346813202, ACC:0.921875\n",
      "Training iteration 123 loss: 0.11469262838363647, ACC:0.953125\n",
      "Training iteration 124 loss: 0.021964306011795998, ACC:1.0\n",
      "Training iteration 125 loss: 0.12952518463134766, ACC:0.953125\n",
      "Training iteration 126 loss: 0.07973510026931763, ACC:0.984375\n",
      "Training iteration 127 loss: 0.1645319014787674, ACC:0.9375\n",
      "Training iteration 128 loss: 0.07322591543197632, ACC:0.984375\n",
      "Training iteration 129 loss: 0.11875158548355103, ACC:0.984375\n",
      "Training iteration 130 loss: 0.02842560037970543, ACC:1.0\n",
      "Training iteration 131 loss: 0.13615299761295319, ACC:0.984375\n",
      "Training iteration 132 loss: 0.14261983335018158, ACC:0.953125\n",
      "Training iteration 133 loss: 0.04100792855024338, ACC:1.0\n",
      "Training iteration 134 loss: 0.0653083473443985, ACC:0.984375\n",
      "Training iteration 135 loss: 0.008031744509935379, ACC:1.0\n",
      "Training iteration 136 loss: 0.017229920253157616, ACC:0.984375\n",
      "Training iteration 137 loss: 0.10847807675600052, ACC:0.953125\n",
      "Training iteration 138 loss: 0.04452451318502426, ACC:0.96875\n",
      "Training iteration 139 loss: 0.028197700157761574, ACC:1.0\n",
      "Training iteration 140 loss: 0.04172923415899277, ACC:0.984375\n",
      "Training iteration 141 loss: 0.13982032239437103, ACC:0.953125\n",
      "Training iteration 142 loss: 0.054929569363594055, ACC:0.96875\n",
      "Training iteration 143 loss: 0.06811248511075974, ACC:0.96875\n",
      "Training iteration 144 loss: 0.09252044558525085, ACC:0.953125\n",
      "Training iteration 145 loss: 0.07517075538635254, ACC:0.96875\n",
      "Training iteration 146 loss: 0.0903155580163002, ACC:0.953125\n",
      "Training iteration 147 loss: 0.02586115524172783, ACC:0.984375\n",
      "Training iteration 148 loss: 0.04439366236329079, ACC:0.984375\n",
      "Training iteration 149 loss: 0.06998250633478165, ACC:0.96875\n",
      "Training iteration 150 loss: 0.07272688299417496, ACC:0.984375\n",
      "Training iteration 151 loss: 0.04239412397146225, ACC:0.96875\n",
      "Training iteration 152 loss: 0.14983077347278595, ACC:0.96875\n",
      "Training iteration 153 loss: 0.014386782422661781, ACC:1.0\n",
      "Training iteration 154 loss: 0.05280717834830284, ACC:0.984375\n",
      "Training iteration 155 loss: 0.03203321993350983, ACC:1.0\n",
      "Training iteration 156 loss: 0.1074354350566864, ACC:0.96875\n",
      "Training iteration 157 loss: 0.12464092671871185, ACC:0.9375\n",
      "Training iteration 158 loss: 0.11357954144477844, ACC:0.9375\n",
      "Training iteration 159 loss: 0.11291934549808502, ACC:0.9375\n",
      "Training iteration 160 loss: 0.13499362766742706, ACC:0.9375\n",
      "Training iteration 161 loss: 0.12305918335914612, ACC:0.953125\n",
      "Training iteration 162 loss: 0.1151413694024086, ACC:0.9375\n",
      "Training iteration 163 loss: 0.10216036438941956, ACC:0.953125\n",
      "Training iteration 164 loss: 0.02846371755003929, ACC:1.0\n",
      "Training iteration 165 loss: 0.05075507611036301, ACC:1.0\n",
      "Training iteration 166 loss: 0.03964788094162941, ACC:0.984375\n",
      "Training iteration 167 loss: 0.03550724312663078, ACC:0.984375\n",
      "Training iteration 168 loss: 0.08997301757335663, ACC:0.96875\n",
      "Training iteration 169 loss: 0.14285889267921448, ACC:0.96875\n",
      "Training iteration 170 loss: 0.12209001183509827, ACC:0.953125\n",
      "Training iteration 171 loss: 0.12715601921081543, ACC:0.9375\n",
      "Training iteration 172 loss: 0.044887933880090714, ACC:1.0\n",
      "Training iteration 173 loss: 0.0650387778878212, ACC:0.984375\n",
      "Training iteration 174 loss: 0.12149843573570251, ACC:0.96875\n",
      "Training iteration 175 loss: 0.039737533777952194, ACC:1.0\n",
      "Training iteration 176 loss: 0.07267430424690247, ACC:0.953125\n",
      "Training iteration 177 loss: 0.08292306959629059, ACC:0.96875\n",
      "Training iteration 178 loss: 0.09788213670253754, ACC:0.96875\n",
      "Training iteration 179 loss: 0.1024322658777237, ACC:0.96875\n",
      "Training iteration 180 loss: 0.1281731128692627, ACC:0.9375\n",
      "Training iteration 181 loss: 0.09523734450340271, ACC:0.96875\n",
      "Training iteration 182 loss: 0.1419595181941986, ACC:0.921875\n",
      "Training iteration 183 loss: 0.12659712135791779, ACC:0.953125\n",
      "Training iteration 184 loss: 0.12785053253173828, ACC:0.984375\n",
      "Training iteration 185 loss: 0.11469975858926773, ACC:0.96875\n",
      "Training iteration 186 loss: 0.08927906304597855, ACC:0.96875\n",
      "Training iteration 187 loss: 0.08741709589958191, ACC:0.953125\n",
      "Training iteration 188 loss: 0.07688549160957336, ACC:0.953125\n",
      "Training iteration 189 loss: 0.11532405763864517, ACC:0.96875\n",
      "Training iteration 190 loss: 0.1274978518486023, ACC:0.953125\n",
      "Training iteration 191 loss: 0.2500004470348358, ACC:0.90625\n",
      "Training iteration 192 loss: 0.03333178907632828, ACC:1.0\n",
      "Training iteration 193 loss: 0.12381061166524887, ACC:0.96875\n",
      "Training iteration 194 loss: 0.05494953691959381, ACC:1.0\n",
      "Training iteration 195 loss: 0.11238943040370941, ACC:0.96875\n",
      "Training iteration 196 loss: 0.05828816816210747, ACC:0.984375\n",
      "Training iteration 197 loss: 0.04161053150892258, ACC:0.984375\n",
      "Training iteration 198 loss: 0.09525517374277115, ACC:0.96875\n",
      "Training iteration 199 loss: 0.03116973675787449, ACC:1.0\n",
      "Training iteration 200 loss: 0.08556782454252243, ACC:0.96875\n",
      "Training iteration 201 loss: 0.06930845230817795, ACC:0.953125\n",
      "Training iteration 202 loss: 0.05569148063659668, ACC:0.984375\n",
      "Training iteration 203 loss: 0.08621250092983246, ACC:0.984375\n",
      "Training iteration 204 loss: 0.06426352262496948, ACC:0.96875\n",
      "Training iteration 205 loss: 0.02901385724544525, ACC:1.0\n",
      "Training iteration 206 loss: 0.0354749858379364, ACC:1.0\n",
      "Training iteration 207 loss: 0.02826230600476265, ACC:0.984375\n",
      "Training iteration 208 loss: 0.08178094029426575, ACC:0.953125\n",
      "Training iteration 209 loss: 0.028859201818704605, ACC:1.0\n",
      "Training iteration 210 loss: 0.03861479461193085, ACC:0.96875\n",
      "Training iteration 211 loss: 0.07663945853710175, ACC:0.984375\n",
      "Training iteration 212 loss: 0.019973542541265488, ACC:1.0\n",
      "Training iteration 213 loss: 0.06592047959566116, ACC:0.96875\n",
      "Training iteration 214 loss: 0.027461476624011993, ACC:1.0\n",
      "Training iteration 215 loss: 0.014832008630037308, ACC:1.0\n",
      "Training iteration 216 loss: 0.07552752643823624, ACC:0.96875\n",
      "Training iteration 217 loss: 0.06747473776340485, ACC:0.984375\n",
      "Training iteration 218 loss: 0.018182191997766495, ACC:1.0\n",
      "Training iteration 219 loss: 0.013015969656407833, ACC:1.0\n",
      "Training iteration 220 loss: 0.022351989522576332, ACC:1.0\n",
      "Training iteration 221 loss: 0.054887499660253525, ACC:0.984375\n",
      "Training iteration 222 loss: 0.08322479575872421, ACC:0.96875\n",
      "Training iteration 223 loss: 0.021955572068691254, ACC:1.0\n",
      "Training iteration 224 loss: 0.026077810674905777, ACC:1.0\n",
      "Training iteration 225 loss: 0.04560873284935951, ACC:0.96875\n",
      "Training iteration 226 loss: 0.021098706871271133, ACC:1.0\n",
      "Training iteration 227 loss: 0.14698632061481476, ACC:0.953125\n",
      "Training iteration 228 loss: 0.09562243521213531, ACC:0.96875\n",
      "Training iteration 229 loss: 0.0436440035700798, ACC:1.0\n",
      "Training iteration 230 loss: 0.17199425399303436, ACC:0.921875\n",
      "Training iteration 231 loss: 0.05989581346511841, ACC:0.984375\n",
      "Training iteration 232 loss: 0.06493581086397171, ACC:0.96875\n",
      "Training iteration 233 loss: 0.055594392120838165, ACC:0.96875\n",
      "Training iteration 234 loss: 0.16693036258220673, ACC:0.9375\n",
      "Training iteration 235 loss: 0.0650351345539093, ACC:0.984375\n",
      "Training iteration 236 loss: 0.12658226490020752, ACC:0.953125\n",
      "Training iteration 237 loss: 0.08555570244789124, ACC:0.96875\n",
      "Training iteration 238 loss: 0.040044426918029785, ACC:1.0\n",
      "Training iteration 239 loss: 0.02543497458100319, ACC:1.0\n",
      "Training iteration 240 loss: 0.054849158972501755, ACC:0.984375\n",
      "Training iteration 241 loss: 0.09191564470529556, ACC:0.96875\n",
      "Training iteration 242 loss: 0.1043095737695694, ACC:0.96875\n",
      "Training iteration 243 loss: 0.049069881439208984, ACC:0.96875\n",
      "Training iteration 244 loss: 0.04988016188144684, ACC:0.984375\n",
      "Training iteration 245 loss: 0.028001397848129272, ACC:1.0\n",
      "Training iteration 246 loss: 0.009033538401126862, ACC:1.0\n",
      "Training iteration 247 loss: 0.09026510268449783, ACC:0.96875\n",
      "Training iteration 248 loss: 0.139926940202713, ACC:0.96875\n",
      "Training iteration 249 loss: 0.18620821833610535, ACC:0.9375\n",
      "Training iteration 250 loss: 0.04681834951043129, ACC:0.984375\n",
      "Training iteration 251 loss: 0.022839227691292763, ACC:1.0\n",
      "Training iteration 252 loss: 0.078849658370018, ACC:0.96875\n",
      "Training iteration 253 loss: 0.110055111348629, ACC:0.9375\n",
      "Training iteration 254 loss: 0.024616938084363937, ACC:0.984375\n",
      "Training iteration 255 loss: 0.027719218283891678, ACC:1.0\n",
      "Training iteration 256 loss: 0.027491897344589233, ACC:1.0\n",
      "Training iteration 257 loss: 0.13255742192268372, ACC:0.96875\n",
      "Training iteration 258 loss: 0.07139213383197784, ACC:0.984375\n",
      "Training iteration 259 loss: 0.08337058126926422, ACC:0.96875\n",
      "Training iteration 260 loss: 0.04729870706796646, ACC:1.0\n",
      "Training iteration 261 loss: 0.1949358433485031, ACC:0.9375\n",
      "Training iteration 262 loss: 0.10819205641746521, ACC:0.953125\n",
      "Training iteration 263 loss: 0.14911626279354095, ACC:0.9375\n",
      "Training iteration 264 loss: 0.09133797883987427, ACC:0.953125\n",
      "Training iteration 265 loss: 0.052450817078351974, ACC:0.984375\n",
      "Training iteration 266 loss: 0.06642419844865799, ACC:0.984375\n",
      "Training iteration 267 loss: 0.018926497548818588, ACC:1.0\n",
      "Training iteration 268 loss: 0.11520911008119583, ACC:0.9375\n",
      "Training iteration 269 loss: 0.059610120952129364, ACC:0.984375\n",
      "Training iteration 270 loss: 0.0676470473408699, ACC:0.984375\n",
      "Training iteration 271 loss: 0.13570061326026917, ACC:0.96875\n",
      "Training iteration 272 loss: 0.0902571901679039, ACC:0.953125\n",
      "Training iteration 273 loss: 0.09364254027605057, ACC:0.96875\n",
      "Training iteration 274 loss: 0.07510510832071304, ACC:0.96875\n",
      "Training iteration 275 loss: 0.04686928912997246, ACC:1.0\n",
      "Training iteration 276 loss: 0.07210148870944977, ACC:0.984375\n",
      "Training iteration 277 loss: 0.15581251680850983, ACC:0.953125\n",
      "Training iteration 278 loss: 0.06616762280464172, ACC:0.96875\n",
      "Training iteration 279 loss: 0.12945522367954254, ACC:0.953125\n",
      "Training iteration 280 loss: 0.06911488622426987, ACC:0.984375\n",
      "Training iteration 281 loss: 0.05354028567671776, ACC:0.984375\n",
      "Training iteration 282 loss: 0.1178780123591423, ACC:0.953125\n",
      "Training iteration 283 loss: 0.04368682950735092, ACC:0.984375\n",
      "Training iteration 284 loss: 0.14416512846946716, ACC:0.9375\n",
      "Training iteration 285 loss: 0.02831868641078472, ACC:1.0\n",
      "Training iteration 286 loss: 0.021010737866163254, ACC:1.0\n",
      "Training iteration 287 loss: 0.07529360055923462, ACC:0.984375\n",
      "Training iteration 288 loss: 0.021749669685959816, ACC:1.0\n",
      "Training iteration 289 loss: 0.023849481716752052, ACC:0.984375\n",
      "Training iteration 290 loss: 0.1553337126970291, ACC:0.9375\n",
      "Training iteration 291 loss: 0.07025673240423203, ACC:0.984375\n",
      "Training iteration 292 loss: 0.04764055088162422, ACC:0.984375\n",
      "Training iteration 293 loss: 0.03950561210513115, ACC:0.984375\n",
      "Training iteration 294 loss: 0.023921554908156395, ACC:1.0\n",
      "Training iteration 295 loss: 0.13344484567642212, ACC:0.953125\n",
      "Training iteration 296 loss: 0.1536722183227539, ACC:0.9375\n",
      "Training iteration 297 loss: 0.11585135012865067, ACC:0.96875\n",
      "Training iteration 298 loss: 0.06926774978637695, ACC:0.984375\n",
      "Training iteration 299 loss: 0.10173134505748749, ACC:0.9375\n",
      "Training iteration 300 loss: 0.10345672070980072, ACC:0.96875\n",
      "Training iteration 301 loss: 0.10888396203517914, ACC:0.9375\n",
      "Training iteration 302 loss: 0.12463171780109406, ACC:0.9375\n",
      "Training iteration 303 loss: 0.10158377140760422, ACC:0.984375\n",
      "Training iteration 304 loss: 0.05879587307572365, ACC:0.984375\n",
      "Training iteration 305 loss: 0.07631009817123413, ACC:0.96875\n",
      "Training iteration 306 loss: 0.05950608476996422, ACC:0.984375\n",
      "Training iteration 307 loss: 0.052848562598228455, ACC:0.984375\n",
      "Training iteration 308 loss: 0.08038076758384705, ACC:0.984375\n",
      "Training iteration 309 loss: 0.050241488963365555, ACC:1.0\n",
      "Training iteration 310 loss: 0.13713331520557404, ACC:0.9375\n",
      "Training iteration 311 loss: 0.07902069389820099, ACC:0.984375\n",
      "Training iteration 312 loss: 0.033151645213365555, ACC:1.0\n",
      "Training iteration 313 loss: 0.053330764174461365, ACC:1.0\n",
      "Training iteration 314 loss: 0.04893838241696358, ACC:0.984375\n",
      "Training iteration 315 loss: 0.10955996066331863, ACC:0.953125\n",
      "Training iteration 316 loss: 0.016668250784277916, ACC:1.0\n",
      "Training iteration 317 loss: 0.04812290146946907, ACC:0.984375\n",
      "Training iteration 318 loss: 0.022901155054569244, ACC:0.984375\n",
      "Training iteration 319 loss: 0.014748292975127697, ACC:1.0\n",
      "Training iteration 320 loss: 0.11483212560415268, ACC:0.96875\n",
      "Training iteration 321 loss: 0.07344760745763779, ACC:0.96875\n",
      "Training iteration 322 loss: 0.10310448706150055, ACC:0.984375\n",
      "Training iteration 323 loss: 0.051510028541088104, ACC:0.984375\n",
      "Training iteration 324 loss: 0.051449526101350784, ACC:0.984375\n",
      "Training iteration 325 loss: 0.11053753644227982, ACC:0.9375\n",
      "Training iteration 326 loss: 0.04572892561554909, ACC:0.984375\n",
      "Training iteration 327 loss: 0.07683295011520386, ACC:0.96875\n",
      "Training iteration 328 loss: 0.08415867388248444, ACC:0.96875\n",
      "Training iteration 329 loss: 0.05066081881523132, ACC:1.0\n",
      "Training iteration 330 loss: 0.12663060426712036, ACC:0.984375\n",
      "Training iteration 331 loss: 0.028978299349546432, ACC:1.0\n",
      "Training iteration 332 loss: 0.11609353125095367, ACC:0.953125\n",
      "Training iteration 333 loss: 0.05810432881116867, ACC:0.984375\n",
      "Training iteration 334 loss: 0.02119617350399494, ACC:1.0\n",
      "Training iteration 335 loss: 0.173603817820549, ACC:0.921875\n",
      "Training iteration 336 loss: 0.04237275943160057, ACC:1.0\n",
      "Training iteration 337 loss: 0.09889193624258041, ACC:0.96875\n",
      "Training iteration 338 loss: 0.10129900276660919, ACC:0.96875\n",
      "Training iteration 339 loss: 0.11364954710006714, ACC:0.9375\n",
      "Training iteration 340 loss: 0.10584067553281784, ACC:0.953125\n",
      "Training iteration 341 loss: 0.028508156538009644, ACC:1.0\n",
      "Training iteration 342 loss: 0.0390431247651577, ACC:1.0\n",
      "Training iteration 343 loss: 0.01207070890814066, ACC:1.0\n",
      "Training iteration 344 loss: 0.05598849058151245, ACC:0.984375\n",
      "Training iteration 345 loss: 0.09880230575799942, ACC:0.953125\n",
      "Training iteration 346 loss: 0.09069272130727768, ACC:0.953125\n",
      "Training iteration 347 loss: 0.253743439912796, ACC:0.921875\n",
      "Training iteration 348 loss: 0.13749131560325623, ACC:0.953125\n",
      "Training iteration 349 loss: 0.09953822195529938, ACC:0.984375\n",
      "Training iteration 350 loss: 0.18239447474479675, ACC:0.953125\n",
      "Training iteration 351 loss: 0.04499569535255432, ACC:0.984375\n",
      "Training iteration 352 loss: 0.057599425315856934, ACC:0.984375\n",
      "Training iteration 353 loss: 0.11504074931144714, ACC:0.96875\n",
      "Training iteration 354 loss: 0.01692708395421505, ACC:1.0\n",
      "Training iteration 355 loss: 0.13063830137252808, ACC:0.953125\n",
      "Training iteration 356 loss: 0.11591760814189911, ACC:0.953125\n",
      "Training iteration 357 loss: 0.0757170245051384, ACC:0.96875\n",
      "Training iteration 358 loss: 0.07218839228153229, ACC:0.984375\n",
      "Training iteration 359 loss: 0.056189727038145065, ACC:0.984375\n",
      "Training iteration 360 loss: 0.0642152950167656, ACC:1.0\n",
      "Training iteration 361 loss: 0.1006852239370346, ACC:0.953125\n",
      "Training iteration 362 loss: 0.10121026635169983, ACC:0.953125\n",
      "Training iteration 363 loss: 0.05107543617486954, ACC:0.984375\n",
      "Training iteration 364 loss: 0.042471565306186676, ACC:1.0\n",
      "Training iteration 365 loss: 0.0432744137942791, ACC:0.984375\n",
      "Training iteration 366 loss: 0.06590109318494797, ACC:0.96875\n",
      "Training iteration 367 loss: 0.10912453383207321, ACC:0.921875\n",
      "Training iteration 368 loss: 0.02234460972249508, ACC:1.0\n",
      "Training iteration 369 loss: 0.03545340150594711, ACC:1.0\n",
      "Training iteration 370 loss: 0.07408501952886581, ACC:0.96875\n",
      "Training iteration 371 loss: 0.11478297412395477, ACC:0.9375\n",
      "Training iteration 372 loss: 0.04435912147164345, ACC:0.984375\n",
      "Training iteration 373 loss: 0.1369950920343399, ACC:0.953125\n",
      "Training iteration 374 loss: 0.1083194836974144, ACC:0.96875\n",
      "Training iteration 375 loss: 0.09391383826732635, ACC:0.96875\n",
      "Training iteration 376 loss: 0.0877506360411644, ACC:0.953125\n",
      "Training iteration 377 loss: 0.05608297139406204, ACC:0.96875\n",
      "Training iteration 378 loss: 0.05311403051018715, ACC:0.984375\n",
      "Training iteration 379 loss: 0.03906292840838432, ACC:1.0\n",
      "Training iteration 380 loss: 0.12225615978240967, ACC:0.96875\n",
      "Training iteration 381 loss: 0.1240776926279068, ACC:0.984375\n",
      "Training iteration 382 loss: 0.07845931500196457, ACC:0.96875\n",
      "Training iteration 383 loss: 0.04150911793112755, ACC:0.984375\n",
      "Training iteration 384 loss: 0.09161684662103653, ACC:0.953125\n",
      "Training iteration 385 loss: 0.15484793484210968, ACC:0.953125\n",
      "Training iteration 386 loss: 0.030300740152597427, ACC:1.0\n",
      "Training iteration 387 loss: 0.09841543436050415, ACC:0.96875\n",
      "Training iteration 388 loss: 0.16871733963489532, ACC:0.9375\n",
      "Training iteration 389 loss: 0.06575528532266617, ACC:0.984375\n",
      "Training iteration 390 loss: 0.0501270517706871, ACC:0.984375\n",
      "Training iteration 391 loss: 0.03083365596830845, ACC:0.984375\n",
      "Training iteration 392 loss: 0.08074498176574707, ACC:0.953125\n",
      "Training iteration 393 loss: 0.045463647693395615, ACC:1.0\n",
      "Training iteration 394 loss: 0.07299186289310455, ACC:0.984375\n",
      "Training iteration 395 loss: 0.10324949026107788, ACC:0.96875\n",
      "Training iteration 396 loss: 0.11401504278182983, ACC:0.953125\n",
      "Training iteration 397 loss: 0.10760518908500671, ACC:0.953125\n",
      "Training iteration 398 loss: 0.0407458059489727, ACC:0.984375\n",
      "Training iteration 399 loss: 0.022071069106459618, ACC:1.0\n",
      "Training iteration 400 loss: 0.049911487847566605, ACC:0.984375\n",
      "Training iteration 401 loss: 0.06551855802536011, ACC:0.984375\n",
      "Training iteration 402 loss: 0.1257191151380539, ACC:0.96875\n",
      "Training iteration 403 loss: 0.032857153564691544, ACC:0.984375\n",
      "Training iteration 404 loss: 0.05242420732975006, ACC:0.984375\n",
      "Training iteration 405 loss: 0.09860150516033173, ACC:0.96875\n",
      "Training iteration 406 loss: 0.031232262030243874, ACC:1.0\n",
      "Training iteration 407 loss: 0.12058401852846146, ACC:0.9375\n",
      "Training iteration 408 loss: 0.018358197063207626, ACC:1.0\n",
      "Training iteration 409 loss: 0.03640275076031685, ACC:1.0\n",
      "Training iteration 410 loss: 0.11080069839954376, ACC:0.9375\n",
      "Training iteration 411 loss: 0.05452362075448036, ACC:0.984375\n",
      "Training iteration 412 loss: 0.07858224958181381, ACC:0.96875\n",
      "Training iteration 413 loss: 0.11212075501680374, ACC:0.96875\n",
      "Training iteration 414 loss: 0.09296853095293045, ACC:0.984375\n",
      "Training iteration 415 loss: 0.1069990023970604, ACC:0.96875\n",
      "Training iteration 416 loss: 0.03232778608798981, ACC:0.984375\n",
      "Training iteration 417 loss: 0.02701875939965248, ACC:1.0\n",
      "Training iteration 418 loss: 0.026638226583600044, ACC:1.0\n",
      "Training iteration 419 loss: 0.09238572418689728, ACC:0.953125\n",
      "Training iteration 420 loss: 0.013081135228276253, ACC:1.0\n",
      "Training iteration 421 loss: 0.015383015386760235, ACC:1.0\n",
      "Training iteration 422 loss: 0.09387210011482239, ACC:0.953125\n",
      "Training iteration 423 loss: 0.04494493082165718, ACC:0.984375\n",
      "Training iteration 424 loss: 0.06905736029148102, ACC:0.96875\n",
      "Training iteration 425 loss: 0.07600115239620209, ACC:0.984375\n",
      "Training iteration 426 loss: 0.07562930881977081, ACC:0.984375\n",
      "Training iteration 427 loss: 0.08535553514957428, ACC:0.96875\n",
      "Training iteration 428 loss: 0.0518670491874218, ACC:0.984375\n",
      "Training iteration 429 loss: 0.08356523513793945, ACC:0.984375\n",
      "Training iteration 430 loss: 0.035043567419052124, ACC:1.0\n",
      "Training iteration 431 loss: 0.1087556779384613, ACC:0.984375\n",
      "Training iteration 432 loss: 0.13586001098155975, ACC:0.96875\n",
      "Training iteration 433 loss: 0.09156015515327454, ACC:0.96875\n",
      "Training iteration 434 loss: 0.11924976110458374, ACC:0.953125\n",
      "Training iteration 435 loss: 0.03382274508476257, ACC:1.0\n",
      "Training iteration 436 loss: 0.09315098077058792, ACC:0.984375\n",
      "Training iteration 437 loss: 0.07579647749662399, ACC:0.953125\n",
      "Training iteration 438 loss: 0.04256899654865265, ACC:0.984375\n",
      "Training iteration 439 loss: 0.0624108649790287, ACC:1.0\n",
      "Training iteration 440 loss: 0.023417538031935692, ACC:1.0\n",
      "Training iteration 441 loss: 0.03917865455150604, ACC:0.984375\n",
      "Training iteration 442 loss: 0.18511733412742615, ACC:0.9375\n",
      "Training iteration 443 loss: 0.06006995588541031, ACC:0.96875\n",
      "Training iteration 444 loss: 0.05251097306609154, ACC:0.984375\n",
      "Training iteration 445 loss: 0.044716015458106995, ACC:0.984375\n",
      "Training iteration 446 loss: 0.020226769149303436, ACC:1.0\n",
      "Training iteration 447 loss: 0.059965845197439194, ACC:0.984375\n",
      "Training iteration 448 loss: 0.08650440722703934, ACC:0.953125\n",
      "Training iteration 449 loss: 0.024504782631993294, ACC:1.0\n",
      "Training iteration 450 loss: 0.041363563388586044, ACC:1.0\n",
      "Validation iteration 451 loss: 0.021016471087932587, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.015285431407392025, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.0904502347111702, ACC: 0.96875\n",
      "Validation iteration 454 loss: 0.036616090685129166, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.022205045446753502, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.15825255215168, ACC: 0.953125\n",
      "Validation iteration 457 loss: 0.04621554911136627, ACC: 0.984375\n",
      "Validation iteration 458 loss: 0.025849053636193275, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.07034249603748322, ACC: 0.96875\n",
      "Validation iteration 460 loss: 0.12101278454065323, ACC: 0.96875\n",
      "Validation iteration 461 loss: 0.03215048089623451, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.03201548382639885, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.06356620043516159, ACC: 0.96875\n",
      "Validation iteration 464 loss: 0.02152862399816513, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.06948291510343552, ACC: 0.96875\n",
      "Validation iteration 466 loss: 0.03659198060631752, ACC: 0.984375\n",
      "Validation iteration 467 loss: 0.0917794406414032, ACC: 0.984375\n",
      "Validation iteration 468 loss: 0.023452505469322205, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.0780988559126854, ACC: 0.984375\n",
      "Validation iteration 470 loss: 0.14574192464351654, ACC: 0.9375\n",
      "Validation iteration 471 loss: 0.018162444233894348, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.06574306637048721, ACC: 0.96875\n",
      "Validation iteration 473 loss: 0.023078978061676025, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.06154482811689377, ACC: 0.984375\n",
      "Validation iteration 475 loss: 0.0523623563349247, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.021582502871751785, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.05385525897145271, ACC: 0.96875\n",
      "Validation iteration 478 loss: 0.040456004440784454, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.0999504029750824, ACC: 0.953125\n",
      "Validation iteration 480 loss: 0.18313799798488617, ACC: 0.953125\n",
      "Validation iteration 481 loss: 0.10034450888633728, ACC: 0.96875\n",
      "Validation iteration 482 loss: 0.09502822905778885, ACC: 0.953125\n",
      "Validation iteration 483 loss: 0.0892685279250145, ACC: 0.96875\n",
      "Validation iteration 484 loss: 0.10578164458274841, ACC: 0.953125\n",
      "Validation iteration 485 loss: 0.026663588359951973, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.02948572300374508, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.12344788759946823, ACC: 0.953125\n",
      "Validation iteration 488 loss: 0.07295196503400803, ACC: 0.984375\n",
      "Validation iteration 489 loss: 0.06332917511463165, ACC: 0.984375\n",
      "Validation iteration 490 loss: 0.03906484320759773, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.16864007711410522, ACC: 0.953125\n",
      "Validation iteration 492 loss: 0.06655139476060867, ACC: 0.984375\n",
      "Validation iteration 493 loss: 0.06746014952659607, ACC: 0.96875\n",
      "Validation iteration 494 loss: 0.060587819665670395, ACC: 0.96875\n",
      "Validation iteration 495 loss: 0.10293784737586975, ACC: 0.96875\n",
      "Validation iteration 496 loss: 0.03521445021033287, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.11640434712171555, ACC: 0.96875\n",
      "Validation iteration 498 loss: 0.026100045070052147, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.08104505389928818, ACC: 0.984375\n",
      "Validation iteration 500 loss: 0.04234347492456436, ACC: 0.984375\n",
      "-- Epoch 6 done -- Train loss: 0.07897083828225732, train ACC: 0.9739930555555556, val loss: 0.06668357426300645, val ACC: 0.9803125\n",
      "<--- 1497.8912308216095 seconds --->\n",
      "Training iteration 1 loss: 0.03950309380888939, ACC:1.0\n",
      "Training iteration 2 loss: 0.05148769170045853, ACC:0.984375\n",
      "Training iteration 3 loss: 0.026093807071447372, ACC:0.984375\n",
      "Training iteration 4 loss: 0.036160796880722046, ACC:0.984375\n",
      "Training iteration 5 loss: 0.01960676535964012, ACC:1.0\n",
      "Training iteration 6 loss: 0.012665929272770882, ACC:1.0\n",
      "Training iteration 7 loss: 0.0824129581451416, ACC:0.96875\n",
      "Training iteration 8 loss: 0.041522152721881866, ACC:0.96875\n",
      "Training iteration 9 loss: 0.0875188559293747, ACC:0.953125\n",
      "Training iteration 10 loss: 0.1572144627571106, ACC:0.9375\n",
      "Training iteration 11 loss: 0.10189434885978699, ACC:0.953125\n",
      "Training iteration 12 loss: 0.17632918059825897, ACC:0.953125\n",
      "Training iteration 13 loss: 0.015877343714237213, ACC:1.0\n",
      "Training iteration 14 loss: 0.02593327686190605, ACC:1.0\n",
      "Training iteration 15 loss: 0.104888916015625, ACC:0.953125\n",
      "Training iteration 16 loss: 0.022151824086904526, ACC:1.0\n",
      "Training iteration 17 loss: 0.24515491724014282, ACC:0.921875\n",
      "Training iteration 18 loss: 0.023800354450941086, ACC:1.0\n",
      "Training iteration 19 loss: 0.08058201521635056, ACC:0.984375\n",
      "Training iteration 20 loss: 0.04002738744020462, ACC:0.96875\n",
      "Training iteration 21 loss: 0.13237841427326202, ACC:0.921875\n",
      "Training iteration 22 loss: 0.04357510805130005, ACC:0.984375\n",
      "Training iteration 23 loss: 0.12431241571903229, ACC:0.953125\n",
      "Training iteration 24 loss: 0.05030675604939461, ACC:0.96875\n",
      "Training iteration 25 loss: 0.020260291174054146, ACC:1.0\n",
      "Training iteration 26 loss: 0.1045403927564621, ACC:0.96875\n",
      "Training iteration 27 loss: 0.04588732495903969, ACC:0.96875\n",
      "Training iteration 28 loss: 0.09099870175123215, ACC:0.953125\n",
      "Training iteration 29 loss: 0.06711475551128387, ACC:0.984375\n",
      "Training iteration 30 loss: 0.048341020941734314, ACC:0.984375\n",
      "Training iteration 31 loss: 0.09933746606111526, ACC:0.984375\n",
      "Training iteration 32 loss: 0.08310778439044952, ACC:0.96875\n",
      "Training iteration 33 loss: 0.06383810937404633, ACC:0.984375\n",
      "Training iteration 34 loss: 0.23904447257518768, ACC:0.9375\n",
      "Training iteration 35 loss: 0.029725927859544754, ACC:1.0\n",
      "Training iteration 36 loss: 0.10489797592163086, ACC:0.96875\n",
      "Training iteration 37 loss: 0.108823761343956, ACC:0.96875\n",
      "Training iteration 38 loss: 0.10918751358985901, ACC:0.953125\n",
      "Training iteration 39 loss: 0.18263782560825348, ACC:0.921875\n",
      "Training iteration 40 loss: 0.09530819952487946, ACC:0.984375\n",
      "Training iteration 41 loss: 0.05758964642882347, ACC:0.96875\n",
      "Training iteration 42 loss: 0.08525579422712326, ACC:0.953125\n",
      "Training iteration 43 loss: 0.09182273596525192, ACC:0.96875\n",
      "Training iteration 44 loss: 0.08411094546318054, ACC:0.953125\n",
      "Training iteration 45 loss: 0.13503172993659973, ACC:0.96875\n",
      "Training iteration 46 loss: 0.13991007208824158, ACC:0.9375\n",
      "Training iteration 47 loss: 0.1442677527666092, ACC:0.953125\n",
      "Training iteration 48 loss: 0.04302547872066498, ACC:0.984375\n",
      "Training iteration 49 loss: 0.03600456938147545, ACC:0.984375\n",
      "Training iteration 50 loss: 0.10557673126459122, ACC:0.953125\n",
      "Training iteration 51 loss: 0.11142454296350479, ACC:0.953125\n",
      "Training iteration 52 loss: 0.02590964548289776, ACC:0.984375\n",
      "Training iteration 53 loss: 0.06926877051591873, ACC:0.96875\n",
      "Training iteration 54 loss: 0.02855483442544937, ACC:1.0\n",
      "Training iteration 55 loss: 0.0518343560397625, ACC:0.984375\n",
      "Training iteration 56 loss: 0.05533023551106453, ACC:0.984375\n",
      "Training iteration 57 loss: 0.1081916019320488, ACC:0.96875\n",
      "Training iteration 58 loss: 0.038755178451538086, ACC:0.984375\n",
      "Training iteration 59 loss: 0.01992396079003811, ACC:1.0\n",
      "Training iteration 60 loss: 0.157530277967453, ACC:0.921875\n",
      "Training iteration 61 loss: 0.08107145875692368, ACC:0.984375\n",
      "Training iteration 62 loss: 0.03193870931863785, ACC:0.984375\n",
      "Training iteration 63 loss: 0.09248849004507065, ACC:0.953125\n",
      "Training iteration 64 loss: 0.10819203406572342, ACC:0.984375\n",
      "Training iteration 65 loss: 0.05062784627079964, ACC:0.984375\n",
      "Training iteration 66 loss: 0.13018524646759033, ACC:0.96875\n",
      "Training iteration 67 loss: 0.190455824136734, ACC:0.96875\n",
      "Training iteration 68 loss: 0.04769507050514221, ACC:0.984375\n",
      "Training iteration 69 loss: 0.11535976082086563, ACC:0.96875\n",
      "Training iteration 70 loss: 0.11825323104858398, ACC:0.96875\n",
      "Training iteration 71 loss: 0.12618902325630188, ACC:0.953125\n",
      "Training iteration 72 loss: 0.04150676727294922, ACC:1.0\n",
      "Training iteration 73 loss: 0.018526112660765648, ACC:1.0\n",
      "Training iteration 74 loss: 0.03632873669266701, ACC:0.984375\n",
      "Training iteration 75 loss: 0.08533311635255814, ACC:0.96875\n",
      "Training iteration 76 loss: 0.06765208393335342, ACC:0.96875\n",
      "Training iteration 77 loss: 0.04185241088271141, ACC:1.0\n",
      "Training iteration 78 loss: 0.09376668930053711, ACC:0.96875\n",
      "Training iteration 79 loss: 0.08185745775699615, ACC:0.96875\n",
      "Training iteration 80 loss: 0.14426538348197937, ACC:0.9375\n",
      "Training iteration 81 loss: 0.07758219540119171, ACC:0.96875\n",
      "Training iteration 82 loss: 0.06826532632112503, ACC:1.0\n",
      "Training iteration 83 loss: 0.12005282938480377, ACC:0.984375\n",
      "Training iteration 84 loss: 0.14358659088611603, ACC:0.953125\n",
      "Training iteration 85 loss: 0.12390174716711044, ACC:0.9375\n",
      "Training iteration 86 loss: 0.05069006234407425, ACC:0.984375\n",
      "Training iteration 87 loss: 0.08342938125133514, ACC:0.96875\n",
      "Training iteration 88 loss: 0.14030422270298004, ACC:0.9375\n",
      "Training iteration 89 loss: 0.16941210627555847, ACC:0.9375\n",
      "Training iteration 90 loss: 0.10490487515926361, ACC:0.96875\n",
      "Training iteration 91 loss: 0.13392207026481628, ACC:0.9375\n",
      "Training iteration 92 loss: 0.0774073526263237, ACC:0.984375\n",
      "Training iteration 93 loss: 0.09075161814689636, ACC:1.0\n",
      "Training iteration 94 loss: 0.07447824627161026, ACC:1.0\n",
      "Training iteration 95 loss: 0.08298201858997345, ACC:1.0\n",
      "Training iteration 96 loss: 0.06870733946561813, ACC:1.0\n",
      "Training iteration 97 loss: 0.07419679313898087, ACC:0.96875\n",
      "Training iteration 98 loss: 0.036644693464040756, ACC:1.0\n",
      "Training iteration 99 loss: 0.07984885573387146, ACC:0.96875\n",
      "Training iteration 100 loss: 0.03715687617659569, ACC:1.0\n",
      "Training iteration 101 loss: 0.04409322515130043, ACC:0.984375\n",
      "Training iteration 102 loss: 0.03485795482993126, ACC:1.0\n",
      "Training iteration 103 loss: 0.04731905087828636, ACC:0.96875\n",
      "Training iteration 104 loss: 0.09732704609632492, ACC:0.953125\n",
      "Training iteration 105 loss: 0.06055658310651779, ACC:0.96875\n",
      "Training iteration 106 loss: 0.16620296239852905, ACC:0.953125\n",
      "Training iteration 107 loss: 0.07584404200315475, ACC:0.96875\n",
      "Training iteration 108 loss: 0.02153325080871582, ACC:1.0\n",
      "Training iteration 109 loss: 0.129273921251297, ACC:0.96875\n",
      "Training iteration 110 loss: 0.03457166254520416, ACC:1.0\n",
      "Training iteration 111 loss: 0.10674992203712463, ACC:0.921875\n",
      "Training iteration 112 loss: 0.04199545085430145, ACC:0.984375\n",
      "Training iteration 113 loss: 0.06129400432109833, ACC:0.96875\n",
      "Training iteration 114 loss: 0.022475143894553185, ACC:1.0\n",
      "Training iteration 115 loss: 0.08190028369426727, ACC:0.96875\n",
      "Training iteration 116 loss: 0.04689604789018631, ACC:1.0\n",
      "Training iteration 117 loss: 0.1555110514163971, ACC:0.9375\n",
      "Training iteration 118 loss: 0.04641261696815491, ACC:0.984375\n",
      "Training iteration 119 loss: 0.07615151256322861, ACC:0.96875\n",
      "Training iteration 120 loss: 0.049449995160102844, ACC:0.984375\n",
      "Training iteration 121 loss: 0.02565128169953823, ACC:1.0\n",
      "Training iteration 122 loss: 0.014447861351072788, ACC:1.0\n",
      "Training iteration 123 loss: 0.1366308629512787, ACC:0.96875\n",
      "Training iteration 124 loss: 0.014483174309134483, ACC:1.0\n",
      "Training iteration 125 loss: 0.0995480865240097, ACC:0.953125\n",
      "Training iteration 126 loss: 0.08984890580177307, ACC:0.96875\n",
      "Training iteration 127 loss: 0.057252753525972366, ACC:0.984375\n",
      "Training iteration 128 loss: 0.10195067524909973, ACC:0.96875\n",
      "Training iteration 129 loss: 0.08819100260734558, ACC:0.984375\n",
      "Training iteration 130 loss: 0.09593372046947479, ACC:0.96875\n",
      "Training iteration 131 loss: 0.053625792264938354, ACC:0.984375\n",
      "Training iteration 132 loss: 0.02132970467209816, ACC:1.0\n",
      "Training iteration 133 loss: 0.04534478858113289, ACC:0.984375\n",
      "Training iteration 134 loss: 0.017918216064572334, ACC:1.0\n",
      "Training iteration 135 loss: 0.20972806215286255, ACC:0.9375\n",
      "Training iteration 136 loss: 0.06362836062908173, ACC:0.96875\n",
      "Training iteration 137 loss: 0.06728556752204895, ACC:0.96875\n",
      "Training iteration 138 loss: 0.06517504900693893, ACC:0.96875\n",
      "Training iteration 139 loss: 0.0700836330652237, ACC:0.984375\n",
      "Training iteration 140 loss: 0.10512692481279373, ACC:0.96875\n",
      "Training iteration 141 loss: 0.1378534436225891, ACC:0.96875\n",
      "Training iteration 142 loss: 0.015178211964666843, ACC:1.0\n",
      "Training iteration 143 loss: 0.07886981219053268, ACC:0.984375\n",
      "Training iteration 144 loss: 0.05338124558329582, ACC:0.96875\n",
      "Training iteration 145 loss: 0.12725742161273956, ACC:0.984375\n",
      "Training iteration 146 loss: 0.027808532118797302, ACC:0.984375\n",
      "Training iteration 147 loss: 0.1433781385421753, ACC:0.953125\n",
      "Training iteration 148 loss: 0.1265295296907425, ACC:0.96875\n",
      "Training iteration 149 loss: 0.14829203486442566, ACC:0.96875\n",
      "Training iteration 150 loss: 0.03004533052444458, ACC:0.984375\n",
      "Training iteration 151 loss: 0.2063610851764679, ACC:0.953125\n",
      "Training iteration 152 loss: 0.03375259414315224, ACC:1.0\n",
      "Training iteration 153 loss: 0.11020375043153763, ACC:0.953125\n",
      "Training iteration 154 loss: 0.06740579754114151, ACC:0.984375\n",
      "Training iteration 155 loss: 0.044179968535900116, ACC:1.0\n",
      "Training iteration 156 loss: 0.15645788609981537, ACC:0.96875\n",
      "Training iteration 157 loss: 0.09231589734554291, ACC:0.953125\n",
      "Training iteration 158 loss: 0.04281073063611984, ACC:0.984375\n",
      "Training iteration 159 loss: 0.11827690154314041, ACC:0.96875\n",
      "Training iteration 160 loss: 0.10603440552949905, ACC:0.953125\n",
      "Training iteration 161 loss: 0.10314181447029114, ACC:0.984375\n",
      "Training iteration 162 loss: 0.07864786684513092, ACC:0.984375\n",
      "Training iteration 163 loss: 0.1697947233915329, ACC:0.9375\n",
      "Training iteration 164 loss: 0.08673524856567383, ACC:0.96875\n",
      "Training iteration 165 loss: 0.08724720776081085, ACC:0.953125\n",
      "Training iteration 166 loss: 0.09277651458978653, ACC:0.96875\n",
      "Training iteration 167 loss: 0.10579512268304825, ACC:0.953125\n",
      "Training iteration 168 loss: 0.14605483412742615, ACC:0.953125\n",
      "Training iteration 169 loss: 0.08263854682445526, ACC:0.96875\n",
      "Training iteration 170 loss: 0.13240575790405273, ACC:0.953125\n",
      "Training iteration 171 loss: 0.06105409935116768, ACC:0.96875\n",
      "Training iteration 172 loss: 0.07264310866594315, ACC:0.96875\n",
      "Training iteration 173 loss: 0.045935455709695816, ACC:0.984375\n",
      "Training iteration 174 loss: 0.04202021658420563, ACC:1.0\n",
      "Training iteration 175 loss: 0.09301196783781052, ACC:0.953125\n",
      "Training iteration 176 loss: 0.019919458776712418, ACC:1.0\n",
      "Training iteration 177 loss: 0.07941203564405441, ACC:0.96875\n",
      "Training iteration 178 loss: 0.04129187390208244, ACC:1.0\n",
      "Training iteration 179 loss: 0.06984075158834457, ACC:0.984375\n",
      "Training iteration 180 loss: 0.0773717537522316, ACC:0.984375\n",
      "Training iteration 181 loss: 0.08691064268350601, ACC:0.96875\n",
      "Training iteration 182 loss: 0.04728386923670769, ACC:0.984375\n",
      "Training iteration 183 loss: 0.021093372255563736, ACC:1.0\n",
      "Training iteration 184 loss: 0.055740442126989365, ACC:0.984375\n",
      "Training iteration 185 loss: 0.13285864889621735, ACC:0.953125\n",
      "Training iteration 186 loss: 0.07791353762149811, ACC:0.984375\n",
      "Training iteration 187 loss: 0.1130039170384407, ACC:0.953125\n",
      "Training iteration 188 loss: 0.09531819075345993, ACC:0.96875\n",
      "Training iteration 189 loss: 0.09653756022453308, ACC:0.953125\n",
      "Training iteration 190 loss: 0.06761746853590012, ACC:0.984375\n",
      "Training iteration 191 loss: 0.12260323017835617, ACC:0.984375\n",
      "Training iteration 192 loss: 0.05808059871196747, ACC:0.984375\n",
      "Training iteration 193 loss: 0.03491372987627983, ACC:1.0\n",
      "Training iteration 194 loss: 0.05968416482210159, ACC:0.96875\n",
      "Training iteration 195 loss: 0.06845946609973907, ACC:0.984375\n",
      "Training iteration 196 loss: 0.08476606011390686, ACC:0.96875\n",
      "Training iteration 197 loss: 0.1933152824640274, ACC:0.90625\n",
      "Training iteration 198 loss: 0.04348335787653923, ACC:1.0\n",
      "Training iteration 199 loss: 0.08619190007448196, ACC:0.984375\n",
      "Training iteration 200 loss: 0.10795900225639343, ACC:0.96875\n",
      "Training iteration 201 loss: 0.05054832249879837, ACC:0.984375\n",
      "Training iteration 202 loss: 0.10071466118097305, ACC:0.984375\n",
      "Training iteration 203 loss: 0.09228477627038956, ACC:0.96875\n",
      "Training iteration 204 loss: 0.10619627684354782, ACC:0.96875\n",
      "Training iteration 205 loss: 0.07356292009353638, ACC:0.984375\n",
      "Training iteration 206 loss: 0.06697076559066772, ACC:0.984375\n",
      "Training iteration 207 loss: 0.08369685709476471, ACC:0.96875\n",
      "Training iteration 208 loss: 0.1537635177373886, ACC:0.953125\n",
      "Training iteration 209 loss: 0.10982682555913925, ACC:0.953125\n",
      "Training iteration 210 loss: 0.09724083542823792, ACC:0.96875\n",
      "Training iteration 211 loss: 0.10400057584047318, ACC:0.96875\n",
      "Training iteration 212 loss: 0.05268069729208946, ACC:1.0\n",
      "Training iteration 213 loss: 0.06536709517240524, ACC:0.984375\n",
      "Training iteration 214 loss: 0.054012782871723175, ACC:0.984375\n",
      "Training iteration 215 loss: 0.03729445859789848, ACC:1.0\n",
      "Training iteration 216 loss: 0.0611720010638237, ACC:0.96875\n",
      "Training iteration 217 loss: 0.026181848719716072, ACC:1.0\n",
      "Training iteration 218 loss: 0.014612769708037376, ACC:1.0\n",
      "Training iteration 219 loss: 0.20057833194732666, ACC:0.9375\n",
      "Training iteration 220 loss: 0.012260634452104568, ACC:1.0\n",
      "Training iteration 221 loss: 0.19240796566009521, ACC:0.9375\n",
      "Training iteration 222 loss: 0.14360876381397247, ACC:0.984375\n",
      "Training iteration 223 loss: 0.11528516560792923, ACC:0.96875\n",
      "Training iteration 224 loss: 0.11028987169265747, ACC:0.96875\n",
      "Training iteration 225 loss: 0.13505952060222626, ACC:0.953125\n",
      "Training iteration 226 loss: 0.1644478738307953, ACC:0.953125\n",
      "Training iteration 227 loss: 0.07397621870040894, ACC:0.984375\n",
      "Training iteration 228 loss: 0.07447607815265656, ACC:0.984375\n",
      "Training iteration 229 loss: 0.03953859955072403, ACC:1.0\n",
      "Training iteration 230 loss: 0.1293054074048996, ACC:0.953125\n",
      "Training iteration 231 loss: 0.038963500410318375, ACC:0.984375\n",
      "Training iteration 232 loss: 0.20681490004062653, ACC:0.90625\n",
      "Training iteration 233 loss: 0.0803440511226654, ACC:0.96875\n",
      "Training iteration 234 loss: 0.06646350771188736, ACC:0.984375\n",
      "Training iteration 235 loss: 0.04607539623975754, ACC:1.0\n",
      "Training iteration 236 loss: 0.06787405908107758, ACC:1.0\n",
      "Training iteration 237 loss: 0.04729875922203064, ACC:0.984375\n",
      "Training iteration 238 loss: 0.055646464228630066, ACC:0.984375\n",
      "Training iteration 239 loss: 0.11733327060937881, ACC:0.9375\n",
      "Training iteration 240 loss: 0.02623998001217842, ACC:1.0\n",
      "Training iteration 241 loss: 0.029458142817020416, ACC:0.984375\n",
      "Training iteration 242 loss: 0.02532186359167099, ACC:1.0\n",
      "Training iteration 243 loss: 0.12012575566768646, ACC:0.953125\n",
      "Training iteration 244 loss: 0.14545069634914398, ACC:0.9375\n",
      "Training iteration 245 loss: 0.1171794980764389, ACC:0.953125\n",
      "Training iteration 246 loss: 0.0881609171628952, ACC:0.984375\n",
      "Training iteration 247 loss: 0.08892452716827393, ACC:0.96875\n",
      "Training iteration 248 loss: 0.11676766723394394, ACC:0.953125\n",
      "Training iteration 249 loss: 0.14332182705402374, ACC:0.9375\n",
      "Training iteration 250 loss: 0.047858599573373795, ACC:1.0\n",
      "Training iteration 251 loss: 0.07947222143411636, ACC:0.953125\n",
      "Training iteration 252 loss: 0.08604378998279572, ACC:0.984375\n",
      "Training iteration 253 loss: 0.0876883789896965, ACC:0.9375\n",
      "Training iteration 254 loss: 0.039488404989242554, ACC:0.984375\n",
      "Training iteration 255 loss: 0.06217223405838013, ACC:0.96875\n",
      "Training iteration 256 loss: 0.06007272005081177, ACC:0.96875\n",
      "Training iteration 257 loss: 0.04839413985610008, ACC:0.96875\n",
      "Training iteration 258 loss: 0.08629163354635239, ACC:0.984375\n",
      "Training iteration 259 loss: 0.022812219336628914, ACC:0.984375\n",
      "Training iteration 260 loss: 0.03348543867468834, ACC:0.984375\n",
      "Training iteration 261 loss: 0.15633755922317505, ACC:0.96875\n",
      "Training iteration 262 loss: 0.04444653168320656, ACC:0.984375\n",
      "Training iteration 263 loss: 0.04790619760751724, ACC:0.984375\n",
      "Training iteration 264 loss: 0.0726725310087204, ACC:0.96875\n",
      "Training iteration 265 loss: 0.032504934817552567, ACC:1.0\n",
      "Training iteration 266 loss: 0.08046524971723557, ACC:0.984375\n",
      "Training iteration 267 loss: 0.09904687851667404, ACC:0.9375\n",
      "Training iteration 268 loss: 0.1110013797879219, ACC:0.953125\n",
      "Training iteration 269 loss: 0.09870783984661102, ACC:0.9375\n",
      "Training iteration 270 loss: 0.06884297728538513, ACC:0.96875\n",
      "Training iteration 271 loss: 0.0590217188000679, ACC:0.96875\n",
      "Training iteration 272 loss: 0.06763175129890442, ACC:0.96875\n",
      "Training iteration 273 loss: 0.04465889558196068, ACC:0.96875\n",
      "Training iteration 274 loss: 0.02262936718761921, ACC:1.0\n",
      "Training iteration 275 loss: 0.09722845256328583, ACC:0.96875\n",
      "Training iteration 276 loss: 0.07177044451236725, ACC:0.984375\n",
      "Training iteration 277 loss: 0.02361088991165161, ACC:1.0\n",
      "Training iteration 278 loss: 0.07406948506832123, ACC:0.984375\n",
      "Training iteration 279 loss: 0.03818817064166069, ACC:0.984375\n",
      "Training iteration 280 loss: 0.02571531943976879, ACC:1.0\n",
      "Training iteration 281 loss: 0.05992552265524864, ACC:0.984375\n",
      "Training iteration 282 loss: 0.0760490819811821, ACC:0.984375\n",
      "Training iteration 283 loss: 0.029103348031640053, ACC:0.984375\n",
      "Training iteration 284 loss: 0.06811941415071487, ACC:0.984375\n",
      "Training iteration 285 loss: 0.09144483506679535, ACC:0.953125\n",
      "Training iteration 286 loss: 0.11025378853082657, ACC:0.953125\n",
      "Training iteration 287 loss: 0.07119898498058319, ACC:0.984375\n",
      "Training iteration 288 loss: 0.04491686820983887, ACC:0.984375\n",
      "Training iteration 289 loss: 0.11893731355667114, ACC:0.96875\n",
      "Training iteration 290 loss: 0.1415909081697464, ACC:0.984375\n",
      "Training iteration 291 loss: 0.03246784955263138, ACC:0.984375\n",
      "Training iteration 292 loss: 0.12401461601257324, ACC:0.96875\n",
      "Training iteration 293 loss: 0.16551335155963898, ACC:0.96875\n",
      "Training iteration 294 loss: 0.08003508299589157, ACC:0.984375\n",
      "Training iteration 295 loss: 0.057805102318525314, ACC:0.984375\n",
      "Training iteration 296 loss: 0.06640801578760147, ACC:0.984375\n",
      "Training iteration 297 loss: 0.08650966733694077, ACC:0.984375\n",
      "Training iteration 298 loss: 0.19340629875659943, ACC:0.953125\n",
      "Training iteration 299 loss: 0.04644269496202469, ACC:1.0\n",
      "Training iteration 300 loss: 0.028057288378477097, ACC:1.0\n",
      "Training iteration 301 loss: 0.08378421515226364, ACC:0.984375\n",
      "Training iteration 302 loss: 0.04630713164806366, ACC:0.984375\n",
      "Training iteration 303 loss: 0.14324089884757996, ACC:0.953125\n",
      "Training iteration 304 loss: 0.06757762283086777, ACC:0.96875\n",
      "Training iteration 305 loss: 0.02958986908197403, ACC:1.0\n",
      "Training iteration 306 loss: 0.02591373771429062, ACC:1.0\n",
      "Training iteration 307 loss: 0.059884846210479736, ACC:0.96875\n",
      "Training iteration 308 loss: 0.11617689579725266, ACC:0.953125\n",
      "Training iteration 309 loss: 0.08932428061962128, ACC:0.984375\n",
      "Training iteration 310 loss: 0.10562382638454437, ACC:0.96875\n",
      "Training iteration 311 loss: 0.10167867690324783, ACC:0.96875\n",
      "Training iteration 312 loss: 0.13111050426959991, ACC:0.953125\n",
      "Training iteration 313 loss: 0.1137099489569664, ACC:0.953125\n",
      "Training iteration 314 loss: 0.05782747641205788, ACC:0.96875\n",
      "Training iteration 315 loss: 0.142300546169281, ACC:0.96875\n",
      "Training iteration 316 loss: 0.10899639129638672, ACC:0.953125\n",
      "Training iteration 317 loss: 0.04261600226163864, ACC:0.984375\n",
      "Training iteration 318 loss: 0.06549501419067383, ACC:1.0\n",
      "Training iteration 319 loss: 0.19920183718204498, ACC:0.90625\n",
      "Training iteration 320 loss: 0.06835371255874634, ACC:0.984375\n",
      "Training iteration 321 loss: 0.09751064330339432, ACC:0.9375\n",
      "Training iteration 322 loss: 0.13258875906467438, ACC:0.96875\n",
      "Training iteration 323 loss: 0.10187257826328278, ACC:0.96875\n",
      "Training iteration 324 loss: 0.15097862482070923, ACC:0.9375\n",
      "Training iteration 325 loss: 0.06402383744716644, ACC:1.0\n",
      "Training iteration 326 loss: 0.11193067580461502, ACC:0.96875\n",
      "Training iteration 327 loss: 0.061309076845645905, ACC:0.984375\n",
      "Training iteration 328 loss: 0.04561704024672508, ACC:0.984375\n",
      "Training iteration 329 loss: 0.07294730097055435, ACC:0.984375\n",
      "Training iteration 330 loss: 0.11975134164094925, ACC:0.953125\n",
      "Training iteration 331 loss: 0.1053895354270935, ACC:0.984375\n",
      "Training iteration 332 loss: 0.05807290971279144, ACC:0.984375\n",
      "Training iteration 333 loss: 0.11228355765342712, ACC:0.96875\n",
      "Training iteration 334 loss: 0.0844619870185852, ACC:0.96875\n",
      "Training iteration 335 loss: 0.10043376684188843, ACC:0.953125\n",
      "Training iteration 336 loss: 0.057339198887348175, ACC:0.96875\n",
      "Training iteration 337 loss: 0.3208577036857605, ACC:0.890625\n",
      "Training iteration 338 loss: 0.1068686693906784, ACC:0.953125\n",
      "Training iteration 339 loss: 0.2573675215244293, ACC:0.90625\n",
      "Training iteration 340 loss: 0.15567055344581604, ACC:0.96875\n",
      "Training iteration 341 loss: 0.16563576459884644, ACC:0.953125\n",
      "Training iteration 342 loss: 0.055558186024427414, ACC:1.0\n",
      "Training iteration 343 loss: 0.15087667107582092, ACC:0.9375\n",
      "Training iteration 344 loss: 0.0401475690305233, ACC:0.984375\n",
      "Training iteration 345 loss: 0.06715916842222214, ACC:0.984375\n",
      "Training iteration 346 loss: 0.0281992070376873, ACC:1.0\n",
      "Training iteration 347 loss: 0.04085353761911392, ACC:1.0\n",
      "Training iteration 348 loss: 0.23940996825695038, ACC:0.9375\n",
      "Training iteration 349 loss: 0.10649804770946503, ACC:0.96875\n",
      "Training iteration 350 loss: 0.0639239177107811, ACC:1.0\n",
      "Training iteration 351 loss: 0.04829662665724754, ACC:1.0\n",
      "Training iteration 352 loss: 0.13047893345355988, ACC:0.953125\n",
      "Training iteration 353 loss: 0.06847839802503586, ACC:0.984375\n",
      "Training iteration 354 loss: 0.12202422320842743, ACC:0.953125\n",
      "Training iteration 355 loss: 0.17875787615776062, ACC:0.921875\n",
      "Training iteration 356 loss: 0.16157354414463043, ACC:0.96875\n",
      "Training iteration 357 loss: 0.1386335790157318, ACC:0.96875\n",
      "Training iteration 358 loss: 0.1174294725060463, ACC:0.9375\n",
      "Training iteration 359 loss: 0.3208886682987213, ACC:0.921875\n",
      "Training iteration 360 loss: 0.12732689082622528, ACC:0.96875\n",
      "Training iteration 361 loss: 0.20682570338249207, ACC:0.921875\n",
      "Training iteration 362 loss: 0.1275763362646103, ACC:0.984375\n",
      "Training iteration 363 loss: 0.15459895133972168, ACC:0.96875\n",
      "Training iteration 364 loss: 0.16883635520935059, ACC:0.9375\n",
      "Training iteration 365 loss: 0.12151650339365005, ACC:0.96875\n",
      "Training iteration 366 loss: 0.1385362446308136, ACC:0.96875\n",
      "Training iteration 367 loss: 0.11546508967876434, ACC:0.96875\n",
      "Training iteration 368 loss: 0.14433839917182922, ACC:0.9375\n",
      "Training iteration 369 loss: 0.18631435930728912, ACC:0.953125\n",
      "Training iteration 370 loss: 0.06014327332377434, ACC:0.984375\n",
      "Training iteration 371 loss: 0.08079982548952103, ACC:0.96875\n",
      "Training iteration 372 loss: 0.08665163069963455, ACC:0.984375\n",
      "Training iteration 373 loss: 0.10898523777723312, ACC:0.96875\n",
      "Training iteration 374 loss: 0.12022639065980911, ACC:0.96875\n",
      "Training iteration 375 loss: 0.06561947613954544, ACC:0.953125\n",
      "Training iteration 376 loss: 0.12902218103408813, ACC:0.953125\n",
      "Training iteration 377 loss: 0.111557736992836, ACC:0.96875\n",
      "Training iteration 378 loss: 0.042680785059928894, ACC:1.0\n",
      "Training iteration 379 loss: 0.05051311105489731, ACC:1.0\n",
      "Training iteration 380 loss: 0.051047876477241516, ACC:0.984375\n",
      "Training iteration 381 loss: 0.17280328273773193, ACC:0.9375\n",
      "Training iteration 382 loss: 0.14060938358306885, ACC:0.953125\n",
      "Training iteration 383 loss: 0.05670506879687309, ACC:0.984375\n",
      "Training iteration 384 loss: 0.1114366427063942, ACC:0.96875\n",
      "Training iteration 385 loss: 0.10429831594228745, ACC:0.96875\n",
      "Training iteration 386 loss: 0.21981267631053925, ACC:0.921875\n",
      "Training iteration 387 loss: 0.0979132130742073, ACC:0.953125\n",
      "Training iteration 388 loss: 0.11203890293836594, ACC:0.96875\n",
      "Training iteration 389 loss: 0.15834268927574158, ACC:0.9375\n",
      "Training iteration 390 loss: 0.06611649692058563, ACC:0.984375\n",
      "Training iteration 391 loss: 0.08582239598035812, ACC:0.953125\n",
      "Training iteration 392 loss: 0.17213909327983856, ACC:0.921875\n",
      "Training iteration 393 loss: 0.19660140573978424, ACC:0.953125\n",
      "Training iteration 394 loss: 0.11443772912025452, ACC:0.9375\n",
      "Training iteration 395 loss: 0.0981634184718132, ACC:0.953125\n",
      "Training iteration 396 loss: 0.05258696526288986, ACC:0.96875\n",
      "Training iteration 397 loss: 0.20723609626293182, ACC:0.953125\n",
      "Training iteration 398 loss: 0.08860762417316437, ACC:0.984375\n",
      "Training iteration 399 loss: 0.11011753976345062, ACC:0.953125\n",
      "Training iteration 400 loss: 0.01685185357928276, ACC:1.0\n",
      "Training iteration 401 loss: 0.04185272008180618, ACC:0.984375\n",
      "Training iteration 402 loss: 0.058285802602767944, ACC:0.96875\n",
      "Training iteration 403 loss: 0.10620056092739105, ACC:0.9375\n",
      "Training iteration 404 loss: 0.0815470740199089, ACC:0.96875\n",
      "Training iteration 405 loss: 0.16051127016544342, ACC:0.921875\n",
      "Training iteration 406 loss: 0.23155419528484344, ACC:0.90625\n",
      "Training iteration 407 loss: 0.07254689931869507, ACC:0.984375\n",
      "Training iteration 408 loss: 0.23759794235229492, ACC:0.921875\n",
      "Training iteration 409 loss: 0.10363148152828217, ACC:1.0\n",
      "Training iteration 410 loss: 0.14401142299175262, ACC:0.9375\n",
      "Training iteration 411 loss: 0.06950119882822037, ACC:0.984375\n",
      "Training iteration 412 loss: 0.11000832170248032, ACC:0.953125\n",
      "Training iteration 413 loss: 0.07359303534030914, ACC:0.984375\n",
      "Training iteration 414 loss: 0.1129641905426979, ACC:0.96875\n",
      "Training iteration 415 loss: 0.06577446311712265, ACC:0.953125\n",
      "Training iteration 416 loss: 0.06403245031833649, ACC:0.96875\n",
      "Training iteration 417 loss: 0.06281571835279465, ACC:0.984375\n",
      "Training iteration 418 loss: 0.0826386958360672, ACC:0.984375\n",
      "Training iteration 419 loss: 0.19480092823505402, ACC:0.921875\n",
      "Training iteration 420 loss: 0.23837843537330627, ACC:0.953125\n",
      "Training iteration 421 loss: 0.04311496391892433, ACC:1.0\n",
      "Training iteration 422 loss: 0.24126912653446198, ACC:0.890625\n",
      "Training iteration 423 loss: 0.27470827102661133, ACC:0.875\n",
      "Training iteration 424 loss: 0.1417018473148346, ACC:0.984375\n",
      "Training iteration 425 loss: 0.1556445211172104, ACC:0.953125\n",
      "Training iteration 426 loss: 0.14936745166778564, ACC:0.953125\n",
      "Training iteration 427 loss: 0.27011215686798096, ACC:0.875\n",
      "Training iteration 428 loss: 0.206380233168602, ACC:0.90625\n",
      "Training iteration 429 loss: 0.1178029403090477, ACC:0.96875\n",
      "Training iteration 430 loss: 0.13576124608516693, ACC:0.9375\n",
      "Training iteration 431 loss: 0.1969461888074875, ACC:0.9375\n",
      "Training iteration 432 loss: 0.22604280710220337, ACC:0.9375\n",
      "Training iteration 433 loss: 0.12280551344156265, ACC:0.9375\n",
      "Training iteration 434 loss: 0.20480622351169586, ACC:0.9375\n",
      "Training iteration 435 loss: 0.12901656329631805, ACC:0.953125\n",
      "Training iteration 436 loss: 0.1871556043624878, ACC:0.921875\n",
      "Training iteration 437 loss: 0.2011583000421524, ACC:0.9375\n",
      "Training iteration 438 loss: 0.2703384459018707, ACC:0.90625\n",
      "Training iteration 439 loss: 0.17177298665046692, ACC:0.90625\n",
      "Training iteration 440 loss: 0.09725580364465714, ACC:0.953125\n",
      "Training iteration 441 loss: 0.1428261548280716, ACC:0.9375\n",
      "Training iteration 442 loss: 0.246740460395813, ACC:0.9375\n",
      "Training iteration 443 loss: 0.04683443531394005, ACC:0.96875\n",
      "Training iteration 444 loss: 0.21426814794540405, ACC:0.9375\n",
      "Training iteration 445 loss: 0.35571378469467163, ACC:0.8125\n",
      "Training iteration 446 loss: 0.24739490449428558, ACC:0.890625\n",
      "Training iteration 447 loss: 0.17938977479934692, ACC:0.9375\n",
      "Training iteration 448 loss: 0.12805314362049103, ACC:0.96875\n",
      "Training iteration 449 loss: 0.18657615780830383, ACC:0.953125\n",
      "Training iteration 450 loss: 0.21875515580177307, ACC:0.953125\n",
      "Validation iteration 451 loss: 0.14684604108333588, ACC: 0.953125\n",
      "Validation iteration 452 loss: 0.11040658503770828, ACC: 0.953125\n",
      "Validation iteration 453 loss: 0.178026482462883, ACC: 0.953125\n",
      "Validation iteration 454 loss: 0.1347055435180664, ACC: 0.953125\n",
      "Validation iteration 455 loss: 0.14438994228839874, ACC: 0.96875\n",
      "Validation iteration 456 loss: 0.20561857521533966, ACC: 0.921875\n",
      "Validation iteration 457 loss: 0.21260268986225128, ACC: 0.921875\n",
      "Validation iteration 458 loss: 0.09848025441169739, ACC: 0.96875\n",
      "Validation iteration 459 loss: 0.1259288489818573, ACC: 0.96875\n",
      "Validation iteration 460 loss: 0.20793135464191437, ACC: 0.90625\n",
      "Validation iteration 461 loss: 0.1012941375374794, ACC: 0.96875\n",
      "Validation iteration 462 loss: 0.22809500992298126, ACC: 0.921875\n",
      "Validation iteration 463 loss: 0.14278219640254974, ACC: 0.96875\n",
      "Validation iteration 464 loss: 0.10153712332248688, ACC: 0.96875\n",
      "Validation iteration 465 loss: 0.07255972176790237, ACC: 0.984375\n",
      "Validation iteration 466 loss: 0.06556687504053116, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.11185504496097565, ACC: 0.96875\n",
      "Validation iteration 468 loss: 0.2379622757434845, ACC: 0.921875\n",
      "Validation iteration 469 loss: 0.15757082402706146, ACC: 0.953125\n",
      "Validation iteration 470 loss: 0.15013250708580017, ACC: 0.953125\n",
      "Validation iteration 471 loss: 0.103257916867733, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.10667308419942856, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.13631410896778107, ACC: 0.953125\n",
      "Validation iteration 474 loss: 0.10052520781755447, ACC: 0.953125\n",
      "Validation iteration 475 loss: 0.16152624785900116, ACC: 0.953125\n",
      "Validation iteration 476 loss: 0.15270556509494781, ACC: 0.953125\n",
      "Validation iteration 477 loss: 0.1028626337647438, ACC: 0.96875\n",
      "Validation iteration 478 loss: 0.15845951437950134, ACC: 0.953125\n",
      "Validation iteration 479 loss: 0.07507206499576569, ACC: 0.984375\n",
      "Validation iteration 480 loss: 0.19376397132873535, ACC: 0.9375\n",
      "Validation iteration 481 loss: 0.12228376418352127, ACC: 0.96875\n",
      "Validation iteration 482 loss: 0.06655420362949371, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.16870743036270142, ACC: 0.9375\n",
      "Validation iteration 484 loss: 0.13964900374412537, ACC: 0.96875\n",
      "Validation iteration 485 loss: 0.15792623162269592, ACC: 0.953125\n",
      "Validation iteration 486 loss: 0.114273801445961, ACC: 0.953125\n",
      "Validation iteration 487 loss: 0.14579881727695465, ACC: 0.953125\n",
      "Validation iteration 488 loss: 0.13254864513874054, ACC: 0.9375\n",
      "Validation iteration 489 loss: 0.21841557323932648, ACC: 0.921875\n",
      "Validation iteration 490 loss: 0.08885372430086136, ACC: 0.96875\n",
      "Validation iteration 491 loss: 0.10052408277988434, ACC: 0.984375\n",
      "Validation iteration 492 loss: 0.060560233891010284, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.19752737879753113, ACC: 0.9375\n",
      "Validation iteration 494 loss: 0.13609527051448822, ACC: 0.953125\n",
      "Validation iteration 495 loss: 0.08700047433376312, ACC: 0.984375\n",
      "Validation iteration 496 loss: 0.20134323835372925, ACC: 0.9375\n",
      "Validation iteration 497 loss: 0.14197595417499542, ACC: 0.96875\n",
      "Validation iteration 498 loss: 0.152171790599823, ACC: 0.953125\n",
      "Validation iteration 499 loss: 0.08907078951597214, ACC: 0.96875\n",
      "Validation iteration 500 loss: 0.06901650130748749, ACC: 0.984375\n",
      "-- Epoch 7 done -- Train loss: 0.09651680981947316, train ACC: 0.9685069444444444, val loss: 0.13631498515605928, val ACC: 0.95875\n",
      "<--- 1746.1531660556793 seconds --->\n",
      "Training iteration 1 loss: 0.20508508384227753, ACC:0.90625\n",
      "Training iteration 2 loss: 0.12551379203796387, ACC:0.96875\n",
      "Training iteration 3 loss: 0.11867190152406693, ACC:0.96875\n",
      "Training iteration 4 loss: 0.31200942397117615, ACC:0.859375\n",
      "Training iteration 5 loss: 0.3283543884754181, ACC:0.875\n",
      "Training iteration 6 loss: 0.1428210735321045, ACC:0.96875\n",
      "Training iteration 7 loss: 0.40779033303260803, ACC:0.90625\n",
      "Training iteration 8 loss: 0.2123742401599884, ACC:0.921875\n",
      "Training iteration 9 loss: 0.5698707699775696, ACC:0.53125\n",
      "Training iteration 10 loss: 0.3880842328071594, ACC:0.875\n",
      "Training iteration 11 loss: 0.2788672149181366, ACC:0.96875\n",
      "Training iteration 12 loss: 0.30116719007492065, ACC:0.9375\n",
      "Training iteration 13 loss: 0.39943358302116394, ACC:0.8125\n",
      "Training iteration 14 loss: 0.24855202436447144, ACC:0.921875\n",
      "Training iteration 15 loss: 0.3494414687156677, ACC:0.859375\n",
      "Training iteration 16 loss: 0.3654075562953949, ACC:0.828125\n",
      "Training iteration 17 loss: 0.34434273838996887, ACC:0.859375\n",
      "Training iteration 18 loss: 0.3896760046482086, ACC:0.828125\n",
      "Training iteration 19 loss: 0.195750892162323, ACC:0.953125\n",
      "Training iteration 20 loss: 0.3019602596759796, ACC:0.890625\n",
      "Training iteration 21 loss: 0.22331781685352325, ACC:0.9375\n",
      "Training iteration 22 loss: 0.2755778431892395, ACC:0.890625\n",
      "Training iteration 23 loss: 0.18373672664165497, ACC:0.96875\n",
      "Training iteration 24 loss: 0.21344484388828278, ACC:0.953125\n",
      "Training iteration 25 loss: 0.22665759921073914, ACC:0.9375\n",
      "Training iteration 26 loss: 0.3701978921890259, ACC:0.859375\n",
      "Training iteration 27 loss: 0.33174949884414673, ACC:0.890625\n",
      "Training iteration 28 loss: 0.1777796894311905, ACC:0.9375\n",
      "Training iteration 29 loss: 0.13279668986797333, ACC:0.96875\n",
      "Training iteration 30 loss: 0.18350723385810852, ACC:0.953125\n",
      "Training iteration 31 loss: 0.3449070155620575, ACC:0.859375\n",
      "Training iteration 32 loss: 0.22294558584690094, ACC:0.921875\n",
      "Training iteration 33 loss: 0.2908233106136322, ACC:0.921875\n",
      "Training iteration 34 loss: 0.2629706561565399, ACC:0.921875\n",
      "Training iteration 35 loss: 0.2201877385377884, ACC:0.9375\n",
      "Training iteration 36 loss: 0.20466825366020203, ACC:0.890625\n",
      "Training iteration 37 loss: 0.2698134183883667, ACC:0.875\n",
      "Training iteration 38 loss: 0.16563689708709717, ACC:0.953125\n",
      "Training iteration 39 loss: 0.15107932686805725, ACC:0.9375\n",
      "Training iteration 40 loss: 0.16562232375144958, ACC:0.90625\n",
      "Training iteration 41 loss: 0.1369199901819229, ACC:0.96875\n",
      "Training iteration 42 loss: 0.19862763583660126, ACC:0.9375\n",
      "Training iteration 43 loss: 0.14943630993366241, ACC:0.953125\n",
      "Training iteration 44 loss: 0.1674685925245285, ACC:0.9375\n",
      "Training iteration 45 loss: 0.07486986368894577, ACC:0.984375\n",
      "Training iteration 46 loss: 0.23027220368385315, ACC:0.921875\n",
      "Training iteration 47 loss: 0.18660695850849152, ACC:0.953125\n",
      "Training iteration 48 loss: 0.3103162944316864, ACC:0.90625\n",
      "Training iteration 49 loss: 0.100067138671875, ACC:0.96875\n",
      "Training iteration 50 loss: 0.26713019609451294, ACC:0.921875\n",
      "Training iteration 51 loss: 0.43574756383895874, ACC:0.765625\n",
      "Training iteration 52 loss: 0.18720783293247223, ACC:0.9375\n",
      "Training iteration 53 loss: 0.2853146195411682, ACC:0.859375\n",
      "Training iteration 54 loss: 0.16507074236869812, ACC:0.921875\n",
      "Training iteration 55 loss: 0.15869252383708954, ACC:0.953125\n",
      "Training iteration 56 loss: 0.30796265602111816, ACC:0.875\n",
      "Training iteration 57 loss: 0.38172614574432373, ACC:0.765625\n",
      "Training iteration 58 loss: 0.27076852321624756, ACC:0.921875\n",
      "Training iteration 59 loss: 0.4179557263851166, ACC:0.859375\n",
      "Training iteration 60 loss: 0.34066200256347656, ACC:0.90625\n",
      "Training iteration 61 loss: 0.2874636948108673, ACC:0.921875\n",
      "Training iteration 62 loss: 0.32719120383262634, ACC:0.859375\n",
      "Training iteration 63 loss: 0.21284563839435577, ACC:0.90625\n",
      "Training iteration 64 loss: 0.18884390592575073, ACC:0.953125\n",
      "Training iteration 65 loss: 0.17633529007434845, ACC:0.921875\n",
      "Training iteration 66 loss: 0.2546271085739136, ACC:0.890625\n",
      "Training iteration 67 loss: 0.43920764327049255, ACC:0.828125\n",
      "Training iteration 68 loss: 0.4442926049232483, ACC:0.828125\n",
      "Training iteration 69 loss: 0.22188273072242737, ACC:0.953125\n",
      "Training iteration 70 loss: 0.1884109377861023, ACC:0.890625\n",
      "Training iteration 71 loss: 0.24587079882621765, ACC:0.890625\n",
      "Training iteration 72 loss: 0.3509516417980194, ACC:0.9375\n",
      "Training iteration 73 loss: 0.18315234780311584, ACC:0.9375\n",
      "Training iteration 74 loss: 0.15934957563877106, ACC:0.96875\n",
      "Training iteration 75 loss: 0.3039530813694, ACC:0.890625\n",
      "Training iteration 76 loss: 0.12927977740764618, ACC:0.96875\n",
      "Training iteration 77 loss: 0.2603602111339569, ACC:0.90625\n",
      "Training iteration 78 loss: 0.2216719537973404, ACC:0.90625\n",
      "Training iteration 79 loss: 0.19741199910640717, ACC:0.96875\n",
      "Training iteration 80 loss: 0.3196110725402832, ACC:0.90625\n",
      "Training iteration 81 loss: 0.18010585010051727, ACC:0.9375\n",
      "Training iteration 82 loss: 0.22544927895069122, ACC:0.90625\n",
      "Training iteration 83 loss: 0.3566044270992279, ACC:0.828125\n",
      "Training iteration 84 loss: 0.5317224860191345, ACC:0.546875\n",
      "Training iteration 85 loss: 0.4145161211490631, ACC:0.84375\n",
      "Training iteration 86 loss: 0.27344173192977905, ACC:0.9375\n",
      "Training iteration 87 loss: 0.4124408960342407, ACC:0.828125\n",
      "Training iteration 88 loss: 0.3237461447715759, ACC:0.90625\n",
      "Training iteration 89 loss: 0.2753223180770874, ACC:0.90625\n",
      "Training iteration 90 loss: 0.3194177448749542, ACC:0.953125\n",
      "Training iteration 91 loss: 0.4047086238861084, ACC:0.9375\n",
      "Training iteration 92 loss: 0.42481157183647156, ACC:0.875\n",
      "Training iteration 93 loss: 0.4334879517555237, ACC:0.890625\n",
      "Training iteration 94 loss: 0.2885378301143646, ACC:0.9375\n",
      "Training iteration 95 loss: 0.23340070247650146, ACC:0.953125\n",
      "Training iteration 96 loss: 0.2379237860441208, ACC:0.953125\n",
      "Training iteration 97 loss: 0.3137091100215912, ACC:0.875\n",
      "Training iteration 98 loss: 0.1871422529220581, ACC:0.96875\n",
      "Training iteration 99 loss: 0.15977078676223755, ACC:0.9375\n",
      "Training iteration 100 loss: 0.21731285750865936, ACC:0.953125\n",
      "Training iteration 101 loss: 0.27230724692344666, ACC:0.890625\n",
      "Training iteration 102 loss: 0.15641005337238312, ACC:0.953125\n",
      "Training iteration 103 loss: 0.20534275472164154, ACC:0.953125\n",
      "Training iteration 104 loss: 0.21033206582069397, ACC:0.921875\n",
      "Training iteration 105 loss: 0.267805814743042, ACC:0.890625\n",
      "Training iteration 106 loss: 0.12707015872001648, ACC:0.921875\n",
      "Training iteration 107 loss: 0.26643410325050354, ACC:0.90625\n",
      "Training iteration 108 loss: 0.34268856048583984, ACC:0.84375\n",
      "Training iteration 109 loss: 0.40099993348121643, ACC:0.8125\n",
      "Training iteration 110 loss: 0.31883955001831055, ACC:0.90625\n",
      "Training iteration 111 loss: 0.2758183777332306, ACC:0.953125\n",
      "Training iteration 112 loss: 0.3202061653137207, ACC:0.9375\n",
      "Training iteration 113 loss: 0.33211949467658997, ACC:0.875\n",
      "Training iteration 114 loss: 0.28879234194755554, ACC:0.90625\n",
      "Training iteration 115 loss: 0.2600252330303192, ACC:0.90625\n",
      "Training iteration 116 loss: 0.2051035463809967, ACC:0.921875\n",
      "Training iteration 117 loss: 0.19117999076843262, ACC:0.953125\n",
      "Training iteration 118 loss: 0.1782180666923523, ACC:0.953125\n",
      "Training iteration 119 loss: 0.2091500163078308, ACC:0.921875\n",
      "Training iteration 120 loss: 0.2264784574508667, ACC:0.96875\n",
      "Training iteration 121 loss: 0.1731598675251007, ACC:0.953125\n",
      "Training iteration 122 loss: 0.2280789464712143, ACC:0.921875\n",
      "Training iteration 123 loss: 0.17109552025794983, ACC:0.9375\n",
      "Training iteration 124 loss: 0.1049884781241417, ACC:0.984375\n",
      "Training iteration 125 loss: 0.1887817531824112, ACC:0.90625\n",
      "Training iteration 126 loss: 0.21453537046909332, ACC:0.9375\n",
      "Training iteration 127 loss: 0.20939572155475616, ACC:0.9375\n",
      "Training iteration 128 loss: 0.1274801641702652, ACC:0.96875\n",
      "Training iteration 129 loss: 0.25782307982444763, ACC:0.921875\n",
      "Training iteration 130 loss: 0.06841635704040527, ACC:1.0\n",
      "Training iteration 131 loss: 0.09599911421537399, ACC:0.96875\n",
      "Training iteration 132 loss: 0.17604772746562958, ACC:0.96875\n",
      "Training iteration 133 loss: 0.20508365333080292, ACC:0.921875\n",
      "Training iteration 134 loss: 0.10817418247461319, ACC:0.96875\n",
      "Training iteration 135 loss: 0.1798916906118393, ACC:0.921875\n",
      "Training iteration 136 loss: 0.13976635038852692, ACC:0.96875\n",
      "Training iteration 137 loss: 0.2478342354297638, ACC:0.875\n",
      "Training iteration 138 loss: 0.22764629125595093, ACC:0.9375\n",
      "Training iteration 139 loss: 0.21328286826610565, ACC:0.859375\n",
      "Training iteration 140 loss: 0.1599053293466568, ACC:0.953125\n",
      "Training iteration 141 loss: 0.11159949004650116, ACC:0.984375\n",
      "Training iteration 142 loss: 0.19028326869010925, ACC:0.921875\n",
      "Training iteration 143 loss: 0.11061868816614151, ACC:0.96875\n",
      "Training iteration 144 loss: 0.13418307900428772, ACC:0.96875\n",
      "Training iteration 145 loss: 0.08715613931417465, ACC:0.96875\n",
      "Training iteration 146 loss: 0.11606552451848984, ACC:0.96875\n",
      "Training iteration 147 loss: 0.198076069355011, ACC:0.921875\n",
      "Training iteration 148 loss: 0.1557305008172989, ACC:0.953125\n",
      "Training iteration 149 loss: 0.15753069519996643, ACC:0.9375\n",
      "Training iteration 150 loss: 0.12072239816188812, ACC:0.96875\n",
      "Training iteration 151 loss: 0.20338498055934906, ACC:0.921875\n",
      "Training iteration 152 loss: 0.23626725375652313, ACC:0.921875\n",
      "Training iteration 153 loss: 0.10842304676771164, ACC:0.953125\n",
      "Training iteration 154 loss: 0.12363165616989136, ACC:0.953125\n",
      "Training iteration 155 loss: 0.2528492212295532, ACC:0.90625\n",
      "Training iteration 156 loss: 0.12090933322906494, ACC:0.984375\n",
      "Training iteration 157 loss: 0.18054163455963135, ACC:0.90625\n",
      "Training iteration 158 loss: 0.1389152556657791, ACC:0.953125\n",
      "Training iteration 159 loss: 0.11486810445785522, ACC:0.96875\n",
      "Training iteration 160 loss: 0.22442935407161713, ACC:0.921875\n",
      "Training iteration 161 loss: 0.2136286497116089, ACC:0.9375\n",
      "Training iteration 162 loss: 0.15241177380084991, ACC:0.984375\n",
      "Training iteration 163 loss: 0.19129174947738647, ACC:0.921875\n",
      "Training iteration 164 loss: 0.13571567833423615, ACC:0.96875\n",
      "Training iteration 165 loss: 0.09858621656894684, ACC:0.96875\n",
      "Training iteration 166 loss: 0.19900934398174286, ACC:0.921875\n",
      "Training iteration 167 loss: 0.12287978082895279, ACC:0.953125\n",
      "Training iteration 168 loss: 0.14332756400108337, ACC:0.953125\n",
      "Training iteration 169 loss: 0.19727809727191925, ACC:0.90625\n",
      "Training iteration 170 loss: 0.23161424696445465, ACC:0.890625\n",
      "Training iteration 171 loss: 0.10386154055595398, ACC:0.984375\n",
      "Training iteration 172 loss: 0.13587626814842224, ACC:0.96875\n",
      "Training iteration 173 loss: 0.0874992087483406, ACC:0.984375\n",
      "Training iteration 174 loss: 0.19454437494277954, ACC:0.890625\n",
      "Training iteration 175 loss: 0.11343757063150406, ACC:0.953125\n",
      "Training iteration 176 loss: 0.12325678020715714, ACC:0.9375\n",
      "Training iteration 177 loss: 0.11611666530370712, ACC:0.96875\n",
      "Training iteration 178 loss: 0.1869460940361023, ACC:0.953125\n",
      "Training iteration 179 loss: 0.2045167237520218, ACC:0.921875\n",
      "Training iteration 180 loss: 0.1610235571861267, ACC:0.953125\n",
      "Training iteration 181 loss: 0.15477024018764496, ACC:0.953125\n",
      "Training iteration 182 loss: 0.13219930231571198, ACC:0.984375\n",
      "Training iteration 183 loss: 0.19429075717926025, ACC:0.953125\n",
      "Training iteration 184 loss: 0.07394086569547653, ACC:0.984375\n",
      "Training iteration 185 loss: 0.130557581782341, ACC:0.984375\n",
      "Training iteration 186 loss: 0.2114398032426834, ACC:0.921875\n",
      "Training iteration 187 loss: 0.12157852947711945, ACC:0.96875\n",
      "Training iteration 188 loss: 0.14406251907348633, ACC:0.9375\n",
      "Training iteration 189 loss: 0.1767549365758896, ACC:0.9375\n",
      "Training iteration 190 loss: 0.09948617964982986, ACC:0.96875\n",
      "Training iteration 191 loss: 0.08430898934602737, ACC:0.96875\n",
      "Training iteration 192 loss: 0.2001471221446991, ACC:0.921875\n",
      "Training iteration 193 loss: 0.08399661630392075, ACC:0.984375\n",
      "Training iteration 194 loss: 0.08327541500329971, ACC:0.984375\n",
      "Training iteration 195 loss: 0.2147667557001114, ACC:0.90625\n",
      "Training iteration 196 loss: 0.15112292766571045, ACC:0.953125\n",
      "Training iteration 197 loss: 0.11086473613977432, ACC:0.953125\n",
      "Training iteration 198 loss: 0.06424769014120102, ACC:0.984375\n",
      "Training iteration 199 loss: 0.15695340931415558, ACC:0.9375\n",
      "Training iteration 200 loss: 0.12345153838396072, ACC:0.96875\n",
      "Training iteration 201 loss: 0.0694296807050705, ACC:0.96875\n",
      "Training iteration 202 loss: 0.07727888971567154, ACC:0.984375\n",
      "Training iteration 203 loss: 0.07399097084999084, ACC:0.984375\n",
      "Training iteration 204 loss: 0.06596457213163376, ACC:0.984375\n",
      "Training iteration 205 loss: 0.1359427273273468, ACC:0.953125\n",
      "Training iteration 206 loss: 0.06279966980218887, ACC:0.96875\n",
      "Training iteration 207 loss: 0.11744347959756851, ACC:0.921875\n",
      "Training iteration 208 loss: 0.17285704612731934, ACC:0.9375\n",
      "Training iteration 209 loss: 0.07174980640411377, ACC:0.96875\n",
      "Training iteration 210 loss: 0.059461601078510284, ACC:0.984375\n",
      "Training iteration 211 loss: 0.09844576567411423, ACC:0.96875\n",
      "Training iteration 212 loss: 0.16276338696479797, ACC:0.9375\n",
      "Training iteration 213 loss: 0.03629542142152786, ACC:1.0\n",
      "Training iteration 214 loss: 0.05476251244544983, ACC:1.0\n",
      "Training iteration 215 loss: 0.05142390355467796, ACC:1.0\n",
      "Training iteration 216 loss: 0.033274877816438675, ACC:1.0\n",
      "Training iteration 217 loss: 0.06529082357883453, ACC:0.984375\n",
      "Training iteration 218 loss: 0.08738221973180771, ACC:0.96875\n",
      "Training iteration 219 loss: 0.14896272122859955, ACC:0.9375\n",
      "Training iteration 220 loss: 0.06148703023791313, ACC:0.984375\n",
      "Training iteration 221 loss: 0.07826106250286102, ACC:1.0\n",
      "Training iteration 222 loss: 0.06077314913272858, ACC:0.984375\n",
      "Training iteration 223 loss: 0.037952225655317307, ACC:1.0\n",
      "Training iteration 224 loss: 0.0648595541715622, ACC:0.984375\n",
      "Training iteration 225 loss: 0.12041303515434265, ACC:0.96875\n",
      "Training iteration 226 loss: 0.04638834670186043, ACC:1.0\n",
      "Training iteration 227 loss: 0.10189849138259888, ACC:0.953125\n",
      "Training iteration 228 loss: 0.0523020401597023, ACC:1.0\n",
      "Training iteration 229 loss: 0.16109830141067505, ACC:0.9375\n",
      "Training iteration 230 loss: 0.10235335677862167, ACC:0.984375\n",
      "Training iteration 231 loss: 0.059713538736104965, ACC:0.984375\n",
      "Training iteration 232 loss: 0.17131786048412323, ACC:0.953125\n",
      "Training iteration 233 loss: 0.1278628408908844, ACC:0.9375\n",
      "Training iteration 234 loss: 0.09516143798828125, ACC:0.96875\n",
      "Training iteration 235 loss: 0.08274051547050476, ACC:0.984375\n",
      "Training iteration 236 loss: 0.11361879110336304, ACC:0.953125\n",
      "Training iteration 237 loss: 0.07205070555210114, ACC:0.984375\n",
      "Training iteration 238 loss: 0.11552143096923828, ACC:0.96875\n",
      "Training iteration 239 loss: 0.09204298257827759, ACC:0.984375\n",
      "Training iteration 240 loss: 0.07150261104106903, ACC:0.96875\n",
      "Training iteration 241 loss: 0.10869912058115005, ACC:0.953125\n",
      "Training iteration 242 loss: 0.07819917052984238, ACC:0.984375\n",
      "Training iteration 243 loss: 0.09941751509904861, ACC:0.96875\n",
      "Training iteration 244 loss: 0.06297224760055542, ACC:0.984375\n",
      "Training iteration 245 loss: 0.07247135788202286, ACC:0.96875\n",
      "Training iteration 246 loss: 0.1040177047252655, ACC:0.984375\n",
      "Training iteration 247 loss: 0.08376028388738632, ACC:0.953125\n",
      "Training iteration 248 loss: 0.04377572238445282, ACC:1.0\n",
      "Training iteration 249 loss: 0.09385824203491211, ACC:0.96875\n",
      "Training iteration 250 loss: 0.1462317407131195, ACC:0.9375\n",
      "Training iteration 251 loss: 0.10679500550031662, ACC:0.953125\n",
      "Training iteration 252 loss: 0.110197052359581, ACC:0.984375\n",
      "Training iteration 253 loss: 0.08981670439243317, ACC:1.0\n",
      "Training iteration 254 loss: 0.06919588893651962, ACC:0.96875\n",
      "Training iteration 255 loss: 0.12856854498386383, ACC:0.953125\n",
      "Training iteration 256 loss: 0.11598636209964752, ACC:0.96875\n",
      "Training iteration 257 loss: 0.07787244766950607, ACC:0.984375\n",
      "Training iteration 258 loss: 0.12041191756725311, ACC:0.953125\n",
      "Training iteration 259 loss: 0.07892096042633057, ACC:0.984375\n",
      "Training iteration 260 loss: 0.11127299815416336, ACC:0.953125\n",
      "Training iteration 261 loss: 0.18264919519424438, ACC:0.9375\n",
      "Training iteration 262 loss: 0.08682860434055328, ACC:0.96875\n",
      "Training iteration 263 loss: 0.1287223845720291, ACC:0.953125\n",
      "Training iteration 264 loss: 0.13748149573802948, ACC:0.96875\n",
      "Training iteration 265 loss: 0.05025576800107956, ACC:0.984375\n",
      "Training iteration 266 loss: 0.16401827335357666, ACC:0.9375\n",
      "Training iteration 267 loss: 0.15752749145030975, ACC:0.953125\n",
      "Training iteration 268 loss: 0.05955176055431366, ACC:1.0\n",
      "Training iteration 269 loss: 0.1650683730840683, ACC:0.953125\n",
      "Training iteration 270 loss: 0.06941931694746017, ACC:0.984375\n",
      "Training iteration 271 loss: 0.1378353387117386, ACC:0.96875\n",
      "Training iteration 272 loss: 0.17755939066410065, ACC:0.953125\n",
      "Training iteration 273 loss: 0.10746394097805023, ACC:0.96875\n",
      "Training iteration 274 loss: 0.06783507764339447, ACC:0.96875\n",
      "Training iteration 275 loss: 0.08001960813999176, ACC:0.953125\n",
      "Training iteration 276 loss: 0.12719812989234924, ACC:0.96875\n",
      "Training iteration 277 loss: 0.2439630627632141, ACC:0.921875\n",
      "Training iteration 278 loss: 0.1311546117067337, ACC:0.953125\n",
      "Training iteration 279 loss: 0.16848400235176086, ACC:0.9375\n",
      "Training iteration 280 loss: 0.16875435411930084, ACC:0.9375\n",
      "Training iteration 281 loss: 0.05048234015703201, ACC:0.984375\n",
      "Training iteration 282 loss: 0.19846069812774658, ACC:0.921875\n",
      "Training iteration 283 loss: 0.18772485852241516, ACC:0.9375\n",
      "Training iteration 284 loss: 0.10804387927055359, ACC:0.96875\n",
      "Training iteration 285 loss: 0.13456261157989502, ACC:0.96875\n",
      "Training iteration 286 loss: 0.16414108872413635, ACC:0.9375\n",
      "Training iteration 287 loss: 0.2019568681716919, ACC:0.921875\n",
      "Training iteration 288 loss: 0.15537023544311523, ACC:0.9375\n",
      "Training iteration 289 loss: 0.21735253930091858, ACC:0.90625\n",
      "Training iteration 290 loss: 0.3036186993122101, ACC:0.890625\n",
      "Training iteration 291 loss: 0.2323031723499298, ACC:0.9375\n",
      "Training iteration 292 loss: 0.14731204509735107, ACC:0.953125\n",
      "Training iteration 293 loss: 0.45701491832733154, ACC:0.875\n",
      "Training iteration 294 loss: 0.20918725430965424, ACC:0.9375\n",
      "Training iteration 295 loss: 0.13295651972293854, ACC:0.984375\n",
      "Training iteration 296 loss: 0.4600342810153961, ACC:0.78125\n",
      "Training iteration 297 loss: 0.1389460563659668, ACC:0.96875\n",
      "Training iteration 298 loss: 0.35743629932403564, ACC:0.90625\n",
      "Training iteration 299 loss: 0.21562105417251587, ACC:0.9375\n",
      "Training iteration 300 loss: 0.3806513249874115, ACC:0.828125\n",
      "Training iteration 301 loss: 0.16086558997631073, ACC:0.984375\n",
      "Training iteration 302 loss: 0.3423517942428589, ACC:0.859375\n",
      "Training iteration 303 loss: 0.41331350803375244, ACC:0.8125\n",
      "Training iteration 304 loss: 0.3222675025463104, ACC:0.90625\n",
      "Training iteration 305 loss: 0.2585620582103729, ACC:0.9375\n",
      "Training iteration 306 loss: 0.3092164695262909, ACC:0.921875\n",
      "Training iteration 307 loss: 0.31592562794685364, ACC:0.90625\n",
      "Training iteration 308 loss: 0.33140361309051514, ACC:0.890625\n",
      "Training iteration 309 loss: 0.26764151453971863, ACC:0.890625\n",
      "Training iteration 310 loss: 0.23762130737304688, ACC:0.921875\n",
      "Training iteration 311 loss: 0.44346025586128235, ACC:0.484375\n",
      "Training iteration 312 loss: 0.3640802204608917, ACC:0.90625\n",
      "Training iteration 313 loss: 0.2669108510017395, ACC:0.90625\n",
      "Training iteration 314 loss: 0.2591477334499359, ACC:0.90625\n",
      "Training iteration 315 loss: 0.21173109114170074, ACC:0.96875\n",
      "Training iteration 316 loss: 0.32935601472854614, ACC:0.84375\n",
      "Training iteration 317 loss: 0.3574959337711334, ACC:0.875\n",
      "Training iteration 318 loss: 0.17587003111839294, ACC:0.921875\n",
      "Training iteration 319 loss: 0.2943471670150757, ACC:0.875\n",
      "Training iteration 320 loss: 0.3327076733112335, ACC:0.890625\n",
      "Training iteration 321 loss: 0.33835768699645996, ACC:0.84375\n",
      "Training iteration 322 loss: 0.18484583497047424, ACC:0.953125\n",
      "Training iteration 323 loss: 0.10414119809865952, ACC:0.96875\n",
      "Training iteration 324 loss: 0.2596804201602936, ACC:0.90625\n",
      "Training iteration 325 loss: 0.17120426893234253, ACC:0.953125\n",
      "Training iteration 326 loss: 0.23384830355644226, ACC:0.90625\n",
      "Training iteration 327 loss: 0.17426642775535583, ACC:0.921875\n",
      "Training iteration 328 loss: 0.19916106760501862, ACC:0.953125\n",
      "Training iteration 329 loss: 0.17007949948310852, ACC:0.953125\n",
      "Training iteration 330 loss: 0.15251509845256805, ACC:0.953125\n",
      "Training iteration 331 loss: 0.12560294568538666, ACC:0.953125\n",
      "Training iteration 332 loss: 0.2479790300130844, ACC:0.9375\n",
      "Training iteration 333 loss: 0.25878363847732544, ACC:0.90625\n",
      "Training iteration 334 loss: 0.21872130036354065, ACC:0.921875\n",
      "Training iteration 335 loss: 0.23464280366897583, ACC:0.953125\n",
      "Training iteration 336 loss: 0.22870662808418274, ACC:0.9375\n",
      "Training iteration 337 loss: 0.2534934878349304, ACC:0.90625\n",
      "Training iteration 338 loss: 1.0775039196014404, ACC:0.546875\n",
      "Training iteration 339 loss: 0.34668925404548645, ACC:0.859375\n",
      "Training iteration 340 loss: 0.4240472912788391, ACC:0.796875\n",
      "Training iteration 341 loss: 0.2966856360435486, ACC:0.890625\n",
      "Training iteration 342 loss: 0.31257364153862, ACC:0.875\n",
      "Training iteration 343 loss: 0.3408483862876892, ACC:0.890625\n",
      "Training iteration 344 loss: 0.25615760684013367, ACC:0.921875\n",
      "Training iteration 345 loss: 0.4827287197113037, ACC:0.578125\n",
      "Training iteration 346 loss: 0.572708010673523, ACC:0.734375\n",
      "Training iteration 347 loss: 0.5370240211486816, ACC:0.625\n",
      "Training iteration 348 loss: 0.47547128796577454, ACC:0.828125\n",
      "Training iteration 349 loss: 0.47902911901474, ACC:0.828125\n",
      "Training iteration 350 loss: 0.4562695622444153, ACC:0.828125\n",
      "Training iteration 351 loss: 0.4090961515903473, ACC:0.84375\n",
      "Training iteration 352 loss: 0.31024181842803955, ACC:0.90625\n",
      "Training iteration 353 loss: 0.5821705460548401, ACC:0.546875\n",
      "Training iteration 354 loss: 0.5398295521736145, ACC:0.59375\n",
      "Training iteration 355 loss: 0.3907819986343384, ACC:0.859375\n",
      "Training iteration 356 loss: 0.446036159992218, ACC:0.796875\n",
      "Training iteration 357 loss: 0.5819709300994873, ACC:0.546875\n",
      "Training iteration 358 loss: 0.4977482557296753, ACC:0.8125\n",
      "Training iteration 359 loss: 0.5446054339408875, ACC:0.796875\n",
      "Training iteration 360 loss: 0.4503292143344879, ACC:0.859375\n",
      "Training iteration 361 loss: 0.40759849548339844, ACC:0.84375\n",
      "Training iteration 362 loss: 0.6746569871902466, ACC:0.5625\n",
      "Training iteration 363 loss: 0.4212532937526703, ACC:0.875\n",
      "Training iteration 364 loss: 0.347542405128479, ACC:0.90625\n",
      "Training iteration 365 loss: 0.5280675888061523, ACC:0.796875\n",
      "Training iteration 366 loss: 0.5556933283805847, ACC:0.703125\n",
      "Training iteration 367 loss: 0.46666407585144043, ACC:0.78125\n",
      "Training iteration 368 loss: 0.46440765261650085, ACC:0.734375\n",
      "Training iteration 369 loss: 0.5590118169784546, ACC:0.671875\n",
      "Training iteration 370 loss: 0.47057852149009705, ACC:0.53125\n",
      "Training iteration 371 loss: 0.4652508497238159, ACC:0.578125\n",
      "Training iteration 372 loss: 0.5186756253242493, ACC:0.46875\n",
      "Training iteration 373 loss: 0.5053839087486267, ACC:0.703125\n",
      "Training iteration 374 loss: 0.5520884990692139, ACC:0.921875\n",
      "Training iteration 375 loss: 0.43619441986083984, ACC:0.875\n",
      "Training iteration 376 loss: 0.35652193427085876, ACC:0.875\n",
      "Training iteration 377 loss: 0.3053516149520874, ACC:0.921875\n",
      "Training iteration 378 loss: 0.29162800312042236, ACC:0.875\n",
      "Training iteration 379 loss: 0.2543206810951233, ACC:0.921875\n",
      "Training iteration 380 loss: 0.33047571778297424, ACC:0.921875\n",
      "Training iteration 381 loss: 0.292845219373703, ACC:0.90625\n",
      "Training iteration 382 loss: 0.3059280216693878, ACC:0.890625\n",
      "Training iteration 383 loss: 0.2656192183494568, ACC:0.9375\n",
      "Training iteration 384 loss: 0.43736010789871216, ACC:0.8125\n",
      "Training iteration 385 loss: 0.3943258821964264, ACC:0.828125\n",
      "Training iteration 386 loss: 0.3468194007873535, ACC:0.859375\n",
      "Training iteration 387 loss: 0.37858283519744873, ACC:0.84375\n",
      "Training iteration 388 loss: 0.539012610912323, ACC:0.734375\n",
      "Training iteration 389 loss: 0.31698712706565857, ACC:0.875\n",
      "Training iteration 390 loss: 0.2293546199798584, ACC:0.953125\n",
      "Training iteration 391 loss: 0.4007589519023895, ACC:0.828125\n",
      "Training iteration 392 loss: 0.274143248796463, ACC:0.890625\n",
      "Training iteration 393 loss: 0.2256489247083664, ACC:0.921875\n",
      "Training iteration 394 loss: 0.3252286911010742, ACC:0.84375\n",
      "Training iteration 395 loss: 0.33324629068374634, ACC:0.84375\n",
      "Training iteration 396 loss: 0.2458259016275406, ACC:0.90625\n",
      "Training iteration 397 loss: 0.3129335939884186, ACC:0.890625\n",
      "Training iteration 398 loss: 0.34154823422431946, ACC:0.890625\n",
      "Training iteration 399 loss: 0.22772836685180664, ACC:0.9375\n",
      "Training iteration 400 loss: 0.19969230890274048, ACC:0.953125\n",
      "Training iteration 401 loss: 0.28872182965278625, ACC:0.921875\n",
      "Training iteration 402 loss: 0.2435462921857834, ACC:0.921875\n",
      "Training iteration 403 loss: 0.19868314266204834, ACC:0.9375\n",
      "Training iteration 404 loss: 0.24364812672138214, ACC:0.890625\n",
      "Training iteration 405 loss: 0.18335582315921783, ACC:0.921875\n",
      "Training iteration 406 loss: 0.4361509680747986, ACC:0.6875\n",
      "Training iteration 407 loss: 0.39856767654418945, ACC:0.84375\n",
      "Training iteration 408 loss: 0.1695251613855362, ACC:0.953125\n",
      "Training iteration 409 loss: 0.21309258043766022, ACC:0.90625\n",
      "Training iteration 410 loss: 0.23468315601348877, ACC:0.9375\n",
      "Training iteration 411 loss: 0.2322750836610794, ACC:0.953125\n",
      "Training iteration 412 loss: 0.2292603850364685, ACC:0.9375\n",
      "Training iteration 413 loss: 0.3508773148059845, ACC:0.875\n",
      "Training iteration 414 loss: 0.2125333547592163, ACC:0.921875\n",
      "Training iteration 415 loss: 0.21791687607765198, ACC:0.9375\n",
      "Training iteration 416 loss: 0.23598133027553558, ACC:0.90625\n",
      "Training iteration 417 loss: 0.31104233860969543, ACC:0.890625\n",
      "Training iteration 418 loss: 0.1525839865207672, ACC:0.984375\n",
      "Training iteration 419 loss: 0.23044219613075256, ACC:0.921875\n",
      "Training iteration 420 loss: 0.21813271939754486, ACC:0.953125\n",
      "Training iteration 421 loss: 0.20828019082546234, ACC:0.921875\n",
      "Training iteration 422 loss: 0.33456745743751526, ACC:0.875\n",
      "Training iteration 423 loss: 0.30300647020339966, ACC:0.890625\n",
      "Training iteration 424 loss: 0.20638507604599, ACC:0.921875\n",
      "Training iteration 425 loss: 0.16140595078468323, ACC:0.96875\n",
      "Training iteration 426 loss: 0.21256279945373535, ACC:0.96875\n",
      "Training iteration 427 loss: 0.13267478346824646, ACC:0.96875\n",
      "Training iteration 428 loss: 0.1385168582201004, ACC:0.984375\n",
      "Training iteration 429 loss: 0.2028040885925293, ACC:0.921875\n",
      "Training iteration 430 loss: 0.15614819526672363, ACC:0.953125\n",
      "Training iteration 431 loss: 0.15931189060211182, ACC:0.921875\n",
      "Training iteration 432 loss: 0.17857517302036285, ACC:0.96875\n",
      "Training iteration 433 loss: 0.20011676847934723, ACC:0.921875\n",
      "Training iteration 434 loss: 0.20505975186824799, ACC:0.953125\n",
      "Training iteration 435 loss: 0.2834610044956207, ACC:0.890625\n",
      "Training iteration 436 loss: 0.13194124400615692, ACC:0.953125\n",
      "Training iteration 437 loss: 0.25566092133522034, ACC:0.890625\n",
      "Training iteration 438 loss: 0.2577628195285797, ACC:0.859375\n",
      "Training iteration 439 loss: 0.34048599004745483, ACC:0.828125\n",
      "Training iteration 440 loss: 0.22578027844429016, ACC:0.9375\n",
      "Training iteration 441 loss: 0.10742641240358353, ACC:0.953125\n",
      "Training iteration 442 loss: 0.17503972351551056, ACC:0.953125\n",
      "Training iteration 443 loss: 0.2099960595369339, ACC:0.953125\n",
      "Training iteration 444 loss: 0.2980304956436157, ACC:0.875\n",
      "Training iteration 445 loss: 0.5067737698554993, ACC:0.78125\n",
      "Training iteration 446 loss: 0.4802027940750122, ACC:0.8125\n",
      "Training iteration 447 loss: 0.2389674186706543, ACC:0.90625\n",
      "Training iteration 448 loss: 0.1763588786125183, ACC:0.96875\n",
      "Training iteration 449 loss: 0.18806882202625275, ACC:0.890625\n",
      "Training iteration 450 loss: 0.27667611837387085, ACC:0.875\n",
      "Validation iteration 451 loss: 0.24315473437309265, ACC: 0.90625\n",
      "Validation iteration 452 loss: 0.26590946316719055, ACC: 0.921875\n",
      "Validation iteration 453 loss: 0.20943893492221832, ACC: 0.921875\n",
      "Validation iteration 454 loss: 0.23522567749023438, ACC: 0.9375\n",
      "Validation iteration 455 loss: 0.21372804045677185, ACC: 0.953125\n",
      "Validation iteration 456 loss: 0.24176327884197235, ACC: 0.921875\n",
      "Validation iteration 457 loss: 0.24321219325065613, ACC: 0.90625\n",
      "Validation iteration 458 loss: 0.29324018955230713, ACC: 0.921875\n",
      "Validation iteration 459 loss: 0.19984261691570282, ACC: 0.953125\n",
      "Validation iteration 460 loss: 0.24884702265262604, ACC: 0.875\n",
      "Validation iteration 461 loss: 0.3271122872829437, ACC: 0.859375\n",
      "Validation iteration 462 loss: 0.2810937762260437, ACC: 0.953125\n",
      "Validation iteration 463 loss: 0.24423745274543762, ACC: 0.921875\n",
      "Validation iteration 464 loss: 0.22580495476722717, ACC: 0.90625\n",
      "Validation iteration 465 loss: 0.24664443731307983, ACC: 0.921875\n",
      "Validation iteration 466 loss: 0.30611029267311096, ACC: 0.84375\n",
      "Validation iteration 467 loss: 0.25930216908454895, ACC: 0.921875\n",
      "Validation iteration 468 loss: 0.2796025574207306, ACC: 0.90625\n",
      "Validation iteration 469 loss: 0.3126446604728699, ACC: 0.859375\n",
      "Validation iteration 470 loss: 0.22059474885463715, ACC: 0.921875\n",
      "Validation iteration 471 loss: 0.24642999470233917, ACC: 0.921875\n",
      "Validation iteration 472 loss: 0.23932521045207977, ACC: 0.921875\n",
      "Validation iteration 473 loss: 0.24199619889259338, ACC: 0.890625\n",
      "Validation iteration 474 loss: 0.2220502644777298, ACC: 0.9375\n",
      "Validation iteration 475 loss: 0.29390838742256165, ACC: 0.875\n",
      "Validation iteration 476 loss: 0.261841744184494, ACC: 0.90625\n",
      "Validation iteration 477 loss: 0.2720887362957001, ACC: 0.90625\n",
      "Validation iteration 478 loss: 0.23274922370910645, ACC: 0.9375\n",
      "Validation iteration 479 loss: 0.2013072967529297, ACC: 0.921875\n",
      "Validation iteration 480 loss: 0.2883090674877167, ACC: 0.90625\n",
      "Validation iteration 481 loss: 0.37218722701072693, ACC: 0.875\n",
      "Validation iteration 482 loss: 0.315717875957489, ACC: 0.859375\n",
      "Validation iteration 483 loss: 0.233554869890213, ACC: 0.921875\n",
      "Validation iteration 484 loss: 0.2970111668109894, ACC: 0.859375\n",
      "Validation iteration 485 loss: 0.3108205199241638, ACC: 0.90625\n",
      "Validation iteration 486 loss: 0.35347169637680054, ACC: 0.84375\n",
      "Validation iteration 487 loss: 0.24444636702537537, ACC: 0.875\n",
      "Validation iteration 488 loss: 0.1780025213956833, ACC: 0.953125\n",
      "Validation iteration 489 loss: 0.2945222854614258, ACC: 0.90625\n",
      "Validation iteration 490 loss: 0.28720876574516296, ACC: 0.921875\n",
      "Validation iteration 491 loss: 0.3085334599018097, ACC: 0.921875\n",
      "Validation iteration 492 loss: 0.1606319546699524, ACC: 0.984375\n",
      "Validation iteration 493 loss: 0.27090197801589966, ACC: 0.890625\n",
      "Validation iteration 494 loss: 0.24618679285049438, ACC: 0.9375\n",
      "Validation iteration 495 loss: 0.2092900574207306, ACC: 0.921875\n",
      "Validation iteration 496 loss: 0.262843519449234, ACC: 0.90625\n",
      "Validation iteration 497 loss: 0.2787990868091583, ACC: 0.890625\n",
      "Validation iteration 498 loss: 0.24139991402626038, ACC: 0.90625\n",
      "Validation iteration 499 loss: 0.2412053346633911, ACC: 0.921875\n",
      "Validation iteration 500 loss: 0.2690066993236542, ACC: 0.90625\n",
      "-- Epoch 8 done -- Train loss: 0.2317483528951804, train ACC: 0.9120486111111111, val loss: 0.2594651541113853, val ACC: 0.909375\n",
      "<--- 1993.8885481357574 seconds --->\n",
      "Training iteration 1 loss: 0.2619827389717102, ACC:0.890625\n",
      "Training iteration 2 loss: 0.18246005475521088, ACC:0.9375\n",
      "Training iteration 3 loss: 0.13339553773403168, ACC:0.96875\n",
      "Training iteration 4 loss: 0.14791570603847504, ACC:0.96875\n",
      "Training iteration 5 loss: 0.2916945815086365, ACC:0.890625\n",
      "Training iteration 6 loss: 0.26818808913230896, ACC:0.9375\n",
      "Training iteration 7 loss: 0.17289958894252777, ACC:0.953125\n",
      "Training iteration 8 loss: 0.22001054883003235, ACC:0.9375\n",
      "Training iteration 9 loss: 0.19686457514762878, ACC:0.9375\n",
      "Training iteration 10 loss: 0.1356704980134964, ACC:0.953125\n",
      "Training iteration 11 loss: 0.10356656461954117, ACC:0.96875\n",
      "Training iteration 12 loss: 0.26491621136665344, ACC:0.890625\n",
      "Training iteration 13 loss: 0.12038379162549973, ACC:0.953125\n",
      "Training iteration 14 loss: 0.12581683695316315, ACC:0.984375\n",
      "Training iteration 15 loss: 0.19949188828468323, ACC:0.9375\n",
      "Training iteration 16 loss: 0.11107055097818375, ACC:0.984375\n",
      "Training iteration 17 loss: 0.3513409495353699, ACC:0.875\n",
      "Training iteration 18 loss: 0.2955286502838135, ACC:0.9375\n",
      "Training iteration 19 loss: 0.2478255033493042, ACC:0.9375\n",
      "Training iteration 20 loss: 0.2297707200050354, ACC:0.953125\n",
      "Training iteration 21 loss: 0.33315208554267883, ACC:0.921875\n",
      "Training iteration 22 loss: 0.3522590398788452, ACC:0.859375\n",
      "Training iteration 23 loss: 0.23222970962524414, ACC:0.890625\n",
      "Training iteration 24 loss: 0.2464740127325058, ACC:0.90625\n",
      "Training iteration 25 loss: 0.3207070529460907, ACC:0.90625\n",
      "Training iteration 26 loss: 0.4004582166671753, ACC:0.859375\n",
      "Training iteration 27 loss: 0.329239159822464, ACC:0.890625\n",
      "Training iteration 28 loss: 0.34198784828186035, ACC:0.921875\n",
      "Training iteration 29 loss: 0.2507171332836151, ACC:0.90625\n",
      "Training iteration 30 loss: 0.3477928638458252, ACC:0.84375\n",
      "Training iteration 31 loss: 0.17211467027664185, ACC:0.96875\n",
      "Training iteration 32 loss: 0.3082156777381897, ACC:0.90625\n",
      "Training iteration 33 loss: 0.1563379317522049, ACC:0.953125\n",
      "Training iteration 34 loss: 0.1922803521156311, ACC:0.953125\n",
      "Training iteration 35 loss: 0.22381025552749634, ACC:0.9375\n",
      "Training iteration 36 loss: 0.3300727903842926, ACC:0.890625\n",
      "Training iteration 37 loss: 0.2705015540122986, ACC:0.859375\n",
      "Training iteration 38 loss: 0.3140498697757721, ACC:0.859375\n",
      "Training iteration 39 loss: 0.29174304008483887, ACC:0.90625\n",
      "Training iteration 40 loss: 0.3519279658794403, ACC:0.859375\n",
      "Training iteration 41 loss: 0.18912488222122192, ACC:0.9375\n",
      "Training iteration 42 loss: 0.3072587549686432, ACC:0.890625\n",
      "Training iteration 43 loss: 0.26402559876441956, ACC:0.90625\n",
      "Training iteration 44 loss: 0.2277132123708725, ACC:0.9375\n",
      "Training iteration 45 loss: 0.12154412269592285, ACC:0.953125\n",
      "Training iteration 46 loss: 0.20859993994235992, ACC:0.921875\n",
      "Training iteration 47 loss: 0.17565935850143433, ACC:0.9375\n",
      "Training iteration 48 loss: 0.20338860154151917, ACC:0.921875\n",
      "Training iteration 49 loss: 0.15168608725070953, ACC:0.921875\n",
      "Training iteration 50 loss: 0.20564010739326477, ACC:0.890625\n",
      "Training iteration 51 loss: 0.1772567182779312, ACC:0.953125\n",
      "Training iteration 52 loss: 0.23227903246879578, ACC:0.921875\n",
      "Training iteration 53 loss: 0.2034093141555786, ACC:0.9375\n",
      "Training iteration 54 loss: 0.3101661503314972, ACC:0.859375\n",
      "Training iteration 55 loss: 0.29470446705818176, ACC:0.8125\n",
      "Training iteration 56 loss: 0.23363110423088074, ACC:0.90625\n",
      "Training iteration 57 loss: 0.3545498549938202, ACC:0.828125\n",
      "Training iteration 58 loss: 0.32741209864616394, ACC:0.84375\n",
      "Training iteration 59 loss: 0.2492893487215042, ACC:0.90625\n",
      "Training iteration 60 loss: 0.28629985451698303, ACC:0.90625\n",
      "Training iteration 61 loss: 0.27102717757225037, ACC:0.875\n",
      "Training iteration 62 loss: 0.26257288455963135, ACC:0.921875\n",
      "Training iteration 63 loss: 0.22548528015613556, ACC:0.9375\n",
      "Training iteration 64 loss: 0.24629759788513184, ACC:0.921875\n",
      "Training iteration 65 loss: 0.21129454672336578, ACC:0.9375\n",
      "Training iteration 66 loss: 0.27629682421684265, ACC:0.921875\n",
      "Training iteration 67 loss: 0.26770201325416565, ACC:0.921875\n",
      "Training iteration 68 loss: 0.530619740486145, ACC:0.53125\n",
      "Training iteration 69 loss: 0.5764581561088562, ACC:0.46875\n",
      "Training iteration 70 loss: 0.5399696826934814, ACC:0.46875\n",
      "Training iteration 71 loss: 0.5107365846633911, ACC:0.875\n",
      "Training iteration 72 loss: 0.4680914878845215, ACC:0.78125\n",
      "Training iteration 73 loss: 0.47598254680633545, ACC:0.84375\n",
      "Training iteration 74 loss: 0.3334701657295227, ACC:0.921875\n",
      "Training iteration 75 loss: 0.33087244629859924, ACC:0.921875\n",
      "Training iteration 76 loss: 0.3190907835960388, ACC:0.90625\n",
      "Training iteration 77 loss: 0.3725515305995941, ACC:0.875\n",
      "Training iteration 78 loss: 0.31040260195732117, ACC:0.90625\n",
      "Training iteration 79 loss: 0.37051305174827576, ACC:0.859375\n",
      "Training iteration 80 loss: 0.40406960248947144, ACC:0.84375\n",
      "Training iteration 81 loss: 0.25340980291366577, ACC:0.953125\n",
      "Training iteration 82 loss: 0.36220166087150574, ACC:0.859375\n",
      "Training iteration 83 loss: 0.3933095932006836, ACC:0.84375\n",
      "Training iteration 84 loss: 0.2707861065864563, ACC:0.96875\n",
      "Training iteration 85 loss: 0.356148362159729, ACC:0.96875\n",
      "Training iteration 86 loss: 0.4363299012184143, ACC:0.90625\n",
      "Training iteration 87 loss: 0.4189395010471344, ACC:0.890625\n",
      "Training iteration 88 loss: 0.47877931594848633, ACC:0.875\n",
      "Training iteration 89 loss: 0.38405904173851013, ACC:0.921875\n",
      "Training iteration 90 loss: 0.31851711869239807, ACC:0.90625\n",
      "Training iteration 91 loss: 0.3454049825668335, ACC:0.921875\n",
      "Training iteration 92 loss: 0.4003992974758148, ACC:0.921875\n",
      "Training iteration 93 loss: 0.32690027356147766, ACC:0.90625\n",
      "Training iteration 94 loss: 0.381420761346817, ACC:0.828125\n",
      "Training iteration 95 loss: 0.3325255215167999, ACC:0.875\n",
      "Training iteration 96 loss: 0.3896690309047699, ACC:0.828125\n",
      "Training iteration 97 loss: 0.4347296357154846, ACC:0.8125\n",
      "Training iteration 98 loss: 0.33321231603622437, ACC:0.890625\n",
      "Training iteration 99 loss: 0.33257508277893066, ACC:0.90625\n",
      "Training iteration 100 loss: 0.4197136163711548, ACC:0.796875\n",
      "Training iteration 101 loss: 0.3276437222957611, ACC:0.921875\n",
      "Training iteration 102 loss: 0.40826350450515747, ACC:0.875\n",
      "Training iteration 103 loss: 0.4068785011768341, ACC:0.875\n",
      "Training iteration 104 loss: 0.3495587408542633, ACC:0.921875\n",
      "Training iteration 105 loss: 0.3359118402004242, ACC:0.90625\n",
      "Training iteration 106 loss: 0.3758350610733032, ACC:0.875\n",
      "Training iteration 107 loss: 0.4617491364479065, ACC:0.796875\n",
      "Training iteration 108 loss: 0.28339430689811707, ACC:0.9375\n",
      "Training iteration 109 loss: 0.36304333806037903, ACC:0.84375\n",
      "Training iteration 110 loss: 0.2982714772224426, ACC:0.921875\n",
      "Training iteration 111 loss: 0.22398217022418976, ACC:1.0\n",
      "Training iteration 112 loss: 0.2581554055213928, ACC:0.9375\n",
      "Training iteration 113 loss: 0.2286553531885147, ACC:0.96875\n",
      "Training iteration 114 loss: 0.30336031317710876, ACC:0.90625\n",
      "Training iteration 115 loss: 0.23242607712745667, ACC:0.96875\n",
      "Training iteration 116 loss: 0.29169610142707825, ACC:0.90625\n",
      "Training iteration 117 loss: 0.21339327096939087, ACC:0.953125\n",
      "Training iteration 118 loss: 0.279975563287735, ACC:0.921875\n",
      "Training iteration 119 loss: 0.22379161417484283, ACC:0.953125\n",
      "Training iteration 120 loss: 0.28501567244529724, ACC:0.9375\n",
      "Training iteration 121 loss: 0.3234002888202667, ACC:0.890625\n",
      "Training iteration 122 loss: 0.2719060182571411, ACC:0.890625\n",
      "Training iteration 123 loss: 0.2690252661705017, ACC:0.859375\n",
      "Training iteration 124 loss: 0.2114468514919281, ACC:0.984375\n",
      "Training iteration 125 loss: 0.20838496088981628, ACC:0.96875\n",
      "Training iteration 126 loss: 0.2860799729824066, ACC:0.953125\n",
      "Training iteration 127 loss: 0.30858027935028076, ACC:0.890625\n",
      "Training iteration 128 loss: 0.24821439385414124, ACC:0.9375\n",
      "Training iteration 129 loss: 0.19661477208137512, ACC:0.984375\n",
      "Training iteration 130 loss: 0.2357664853334427, ACC:0.9375\n",
      "Training iteration 131 loss: 0.16811524331569672, ACC:0.9375\n",
      "Training iteration 132 loss: 0.23125356435775757, ACC:0.90625\n",
      "Training iteration 133 loss: 0.260984867811203, ACC:0.921875\n",
      "Training iteration 134 loss: 0.23584601283073425, ACC:0.90625\n",
      "Training iteration 135 loss: 0.17865322530269623, ACC:0.953125\n",
      "Training iteration 136 loss: 0.15013420581817627, ACC:0.96875\n",
      "Training iteration 137 loss: 0.1923440396785736, ACC:0.953125\n",
      "Training iteration 138 loss: 0.17767731845378876, ACC:0.953125\n",
      "Training iteration 139 loss: 0.19008612632751465, ACC:0.96875\n",
      "Training iteration 140 loss: 0.1515340805053711, ACC:0.96875\n",
      "Training iteration 141 loss: 0.2363242506980896, ACC:0.9375\n",
      "Training iteration 142 loss: 0.16818201541900635, ACC:0.953125\n",
      "Training iteration 143 loss: 0.2787628471851349, ACC:0.875\n",
      "Training iteration 144 loss: 0.2240196168422699, ACC:0.890625\n",
      "Training iteration 145 loss: 0.28640517592430115, ACC:0.859375\n",
      "Training iteration 146 loss: 0.2277512550354004, ACC:0.9375\n",
      "Training iteration 147 loss: 0.21311256289482117, ACC:0.96875\n",
      "Training iteration 148 loss: 0.21262985467910767, ACC:0.953125\n",
      "Training iteration 149 loss: 0.18124689161777496, ACC:0.9375\n",
      "Training iteration 150 loss: 0.15308475494384766, ACC:0.984375\n",
      "Training iteration 151 loss: 0.2155369371175766, ACC:0.9375\n",
      "Training iteration 152 loss: 0.1678294837474823, ACC:0.921875\n",
      "Training iteration 153 loss: 0.20603777468204498, ACC:0.90625\n",
      "Training iteration 154 loss: 0.23081384599208832, ACC:0.90625\n",
      "Training iteration 155 loss: 0.17521509528160095, ACC:0.96875\n",
      "Training iteration 156 loss: 0.2006983458995819, ACC:0.96875\n",
      "Training iteration 157 loss: 0.4882935583591461, ACC:0.75\n",
      "Training iteration 158 loss: 0.12017108500003815, ACC:0.96875\n",
      "Training iteration 159 loss: 0.1990090161561966, ACC:0.9375\n",
      "Training iteration 160 loss: 0.23309269547462463, ACC:0.90625\n",
      "Training iteration 161 loss: 0.3177102208137512, ACC:0.890625\n",
      "Training iteration 162 loss: 0.19623953104019165, ACC:0.921875\n",
      "Training iteration 163 loss: 0.19933834671974182, ACC:0.953125\n",
      "Training iteration 164 loss: 0.2464231550693512, ACC:0.9375\n",
      "Training iteration 165 loss: 0.22756634652614594, ACC:0.9375\n",
      "Training iteration 166 loss: 0.20763742923736572, ACC:0.921875\n",
      "Training iteration 167 loss: 0.22727541625499725, ACC:0.953125\n",
      "Training iteration 168 loss: 0.2183283120393753, ACC:0.96875\n",
      "Training iteration 169 loss: 0.2607906758785248, ACC:0.90625\n",
      "Training iteration 170 loss: 0.26929953694343567, ACC:0.9375\n",
      "Training iteration 171 loss: 0.28867921233177185, ACC:0.90625\n",
      "Training iteration 172 loss: 0.37626373767852783, ACC:0.875\n",
      "Training iteration 173 loss: 0.3742944300174713, ACC:0.8125\n",
      "Training iteration 174 loss: 0.2743510603904724, ACC:0.890625\n",
      "Training iteration 175 loss: 0.30036869645118713, ACC:0.90625\n",
      "Training iteration 176 loss: 0.31306517124176025, ACC:0.84375\n",
      "Training iteration 177 loss: 0.3223389685153961, ACC:0.859375\n",
      "Training iteration 178 loss: 0.3459228575229645, ACC:0.859375\n",
      "Training iteration 179 loss: 0.2776446044445038, ACC:0.921875\n",
      "Training iteration 180 loss: 0.32664990425109863, ACC:0.859375\n",
      "Training iteration 181 loss: 0.3215738832950592, ACC:0.828125\n",
      "Training iteration 182 loss: 0.24551606178283691, ACC:0.921875\n",
      "Training iteration 183 loss: 0.1726321429014206, ACC:0.953125\n",
      "Training iteration 184 loss: 0.2952632009983063, ACC:0.90625\n",
      "Training iteration 185 loss: 0.26207756996154785, ACC:0.90625\n",
      "Training iteration 186 loss: 0.243837371468544, ACC:0.921875\n",
      "Training iteration 187 loss: 0.27907297015190125, ACC:0.90625\n",
      "Training iteration 188 loss: 0.26715394854545593, ACC:0.890625\n",
      "Training iteration 189 loss: 0.300005167722702, ACC:0.9375\n",
      "Training iteration 190 loss: 0.1784808486700058, ACC:0.984375\n",
      "Training iteration 191 loss: 0.30672186613082886, ACC:0.875\n",
      "Training iteration 192 loss: 0.24614976346492767, ACC:0.90625\n",
      "Training iteration 193 loss: 0.29218024015426636, ACC:0.921875\n",
      "Training iteration 194 loss: 0.22229745984077454, ACC:0.9375\n",
      "Training iteration 195 loss: 0.27488094568252563, ACC:0.890625\n",
      "Training iteration 196 loss: 0.24747271835803986, ACC:0.921875\n",
      "Training iteration 197 loss: 0.2307979166507721, ACC:0.9375\n",
      "Training iteration 198 loss: 0.36220741271972656, ACC:0.875\n",
      "Training iteration 199 loss: 0.3232005536556244, ACC:0.9375\n",
      "Training iteration 200 loss: 0.28513357043266296, ACC:0.9375\n",
      "Training iteration 201 loss: 0.32014748454093933, ACC:0.9375\n",
      "Training iteration 202 loss: 0.2434559017419815, ACC:0.96875\n",
      "Training iteration 203 loss: 0.3749012351036072, ACC:0.890625\n",
      "Training iteration 204 loss: 0.2346164584159851, ACC:0.9375\n",
      "Training iteration 205 loss: 0.36259761452674866, ACC:0.921875\n",
      "Training iteration 206 loss: 0.27820464968681335, ACC:0.9375\n",
      "Training iteration 207 loss: 0.43243607878685, ACC:0.90625\n",
      "Training iteration 208 loss: 0.30174723267555237, ACC:0.875\n",
      "Training iteration 209 loss: 0.35212182998657227, ACC:0.875\n",
      "Training iteration 210 loss: 0.34739115834236145, ACC:0.875\n",
      "Training iteration 211 loss: 0.2653527855873108, ACC:0.9375\n",
      "Training iteration 212 loss: 0.25415167212486267, ACC:0.90625\n",
      "Training iteration 213 loss: 0.31510263681411743, ACC:0.859375\n",
      "Training iteration 214 loss: 0.2083328664302826, ACC:0.9375\n",
      "Training iteration 215 loss: 0.3352491557598114, ACC:0.890625\n",
      "Training iteration 216 loss: 0.3016239106655121, ACC:0.9375\n",
      "Training iteration 217 loss: 0.33932173252105713, ACC:0.875\n",
      "Training iteration 218 loss: 0.23366187512874603, ACC:0.953125\n",
      "Training iteration 219 loss: 0.2602183520793915, ACC:0.953125\n",
      "Training iteration 220 loss: 0.2481880784034729, ACC:0.984375\n",
      "Training iteration 221 loss: 0.22877304255962372, ACC:0.953125\n",
      "Training iteration 222 loss: 0.24434557557106018, ACC:0.921875\n",
      "Training iteration 223 loss: 0.2707676291465759, ACC:0.9375\n",
      "Training iteration 224 loss: 0.24925149977207184, ACC:0.90625\n",
      "Training iteration 225 loss: 0.34770116209983826, ACC:0.875\n",
      "Training iteration 226 loss: 0.28375786542892456, ACC:0.890625\n",
      "Training iteration 227 loss: 0.26174870133399963, ACC:0.90625\n",
      "Training iteration 228 loss: 0.22380530834197998, ACC:0.9375\n",
      "Training iteration 229 loss: 0.25003355741500854, ACC:0.90625\n",
      "Training iteration 230 loss: 0.2700195610523224, ACC:0.90625\n",
      "Training iteration 231 loss: 0.25383108854293823, ACC:0.96875\n",
      "Training iteration 232 loss: 0.3212108016014099, ACC:0.890625\n",
      "Training iteration 233 loss: 0.231913223862648, ACC:0.921875\n",
      "Training iteration 234 loss: 0.19600753486156464, ACC:0.953125\n",
      "Training iteration 235 loss: 0.21091872453689575, ACC:0.9375\n",
      "Training iteration 236 loss: 0.2590913474559784, ACC:0.890625\n",
      "Training iteration 237 loss: 0.1850273609161377, ACC:0.953125\n",
      "Training iteration 238 loss: 0.1641390472650528, ACC:0.953125\n",
      "Training iteration 239 loss: 0.2295350879430771, ACC:0.921875\n",
      "Training iteration 240 loss: 0.2554255425930023, ACC:0.890625\n",
      "Training iteration 241 loss: 0.20039884746074677, ACC:0.9375\n",
      "Training iteration 242 loss: 0.1923789083957672, ACC:0.9375\n",
      "Training iteration 243 loss: 0.10629761964082718, ACC:1.0\n",
      "Training iteration 244 loss: 0.2961045503616333, ACC:0.90625\n",
      "Training iteration 245 loss: 0.2092462033033371, ACC:0.9375\n",
      "Training iteration 246 loss: 0.1674690991640091, ACC:0.953125\n",
      "Training iteration 247 loss: 0.22789156436920166, ACC:0.9375\n",
      "Training iteration 248 loss: 0.3643820583820343, ACC:0.796875\n",
      "Training iteration 249 loss: 0.22487036883831024, ACC:0.9375\n",
      "Training iteration 250 loss: 0.22298574447631836, ACC:0.9375\n",
      "Training iteration 251 loss: 0.14707832038402557, ACC:0.953125\n",
      "Training iteration 252 loss: 0.1508907973766327, ACC:0.921875\n",
      "Training iteration 253 loss: 0.2753327786922455, ACC:0.875\n",
      "Training iteration 254 loss: 0.22243967652320862, ACC:0.921875\n",
      "Training iteration 255 loss: 0.14406830072402954, ACC:0.953125\n",
      "Training iteration 256 loss: 0.15271435678005219, ACC:0.984375\n",
      "Training iteration 257 loss: 0.25525009632110596, ACC:0.921875\n",
      "Training iteration 258 loss: 0.14380402863025665, ACC:0.96875\n",
      "Training iteration 259 loss: 0.15099264681339264, ACC:0.96875\n",
      "Training iteration 260 loss: 0.18712644279003143, ACC:0.953125\n",
      "Training iteration 261 loss: 0.18922598659992218, ACC:0.9375\n",
      "Training iteration 262 loss: 0.27364271879196167, ACC:0.90625\n",
      "Training iteration 263 loss: 0.1892368495464325, ACC:0.953125\n",
      "Training iteration 264 loss: 0.19825181365013123, ACC:0.890625\n",
      "Training iteration 265 loss: 0.11023903638124466, ACC:0.96875\n",
      "Training iteration 266 loss: 0.1144849881529808, ACC:0.96875\n",
      "Training iteration 267 loss: 0.14918528497219086, ACC:0.953125\n",
      "Training iteration 268 loss: 0.27450239658355713, ACC:0.890625\n",
      "Training iteration 269 loss: 0.18401655554771423, ACC:0.9375\n",
      "Training iteration 270 loss: 0.3079797923564911, ACC:0.921875\n",
      "Training iteration 271 loss: 0.26786336302757263, ACC:0.90625\n",
      "Training iteration 272 loss: 0.24530881643295288, ACC:0.96875\n",
      "Training iteration 273 loss: 0.16090145707130432, ACC:0.96875\n",
      "Training iteration 274 loss: 0.08664810657501221, ACC:1.0\n",
      "Training iteration 275 loss: 0.11679831892251968, ACC:0.984375\n",
      "Training iteration 276 loss: 0.2429051250219345, ACC:0.890625\n",
      "Training iteration 277 loss: 0.1951049119234085, ACC:0.9375\n",
      "Training iteration 278 loss: 0.13602018356323242, ACC:0.9375\n",
      "Training iteration 279 loss: 0.18489590287208557, ACC:0.9375\n",
      "Training iteration 280 loss: 0.13494570553302765, ACC:0.9375\n",
      "Training iteration 281 loss: 0.1627495139837265, ACC:0.953125\n",
      "Training iteration 282 loss: 0.07502081245183945, ACC:1.0\n",
      "Training iteration 283 loss: 0.18764770030975342, ACC:0.921875\n",
      "Training iteration 284 loss: 0.1721142679452896, ACC:0.9375\n",
      "Training iteration 285 loss: 0.12623368203639984, ACC:0.96875\n",
      "Training iteration 286 loss: 0.05961429327726364, ACC:1.0\n",
      "Training iteration 287 loss: 0.10815189778804779, ACC:0.984375\n",
      "Training iteration 288 loss: 0.1817944496870041, ACC:0.984375\n",
      "Training iteration 289 loss: 0.1614290028810501, ACC:0.953125\n",
      "Training iteration 290 loss: 0.09660317003726959, ACC:0.96875\n",
      "Training iteration 291 loss: 0.14990130066871643, ACC:0.953125\n",
      "Training iteration 292 loss: 0.09259624779224396, ACC:0.984375\n",
      "Training iteration 293 loss: 0.11609674990177155, ACC:0.96875\n",
      "Training iteration 294 loss: 0.14155331254005432, ACC:0.96875\n",
      "Training iteration 295 loss: 0.13106116652488708, ACC:0.96875\n",
      "Training iteration 296 loss: 0.11417178064584732, ACC:0.953125\n",
      "Training iteration 297 loss: 0.14695225656032562, ACC:0.953125\n",
      "Training iteration 298 loss: 0.23160649836063385, ACC:0.921875\n",
      "Training iteration 299 loss: 0.10461009293794632, ACC:0.96875\n",
      "Training iteration 300 loss: 0.20377366244792938, ACC:0.90625\n",
      "Training iteration 301 loss: 0.10762856155633926, ACC:0.984375\n",
      "Training iteration 302 loss: 0.2003941386938095, ACC:0.953125\n",
      "Training iteration 303 loss: 0.15469494462013245, ACC:0.9375\n",
      "Training iteration 304 loss: 0.14404451847076416, ACC:0.96875\n",
      "Training iteration 305 loss: 0.2065078169107437, ACC:0.921875\n",
      "Training iteration 306 loss: 0.21522477269172668, ACC:0.9375\n",
      "Training iteration 307 loss: 0.22370384633541107, ACC:0.921875\n",
      "Training iteration 308 loss: 0.23556514084339142, ACC:0.9375\n",
      "Training iteration 309 loss: 0.11647972464561462, ACC:0.96875\n",
      "Training iteration 310 loss: 0.2101222574710846, ACC:0.921875\n",
      "Training iteration 311 loss: 0.17718933522701263, ACC:0.921875\n",
      "Training iteration 312 loss: 0.11782588064670563, ACC:0.984375\n",
      "Training iteration 313 loss: 0.1859540492296219, ACC:0.96875\n",
      "Training iteration 314 loss: 0.087990403175354, ACC:1.0\n",
      "Training iteration 315 loss: 0.20176635682582855, ACC:0.953125\n",
      "Training iteration 316 loss: 0.10747794806957245, ACC:0.984375\n",
      "Training iteration 317 loss: 0.16585028171539307, ACC:0.953125\n",
      "Training iteration 318 loss: 0.21095624566078186, ACC:0.890625\n",
      "Training iteration 319 loss: 0.14411568641662598, ACC:0.96875\n",
      "Training iteration 320 loss: 0.17358501255512238, ACC:0.953125\n",
      "Training iteration 321 loss: 0.10259930044412613, ACC:0.984375\n",
      "Training iteration 322 loss: 0.15759426355361938, ACC:0.921875\n",
      "Training iteration 323 loss: 0.24109184741973877, ACC:0.875\n",
      "Training iteration 324 loss: 0.08115453273057938, ACC:1.0\n",
      "Training iteration 325 loss: 0.20778429508209229, ACC:0.921875\n",
      "Training iteration 326 loss: 0.20768487453460693, ACC:0.90625\n",
      "Training iteration 327 loss: 0.10563522577285767, ACC:0.953125\n",
      "Training iteration 328 loss: 0.22375622391700745, ACC:0.90625\n",
      "Training iteration 329 loss: 0.10747312009334564, ACC:0.96875\n",
      "Training iteration 330 loss: 0.14494363963603973, ACC:0.953125\n",
      "Training iteration 331 loss: 0.11398739367723465, ACC:0.96875\n",
      "Training iteration 332 loss: 0.11861398071050644, ACC:0.96875\n",
      "Training iteration 333 loss: 0.13350467383861542, ACC:0.96875\n",
      "Training iteration 334 loss: 0.15260735154151917, ACC:0.96875\n",
      "Training iteration 335 loss: 0.054629839956760406, ACC:0.984375\n",
      "Training iteration 336 loss: 0.05901427939534187, ACC:0.984375\n",
      "Training iteration 337 loss: 0.14190305769443512, ACC:0.953125\n",
      "Training iteration 338 loss: 0.16724002361297607, ACC:0.953125\n",
      "Training iteration 339 loss: 0.1390291303396225, ACC:0.96875\n",
      "Training iteration 340 loss: 0.08445010334253311, ACC:0.984375\n",
      "Training iteration 341 loss: 0.1269175112247467, ACC:0.953125\n",
      "Training iteration 342 loss: 0.18862919509410858, ACC:0.90625\n",
      "Training iteration 343 loss: 0.21272772550582886, ACC:0.9375\n",
      "Training iteration 344 loss: 0.11242911964654922, ACC:0.984375\n",
      "Training iteration 345 loss: 0.12671710550785065, ACC:0.9375\n",
      "Training iteration 346 loss: 0.060516878962516785, ACC:1.0\n",
      "Training iteration 347 loss: 0.05639181658625603, ACC:1.0\n",
      "Training iteration 348 loss: 0.11849411576986313, ACC:0.96875\n",
      "Training iteration 349 loss: 0.08144032955169678, ACC:0.984375\n",
      "Training iteration 350 loss: 0.10587392747402191, ACC:0.96875\n",
      "Training iteration 351 loss: 0.10726367682218552, ACC:0.96875\n",
      "Training iteration 352 loss: 0.11527106910943985, ACC:0.984375\n",
      "Training iteration 353 loss: 0.18470509350299835, ACC:0.9375\n",
      "Training iteration 354 loss: 0.07865714281797409, ACC:0.984375\n",
      "Training iteration 355 loss: 0.1050499677658081, ACC:0.96875\n",
      "Training iteration 356 loss: 0.08510179817676544, ACC:0.984375\n",
      "Training iteration 357 loss: 0.10393094271421432, ACC:0.953125\n",
      "Training iteration 358 loss: 0.15755443274974823, ACC:0.953125\n",
      "Training iteration 359 loss: 0.17405244708061218, ACC:0.90625\n",
      "Training iteration 360 loss: 0.23037447035312653, ACC:0.96875\n",
      "Training iteration 361 loss: 0.16161789000034332, ACC:0.9375\n",
      "Training iteration 362 loss: 0.07684679329395294, ACC:0.96875\n",
      "Training iteration 363 loss: 0.09786095470190048, ACC:0.96875\n",
      "Training iteration 364 loss: 0.15320906043052673, ACC:0.953125\n",
      "Training iteration 365 loss: 0.15492506325244904, ACC:0.953125\n",
      "Training iteration 366 loss: 0.08740152418613434, ACC:0.984375\n",
      "Training iteration 367 loss: 0.12391838431358337, ACC:0.953125\n",
      "Training iteration 368 loss: 0.12247589975595474, ACC:0.984375\n",
      "Training iteration 369 loss: 0.09091141819953918, ACC:0.984375\n",
      "Training iteration 370 loss: 0.1456485390663147, ACC:0.96875\n",
      "Training iteration 371 loss: 0.1417783945798874, ACC:0.96875\n",
      "Training iteration 372 loss: 0.12204811722040176, ACC:0.96875\n",
      "Training iteration 373 loss: 0.05622822418808937, ACC:1.0\n",
      "Training iteration 374 loss: 0.08245915174484253, ACC:0.984375\n",
      "Training iteration 375 loss: 0.07312340289354324, ACC:0.984375\n",
      "Training iteration 376 loss: 0.114190012216568, ACC:0.984375\n",
      "Training iteration 377 loss: 0.049178678542375565, ACC:1.0\n",
      "Training iteration 378 loss: 0.10870043188333511, ACC:0.96875\n",
      "Training iteration 379 loss: 0.07163675129413605, ACC:0.984375\n",
      "Training iteration 380 loss: 0.042445987462997437, ACC:1.0\n",
      "Training iteration 381 loss: 0.2379603087902069, ACC:0.921875\n",
      "Training iteration 382 loss: 0.14910158514976501, ACC:0.953125\n",
      "Training iteration 383 loss: 0.08228956162929535, ACC:0.96875\n",
      "Training iteration 384 loss: 0.0907721146941185, ACC:0.96875\n",
      "Training iteration 385 loss: 0.10067406296730042, ACC:0.953125\n",
      "Training iteration 386 loss: 0.07640913128852844, ACC:0.96875\n",
      "Training iteration 387 loss: 0.16351336240768433, ACC:0.9375\n",
      "Training iteration 388 loss: 0.09734369069337845, ACC:0.953125\n",
      "Training iteration 389 loss: 0.12166031450033188, ACC:0.96875\n",
      "Training iteration 390 loss: 0.0803726464509964, ACC:0.984375\n",
      "Training iteration 391 loss: 0.07292281091213226, ACC:0.984375\n",
      "Training iteration 392 loss: 0.07390473037958145, ACC:0.96875\n",
      "Training iteration 393 loss: 0.20423680543899536, ACC:0.953125\n",
      "Training iteration 394 loss: 0.06337693333625793, ACC:0.984375\n",
      "Training iteration 395 loss: 0.15907035768032074, ACC:0.921875\n",
      "Training iteration 396 loss: 0.11918945610523224, ACC:0.96875\n",
      "Training iteration 397 loss: 0.2269442081451416, ACC:0.890625\n",
      "Training iteration 398 loss: 0.09943877160549164, ACC:0.984375\n",
      "Training iteration 399 loss: 0.2256748378276825, ACC:0.9375\n",
      "Training iteration 400 loss: 0.2491050809621811, ACC:0.90625\n",
      "Training iteration 401 loss: 0.17235879600048065, ACC:0.953125\n",
      "Training iteration 402 loss: 0.1902155876159668, ACC:0.953125\n",
      "Training iteration 403 loss: 0.17720387876033783, ACC:0.96875\n",
      "Training iteration 404 loss: 0.2116219848394394, ACC:0.9375\n",
      "Training iteration 405 loss: 0.16611886024475098, ACC:0.96875\n",
      "Training iteration 406 loss: 0.17398588359355927, ACC:0.953125\n",
      "Training iteration 407 loss: 0.46615123748779297, ACC:0.828125\n",
      "Training iteration 408 loss: 0.47787466645240784, ACC:0.515625\n",
      "Training iteration 409 loss: 0.4047251045703888, ACC:0.9375\n",
      "Training iteration 410 loss: 0.43557295203208923, ACC:0.859375\n",
      "Training iteration 411 loss: 0.3477763533592224, ACC:0.953125\n",
      "Training iteration 412 loss: 0.3822953999042511, ACC:0.921875\n",
      "Training iteration 413 loss: 0.3625013530254364, ACC:0.875\n",
      "Training iteration 414 loss: 0.29995226860046387, ACC:0.90625\n",
      "Training iteration 415 loss: 0.24396517872810364, ACC:0.953125\n",
      "Training iteration 416 loss: 0.23932918906211853, ACC:0.96875\n",
      "Training iteration 417 loss: 0.22795943915843964, ACC:0.953125\n",
      "Training iteration 418 loss: 0.19031761586666107, ACC:0.96875\n",
      "Training iteration 419 loss: 0.2090913951396942, ACC:0.9375\n",
      "Training iteration 420 loss: 0.12437079846858978, ACC:0.984375\n",
      "Training iteration 421 loss: 0.15617288649082184, ACC:0.96875\n",
      "Training iteration 422 loss: 0.22182559967041016, ACC:0.921875\n",
      "Training iteration 423 loss: 0.1349615454673767, ACC:0.953125\n",
      "Training iteration 424 loss: 0.16928008198738098, ACC:0.9375\n",
      "Training iteration 425 loss: 0.1238754466176033, ACC:0.96875\n",
      "Training iteration 426 loss: 0.10865035653114319, ACC:0.984375\n",
      "Training iteration 427 loss: 0.09828589856624603, ACC:1.0\n",
      "Training iteration 428 loss: 0.17628827691078186, ACC:0.90625\n",
      "Training iteration 429 loss: 0.170478954911232, ACC:0.921875\n",
      "Training iteration 430 loss: 0.11346392333507538, ACC:0.9375\n",
      "Training iteration 431 loss: 0.13138845562934875, ACC:0.96875\n",
      "Training iteration 432 loss: 0.13041433691978455, ACC:0.96875\n",
      "Training iteration 433 loss: 0.1140342727303505, ACC:0.9375\n",
      "Training iteration 434 loss: 0.11324339359998703, ACC:0.96875\n",
      "Training iteration 435 loss: 0.0648735910654068, ACC:1.0\n",
      "Training iteration 436 loss: 0.12569788098335266, ACC:0.953125\n",
      "Training iteration 437 loss: 0.09783636033535004, ACC:0.96875\n",
      "Training iteration 438 loss: 0.10258786380290985, ACC:0.96875\n",
      "Training iteration 439 loss: 0.24062713980674744, ACC:0.890625\n",
      "Training iteration 440 loss: 0.1622626781463623, ACC:0.96875\n",
      "Training iteration 441 loss: 0.17401495575904846, ACC:0.953125\n",
      "Training iteration 442 loss: 0.2570500075817108, ACC:0.859375\n",
      "Training iteration 443 loss: 0.17093369364738464, ACC:0.953125\n",
      "Training iteration 444 loss: 0.16206210851669312, ACC:0.953125\n",
      "Training iteration 445 loss: 0.16434577107429504, ACC:0.9375\n",
      "Training iteration 446 loss: 0.12133625894784927, ACC:0.984375\n",
      "Training iteration 447 loss: 0.2112698256969452, ACC:0.9375\n",
      "Training iteration 448 loss: 0.27307119965553284, ACC:0.921875\n",
      "Training iteration 449 loss: 0.17639832198619843, ACC:0.96875\n",
      "Training iteration 450 loss: 0.2131236344575882, ACC:0.96875\n",
      "Validation iteration 451 loss: 0.15689612925052643, ACC: 0.96875\n",
      "Validation iteration 452 loss: 0.1279396414756775, ACC: 0.96875\n",
      "Validation iteration 453 loss: 0.1339349001646042, ACC: 0.984375\n",
      "Validation iteration 454 loss: 0.15654781460762024, ACC: 0.96875\n",
      "Validation iteration 455 loss: 0.21097128093242645, ACC: 0.9375\n",
      "Validation iteration 456 loss: 0.15719476342201233, ACC: 0.96875\n",
      "Validation iteration 457 loss: 0.21293145418167114, ACC: 0.90625\n",
      "Validation iteration 458 loss: 0.2334395796060562, ACC: 0.890625\n",
      "Validation iteration 459 loss: 0.13719838857650757, ACC: 0.984375\n",
      "Validation iteration 460 loss: 0.12181773036718369, ACC: 0.96875\n",
      "Validation iteration 461 loss: 0.13197468221187592, ACC: 0.984375\n",
      "Validation iteration 462 loss: 0.2498515248298645, ACC: 0.9375\n",
      "Validation iteration 463 loss: 0.19175709784030914, ACC: 0.9375\n",
      "Validation iteration 464 loss: 0.14123134315013885, ACC: 0.96875\n",
      "Validation iteration 465 loss: 0.1979893296957016, ACC: 0.921875\n",
      "Validation iteration 466 loss: 0.11748708039522171, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.2854633629322052, ACC: 0.90625\n",
      "Validation iteration 468 loss: 0.18597804009914398, ACC: 0.953125\n",
      "Validation iteration 469 loss: 0.2433125376701355, ACC: 0.90625\n",
      "Validation iteration 470 loss: 0.17446564137935638, ACC: 0.953125\n",
      "Validation iteration 471 loss: 0.32542097568511963, ACC: 0.875\n",
      "Validation iteration 472 loss: 0.17503675818443298, ACC: 0.921875\n",
      "Validation iteration 473 loss: 0.18404965102672577, ACC: 0.90625\n",
      "Validation iteration 474 loss: 0.20247459411621094, ACC: 0.9375\n",
      "Validation iteration 475 loss: 0.16775529086589813, ACC: 0.953125\n",
      "Validation iteration 476 loss: 0.12755635380744934, ACC: 0.96875\n",
      "Validation iteration 477 loss: 0.10303312540054321, ACC: 0.984375\n",
      "Validation iteration 478 loss: 0.158725768327713, ACC: 0.96875\n",
      "Validation iteration 479 loss: 0.1659575253725052, ACC: 0.9375\n",
      "Validation iteration 480 loss: 0.16303423047065735, ACC: 0.953125\n",
      "Validation iteration 481 loss: 0.11405634880065918, ACC: 0.96875\n",
      "Validation iteration 482 loss: 0.150435209274292, ACC: 0.953125\n",
      "Validation iteration 483 loss: 0.16718009114265442, ACC: 0.953125\n",
      "Validation iteration 484 loss: 0.25986719131469727, ACC: 0.90625\n",
      "Validation iteration 485 loss: 0.21404393017292023, ACC: 0.921875\n",
      "Validation iteration 486 loss: 0.1404719054698944, ACC: 0.96875\n",
      "Validation iteration 487 loss: 0.17730355262756348, ACC: 0.953125\n",
      "Validation iteration 488 loss: 0.11933816969394684, ACC: 0.984375\n",
      "Validation iteration 489 loss: 0.13487468659877777, ACC: 0.96875\n",
      "Validation iteration 490 loss: 0.1762554943561554, ACC: 0.9375\n",
      "Validation iteration 491 loss: 0.18171502649784088, ACC: 0.953125\n",
      "Validation iteration 492 loss: 0.156030535697937, ACC: 0.96875\n",
      "Validation iteration 493 loss: 0.11224080622196198, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.169580340385437, ACC: 0.953125\n",
      "Validation iteration 495 loss: 0.1494341492652893, ACC: 0.96875\n",
      "Validation iteration 496 loss: 0.20461058616638184, ACC: 0.921875\n",
      "Validation iteration 497 loss: 0.1434624195098877, ACC: 0.96875\n",
      "Validation iteration 498 loss: 0.22385527193546295, ACC: 0.890625\n",
      "Validation iteration 499 loss: 0.13682563602924347, ACC: 0.96875\n",
      "Validation iteration 500 loss: 0.24360662698745728, ACC: 0.921875\n",
      "-- Epoch 9 done -- Train loss: 0.22299226088656318, train ACC: 0.9277430555555556, val loss: 0.17433229148387908, val ACC: 0.9490625\n",
      "<--- 2241.5442855358124 seconds --->\n",
      "Training iteration 1 loss: 0.15241684019565582, ACC:0.96875\n",
      "Training iteration 2 loss: 0.21030481159687042, ACC:0.9375\n",
      "Training iteration 3 loss: 0.18690159916877747, ACC:0.9375\n",
      "Training iteration 4 loss: 0.21005527675151825, ACC:0.90625\n",
      "Training iteration 5 loss: 0.08919140696525574, ACC:0.984375\n",
      "Training iteration 6 loss: 0.13586680591106415, ACC:0.953125\n",
      "Training iteration 7 loss: 0.21935003995895386, ACC:0.90625\n",
      "Training iteration 8 loss: 0.11823976039886475, ACC:0.984375\n",
      "Training iteration 9 loss: 0.13362203538417816, ACC:0.96875\n",
      "Training iteration 10 loss: 0.3508136570453644, ACC:0.859375\n",
      "Training iteration 11 loss: 0.1086072325706482, ACC:0.953125\n",
      "Training iteration 12 loss: 0.09329231083393097, ACC:0.96875\n",
      "Training iteration 13 loss: 0.11620813608169556, ACC:0.96875\n",
      "Training iteration 14 loss: 0.20747549831867218, ACC:0.921875\n",
      "Training iteration 15 loss: 0.1378650665283203, ACC:0.96875\n",
      "Training iteration 16 loss: 0.07638433575630188, ACC:0.984375\n",
      "Training iteration 17 loss: 0.2552177608013153, ACC:0.90625\n",
      "Training iteration 18 loss: 0.18829546868801117, ACC:0.90625\n",
      "Training iteration 19 loss: 0.10452818870544434, ACC:0.96875\n",
      "Training iteration 20 loss: 0.16962090134620667, ACC:0.921875\n",
      "Training iteration 21 loss: 0.26449641585350037, ACC:0.921875\n",
      "Training iteration 22 loss: 0.12183085829019547, ACC:0.984375\n",
      "Training iteration 23 loss: 0.15385514497756958, ACC:0.96875\n",
      "Training iteration 24 loss: 0.15252944827079773, ACC:0.953125\n",
      "Training iteration 25 loss: 0.3135097920894623, ACC:0.828125\n",
      "Training iteration 26 loss: 0.13530856370925903, ACC:0.984375\n",
      "Training iteration 27 loss: 0.25629013776779175, ACC:0.921875\n",
      "Training iteration 28 loss: 0.3312223553657532, ACC:0.875\n",
      "Training iteration 29 loss: 0.27634647488594055, ACC:0.90625\n",
      "Training iteration 30 loss: 0.2537907660007477, ACC:0.890625\n",
      "Training iteration 31 loss: 0.2623068690299988, ACC:0.921875\n",
      "Training iteration 32 loss: 0.26234686374664307, ACC:0.90625\n",
      "Training iteration 33 loss: 0.2963149845600128, ACC:0.890625\n",
      "Training iteration 34 loss: 0.3330010771751404, ACC:0.890625\n",
      "Training iteration 35 loss: 0.43134263157844543, ACC:0.828125\n",
      "Training iteration 36 loss: 0.3831923007965088, ACC:0.84375\n",
      "Training iteration 37 loss: 0.33242130279541016, ACC:0.875\n",
      "Training iteration 38 loss: 0.17432279884815216, ACC:0.953125\n",
      "Training iteration 39 loss: 0.22161538898944855, ACC:0.953125\n",
      "Training iteration 40 loss: 0.2848167419433594, ACC:0.890625\n",
      "Training iteration 41 loss: 0.29306116700172424, ACC:0.890625\n",
      "Training iteration 42 loss: 0.2128603756427765, ACC:0.953125\n",
      "Training iteration 43 loss: 0.22085772454738617, ACC:0.9375\n",
      "Training iteration 44 loss: 0.25547346472740173, ACC:0.859375\n",
      "Training iteration 45 loss: 0.22336030006408691, ACC:0.9375\n",
      "Training iteration 46 loss: 0.16353611648082733, ACC:0.953125\n",
      "Training iteration 47 loss: 0.25306427478790283, ACC:0.921875\n",
      "Training iteration 48 loss: 0.2088903933763504, ACC:0.921875\n",
      "Training iteration 49 loss: 0.30448654294013977, ACC:0.90625\n",
      "Training iteration 50 loss: 0.271049827337265, ACC:0.921875\n",
      "Training iteration 51 loss: 0.20926442742347717, ACC:0.9375\n",
      "Training iteration 52 loss: 0.3148363530635834, ACC:0.859375\n",
      "Training iteration 53 loss: 0.2967095971107483, ACC:0.90625\n",
      "Training iteration 54 loss: 0.24098417162895203, ACC:0.9375\n",
      "Training iteration 55 loss: 0.18629738688468933, ACC:0.96875\n",
      "Training iteration 56 loss: 0.257464200258255, ACC:0.9375\n",
      "Training iteration 57 loss: 0.26433730125427246, ACC:0.9375\n",
      "Training iteration 58 loss: 0.28440481424331665, ACC:0.90625\n",
      "Training iteration 59 loss: 0.3009052574634552, ACC:0.890625\n",
      "Training iteration 60 loss: 0.21703463792800903, ACC:0.953125\n",
      "Training iteration 61 loss: 0.23141881823539734, ACC:0.90625\n",
      "Training iteration 62 loss: 0.3540598154067993, ACC:0.828125\n",
      "Training iteration 63 loss: 0.1360672116279602, ACC:0.984375\n",
      "Training iteration 64 loss: 0.19448736310005188, ACC:0.953125\n",
      "Training iteration 65 loss: 0.23124663531780243, ACC:0.890625\n",
      "Training iteration 66 loss: 0.19635440409183502, ACC:0.953125\n",
      "Training iteration 67 loss: 0.11519840359687805, ACC:0.96875\n",
      "Training iteration 68 loss: 0.15110184252262115, ACC:0.953125\n",
      "Training iteration 69 loss: 0.1628454476594925, ACC:0.953125\n",
      "Training iteration 70 loss: 0.1498550921678543, ACC:0.953125\n",
      "Training iteration 71 loss: 0.28322064876556396, ACC:0.859375\n",
      "Training iteration 72 loss: 0.2645030915737152, ACC:0.921875\n",
      "Training iteration 73 loss: 0.2314535528421402, ACC:0.9375\n",
      "Training iteration 74 loss: 0.22003154456615448, ACC:0.9375\n",
      "Training iteration 75 loss: 0.20627963542938232, ACC:0.921875\n",
      "Training iteration 76 loss: 0.2276780903339386, ACC:0.890625\n",
      "Training iteration 77 loss: 0.2515985667705536, ACC:0.890625\n",
      "Training iteration 78 loss: 0.1583273708820343, ACC:0.96875\n",
      "Training iteration 79 loss: 0.21444031596183777, ACC:0.921875\n",
      "Training iteration 80 loss: 0.18956156075000763, ACC:0.875\n",
      "Training iteration 81 loss: 0.2001548409461975, ACC:0.90625\n",
      "Training iteration 82 loss: 0.2598164975643158, ACC:0.890625\n",
      "Training iteration 83 loss: 0.19496381282806396, ACC:0.921875\n",
      "Training iteration 84 loss: 0.3132607936859131, ACC:0.828125\n",
      "Training iteration 85 loss: 0.17465096712112427, ACC:0.9375\n",
      "Training iteration 86 loss: 0.16897828876972198, ACC:0.90625\n",
      "Training iteration 87 loss: 0.26482027769088745, ACC:0.875\n",
      "Training iteration 88 loss: 0.17221683263778687, ACC:0.9375\n",
      "Training iteration 89 loss: 0.16921378672122955, ACC:0.96875\n",
      "Training iteration 90 loss: 0.19370576739311218, ACC:0.90625\n",
      "Training iteration 91 loss: 0.1582786589860916, ACC:0.953125\n",
      "Training iteration 92 loss: 0.21322079002857208, ACC:0.921875\n",
      "Training iteration 93 loss: 0.22731532156467438, ACC:0.921875\n",
      "Training iteration 94 loss: 0.11945485323667526, ACC:0.984375\n",
      "Training iteration 95 loss: 0.09390801191329956, ACC:0.984375\n",
      "Training iteration 96 loss: 0.18923231959342957, ACC:0.953125\n",
      "Training iteration 97 loss: 0.12623468041419983, ACC:0.984375\n",
      "Training iteration 98 loss: 0.10546761006116867, ACC:0.96875\n",
      "Training iteration 99 loss: 0.1412506252527237, ACC:0.984375\n",
      "Training iteration 100 loss: 0.12924666702747345, ACC:0.96875\n",
      "Training iteration 101 loss: 0.09904718399047852, ACC:0.953125\n",
      "Training iteration 102 loss: 0.20503884553909302, ACC:0.9375\n",
      "Training iteration 103 loss: 0.1437307447195053, ACC:0.953125\n",
      "Training iteration 104 loss: 0.10123173892498016, ACC:0.984375\n",
      "Training iteration 105 loss: 0.15624366700649261, ACC:0.953125\n",
      "Training iteration 106 loss: 0.1904900223016739, ACC:0.9375\n",
      "Training iteration 107 loss: 0.06980590522289276, ACC:0.984375\n",
      "Training iteration 108 loss: 0.09594767540693283, ACC:0.96875\n",
      "Training iteration 109 loss: 0.11718123406171799, ACC:0.984375\n",
      "Training iteration 110 loss: 0.1955861747264862, ACC:0.953125\n",
      "Training iteration 111 loss: 0.09714141488075256, ACC:0.984375\n",
      "Training iteration 112 loss: 0.15616555511951447, ACC:0.953125\n",
      "Training iteration 113 loss: 0.09963080286979675, ACC:0.96875\n",
      "Training iteration 114 loss: 0.10648994892835617, ACC:0.953125\n",
      "Training iteration 115 loss: 0.16205114126205444, ACC:0.953125\n",
      "Training iteration 116 loss: 0.1248026192188263, ACC:0.96875\n",
      "Training iteration 117 loss: 0.11680740863084793, ACC:0.984375\n",
      "Training iteration 118 loss: 0.1225152462720871, ACC:0.96875\n",
      "Training iteration 119 loss: 0.04538706690073013, ACC:1.0\n",
      "Training iteration 120 loss: 0.05944833159446716, ACC:1.0\n",
      "Training iteration 121 loss: 0.20933780074119568, ACC:0.90625\n",
      "Training iteration 122 loss: 0.1924169510602951, ACC:0.9375\n",
      "Training iteration 123 loss: 0.35546332597732544, ACC:0.890625\n",
      "Training iteration 124 loss: 0.1852484941482544, ACC:0.921875\n",
      "Training iteration 125 loss: 0.17566294968128204, ACC:0.9375\n",
      "Training iteration 126 loss: 0.07897661626338959, ACC:0.96875\n",
      "Training iteration 127 loss: 0.1401715725660324, ACC:0.953125\n",
      "Training iteration 128 loss: 0.13375037908554077, ACC:0.953125\n",
      "Training iteration 129 loss: 0.2848567068576813, ACC:0.890625\n",
      "Training iteration 130 loss: 0.10522815585136414, ACC:0.9375\n",
      "Training iteration 131 loss: 0.11382073163986206, ACC:0.953125\n",
      "Training iteration 132 loss: 0.1322365552186966, ACC:0.953125\n",
      "Training iteration 133 loss: 0.2254658192396164, ACC:0.90625\n",
      "Training iteration 134 loss: 0.18065126240253448, ACC:0.96875\n",
      "Training iteration 135 loss: 0.22410355508327484, ACC:0.90625\n",
      "Training iteration 136 loss: 0.20354294776916504, ACC:0.921875\n",
      "Training iteration 137 loss: 0.13642381131649017, ACC:0.96875\n",
      "Training iteration 138 loss: 0.13139496743679047, ACC:0.96875\n",
      "Training iteration 139 loss: 0.13933849334716797, ACC:0.953125\n",
      "Training iteration 140 loss: 0.16403137147426605, ACC:0.953125\n",
      "Training iteration 141 loss: 0.10969497263431549, ACC:0.953125\n",
      "Training iteration 142 loss: 0.06237306073307991, ACC:0.984375\n",
      "Training iteration 143 loss: 0.148260235786438, ACC:0.96875\n",
      "Training iteration 144 loss: 0.14122869074344635, ACC:0.96875\n",
      "Training iteration 145 loss: 0.06099376454949379, ACC:1.0\n",
      "Training iteration 146 loss: 0.14819912612438202, ACC:0.921875\n",
      "Training iteration 147 loss: 0.09648739546537399, ACC:0.984375\n",
      "Training iteration 148 loss: 0.15201187133789062, ACC:0.953125\n",
      "Training iteration 149 loss: 0.2568103075027466, ACC:0.9375\n",
      "Training iteration 150 loss: 0.19233733415603638, ACC:0.953125\n",
      "Training iteration 151 loss: 0.11684443056583405, ACC:0.953125\n",
      "Training iteration 152 loss: 0.28699198365211487, ACC:0.890625\n",
      "Training iteration 153 loss: 0.2078390121459961, ACC:0.90625\n",
      "Training iteration 154 loss: 0.1676541119813919, ACC:0.953125\n",
      "Training iteration 155 loss: 0.25623032450675964, ACC:0.921875\n",
      "Training iteration 156 loss: 0.3757569491863251, ACC:0.859375\n",
      "Training iteration 157 loss: 0.19612163305282593, ACC:0.953125\n",
      "Training iteration 158 loss: 0.24724233150482178, ACC:0.921875\n",
      "Training iteration 159 loss: 0.224100261926651, ACC:0.96875\n",
      "Training iteration 160 loss: 0.16346484422683716, ACC:0.953125\n",
      "Training iteration 161 loss: 0.21924373507499695, ACC:0.9375\n",
      "Training iteration 162 loss: 0.1786012202501297, ACC:0.90625\n",
      "Training iteration 163 loss: 0.3192637264728546, ACC:0.890625\n",
      "Training iteration 164 loss: 0.13456398248672485, ACC:0.96875\n",
      "Training iteration 165 loss: 0.12074165791273117, ACC:0.953125\n",
      "Training iteration 166 loss: 0.1162225529551506, ACC:0.953125\n",
      "Training iteration 167 loss: 0.2549389600753784, ACC:0.921875\n",
      "Training iteration 168 loss: 0.07534105330705643, ACC:0.96875\n",
      "Training iteration 169 loss: 0.1620149314403534, ACC:0.953125\n",
      "Training iteration 170 loss: 0.1821119785308838, ACC:0.96875\n",
      "Training iteration 171 loss: 0.23209352791309357, ACC:0.875\n",
      "Training iteration 172 loss: 0.15706029534339905, ACC:0.953125\n",
      "Training iteration 173 loss: 0.24029597640037537, ACC:0.90625\n",
      "Training iteration 174 loss: 0.29772794246673584, ACC:0.875\n",
      "Training iteration 175 loss: 0.12172245234251022, ACC:0.96875\n",
      "Training iteration 176 loss: 0.1198003739118576, ACC:1.0\n",
      "Training iteration 177 loss: 0.12946052849292755, ACC:0.953125\n",
      "Training iteration 178 loss: 0.10703335702419281, ACC:0.96875\n",
      "Training iteration 179 loss: 0.14637920260429382, ACC:0.96875\n",
      "Training iteration 180 loss: 0.11360163986682892, ACC:0.953125\n",
      "Training iteration 181 loss: 0.21906925737857819, ACC:0.890625\n",
      "Training iteration 182 loss: 0.2729814052581787, ACC:0.90625\n",
      "Training iteration 183 loss: 0.16116589307785034, ACC:0.953125\n",
      "Training iteration 184 loss: 0.17407137155532837, ACC:0.921875\n",
      "Training iteration 185 loss: 0.10555422306060791, ACC:1.0\n",
      "Training iteration 186 loss: 0.24447928369045258, ACC:0.875\n",
      "Training iteration 187 loss: 0.18990448117256165, ACC:0.9375\n",
      "Training iteration 188 loss: 0.14939677715301514, ACC:0.9375\n",
      "Training iteration 189 loss: 0.12594535946846008, ACC:0.953125\n",
      "Training iteration 190 loss: 0.10922452062368393, ACC:0.953125\n",
      "Training iteration 191 loss: 0.15808604657649994, ACC:0.953125\n",
      "Training iteration 192 loss: 0.14378468692302704, ACC:0.96875\n",
      "Training iteration 193 loss: 0.18747322261333466, ACC:0.953125\n",
      "Training iteration 194 loss: 0.10217706859111786, ACC:0.96875\n",
      "Training iteration 195 loss: 0.10052435100078583, ACC:0.96875\n",
      "Training iteration 196 loss: 0.10892657935619354, ACC:0.984375\n",
      "Training iteration 197 loss: 0.12685543298721313, ACC:0.96875\n",
      "Training iteration 198 loss: 0.1149972453713417, ACC:0.96875\n",
      "Training iteration 199 loss: 0.13202112913131714, ACC:0.96875\n",
      "Training iteration 200 loss: 0.1535273790359497, ACC:0.9375\n",
      "Training iteration 201 loss: 0.14955593645572662, ACC:0.921875\n",
      "Training iteration 202 loss: 0.1297568827867508, ACC:0.96875\n",
      "Training iteration 203 loss: 0.13069336116313934, ACC:0.953125\n",
      "Training iteration 204 loss: 0.12346327304840088, ACC:0.96875\n",
      "Training iteration 205 loss: 0.09728053212165833, ACC:0.984375\n",
      "Training iteration 206 loss: 0.31214478611946106, ACC:0.859375\n",
      "Training iteration 207 loss: 0.2091139703989029, ACC:0.9375\n",
      "Training iteration 208 loss: 0.19936825335025787, ACC:0.90625\n",
      "Training iteration 209 loss: 0.17169089615345, ACC:0.921875\n",
      "Training iteration 210 loss: 0.1403561532497406, ACC:0.9375\n",
      "Training iteration 211 loss: 0.2507217824459076, ACC:0.875\n",
      "Training iteration 212 loss: 0.3125145435333252, ACC:0.90625\n",
      "Training iteration 213 loss: 0.2001180797815323, ACC:0.90625\n",
      "Training iteration 214 loss: 0.33279767632484436, ACC:0.859375\n",
      "Training iteration 215 loss: 0.36562737822532654, ACC:0.796875\n",
      "Training iteration 216 loss: 0.21146070957183838, ACC:0.890625\n",
      "Training iteration 217 loss: 0.2905580997467041, ACC:0.859375\n",
      "Training iteration 218 loss: 0.11111524701118469, ACC:0.9375\n",
      "Training iteration 219 loss: 0.25487568974494934, ACC:0.953125\n",
      "Training iteration 220 loss: 0.20608524978160858, ACC:0.96875\n",
      "Training iteration 221 loss: 0.20160448551177979, ACC:0.9375\n",
      "Training iteration 222 loss: 0.17296436429023743, ACC:0.96875\n",
      "Training iteration 223 loss: 0.1629868447780609, ACC:0.96875\n",
      "Training iteration 224 loss: 0.1678360402584076, ACC:0.96875\n",
      "Training iteration 225 loss: 0.2031049281358719, ACC:0.9375\n",
      "Training iteration 226 loss: 0.12281177192926407, ACC:1.0\n",
      "Training iteration 227 loss: 0.18137328326702118, ACC:0.953125\n",
      "Training iteration 228 loss: 0.18531177937984467, ACC:0.953125\n",
      "Training iteration 229 loss: 0.18408577144145966, ACC:0.9375\n",
      "Training iteration 230 loss: 0.14561252295970917, ACC:0.96875\n",
      "Training iteration 231 loss: 0.1484285145998001, ACC:0.984375\n",
      "Training iteration 232 loss: 0.2108517587184906, ACC:0.953125\n",
      "Training iteration 233 loss: 0.10052356868982315, ACC:1.0\n",
      "Training iteration 234 loss: 0.15747904777526855, ACC:0.96875\n",
      "Training iteration 235 loss: 0.1378994584083557, ACC:0.953125\n",
      "Training iteration 236 loss: 0.13725724816322327, ACC:0.984375\n",
      "Training iteration 237 loss: 0.07859179377555847, ACC:1.0\n",
      "Training iteration 238 loss: 0.13175097107887268, ACC:0.984375\n",
      "Training iteration 239 loss: 0.11076126247644424, ACC:0.984375\n",
      "Training iteration 240 loss: 0.0974324494600296, ACC:0.984375\n",
      "Training iteration 241 loss: 0.08782043308019638, ACC:0.96875\n",
      "Training iteration 242 loss: 0.1600123792886734, ACC:0.9375\n",
      "Training iteration 243 loss: 0.16520990431308746, ACC:0.9375\n",
      "Training iteration 244 loss: 0.13345511257648468, ACC:0.9375\n",
      "Training iteration 245 loss: 0.12964285910129547, ACC:0.984375\n",
      "Training iteration 246 loss: 0.10765934735536575, ACC:0.96875\n",
      "Training iteration 247 loss: 0.07098345458507538, ACC:0.984375\n",
      "Training iteration 248 loss: 0.04538729786872864, ACC:1.0\n",
      "Training iteration 249 loss: 0.06708385050296783, ACC:1.0\n",
      "Training iteration 250 loss: 0.14123009145259857, ACC:0.953125\n",
      "Training iteration 251 loss: 0.1485145092010498, ACC:0.9375\n",
      "Training iteration 252 loss: 0.14323054254055023, ACC:0.90625\n",
      "Training iteration 253 loss: 0.10324717313051224, ACC:0.953125\n",
      "Training iteration 254 loss: 0.15485543012619019, ACC:0.9375\n",
      "Training iteration 255 loss: 0.10943679511547089, ACC:0.96875\n",
      "Training iteration 256 loss: 0.06565091013908386, ACC:0.984375\n",
      "Training iteration 257 loss: 0.0843784511089325, ACC:0.96875\n",
      "Training iteration 258 loss: 0.19219958782196045, ACC:0.90625\n",
      "Training iteration 259 loss: 0.08036686480045319, ACC:0.984375\n",
      "Training iteration 260 loss: 0.2682945728302002, ACC:0.90625\n",
      "Training iteration 261 loss: 0.09981273114681244, ACC:0.96875\n",
      "Training iteration 262 loss: 0.05965510010719299, ACC:0.984375\n",
      "Training iteration 263 loss: 0.12154635787010193, ACC:0.9375\n",
      "Training iteration 264 loss: 0.08447972685098648, ACC:0.953125\n",
      "Training iteration 265 loss: 0.19282490015029907, ACC:0.90625\n",
      "Training iteration 266 loss: 0.09559743106365204, ACC:0.96875\n",
      "Training iteration 267 loss: 0.15119154751300812, ACC:0.9375\n",
      "Training iteration 268 loss: 0.06406694650650024, ACC:0.96875\n",
      "Training iteration 269 loss: 0.13876092433929443, ACC:0.9375\n",
      "Training iteration 270 loss: 0.07893544435501099, ACC:0.984375\n",
      "Training iteration 271 loss: 0.13564299046993256, ACC:0.953125\n",
      "Training iteration 272 loss: 0.13958613574504852, ACC:0.9375\n",
      "Training iteration 273 loss: 0.037210918962955475, ACC:1.0\n",
      "Training iteration 274 loss: 0.04830135777592659, ACC:1.0\n",
      "Training iteration 275 loss: 0.03741315007209778, ACC:1.0\n",
      "Training iteration 276 loss: 0.07806594669818878, ACC:0.984375\n",
      "Training iteration 277 loss: 0.23842360079288483, ACC:0.921875\n",
      "Training iteration 278 loss: 0.10938655585050583, ACC:0.96875\n",
      "Training iteration 279 loss: 0.07469841092824936, ACC:0.984375\n",
      "Training iteration 280 loss: 0.11493811756372452, ACC:0.96875\n",
      "Training iteration 281 loss: 0.09428330510854721, ACC:0.953125\n",
      "Training iteration 282 loss: 0.1775340437889099, ACC:0.90625\n",
      "Training iteration 283 loss: 0.06697209924459457, ACC:0.984375\n",
      "Training iteration 284 loss: 0.17042355239391327, ACC:0.953125\n",
      "Training iteration 285 loss: 0.24040375649929047, ACC:0.875\n",
      "Training iteration 286 loss: 0.09302522987127304, ACC:0.984375\n",
      "Training iteration 287 loss: 0.18060022592544556, ACC:0.953125\n",
      "Training iteration 288 loss: 0.13556045293807983, ACC:0.96875\n",
      "Training iteration 289 loss: 0.19098079204559326, ACC:0.9375\n",
      "Training iteration 290 loss: 0.15393440425395966, ACC:0.9375\n",
      "Training iteration 291 loss: 0.13883768022060394, ACC:0.953125\n",
      "Training iteration 292 loss: 0.1329154670238495, ACC:0.96875\n",
      "Training iteration 293 loss: 0.20477169752120972, ACC:0.9375\n",
      "Training iteration 294 loss: 0.21040812134742737, ACC:0.9375\n",
      "Training iteration 295 loss: 0.12672021985054016, ACC:0.96875\n",
      "Training iteration 296 loss: 0.1749364733695984, ACC:0.9375\n",
      "Training iteration 297 loss: 0.16699352860450745, ACC:0.9375\n",
      "Training iteration 298 loss: 0.21254457533359528, ACC:0.890625\n",
      "Training iteration 299 loss: 0.12458669394254684, ACC:0.96875\n",
      "Training iteration 300 loss: 0.05554609000682831, ACC:1.0\n",
      "Training iteration 301 loss: 0.12690924108028412, ACC:0.96875\n",
      "Training iteration 302 loss: 0.06568224728107452, ACC:0.984375\n",
      "Training iteration 303 loss: 0.0424671433866024, ACC:1.0\n",
      "Training iteration 304 loss: 0.04546661674976349, ACC:1.0\n",
      "Training iteration 305 loss: 0.16636763513088226, ACC:0.953125\n",
      "Training iteration 306 loss: 0.10904402285814285, ACC:0.953125\n",
      "Training iteration 307 loss: 0.20132581889629364, ACC:0.953125\n",
      "Training iteration 308 loss: 0.13663862645626068, ACC:0.953125\n",
      "Training iteration 309 loss: 0.1496693342924118, ACC:0.9375\n",
      "Training iteration 310 loss: 0.12929894030094147, ACC:0.953125\n",
      "Training iteration 311 loss: 0.11302616447210312, ACC:0.96875\n",
      "Training iteration 312 loss: 0.09839820116758347, ACC:0.96875\n",
      "Training iteration 313 loss: 0.036381371319293976, ACC:1.0\n",
      "Training iteration 314 loss: 0.08028179407119751, ACC:0.96875\n",
      "Training iteration 315 loss: 0.14101660251617432, ACC:0.96875\n",
      "Training iteration 316 loss: 0.0953429639339447, ACC:0.96875\n",
      "Training iteration 317 loss: 0.05011617764830589, ACC:0.984375\n",
      "Training iteration 318 loss: 0.14992624521255493, ACC:0.921875\n",
      "Training iteration 319 loss: 0.21431545913219452, ACC:0.921875\n",
      "Training iteration 320 loss: 0.0795893669128418, ACC:0.984375\n",
      "Training iteration 321 loss: 0.13876895606517792, ACC:0.96875\n",
      "Training iteration 322 loss: 0.0603955052793026, ACC:0.984375\n",
      "Training iteration 323 loss: 0.03991058096289635, ACC:1.0\n",
      "Training iteration 324 loss: 0.12504929304122925, ACC:0.96875\n",
      "Training iteration 325 loss: 0.13266520202159882, ACC:0.953125\n",
      "Training iteration 326 loss: 0.1931406706571579, ACC:0.9375\n",
      "Training iteration 327 loss: 0.24226118624210358, ACC:0.90625\n",
      "Training iteration 328 loss: 0.08633307367563248, ACC:0.984375\n",
      "Training iteration 329 loss: 0.1055273786187172, ACC:0.953125\n",
      "Training iteration 330 loss: 0.08595190942287445, ACC:0.96875\n",
      "Training iteration 331 loss: 0.12979301810264587, ACC:0.953125\n",
      "Training iteration 332 loss: 0.1097155511379242, ACC:0.953125\n",
      "Training iteration 333 loss: 0.15812982618808746, ACC:0.9375\n",
      "Training iteration 334 loss: 0.07255272567272186, ACC:0.984375\n",
      "Training iteration 335 loss: 0.10641945153474808, ACC:0.953125\n",
      "Training iteration 336 loss: 0.07415469735860825, ACC:0.96875\n",
      "Training iteration 337 loss: 0.25333818793296814, ACC:0.890625\n",
      "Training iteration 338 loss: 0.10481267422437668, ACC:0.953125\n",
      "Training iteration 339 loss: 0.12950122356414795, ACC:0.953125\n",
      "Training iteration 340 loss: 0.14378860592842102, ACC:0.953125\n",
      "Training iteration 341 loss: 0.3006870448589325, ACC:0.890625\n",
      "Training iteration 342 loss: 0.19544944167137146, ACC:0.921875\n",
      "Training iteration 343 loss: 0.0734967589378357, ACC:0.953125\n",
      "Training iteration 344 loss: 0.25913459062576294, ACC:0.90625\n",
      "Training iteration 345 loss: 0.10449444502592087, ACC:0.9375\n",
      "Training iteration 346 loss: 0.2741655707359314, ACC:0.875\n",
      "Training iteration 347 loss: 0.10663247853517532, ACC:0.96875\n",
      "Training iteration 348 loss: 0.1516222357749939, ACC:0.9375\n",
      "Training iteration 349 loss: 0.22280353307724, ACC:0.9375\n",
      "Training iteration 350 loss: 0.29439520835876465, ACC:0.953125\n",
      "Training iteration 351 loss: 0.17225071787834167, ACC:0.96875\n",
      "Training iteration 352 loss: 0.07320947200059891, ACC:0.96875\n",
      "Training iteration 353 loss: 0.10344366729259491, ACC:0.96875\n",
      "Training iteration 354 loss: 0.13090980052947998, ACC:0.953125\n",
      "Training iteration 355 loss: 0.12129097431898117, ACC:0.96875\n",
      "Training iteration 356 loss: 0.0789368599653244, ACC:0.984375\n",
      "Training iteration 357 loss: 0.14460401237010956, ACC:0.953125\n",
      "Training iteration 358 loss: 0.2599560022354126, ACC:0.921875\n",
      "Training iteration 359 loss: 0.13974158465862274, ACC:0.984375\n",
      "Training iteration 360 loss: 0.2862238585948944, ACC:0.890625\n",
      "Training iteration 361 loss: 0.13512760400772095, ACC:0.96875\n",
      "Training iteration 362 loss: 0.17170065641403198, ACC:0.921875\n",
      "Training iteration 363 loss: 0.1088341474533081, ACC:0.96875\n",
      "Training iteration 364 loss: 0.10533876717090607, ACC:0.96875\n",
      "Training iteration 365 loss: 0.18903577327728271, ACC:0.9375\n",
      "Training iteration 366 loss: 0.19311168789863586, ACC:0.921875\n",
      "Training iteration 367 loss: 0.17109628021717072, ACC:0.953125\n",
      "Training iteration 368 loss: 0.14187291264533997, ACC:0.953125\n",
      "Training iteration 369 loss: 0.07530725002288818, ACC:0.984375\n",
      "Training iteration 370 loss: 0.18455027043819427, ACC:0.921875\n",
      "Training iteration 371 loss: 0.0945090800523758, ACC:0.984375\n",
      "Training iteration 372 loss: 0.23282691836357117, ACC:0.9375\n",
      "Training iteration 373 loss: 0.1769091635942459, ACC:0.96875\n",
      "Training iteration 374 loss: 0.05754438415169716, ACC:0.984375\n",
      "Training iteration 375 loss: 0.26484763622283936, ACC:0.90625\n",
      "Training iteration 376 loss: 0.29295599460601807, ACC:0.890625\n",
      "Training iteration 377 loss: 0.18229252099990845, ACC:0.953125\n",
      "Training iteration 378 loss: 0.12317650020122528, ACC:0.953125\n",
      "Training iteration 379 loss: 0.09354636818170547, ACC:0.984375\n",
      "Training iteration 380 loss: 0.16596971452236176, ACC:0.953125\n",
      "Training iteration 381 loss: 0.1411179006099701, ACC:0.921875\n",
      "Training iteration 382 loss: 0.07362107932567596, ACC:0.96875\n",
      "Training iteration 383 loss: 0.15538246929645538, ACC:0.96875\n",
      "Training iteration 384 loss: 0.19403482973575592, ACC:0.921875\n",
      "Training iteration 385 loss: 0.06842096894979477, ACC:0.984375\n",
      "Training iteration 386 loss: 0.13945665955543518, ACC:0.96875\n",
      "Training iteration 387 loss: 0.04720678552985191, ACC:1.0\n",
      "Training iteration 388 loss: 0.20784829556941986, ACC:0.921875\n",
      "Training iteration 389 loss: 0.11161449551582336, ACC:0.9375\n",
      "Training iteration 390 loss: 0.0606062076985836, ACC:0.984375\n",
      "Training iteration 391 loss: 0.15190087258815765, ACC:0.9375\n",
      "Training iteration 392 loss: 0.06489022821187973, ACC:1.0\n",
      "Training iteration 393 loss: 0.05281292274594307, ACC:1.0\n",
      "Training iteration 394 loss: 0.1966279000043869, ACC:0.921875\n",
      "Training iteration 395 loss: 0.07005635648965836, ACC:1.0\n",
      "Training iteration 396 loss: 0.1868710070848465, ACC:0.953125\n",
      "Training iteration 397 loss: 0.05669056996703148, ACC:0.984375\n",
      "Training iteration 398 loss: 0.04136432334780693, ACC:1.0\n",
      "Training iteration 399 loss: 0.10819826275110245, ACC:0.984375\n",
      "Training iteration 400 loss: 0.16552570462226868, ACC:0.953125\n",
      "Training iteration 401 loss: 0.073971688747406, ACC:0.984375\n",
      "Training iteration 402 loss: 0.11859311163425446, ACC:0.953125\n",
      "Training iteration 403 loss: 0.15834201872348785, ACC:0.953125\n",
      "Training iteration 404 loss: 0.13023890554904938, ACC:0.96875\n",
      "Training iteration 405 loss: 0.11648764461278915, ACC:0.953125\n",
      "Training iteration 406 loss: 0.058352455496788025, ACC:0.984375\n",
      "Training iteration 407 loss: 0.10450541228055954, ACC:0.96875\n",
      "Training iteration 408 loss: 0.04320757836103439, ACC:1.0\n",
      "Training iteration 409 loss: 0.053527552634477615, ACC:0.984375\n",
      "Training iteration 410 loss: 0.12944744527339935, ACC:0.953125\n",
      "Training iteration 411 loss: 0.10976352542638779, ACC:0.96875\n",
      "Training iteration 412 loss: 0.13148023188114166, ACC:0.96875\n",
      "Training iteration 413 loss: 0.08404239267110825, ACC:0.96875\n",
      "Training iteration 414 loss: 0.02696182206273079, ACC:1.0\n",
      "Training iteration 415 loss: 0.07621574401855469, ACC:0.984375\n",
      "Training iteration 416 loss: 0.09530071169137955, ACC:0.96875\n",
      "Training iteration 417 loss: 0.15134330093860626, ACC:0.953125\n",
      "Training iteration 418 loss: 0.060160111635923386, ACC:0.984375\n",
      "Training iteration 419 loss: 0.15617787837982178, ACC:0.953125\n",
      "Training iteration 420 loss: 0.19097046554088593, ACC:0.9375\n",
      "Training iteration 421 loss: 0.05322607606649399, ACC:0.984375\n",
      "Training iteration 422 loss: 0.10475999116897583, ACC:0.96875\n",
      "Training iteration 423 loss: 0.07506183534860611, ACC:0.96875\n",
      "Training iteration 424 loss: 0.15596984326839447, ACC:0.953125\n",
      "Training iteration 425 loss: 0.16241692006587982, ACC:0.953125\n",
      "Training iteration 426 loss: 0.22465063631534576, ACC:0.921875\n",
      "Training iteration 427 loss: 0.1088397204875946, ACC:0.953125\n",
      "Training iteration 428 loss: 0.08890601992607117, ACC:0.9375\n",
      "Training iteration 429 loss: 0.09750232100486755, ACC:0.96875\n",
      "Training iteration 430 loss: 0.13237152993679047, ACC:0.953125\n",
      "Training iteration 431 loss: 0.17185235023498535, ACC:0.9375\n",
      "Training iteration 432 loss: 0.08281762897968292, ACC:0.984375\n",
      "Training iteration 433 loss: 0.06640399247407913, ACC:0.96875\n",
      "Training iteration 434 loss: 0.041025955229997635, ACC:0.984375\n",
      "Training iteration 435 loss: 0.08916910737752914, ACC:0.96875\n",
      "Training iteration 436 loss: 0.10945755243301392, ACC:0.9375\n",
      "Training iteration 437 loss: 0.11858437955379486, ACC:0.953125\n",
      "Training iteration 438 loss: 0.037437550723552704, ACC:1.0\n",
      "Training iteration 439 loss: 0.045444972813129425, ACC:1.0\n",
      "Training iteration 440 loss: 0.02943207509815693, ACC:1.0\n",
      "Training iteration 441 loss: 0.047706395387649536, ACC:0.984375\n",
      "Training iteration 442 loss: 0.21008452773094177, ACC:0.9375\n",
      "Training iteration 443 loss: 0.10693448781967163, ACC:0.953125\n",
      "Training iteration 444 loss: 0.04252898693084717, ACC:0.984375\n",
      "Training iteration 445 loss: 0.15840688347816467, ACC:0.953125\n",
      "Training iteration 446 loss: 0.08109373599290848, ACC:0.984375\n",
      "Training iteration 447 loss: 0.10922821611166, ACC:0.953125\n",
      "Training iteration 448 loss: 0.08556516468524933, ACC:0.984375\n",
      "Training iteration 449 loss: 0.13333559036254883, ACC:0.96875\n",
      "Training iteration 450 loss: 0.07120269536972046, ACC:0.984375\n",
      "Validation iteration 451 loss: 0.11454188078641891, ACC: 0.953125\n",
      "Validation iteration 452 loss: 0.07329755276441574, ACC: 0.984375\n",
      "Validation iteration 453 loss: 0.12402693182229996, ACC: 0.984375\n",
      "Validation iteration 454 loss: 0.08383876085281372, ACC: 0.953125\n",
      "Validation iteration 455 loss: 0.05363577976822853, ACC: 0.984375\n",
      "Validation iteration 456 loss: 0.1445380002260208, ACC: 0.9375\n",
      "Validation iteration 457 loss: 0.06444663554430008, ACC: 0.984375\n",
      "Validation iteration 458 loss: 0.11196722835302353, ACC: 0.96875\n",
      "Validation iteration 459 loss: 0.07683095335960388, ACC: 0.96875\n",
      "Validation iteration 460 loss: 0.14016596972942352, ACC: 0.9375\n",
      "Validation iteration 461 loss: 0.1056165099143982, ACC: 0.96875\n",
      "Validation iteration 462 loss: 0.038261085748672485, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.07726335525512695, ACC: 0.96875\n",
      "Validation iteration 464 loss: 0.06335066258907318, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.04936268553137779, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.07074142247438431, ACC: 0.984375\n",
      "Validation iteration 467 loss: 0.24730880558490753, ACC: 0.90625\n",
      "Validation iteration 468 loss: 0.10344327986240387, ACC: 0.984375\n",
      "Validation iteration 469 loss: 0.195822075009346, ACC: 0.921875\n",
      "Validation iteration 470 loss: 0.1400044709444046, ACC: 0.9375\n",
      "Validation iteration 471 loss: 0.12773171067237854, ACC: 0.953125\n",
      "Validation iteration 472 loss: 0.07061238586902618, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.1807156503200531, ACC: 0.9375\n",
      "Validation iteration 474 loss: 0.08801580965518951, ACC: 0.984375\n",
      "Validation iteration 475 loss: 0.08987931907176971, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.07482286542654037, ACC: 0.953125\n",
      "Validation iteration 477 loss: 0.09550163894891739, ACC: 0.96875\n",
      "Validation iteration 478 loss: 0.07072333991527557, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.06925307959318161, ACC: 0.984375\n",
      "Validation iteration 480 loss: 0.10033784806728363, ACC: 0.984375\n",
      "Validation iteration 481 loss: 0.11107661575078964, ACC: 0.96875\n",
      "Validation iteration 482 loss: 0.14386342465877533, ACC: 0.953125\n",
      "Validation iteration 483 loss: 0.06547395139932632, ACC: 0.96875\n",
      "Validation iteration 484 loss: 0.11617536842823029, ACC: 0.953125\n",
      "Validation iteration 485 loss: 0.10578644275665283, ACC: 0.96875\n",
      "Validation iteration 486 loss: 0.07917024195194244, ACC: 0.984375\n",
      "Validation iteration 487 loss: 0.10164473950862885, ACC: 0.953125\n",
      "Validation iteration 488 loss: 0.05709245428442955, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.2122076153755188, ACC: 0.9375\n",
      "Validation iteration 490 loss: 0.13292144238948822, ACC: 0.9375\n",
      "Validation iteration 491 loss: 0.09484566748142242, ACC: 0.953125\n",
      "Validation iteration 492 loss: 0.1821567416191101, ACC: 0.953125\n",
      "Validation iteration 493 loss: 0.09192132949829102, ACC: 0.96875\n",
      "Validation iteration 494 loss: 0.07243725657463074, ACC: 0.96875\n",
      "Validation iteration 495 loss: 0.09722312539815903, ACC: 0.96875\n",
      "Validation iteration 496 loss: 0.10160978883504868, ACC: 0.96875\n",
      "Validation iteration 497 loss: 0.18969419598579407, ACC: 0.90625\n",
      "Validation iteration 498 loss: 0.11955495923757553, ACC: 0.953125\n",
      "Validation iteration 499 loss: 0.08806885033845901, ACC: 0.984375\n",
      "Validation iteration 500 loss: 0.056566622108221054, ACC: 0.984375\n",
      "-- Epoch 10 done -- Train loss: 0.1577697400872906, train ACC: 0.9478125, val loss: 0.10531097054481506, val ACC: 0.965625\n",
      "<--- 2490.9210212230682 seconds --->\n"
     ]
    }
   ],
   "source": [
    "# for timing model training purposes\n",
    "start_time = time.time()\n",
    "\n",
    "# input variables\n",
    "epoch_num = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# train model\n",
    "model = LeNet5()\n",
    "cost_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# to store loss for training & validation set\n",
    "jw_train_epoch = []  ## to store loss for training set by epoch (average loss of iterations)\n",
    "jw_val_epoch = []    ## to store loss for validation set by epoch (average loss of iterations)\n",
    "\n",
    "# to store accuracy of training & validation set\n",
    "acc_train_epoch = []\n",
    "acc_val_epoch = []\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "\n",
    "    # track train & validation loss & accuracy by iteration for each epoch\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    test_counter = 1\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ''' include below IF statement if want to limit number of batches '''\n",
    "        if i+1 == 501:  # should have total of 500 batches after train & val sets\n",
    "            break\n",
    "        \n",
    "        # use 80% for training, 20% for testing, and 10% for validation of the 40k training samples\n",
    "        ## batch size=64 so 625 batches total: 450 train batches, 50 val batches, and 125 test batches\n",
    "\n",
    "        if i+1 > 450:  # validate model with validation set (10% of total train data)\n",
    "            inputs, labels = data\n",
    "            \n",
    "            logits, outputs = model(inputs)\n",
    "            cost = cost_fn(logits, labels)\n",
    "            \n",
    "            jw_val = float(cost)\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "            correct = pred == labels\n",
    "            acc = sum(np.array(correct))/len(np.array(correct))\n",
    "\n",
    "            acc_val = acc\n",
    "\n",
    "            val_loss.append(jw_val)\n",
    "            val_acc.append(acc_val)\n",
    "            \n",
    "            print(f'Validation iteration {i+1} loss: {jw_val}, ACC: {acc_val}')\n",
    "            \n",
    "        else:  # train model with training set\n",
    "\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()            # zero the parameter gradients\n",
    "            logits, outputs = model(inputs)  # forward\n",
    "            cost = cost_fn(logits, labels)   # input logits prior to softmax activation into cost function\n",
    "            cost.backward()                  # backward\n",
    "            optimizer.step()                 # optimize\n",
    "\n",
    "            jw_train = float(cost)\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred = torch.Tensor.argmax(outputs, dim=1)            # get labels of prediction with highest probability\n",
    "            correct = pred == labels                              # compare to actual labels and see which was predicted correctly\n",
    "\n",
    "            acc = sum(np.array(correct))/len(np.array(correct))   # calculate accuracy\n",
    "\n",
    "            acc_train = acc\n",
    "\n",
    "            train_loss.append(jw_train)\n",
    "            train_acc.append(acc_train)\n",
    "            \n",
    "            print(f'Training iteration {i+1} loss: {jw_train}, ACC:{acc_train}')\n",
    "\n",
    "    # to save time, epoch loss = the lowest loss, epoch acc = highest acc in training\n",
    "    epoch_jw = np.mean(np.array(train_loss))\n",
    "    epoch_acc = np.mean(np.array(train_acc))\n",
    "\n",
    "    jw_train_epoch.append(epoch_jw)\n",
    "    jw_val_epoch.append(np.mean(val_loss))\n",
    "    acc_train_epoch.append(epoch_acc)\n",
    "    acc_val_epoch.append(np.mean(val_acc))\n",
    "    \n",
    "    print(f'-- Epoch {epoch+1} done -- Train loss: {epoch_jw}, train ACC: {epoch_acc}, val loss: {np.mean(val_loss)}, val ACC: {np.mean(val_acc)}')\n",
    "    \n",
    "    print(\"<--- %s seconds --->\" % (time.time() - start_time))\n",
    "\n",
    "    # save model at every epoch\n",
    "    path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_cpu_epoch{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwdS7oqJweUo"
   },
   "source": [
    "Plot loss and accuracy over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1636883674385,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "hZPUGYuZYBLH",
    "outputId": "d66fcf71-05ab-414e-ee28-792459be554f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss over epoch\n",
    "plt.plot([i+1 for i in range(len(jw_train_epoch))],jw_train_epoch)\n",
    "plt.plot([i+1 for i in range(len(jw_val_epoch))],jw_val_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Loss with learning rate {learning_rate}')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 741,
     "status": "ok",
     "timestamp": 1636883675108,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "j8coUlN-YBLI",
    "outputId": "e4a7ae4b-edac-4ea9-ec02-2c6aa53350c2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracy over epoch\n",
    "plt.plot([i+1 for i in range(len(acc_train_epoch))],acc_train_epoch)\n",
    "plt.plot([i+1 for i in range(len(acc_val_epoch))],acc_val_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Accuracy with learning rate {learning_rate}')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJQvqXWnYBLI"
   },
   "source": [
    "**Calculate test accuracy for model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117399,
     "status": "ok",
     "timestamp": 1636903241411,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "EoO1l_KoBvaf",
    "outputId": "98efc5b0-6725-4ef6-89e4-d8f6d951fe69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=5880, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing accuracy = 96.6%\n"
     ]
    }
   ],
   "source": [
    "# load saved model \n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_cpu_epoch{10}.pth\"\n",
    "\n",
    "model = LeNet5()\n",
    "model.load_state_dict(torch.load(path))\n",
    "print(model.eval())\n",
    "\n",
    "# or use model in current session\n",
    "# model = model\n",
    "\n",
    "# calculate test accuracy of model at a given epoch\n",
    "\n",
    "testset_path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_testset.pth\"\n",
    "inputs, labels = torch.load(testset_path)\n",
    "\n",
    "logits, outputs = model(inputs)\n",
    "\n",
    "# calculate overall accuracy across all classes\n",
    "pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "correct = pred == labels\n",
    "\n",
    "correct_cnt = sum(np.array(correct))\n",
    "data_cnt = len(np.array(correct))\n",
    "\n",
    "print(f'\\n Testing accuracy = {(correct_cnt/data_cnt)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145201,
     "status": "ok",
     "timestamp": 1636903892569,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "Wh_BILNjDqqt",
    "outputId": "2ffc3e15-8608-4d01-fd4b-15220861cda5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=5880, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing accuracy = 98.2%\n"
     ]
    }
   ],
   "source": [
    "# load saved model \n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_cpu_epoch{6}.pth\"\n",
    "\n",
    "model = LeNet5()\n",
    "model.load_state_dict(torch.load(path))\n",
    "print(model.eval())\n",
    "\n",
    "# or use model in current session\n",
    "# model = model\n",
    "\n",
    "# calculate test accuracy of model at a given epoch\n",
    "\n",
    "testset_path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_testset.pth\"\n",
    "inputs, labels = torch.load(testset_path)\n",
    "\n",
    "logits, outputs = model(inputs)\n",
    "\n",
    "# calculate overall accuracy across all classes\n",
    "pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "correct = pred == labels\n",
    "\n",
    "correct_cnt = sum(np.array(correct))\n",
    "data_cnt = len(np.array(correct))\n",
    "\n",
    "print(f'\\n Testing accuracy = {(correct_cnt/data_cnt)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145167,
     "status": "ok",
     "timestamp": 1636904351561,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "2O2WJpSOF-LG",
    "outputId": "e5905495-c30e-4483-dc76-1b3f1792156b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=5880, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing accuracy = 97.2%\n"
     ]
    }
   ],
   "source": [
    "# load saved model \n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_cpu_epoch{5}.pth\"\n",
    "\n",
    "model = LeNet5()\n",
    "model.load_state_dict(torch.load(path))\n",
    "print(model.eval())\n",
    "\n",
    "# or use model in current session\n",
    "# model = model\n",
    "\n",
    "# calculate test accuracy of model at a given epoch\n",
    "\n",
    "testset_path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_testset.pth\"\n",
    "inputs, labels = torch.load(testset_path)\n",
    "\n",
    "logits, outputs = model(inputs)\n",
    "\n",
    "# calculate overall accuracy across all classes\n",
    "pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "correct = pred == labels\n",
    "\n",
    "correct_cnt = sum(np.array(correct))\n",
    "data_cnt = len(np.array(correct))\n",
    "\n",
    "print(f'\\n Testing accuracy = {(correct_cnt/data_cnt)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8712,
     "status": "ok",
     "timestamp": 1636923837550,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "a62LgfqSQkEI",
    "outputId": "5fdc70ff-7bd6-475a-940b-ffdeb0bdaee3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=5880, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing accuracy = 71.8%\n"
     ]
    }
   ],
   "source": [
    "# test accuracy with 1000 augmented images to see if model can handle \"new\" inputs\n",
    "\n",
    "# load saved model \n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_cpu_epoch{6}.pth\"\n",
    "\n",
    "model = LeNet5()\n",
    "model.load_state_dict(torch.load(path))\n",
    "print(model.eval())\n",
    "\n",
    "# or use model in current session\n",
    "# model = model\n",
    "\n",
    "# calculate test accuracy of model at a given epoch\n",
    "\n",
    "testset_path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/test_sets/small_1000_set.pth\"\n",
    "inputs, labels = torch.load(testset_path)\n",
    "\n",
    "logits, outputs = model(inputs)\n",
    "\n",
    "# calculate overall accuracy across all classes\n",
    "pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "correct = pred == labels\n",
    "\n",
    "correct_cnt = sum(np.array(correct))\n",
    "data_cnt = len(np.array(correct))\n",
    "\n",
    "print(f'\\n Testing accuracy = {(correct_cnt/data_cnt)*100:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LeNet5_model_noAugmentation_lr0.001.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
