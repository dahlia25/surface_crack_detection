{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLEh-LthYBKv"
   },
   "source": [
    "# Surface Crack Images - LeNet5 model\n",
    "This notebook contains the code training the LeNet5 model (without augmented data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEb3XLxjYBK4"
   },
   "source": [
    "**Load packages/modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30206,
     "status": "ok",
     "timestamp": 1636904583335,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "H-oXyKDyYEED",
    "outputId": "3b462d4c-55b1-4939-e040-28b3ce43c82d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27569,
     "status": "ok",
     "timestamp": 1636904834025,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "9AKkpfhtYBK5",
    "outputId": "d3bba900-f4c9-4420-c2de-c3c1b4576e84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.10.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print(f'Torch version: {torch .__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3527,
     "status": "ok",
     "timestamp": 1636904837523,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "i_OfDGEmYBK8",
    "outputId": "db028559-de89-4a17-e220-f2cef647e3cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "%pip install torchsummary\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysKmpc9cYBK8"
   },
   "source": [
    "**Load data into tensors first**<br>\n",
    "Then concatenate the tensors with original and augmented data into one for ease of model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 109222,
     "status": "ok",
     "timestamp": 1636904947291,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "syPC_CMNYBK_"
   },
   "outputs": [],
   "source": [
    "# load data into tensors first\n",
    "data_dir = '/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/archive/original'\n",
    "# aug_data_dir = '/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/archive/augmented'\n",
    "batch_size = 64 \n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "data = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(data, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True, \n",
    "                                         pin_memory=True)\n",
    "\n",
    "# aug_dataloader = torch.utils.data.DataLoader(aug_data,\n",
    "#                                              batch_size=batch_size*9,  # multiply by 9 since augmented x9 images for each original\n",
    "#                                              shuffle=True,\n",
    "#                                              pin_memory=True)\n",
    "\n",
    "# images, labels = next(iter(dataloader))\n",
    "# aug_images, aug_labels = next(iter(aug_dataloader))\n",
    "\n",
    "# print(f'Original images shape: {images.shape}')\n",
    "# print(f'Augmented images shape: {aug_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9C4r55Yj-Pf"
   },
   "outputs": [],
   "source": [
    "# generate and save test set\n",
    "test_counter = 1\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    if i+1 > 500:\n",
    "        if test_counter == 1:\n",
    "            test_inputs, test_labels = data \n",
    "            test_counter += 1\n",
    "        else:\n",
    "            new_inputs, new_labels = data\n",
    "            test_inputs = torch.concat((test_inputs, new_inputs), 0)\n",
    "            test_labels = torch.concat((test_labels, new_labels), 0)\n",
    "            test_counter += 1\n",
    "\n",
    "        # print(f'Batch {i+1} for test set complete.')\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# print(f'Saving test inputs {test_inputs.shape}, test labels {test_labels.shape}')\n",
    "\n",
    "# save test set at the end of training\n",
    "testset = [test_inputs, test_labels]\n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_testset.pth\"\n",
    "torch.save(testset, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZaeGsiNYBLC"
   },
   "source": [
    "## Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1636904970603,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "-0gG2OoGYBLD"
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, padding=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=2, stride=2)       \n",
    "        self.conv3 = nn.Conv2d(16, 120, 5, padding=2, stride=2)\n",
    "\n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(120*49, 84)\n",
    "        self.fc2 = nn.Linear(84, 2)   # have two classes (has crack/no crack)\n",
    "        \n",
    "        # softmax layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # pooling layer\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2)  # Average pool 2x2\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = F.tanh(self.conv1(x)) # layer 1\n",
    "        x = self.pool(x)          # layer 2\n",
    "        \n",
    "        x = F.tanh(self.conv2(x)) # layer 3\n",
    "        x = self.pool(x)          # layer 4\n",
    "        \n",
    "        x = F.tanh(self.conv3(x)) # layer 5\n",
    "        \n",
    "        x = torch.flatten(x, 1)   # flatten\n",
    "        \n",
    "        x = F.tanh(self.fc1(x))   # reshape layer\n",
    "        logit = self.fc2(x)       # layer 6\n",
    "        \n",
    "        output = self.softmax(logit) # layer 7\n",
    "\n",
    "        return logit, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1636904974879,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "0gbcU53BYBLE",
    "outputId": "3bfbb3f9-8c69-4fc1-d20e-0d2be7e5bb26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 114, 114]             456\n",
      "         AvgPool2d-2            [-1, 6, 57, 57]               0\n",
      "            Conv2d-3           [-1, 16, 29, 29]           2,416\n",
      "         AvgPool2d-4           [-1, 16, 14, 14]               0\n",
      "            Conv2d-5            [-1, 120, 7, 7]          48,120\n",
      "            Linear-6                   [-1, 84]         494,004\n",
      "            Linear-7                    [-1, 2]             170\n",
      "           Softmax-8                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 545,166\n",
      "Trainable params: 545,166\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 0.92\n",
      "Params size (MB): 2.08\n",
      "Estimated Total Size (MB): 3.59\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "# print model summary\n",
    "\n",
    "model = LeNet5()\n",
    "\n",
    "channels = 3\n",
    "H = 227\n",
    "W = 227\n",
    "\n",
    "summary(model, (channels, H, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp89_6KnjowJ"
   },
   "source": [
    "### Find optimal learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiN5vfU2U_-j"
   },
   "source": [
    "**Train model with learning rates by starting with small number of batches**, so can confirm that architecture is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10014194,
     "status": "ok",
     "timestamp": 1636915057468,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "0acPeyXXsDdk",
    "outputId": "47b74afc-4526-4e08-a1d3-7bcaaa38071e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 1 loss: 0.6936317086219788, ACC:0.5\n",
      "Training iteration 2 loss: 3.4028635025024414, ACC:0.578125\n",
      "Training iteration 3 loss: 1.9545999765396118, ACC:0.515625\n",
      "Training iteration 4 loss: 1.8310450315475464, ACC:0.453125\n",
      "Training iteration 5 loss: 1.8549911975860596, ACC:0.515625\n",
      "Training iteration 6 loss: 3.036938428878784, ACC:0.375\n",
      "Training iteration 7 loss: 0.6755508184432983, ACC:0.59375\n",
      "Training iteration 8 loss: 1.5699164867401123, ACC:0.5625\n",
      "Training iteration 9 loss: 1.6375796794891357, ACC:0.578125\n",
      "Training iteration 10 loss: 1.0434997081756592, ACC:0.421875\n",
      "Training iteration 11 loss: 1.5625298023223877, ACC:0.5\n",
      "Training iteration 12 loss: 2.2749226093292236, ACC:0.453125\n",
      "Training iteration 13 loss: 1.1759434938430786, ACC:0.5\n",
      "Training iteration 14 loss: 0.9440804123878479, ACC:0.546875\n",
      "Training iteration 15 loss: 1.6352301836013794, ACC:0.546875\n",
      "Training iteration 16 loss: 1.396681308746338, ACC:0.546875\n",
      "Training iteration 17 loss: 0.7184805870056152, ACC:0.5\n",
      "Training iteration 18 loss: 1.2811723947525024, ACC:0.5\n",
      "Training iteration 19 loss: 1.6133406162261963, ACC:0.453125\n",
      "Training iteration 20 loss: 0.8769151568412781, ACC:0.40625\n",
      "Training iteration 21 loss: 1.3833861351013184, ACC:0.4375\n",
      "Training iteration 22 loss: 1.82855224609375, ACC:0.390625\n",
      "Training iteration 23 loss: 0.727541983127594, ACC:0.578125\n",
      "Training iteration 24 loss: 0.8769049644470215, ACC:0.546875\n",
      "Training iteration 25 loss: 1.2341338396072388, ACC:0.53125\n",
      "Training iteration 26 loss: 0.800986111164093, ACC:0.609375\n",
      "Training iteration 27 loss: 0.7074061632156372, ACC:0.46875\n",
      "Training iteration 28 loss: 0.9615479111671448, ACC:0.484375\n",
      "Training iteration 29 loss: 0.8100881576538086, ACC:0.515625\n",
      "Training iteration 30 loss: 0.7201091647148132, ACC:0.46875\n",
      "Training iteration 31 loss: 0.8500335216522217, ACC:0.484375\n",
      "Training iteration 32 loss: 0.7484568953514099, ACC:0.453125\n",
      "Training iteration 33 loss: 0.7246739864349365, ACC:0.5625\n",
      "Training iteration 34 loss: 1.008307695388794, ACC:0.4375\n",
      "Training iteration 35 loss: 0.7187882661819458, ACC:0.375\n",
      "Training iteration 36 loss: 1.1720330715179443, ACC:0.421875\n",
      "Training iteration 37 loss: 0.7701778411865234, ACC:0.609375\n",
      "Training iteration 38 loss: 0.710272490978241, ACC:0.421875\n",
      "Training iteration 39 loss: 1.1110446453094482, ACC:0.40625\n",
      "Training iteration 40 loss: 0.8616573810577393, ACC:0.46875\n",
      "Training iteration 41 loss: 0.7754389047622681, ACC:0.46875\n",
      "Training iteration 42 loss: 0.8989257216453552, ACC:0.5\n",
      "Training iteration 43 loss: 0.7280614376068115, ACC:0.484375\n",
      "Training iteration 44 loss: 0.8028897643089294, ACC:0.5\n",
      "Training iteration 45 loss: 0.8125594258308411, ACC:0.53125\n",
      "Training iteration 46 loss: 0.702878475189209, ACC:0.46875\n",
      "Training iteration 47 loss: 0.8488544225692749, ACC:0.484375\n",
      "Training iteration 48 loss: 0.7893190383911133, ACC:0.5\n",
      "Training iteration 49 loss: 0.7238296270370483, ACC:0.453125\n",
      "Training iteration 50 loss: 0.8054372668266296, ACC:0.453125\n",
      "Training iteration 51 loss: 0.6934787034988403, ACC:0.484375\n",
      "Training iteration 52 loss: 0.7744297385215759, ACC:0.453125\n",
      "Training iteration 53 loss: 0.6842252612113953, ACC:0.578125\n",
      "Training iteration 54 loss: 0.696657121181488, ACC:0.46875\n",
      "Training iteration 55 loss: 0.6917567849159241, ACC:0.578125\n",
      "Training iteration 56 loss: 0.6866084933280945, ACC:0.5625\n",
      "Training iteration 57 loss: 0.7677662372589111, ACC:0.421875\n",
      "Training iteration 58 loss: 0.6969870924949646, ACC:0.546875\n",
      "Training iteration 59 loss: 0.8548035025596619, ACC:0.453125\n",
      "Training iteration 60 loss: 0.6941855549812317, ACC:0.484375\n",
      "Training iteration 61 loss: 0.7636065483093262, ACC:0.515625\n",
      "Training iteration 62 loss: 0.7686966061592102, ACC:0.421875\n",
      "Training iteration 63 loss: 0.8064264059066772, ACC:0.484375\n",
      "Training iteration 64 loss: 0.8236950635910034, ACC:0.46875\n",
      "Training iteration 65 loss: 0.7262513041496277, ACC:0.484375\n",
      "Training iteration 66 loss: 0.8231627345085144, ACC:0.484375\n",
      "Training iteration 67 loss: 0.6957968473434448, ACC:0.46875\n",
      "Training iteration 68 loss: 0.901721179485321, ACC:0.421875\n",
      "Training iteration 69 loss: 0.7121912837028503, ACC:0.4375\n",
      "Training iteration 70 loss: 0.7500264048576355, ACC:0.59375\n",
      "Training iteration 71 loss: 0.8709388971328735, ACC:0.546875\n",
      "Training iteration 72 loss: 0.7017107009887695, ACC:0.46875\n",
      "Training iteration 73 loss: 0.7737516164779663, ACC:0.609375\n",
      "Training iteration 74 loss: 0.9429407119750977, ACC:0.546875\n",
      "Training iteration 75 loss: 0.695322573184967, ACC:0.515625\n",
      "Training iteration 76 loss: 0.9119453430175781, ACC:0.5\n",
      "Training iteration 77 loss: 0.8223068714141846, ACC:0.5\n",
      "Training iteration 78 loss: 0.7484878897666931, ACC:0.5\n",
      "Training iteration 79 loss: 0.7610300183296204, ACC:0.59375\n",
      "Training iteration 80 loss: 0.7614179253578186, ACC:0.484375\n",
      "Training iteration 81 loss: 0.8220054507255554, ACC:0.46875\n",
      "Training iteration 82 loss: 0.7742789387702942, ACC:0.515625\n",
      "Training iteration 83 loss: 0.6993179321289062, ACC:0.515625\n",
      "Training iteration 84 loss: 0.7533295750617981, ACC:0.5625\n",
      "Training iteration 85 loss: 0.7998557090759277, ACC:0.40625\n",
      "Training iteration 86 loss: 0.8107741475105286, ACC:0.546875\n",
      "Training iteration 87 loss: 0.9780102372169495, ACC:0.484375\n",
      "Training iteration 88 loss: 0.7302398085594177, ACC:0.390625\n",
      "Training iteration 89 loss: 0.7457002401351929, ACC:0.53125\n",
      "Training iteration 90 loss: 0.7200482487678528, ACC:0.453125\n",
      "Training iteration 91 loss: 0.7613968253135681, ACC:0.53125\n",
      "Training iteration 92 loss: 0.8558826446533203, ACC:0.421875\n",
      "Training iteration 93 loss: 0.7469671964645386, ACC:0.546875\n",
      "Training iteration 94 loss: 0.8769357800483704, ACC:0.546875\n",
      "Training iteration 95 loss: 0.7091900706291199, ACC:0.5\n",
      "Training iteration 96 loss: 0.9232583045959473, ACC:0.453125\n",
      "Training iteration 97 loss: 0.7146039009094238, ACC:0.546875\n",
      "Training iteration 98 loss: 0.7298636436462402, ACC:0.5\n",
      "Training iteration 99 loss: 0.7571684122085571, ACC:0.515625\n",
      "Training iteration 100 loss: 0.6893464922904968, ACC:0.5625\n",
      "Training iteration 101 loss: 0.9523599743843079, ACC:0.40625\n",
      "Training iteration 102 loss: 0.6837965846061707, ACC:0.59375\n",
      "Training iteration 103 loss: 1.029113531112671, ACC:0.484375\n",
      "Training iteration 104 loss: 0.7703843712806702, ACC:0.5\n",
      "Training iteration 105 loss: 0.8386226892471313, ACC:0.515625\n",
      "Training iteration 106 loss: 0.9360675811767578, ACC:0.484375\n",
      "Training iteration 107 loss: 0.721958339214325, ACC:0.484375\n",
      "Training iteration 108 loss: 0.9900177717208862, ACC:0.421875\n",
      "Training iteration 109 loss: 0.6956399083137512, ACC:0.515625\n",
      "Training iteration 110 loss: 0.7558115124702454, ACC:0.609375\n",
      "Training iteration 111 loss: 0.8414429426193237, ACC:0.515625\n",
      "Training iteration 112 loss: 0.6616886258125305, ACC:0.625\n",
      "Training iteration 113 loss: 1.2144527435302734, ACC:0.46875\n",
      "Training iteration 114 loss: 0.7875451445579529, ACC:0.46875\n",
      "Training iteration 115 loss: 1.0402100086212158, ACC:0.5\n",
      "Training iteration 116 loss: 0.9644309878349304, ACC:0.546875\n",
      "Training iteration 117 loss: 0.69845050573349, ACC:0.484375\n",
      "Training iteration 118 loss: 1.0209877490997314, ACC:0.46875\n",
      "Training iteration 119 loss: 0.7758496403694153, ACC:0.421875\n",
      "Training iteration 120 loss: 1.0731651782989502, ACC:0.46875\n",
      "Training iteration 121 loss: 0.8851768970489502, ACC:0.515625\n",
      "Training iteration 122 loss: 0.7123127579689026, ACC:0.5625\n",
      "Training iteration 123 loss: 1.1478586196899414, ACC:0.46875\n",
      "Training iteration 124 loss: 0.688754677772522, ACC:0.5625\n",
      "Training iteration 125 loss: 0.9301681518554688, ACC:0.4375\n",
      "Training iteration 126 loss: 0.6786981821060181, ACC:0.59375\n",
      "Training iteration 127 loss: 0.7367222905158997, ACC:0.375\n",
      "Training iteration 128 loss: 0.6888395547866821, ACC:0.546875\n",
      "Training iteration 129 loss: 0.7204089164733887, ACC:0.53125\n",
      "Training iteration 130 loss: 0.6785280108451843, ACC:0.59375\n",
      "Training iteration 131 loss: 0.6983609199523926, ACC:0.46875\n",
      "Training iteration 132 loss: 0.7157180905342102, ACC:0.484375\n",
      "Training iteration 133 loss: 0.7018209099769592, ACC:0.4375\n",
      "Training iteration 134 loss: 0.7543456554412842, ACC:0.5\n",
      "Training iteration 135 loss: 0.6809712648391724, ACC:0.578125\n",
      "Training iteration 136 loss: 0.6932664513587952, ACC:0.5\n",
      "Training iteration 137 loss: 0.7105262875556946, ACC:0.484375\n",
      "Training iteration 138 loss: 0.6962658166885376, ACC:0.4375\n",
      "Training iteration 139 loss: 0.6875666379928589, ACC:0.5625\n",
      "Training iteration 140 loss: 0.7070913314819336, ACC:0.53125\n",
      "Training iteration 141 loss: 0.67073655128479, ACC:0.625\n",
      "Training iteration 142 loss: 0.697395384311676, ACC:0.546875\n",
      "Training iteration 143 loss: 0.6991111636161804, ACC:0.5\n",
      "Training iteration 144 loss: 0.7149103879928589, ACC:0.484375\n",
      "Training iteration 145 loss: 0.7052314281463623, ACC:0.453125\n",
      "Training iteration 146 loss: 0.7571864128112793, ACC:0.46875\n",
      "Training iteration 147 loss: 0.7022594213485718, ACC:0.421875\n",
      "Training iteration 148 loss: 0.8600643873214722, ACC:0.46875\n",
      "Training iteration 149 loss: 0.6704054474830627, ACC:0.625\n",
      "Training iteration 150 loss: 0.6885033845901489, ACC:0.59375\n",
      "Training iteration 151 loss: 0.6759989261627197, ACC:0.625\n",
      "Training iteration 152 loss: 0.8271108269691467, ACC:0.5\n",
      "Training iteration 153 loss: 0.7506963610649109, ACC:0.421875\n",
      "Training iteration 154 loss: 0.7724494338035583, ACC:0.40625\n",
      "Training iteration 155 loss: 0.7247857451438904, ACC:0.578125\n",
      "Training iteration 156 loss: 0.9013326168060303, ACC:0.484375\n",
      "Training iteration 157 loss: 0.7203918099403381, ACC:0.5\n",
      "Training iteration 158 loss: 0.9320786595344543, ACC:0.453125\n",
      "Training iteration 159 loss: 0.7128306031227112, ACC:0.46875\n",
      "Training iteration 160 loss: 0.788495659828186, ACC:0.515625\n",
      "Training iteration 161 loss: 0.708706259727478, ACC:0.390625\n",
      "Training iteration 162 loss: 1.0090810060501099, ACC:0.484375\n",
      "Training iteration 163 loss: 0.7840372323989868, ACC:0.46875\n",
      "Training iteration 164 loss: 0.9434148669242859, ACC:0.5\n",
      "Training iteration 165 loss: 0.7895671129226685, ACC:0.578125\n",
      "Training iteration 166 loss: 0.7220972180366516, ACC:0.4375\n",
      "Training iteration 167 loss: 0.8070849180221558, ACC:0.453125\n",
      "Training iteration 168 loss: 0.735962450504303, ACC:0.4375\n",
      "Training iteration 169 loss: 0.7032145261764526, ACC:0.515625\n",
      "Training iteration 170 loss: 0.7081155776977539, ACC:0.4375\n",
      "Training iteration 171 loss: 0.698248028755188, ACC:0.453125\n",
      "Training iteration 172 loss: 0.6913034319877625, ACC:0.53125\n",
      "Training iteration 173 loss: 0.7161544561386108, ACC:0.46875\n",
      "Training iteration 174 loss: 0.6857263445854187, ACC:0.5625\n",
      "Training iteration 175 loss: 0.7428404688835144, ACC:0.53125\n",
      "Training iteration 176 loss: 0.6964337825775146, ACC:0.484375\n",
      "Training iteration 177 loss: 0.7085767388343811, ACC:0.578125\n",
      "Training iteration 178 loss: 0.7632535099983215, ACC:0.515625\n",
      "Training iteration 179 loss: 0.7230263352394104, ACC:0.484375\n",
      "Training iteration 180 loss: 0.7633885145187378, ACC:0.484375\n",
      "Training iteration 181 loss: 0.7046695351600647, ACC:0.515625\n",
      "Training iteration 182 loss: 0.8183363080024719, ACC:0.4375\n",
      "Training iteration 183 loss: 0.7616225481033325, ACC:0.46875\n",
      "Training iteration 184 loss: 0.7360801696777344, ACC:0.5\n",
      "Training iteration 185 loss: 0.7733741998672485, ACC:0.390625\n",
      "Training iteration 186 loss: 0.6976686716079712, ACC:0.5\n",
      "Training iteration 187 loss: 0.767484724521637, ACC:0.390625\n",
      "Training iteration 188 loss: 0.7212765216827393, ACC:0.578125\n",
      "Training iteration 189 loss: 1.0153337717056274, ACC:0.359375\n",
      "Training iteration 190 loss: 0.8811440467834473, ACC:0.546875\n",
      "Training iteration 191 loss: 1.106856107711792, ACC:0.5\n",
      "Training iteration 192 loss: 0.6914064884185791, ACC:0.546875\n",
      "Training iteration 193 loss: 0.9792861938476562, ACC:0.59375\n",
      "Training iteration 194 loss: 0.8093134760856628, ACC:0.609375\n",
      "Training iteration 195 loss: 0.7136479616165161, ACC:0.515625\n",
      "Training iteration 196 loss: 1.118776798248291, ACC:0.421875\n",
      "Training iteration 197 loss: 0.7415174841880798, ACC:0.375\n",
      "Training iteration 198 loss: 0.7291994094848633, ACC:0.515625\n",
      "Training iteration 199 loss: 0.6979387402534485, ACC:0.421875\n",
      "Training iteration 200 loss: 0.6928079724311829, ACC:0.5625\n",
      "Training iteration 201 loss: 0.7085266709327698, ACC:0.53125\n",
      "Training iteration 202 loss: 0.7035365700721741, ACC:0.5\n",
      "Training iteration 203 loss: 0.6822406649589539, ACC:0.578125\n",
      "Training iteration 204 loss: 0.7587761878967285, ACC:0.53125\n",
      "Training iteration 205 loss: 0.6914446353912354, ACC:0.59375\n",
      "Training iteration 206 loss: 0.7029328942298889, ACC:0.421875\n",
      "Training iteration 207 loss: 0.6871779561042786, ACC:0.578125\n",
      "Training iteration 208 loss: 0.7701736688613892, ACC:0.484375\n",
      "Training iteration 209 loss: 0.7233414649963379, ACC:0.5\n",
      "Training iteration 210 loss: 0.7331070899963379, ACC:0.53125\n",
      "Training iteration 211 loss: 0.7056480050086975, ACC:0.40625\n",
      "Training iteration 212 loss: 0.691516637802124, ACC:0.546875\n",
      "Training iteration 213 loss: 0.7679307460784912, ACC:0.390625\n",
      "Training iteration 214 loss: 0.7213109731674194, ACC:0.5625\n",
      "Training iteration 215 loss: 0.7950475811958313, ACC:0.515625\n",
      "Training iteration 216 loss: 0.6691081523895264, ACC:0.609375\n",
      "Training iteration 217 loss: 1.2191987037658691, ACC:0.390625\n",
      "Training iteration 218 loss: 0.6398524641990662, ACC:0.671875\n",
      "Training iteration 219 loss: 1.5369292497634888, ACC:0.484375\n",
      "Training iteration 220 loss: 1.2295557260513306, ACC:0.359375\n",
      "Training iteration 221 loss: 1.3328568935394287, ACC:0.5625\n",
      "Training iteration 222 loss: 1.6669025421142578, ACC:0.609375\n",
      "Training iteration 223 loss: 1.369115948677063, ACC:0.515625\n",
      "Training iteration 224 loss: 1.0968767404556274, ACC:0.453125\n",
      "Training iteration 225 loss: 1.520031213760376, ACC:0.484375\n",
      "Training iteration 226 loss: 0.7091614007949829, ACC:0.546875\n",
      "Training iteration 227 loss: 1.1399015188217163, ACC:0.515625\n",
      "Training iteration 228 loss: 1.2631570100784302, ACC:0.40625\n",
      "Training iteration 229 loss: 0.9223552346229553, ACC:0.53125\n",
      "Training iteration 230 loss: 1.409433126449585, ACC:0.484375\n",
      "Training iteration 231 loss: 0.7392218708992004, ACC:0.4375\n",
      "Training iteration 232 loss: 1.246498942375183, ACC:0.5625\n",
      "Training iteration 233 loss: 1.1803754568099976, ACC:0.609375\n",
      "Training iteration 234 loss: 0.8520854115486145, ACC:0.359375\n",
      "Training iteration 235 loss: 1.5210553407669067, ACC:0.5625\n",
      "Training iteration 236 loss: 1.6861215829849243, ACC:0.609375\n",
      "Training iteration 237 loss: 1.3959290981292725, ACC:0.46875\n",
      "Training iteration 238 loss: 1.363767385482788, ACC:0.4375\n",
      "Training iteration 239 loss: 1.3744754791259766, ACC:0.59375\n",
      "Training iteration 240 loss: 0.8636415004730225, ACC:0.59375\n",
      "Training iteration 241 loss: 0.9046721458435059, ACC:0.515625\n",
      "Training iteration 242 loss: 1.3805652856826782, ACC:0.4375\n",
      "Training iteration 243 loss: 0.6948117017745972, ACC:0.5\n",
      "Training iteration 244 loss: 1.1588281393051147, ACC:0.5\n",
      "Training iteration 245 loss: 0.9472669959068298, ACC:0.4375\n",
      "Training iteration 246 loss: 1.0807563066482544, ACC:0.484375\n",
      "Training iteration 247 loss: 1.0366051197052002, ACC:0.546875\n",
      "Training iteration 248 loss: 0.6940209865570068, ACC:0.484375\n",
      "Training iteration 249 loss: 1.024009108543396, ACC:0.5\n",
      "Training iteration 250 loss: 0.7900570631027222, ACC:0.5\n",
      "Training iteration 251 loss: 0.7797736525535583, ACC:0.578125\n",
      "Training iteration 252 loss: 0.9838255047798157, ACC:0.546875\n",
      "Training iteration 253 loss: 0.7220278382301331, ACC:0.421875\n",
      "Training iteration 254 loss: 1.276669979095459, ACC:0.453125\n",
      "Training iteration 255 loss: 0.8785869479179382, ACC:0.515625\n",
      "Training iteration 256 loss: 0.7746166586875916, ACC:0.578125\n",
      "Training iteration 257 loss: 1.076151728630066, ACC:0.5625\n",
      "Training iteration 258 loss: 0.7151343822479248, ACC:0.578125\n",
      "Training iteration 259 loss: 0.8457896113395691, ACC:0.515625\n",
      "Training iteration 260 loss: 0.7818061113357544, ACC:0.59375\n",
      "Training iteration 261 loss: 0.7032087445259094, ACC:0.453125\n",
      "Training iteration 262 loss: 0.9778400659561157, ACC:0.484375\n",
      "Training iteration 263 loss: 0.7406513690948486, ACC:0.53125\n",
      "Training iteration 264 loss: 0.7949931025505066, ACC:0.5\n",
      "Training iteration 265 loss: 0.7810676693916321, ACC:0.53125\n",
      "Training iteration 266 loss: 0.7189682722091675, ACC:0.4375\n",
      "Training iteration 267 loss: 0.7460035681724548, ACC:0.46875\n",
      "Training iteration 268 loss: 0.7004203200340271, ACC:0.515625\n",
      "Training iteration 269 loss: 0.7057902216911316, ACC:0.5625\n",
      "Training iteration 270 loss: 0.7412145733833313, ACC:0.390625\n",
      "Training iteration 271 loss: 0.9798257946968079, ACC:0.421875\n",
      "Training iteration 272 loss: 0.7284711599349976, ACC:0.40625\n",
      "Training iteration 273 loss: 1.2353720664978027, ACC:0.375\n",
      "Training iteration 274 loss: 0.6958855390548706, ACC:0.53125\n",
      "Training iteration 275 loss: 0.8397890329360962, ACC:0.53125\n",
      "Training iteration 276 loss: 0.8004118800163269, ACC:0.515625\n",
      "Training iteration 277 loss: 0.833524763584137, ACC:0.375\n",
      "Training iteration 278 loss: 0.7094775438308716, ACC:0.46875\n",
      "Training iteration 279 loss: 0.704447329044342, ACC:0.578125\n",
      "Training iteration 280 loss: 0.8242408633232117, ACC:0.484375\n",
      "Training iteration 281 loss: 0.7845816612243652, ACC:0.359375\n",
      "Training iteration 282 loss: 0.6982640027999878, ACC:0.390625\n",
      "Training iteration 283 loss: 0.8134536743164062, ACC:0.53125\n",
      "Training iteration 284 loss: 0.7921156883239746, ACC:0.4375\n",
      "Training iteration 285 loss: 0.8235266208648682, ACC:0.546875\n",
      "Training iteration 286 loss: 1.0010806322097778, ACC:0.453125\n",
      "Training iteration 287 loss: 0.7979279160499573, ACC:0.46875\n",
      "Training iteration 288 loss: 0.9404137134552002, ACC:0.46875\n",
      "Training iteration 289 loss: 0.7167668342590332, ACC:0.484375\n",
      "Training iteration 290 loss: 0.8574433326721191, ACC:0.5\n",
      "Training iteration 291 loss: 0.7065978050231934, ACC:0.390625\n",
      "Training iteration 292 loss: 1.045596718788147, ACC:0.5\n",
      "Training iteration 293 loss: 0.9311627745628357, ACC:0.40625\n",
      "Training iteration 294 loss: 1.01265549659729, ACC:0.53125\n",
      "Training iteration 295 loss: 1.0436625480651855, ACC:0.578125\n",
      "Training iteration 296 loss: 0.706024706363678, ACC:0.515625\n",
      "Training iteration 297 loss: 1.0014259815216064, ACC:0.53125\n",
      "Training iteration 298 loss: 1.0282838344573975, ACC:0.46875\n",
      "Training iteration 299 loss: 0.7917042374610901, ACC:0.53125\n",
      "Training iteration 300 loss: 1.1317596435546875, ACC:0.5\n",
      "Training iteration 301 loss: 0.7007203698158264, ACC:0.515625\n",
      "Training iteration 302 loss: 1.0210574865341187, ACC:0.484375\n",
      "Training iteration 303 loss: 0.8191847205162048, ACC:0.5\n",
      "Training iteration 304 loss: 0.8734105825424194, ACC:0.46875\n",
      "Training iteration 305 loss: 0.9328014850616455, ACC:0.421875\n",
      "Training iteration 306 loss: 0.833557665348053, ACC:0.5\n",
      "Training iteration 307 loss: 0.9045838713645935, ACC:0.515625\n",
      "Training iteration 308 loss: 0.6955971717834473, ACC:0.515625\n",
      "Training iteration 309 loss: 0.9987751245498657, ACC:0.453125\n",
      "Training iteration 310 loss: 0.6913325190544128, ACC:0.53125\n",
      "Training iteration 311 loss: 0.8084547519683838, ACC:0.515625\n",
      "Training iteration 312 loss: 0.731809675693512, ACC:0.515625\n",
      "Training iteration 313 loss: 0.6957011222839355, ACC:0.578125\n",
      "Training iteration 314 loss: 0.8848037123680115, ACC:0.5\n",
      "Training iteration 315 loss: 0.695925235748291, ACC:0.46875\n",
      "Training iteration 316 loss: 0.7736222743988037, ACC:0.53125\n",
      "Training iteration 317 loss: 0.720173716545105, ACC:0.5\n",
      "Training iteration 318 loss: 0.761660099029541, ACC:0.5\n",
      "Training iteration 319 loss: 0.7626078724861145, ACC:0.46875\n",
      "Training iteration 320 loss: 0.8122506737709045, ACC:0.421875\n",
      "Training iteration 321 loss: 0.7271148562431335, ACC:0.390625\n",
      "Training iteration 322 loss: 0.817484438419342, ACC:0.5625\n",
      "Training iteration 323 loss: 0.826580822467804, ACC:0.53125\n",
      "Training iteration 324 loss: 0.7272599935531616, ACC:0.5\n",
      "Training iteration 325 loss: 0.9045541882514954, ACC:0.46875\n",
      "Training iteration 326 loss: 0.7090810537338257, ACC:0.453125\n",
      "Training iteration 327 loss: 0.8006683588027954, ACC:0.46875\n",
      "Training iteration 328 loss: 0.6940106153488159, ACC:0.515625\n",
      "Training iteration 329 loss: 0.814521849155426, ACC:0.46875\n",
      "Training iteration 330 loss: 0.6791538000106812, ACC:0.65625\n",
      "Training iteration 331 loss: 1.1394622325897217, ACC:0.46875\n",
      "Training iteration 332 loss: 0.7410650253295898, ACC:0.53125\n",
      "Training iteration 333 loss: 0.9202022552490234, ACC:0.46875\n",
      "Training iteration 334 loss: 0.8693472146987915, ACC:0.4375\n",
      "Training iteration 335 loss: 0.8748077750205994, ACC:0.5\n",
      "Training iteration 336 loss: 0.9105405807495117, ACC:0.5\n",
      "Training iteration 337 loss: 0.7367602586746216, ACC:0.484375\n",
      "Training iteration 338 loss: 0.956911563873291, ACC:0.4375\n",
      "Training iteration 339 loss: 0.7454128861427307, ACC:0.421875\n",
      "Training iteration 340 loss: 0.7195586562156677, ACC:0.546875\n",
      "Training iteration 341 loss: 0.6976643204689026, ACC:0.484375\n",
      "Training iteration 342 loss: 0.8416786789894104, ACC:0.375\n",
      "Training iteration 343 loss: 0.7037898302078247, ACC:0.546875\n",
      "Training iteration 344 loss: 0.73142009973526, ACC:0.59375\n",
      "Training iteration 345 loss: 0.8015360832214355, ACC:0.359375\n",
      "Training iteration 346 loss: 1.1692227125167847, ACC:0.4375\n",
      "Training iteration 347 loss: 0.8547965884208679, ACC:0.46875\n",
      "Training iteration 348 loss: 0.9242109060287476, ACC:0.515625\n",
      "Training iteration 349 loss: 0.8909738063812256, ACC:0.578125\n",
      "Training iteration 350 loss: 0.6959274411201477, ACC:0.484375\n",
      "Training iteration 351 loss: 0.9712401032447815, ACC:0.515625\n",
      "Training iteration 352 loss: 0.8488564491271973, ACC:0.46875\n",
      "Training iteration 353 loss: 0.9253748059272766, ACC:0.484375\n",
      "Training iteration 354 loss: 0.9340307116508484, ACC:0.484375\n",
      "Training iteration 355 loss: 0.7009515762329102, ACC:0.578125\n",
      "Training iteration 356 loss: 1.2073003053665161, ACC:0.453125\n",
      "Training iteration 357 loss: 0.6854998469352722, ACC:0.5625\n",
      "Training iteration 358 loss: 0.8926528692245483, ACC:0.5\n",
      "Training iteration 359 loss: 0.6214682459831238, ACC:0.6875\n",
      "Training iteration 360 loss: 0.7180189490318298, ACC:0.453125\n",
      "Training iteration 361 loss: 0.7223816514015198, ACC:0.578125\n",
      "Training iteration 362 loss: 0.8325738906860352, ACC:0.5\n",
      "Training iteration 363 loss: 0.7592547535896301, ACC:0.40625\n",
      "Training iteration 364 loss: 0.6690355539321899, ACC:0.609375\n",
      "Training iteration 365 loss: 0.7243695855140686, ACC:0.484375\n",
      "Training iteration 366 loss: 0.7131273150444031, ACC:0.5\n",
      "Training iteration 367 loss: 0.7071461081504822, ACC:0.53125\n",
      "Training iteration 368 loss: 0.6927261352539062, ACC:0.515625\n",
      "Training iteration 369 loss: 0.7739014029502869, ACC:0.421875\n",
      "Training iteration 370 loss: 0.6829363107681274, ACC:0.578125\n",
      "Training iteration 371 loss: 0.8269517421722412, ACC:0.515625\n",
      "Training iteration 372 loss: 0.7002633810043335, ACC:0.4375\n",
      "Training iteration 373 loss: 0.7284071445465088, ACC:0.640625\n",
      "Training iteration 374 loss: 1.0037968158721924, ACC:0.5\n",
      "Training iteration 375 loss: 0.7651809453964233, ACC:0.40625\n",
      "Training iteration 376 loss: 0.861190915107727, ACC:0.4375\n",
      "Training iteration 377 loss: 0.706577718257904, ACC:0.546875\n",
      "Training iteration 378 loss: 0.6888116598129272, ACC:0.65625\n",
      "Training iteration 379 loss: 0.9299660325050354, ACC:0.40625\n",
      "Training iteration 380 loss: 0.8046175837516785, ACC:0.59375\n",
      "Training iteration 381 loss: 1.3649389743804932, ACC:0.4375\n",
      "Training iteration 382 loss: 0.6974334716796875, ACC:0.515625\n",
      "Training iteration 383 loss: 1.3350974321365356, ACC:0.4375\n",
      "Training iteration 384 loss: 0.7508857250213623, ACC:0.515625\n",
      "Training iteration 385 loss: 0.9520466923713684, ACC:0.53125\n",
      "Training iteration 386 loss: 1.130976676940918, ACC:0.4375\n",
      "Training iteration 387 loss: 0.7559598088264465, ACC:0.5625\n",
      "Training iteration 388 loss: 1.3956739902496338, ACC:0.4375\n",
      "Training iteration 389 loss: 0.6774824857711792, ACC:0.59375\n",
      "Training iteration 390 loss: 1.1367968320846558, ACC:0.34375\n",
      "Training iteration 391 loss: 0.692692220211029, ACC:0.53125\n",
      "Training iteration 392 loss: 1.0286853313446045, ACC:0.453125\n",
      "Training iteration 393 loss: 0.6831016540527344, ACC:0.578125\n",
      "Training iteration 394 loss: 0.7462384104728699, ACC:0.53125\n",
      "Training iteration 395 loss: 0.7381422519683838, ACC:0.5\n",
      "Training iteration 396 loss: 0.8136610984802246, ACC:0.359375\n",
      "Training iteration 397 loss: 0.7068101763725281, ACC:0.484375\n",
      "Training iteration 398 loss: 0.7054255604743958, ACC:0.53125\n",
      "Training iteration 399 loss: 0.6937756538391113, ACC:0.5\n",
      "Training iteration 400 loss: 0.6945613622665405, ACC:0.546875\n",
      "Training iteration 401 loss: 0.6912024617195129, ACC:0.5625\n",
      "Training iteration 402 loss: 0.6862145066261292, ACC:0.5625\n",
      "Training iteration 403 loss: 0.6938620209693909, ACC:0.484375\n",
      "Training iteration 404 loss: 0.6892274618148804, ACC:0.546875\n",
      "Training iteration 405 loss: 0.7006629109382629, ACC:0.53125\n",
      "Training iteration 406 loss: 0.6915666460990906, ACC:0.53125\n",
      "Training iteration 407 loss: 0.7100969552993774, ACC:0.390625\n",
      "Training iteration 408 loss: 0.7376134991645813, ACC:0.515625\n",
      "Training iteration 409 loss: 0.7416480183601379, ACC:0.421875\n",
      "Training iteration 410 loss: 0.8298717141151428, ACC:0.5\n",
      "Training iteration 411 loss: 0.6972623467445374, ACC:0.578125\n",
      "Training iteration 412 loss: 0.6894340515136719, ACC:0.546875\n",
      "Training iteration 413 loss: 0.7336019277572632, ACC:0.578125\n",
      "Training iteration 414 loss: 0.7276102304458618, ACC:0.515625\n",
      "Training iteration 415 loss: 0.7872267365455627, ACC:0.4375\n",
      "Training iteration 416 loss: 0.708450436592102, ACC:0.46875\n",
      "Training iteration 417 loss: 0.7651825547218323, ACC:0.5\n",
      "Training iteration 418 loss: 0.745258092880249, ACC:0.421875\n",
      "Training iteration 419 loss: 0.8900744915008545, ACC:0.46875\n",
      "Training iteration 420 loss: 0.7445484399795532, ACC:0.46875\n",
      "Training iteration 421 loss: 0.7987909913063049, ACC:0.546875\n",
      "Training iteration 422 loss: 0.9153871536254883, ACC:0.453125\n",
      "Training iteration 423 loss: 0.8225318193435669, ACC:0.484375\n",
      "Training iteration 424 loss: 0.8726376891136169, ACC:0.5\n",
      "Training iteration 425 loss: 0.7107717394828796, ACC:0.515625\n",
      "Training iteration 426 loss: 0.8716756701469421, ACC:0.515625\n",
      "Training iteration 427 loss: 0.7091391682624817, ACC:0.4375\n",
      "Training iteration 428 loss: 0.8541890978813171, ACC:0.578125\n",
      "Training iteration 429 loss: 0.9921112656593323, ACC:0.453125\n",
      "Training iteration 430 loss: 0.7848466634750366, ACC:0.5625\n",
      "Training iteration 431 loss: 1.2017312049865723, ACC:0.484375\n",
      "Training iteration 432 loss: 0.6994041800498962, ACC:0.453125\n",
      "Training iteration 433 loss: 1.2435340881347656, ACC:0.484375\n",
      "Training iteration 434 loss: 0.939468264579773, ACC:0.484375\n",
      "Training iteration 435 loss: 0.8442693948745728, ACC:0.578125\n",
      "Training iteration 436 loss: 1.280082106590271, ACC:0.515625\n",
      "Training iteration 437 loss: 0.7082778215408325, ACC:0.53125\n",
      "Training iteration 438 loss: 1.1841049194335938, ACC:0.453125\n",
      "Training iteration 439 loss: 0.8595099449157715, ACC:0.5\n",
      "Training iteration 440 loss: 0.8727863430976868, ACC:0.515625\n",
      "Training iteration 441 loss: 0.9568290710449219, ACC:0.53125\n",
      "Training iteration 442 loss: 0.6779004335403442, ACC:0.625\n",
      "Training iteration 443 loss: 1.4735616445541382, ACC:0.421875\n",
      "Training iteration 444 loss: 0.8957280516624451, ACC:0.421875\n",
      "Training iteration 445 loss: 1.1656790971755981, ACC:0.546875\n",
      "Training iteration 446 loss: 1.1394054889678955, ACC:0.625\n",
      "Training iteration 447 loss: 0.9392066597938538, ACC:0.421875\n",
      "Training iteration 448 loss: 1.3869655132293701, ACC:0.515625\n",
      "Training iteration 449 loss: 1.6974728107452393, ACC:0.5\n",
      "Training iteration 450 loss: 0.7507714033126831, ACC:0.5\n",
      "Validation iteration 451 loss: 1.5099200010299683, ACC: 0.484375\n",
      "Validation iteration 452 loss: 1.3779406547546387, ACC: 0.53125\n",
      "Validation iteration 453 loss: 1.4659268856048584, ACC: 0.5\n",
      "Validation iteration 454 loss: 1.4219337701797485, ACC: 0.515625\n",
      "Validation iteration 455 loss: 1.5539131164550781, ACC: 0.46875\n",
      "Validation iteration 456 loss: 1.4219337701797485, ACC: 0.515625\n",
      "Validation iteration 457 loss: 1.3339475393295288, ACC: 0.546875\n",
      "Validation iteration 458 loss: 1.4219337701797485, ACC: 0.515625\n",
      "Validation iteration 459 loss: 1.5539131164550781, ACC: 0.46875\n",
      "Validation iteration 460 loss: 1.6418993473052979, ACC: 0.4375\n",
      "Validation iteration 461 loss: 1.5539131164550781, ACC: 0.46875\n",
      "Validation iteration 462 loss: 1.2899545431137085, ACC: 0.5625\n",
      "Validation iteration 463 loss: 1.6858924627304077, ACC: 0.421875\n",
      "Validation iteration 464 loss: 1.2899545431137085, ACC: 0.5625\n",
      "Validation iteration 465 loss: 1.3339475393295288, ACC: 0.546875\n",
      "Validation iteration 466 loss: 1.6858924627304077, ACC: 0.421875\n",
      "Validation iteration 467 loss: 1.4219337701797485, ACC: 0.515625\n",
      "Validation iteration 468 loss: 1.4659268856048584, ACC: 0.5\n",
      "Validation iteration 469 loss: 1.5539131164550781, ACC: 0.46875\n",
      "Validation iteration 470 loss: 1.6858924627304077, ACC: 0.421875\n",
      "Validation iteration 471 loss: 1.4219337701797485, ACC: 0.515625\n",
      "Validation iteration 472 loss: 1.3779406547546387, ACC: 0.53125\n",
      "Validation iteration 473 loss: 1.157975196838379, ACC: 0.609375\n",
      "Validation iteration 474 loss: 1.5099200010299683, ACC: 0.484375\n",
      "Validation iteration 475 loss: 1.4219337701797485, ACC: 0.515625\n",
      "Validation iteration 476 loss: 1.5099200010299683, ACC: 0.484375\n",
      "Validation iteration 477 loss: 1.7298855781555176, ACC: 0.40625\n",
      "Validation iteration 478 loss: 1.4659268856048584, ACC: 0.5\n",
      "Validation iteration 479 loss: 1.4659268856048584, ACC: 0.5\n",
      "Validation iteration 480 loss: 1.4219337701797485, ACC: 0.515625\n",
      "Validation iteration 481 loss: 1.3779406547546387, ACC: 0.53125\n",
      "Validation iteration 482 loss: 1.5099200010299683, ACC: 0.484375\n",
      "Validation iteration 483 loss: 1.245961308479309, ACC: 0.578125\n",
      "Validation iteration 484 loss: 1.7298855781555176, ACC: 0.40625\n",
      "Validation iteration 485 loss: 1.6858924627304077, ACC: 0.421875\n",
      "Validation iteration 486 loss: 1.3339475393295288, ACC: 0.546875\n",
      "Validation iteration 487 loss: 1.2019681930541992, ACC: 0.59375\n",
      "Validation iteration 488 loss: 1.5539131164550781, ACC: 0.46875\n",
      "Validation iteration 489 loss: 1.5099200010299683, ACC: 0.484375\n",
      "Validation iteration 490 loss: 1.6418993473052979, ACC: 0.4375\n",
      "Validation iteration 491 loss: 1.4659268856048584, ACC: 0.5\n",
      "Validation iteration 492 loss: 1.289954423904419, ACC: 0.5625\n",
      "Validation iteration 493 loss: 1.7738786935806274, ACC: 0.390625\n",
      "Validation iteration 494 loss: 1.8618649244308472, ACC: 0.359375\n",
      "Validation iteration 495 loss: 1.7298855781555176, ACC: 0.40625\n",
      "Validation iteration 496 loss: 1.8618649244308472, ACC: 0.359375\n",
      "Validation iteration 497 loss: 1.5099200010299683, ACC: 0.484375\n",
      "Validation iteration 498 loss: 1.157975196838379, ACC: 0.609375\n",
      "Validation iteration 499 loss: 1.5539131164550781, ACC: 0.46875\n",
      "Validation iteration 500 loss: 1.7738786935806274, ACC: 0.390625\n",
      "-- Epoch 1 done -- Train loss: 0.8756989591651493, train ACC: 0.4987847222222222, val loss: 1.4984818005561829, val ACC: 0.4884375\n",
      "<--- 7233.349992752075 seconds --->\n",
      "Training iteration 1 loss: 1.289954423904419, ACC:0.5625\n",
      "Training iteration 2 loss: 1.8234237432479858, ACC:0.46875\n",
      "Training iteration 3 loss: 0.6309711933135986, ACC:0.6875\n",
      "Training iteration 4 loss: 0.9995687007904053, ACC:0.5\n",
      "Training iteration 5 loss: 0.9036245942115784, ACC:0.484375\n",
      "Training iteration 6 loss: 0.6797563433647156, ACC:0.640625\n",
      "Training iteration 7 loss: 1.2425605058670044, ACC:0.53125\n",
      "Training iteration 8 loss: 0.8868901133537292, ACC:0.453125\n",
      "Training iteration 9 loss: 1.0859966278076172, ACC:0.546875\n",
      "Training iteration 10 loss: 1.7184486389160156, ACC:0.390625\n",
      "Training iteration 11 loss: 0.7340249419212341, ACC:0.46875\n",
      "Training iteration 12 loss: 1.2293622493743896, ACC:0.515625\n",
      "Training iteration 13 loss: 1.0430773496627808, ACC:0.40625\n",
      "Training iteration 14 loss: 1.075636386871338, ACC:0.546875\n",
      "Training iteration 15 loss: 1.6206073760986328, ACC:0.46875\n",
      "Training iteration 16 loss: 0.7375839948654175, ACC:0.46875\n",
      "Training iteration 17 loss: 1.176466941833496, ACC:0.59375\n",
      "Training iteration 18 loss: 1.6521209478378296, ACC:0.515625\n",
      "Training iteration 19 loss: 0.8009582757949829, ACC:0.484375\n",
      "Training iteration 20 loss: 1.3768423795700073, ACC:0.53125\n",
      "Training iteration 21 loss: 1.7833998203277588, ACC:0.484375\n",
      "Training iteration 22 loss: 0.7453749179840088, ACC:0.515625\n",
      "Training iteration 23 loss: 1.6833447217941284, ACC:0.40625\n",
      "Training iteration 24 loss: 1.393513798713684, ACC:0.4375\n",
      "Training iteration 25 loss: 0.8005037903785706, ACC:0.546875\n",
      "Training iteration 26 loss: 1.7649260759353638, ACC:0.40625\n",
      "Training iteration 27 loss: 0.7585143446922302, ACC:0.53125\n",
      "Training iteration 28 loss: 1.3502198457717896, ACC:0.40625\n",
      "Training iteration 29 loss: 0.8765361905097961, ACC:0.5625\n",
      "Training iteration 30 loss: 0.7587149143218994, ACC:0.46875\n",
      "Training iteration 31 loss: 1.010282278060913, ACC:0.4375\n",
      "Training iteration 32 loss: 0.7104058265686035, ACC:0.46875\n",
      "Training iteration 33 loss: 0.9178898334503174, ACC:0.4375\n",
      "Training iteration 34 loss: 0.7279580235481262, ACC:0.3125\n",
      "Training iteration 35 loss: 0.6985728740692139, ACC:0.484375\n",
      "Training iteration 36 loss: 0.7026257514953613, ACC:0.40625\n",
      "Training iteration 37 loss: 0.7012307047843933, ACC:0.578125\n",
      "Training iteration 38 loss: 0.8431865572929382, ACC:0.421875\n",
      "Training iteration 39 loss: 0.6355463266372681, ACC:0.671875\n",
      "Training iteration 40 loss: 1.2368170022964478, ACC:0.484375\n",
      "Training iteration 41 loss: 0.767189621925354, ACC:0.46875\n",
      "Training iteration 42 loss: 1.2382848262786865, ACC:0.4375\n",
      "Training iteration 43 loss: 0.846114456653595, ACC:0.5625\n",
      "Training iteration 44 loss: 0.7519707083702087, ACC:0.484375\n",
      "Training iteration 45 loss: 0.9668573141098022, ACC:0.46875\n",
      "Training iteration 46 loss: 0.6939529776573181, ACC:0.5\n",
      "Training iteration 47 loss: 0.8680687546730042, ACC:0.515625\n",
      "Training iteration 48 loss: 0.7116033434867859, ACC:0.546875\n",
      "Training iteration 49 loss: 0.7466070055961609, ACC:0.5\n",
      "Training iteration 50 loss: 0.7295255064964294, ACC:0.546875\n",
      "Training iteration 51 loss: 0.695327639579773, ACC:0.421875\n",
      "Training iteration 52 loss: 0.7064095139503479, ACC:0.453125\n",
      "Training iteration 53 loss: 0.7075421810150146, ACC:0.484375\n",
      "Training iteration 54 loss: 0.6912292242050171, ACC:0.53125\n",
      "Training iteration 55 loss: 0.6931668519973755, ACC:0.5\n",
      "Training iteration 56 loss: 0.6932498216629028, ACC:0.515625\n",
      "Training iteration 57 loss: 0.689240038394928, ACC:0.546875\n",
      "Training iteration 58 loss: 0.6993457078933716, ACC:0.5\n",
      "Training iteration 59 loss: 0.6944271326065063, ACC:0.484375\n",
      "Training iteration 60 loss: 0.6894835233688354, ACC:0.546875\n",
      "Training iteration 61 loss: 0.7079225778579712, ACC:0.484375\n",
      "Training iteration 62 loss: 0.691198468208313, ACC:0.53125\n",
      "Training iteration 63 loss: 0.7238972187042236, ACC:0.5\n",
      "Training iteration 64 loss: 0.6953242421150208, ACC:0.359375\n",
      "Training iteration 65 loss: 0.8721229434013367, ACC:0.53125\n",
      "Training iteration 66 loss: 0.8881526589393616, ACC:0.375\n",
      "Training iteration 67 loss: 0.995637834072113, ACC:0.53125\n",
      "Training iteration 68 loss: 1.2234108448028564, ACC:0.453125\n",
      "Training iteration 69 loss: 0.775550365447998, ACC:0.453125\n",
      "Training iteration 70 loss: 1.2769752740859985, ACC:0.34375\n",
      "Training iteration 71 loss: 0.7847517132759094, ACC:0.453125\n",
      "Training iteration 72 loss: 1.0160378217697144, ACC:0.4375\n",
      "Training iteration 73 loss: 0.6979624629020691, ACC:0.53125\n",
      "Training iteration 74 loss: 0.9040213823318481, ACC:0.546875\n",
      "Training iteration 75 loss: 0.7479792237281799, ACC:0.53125\n",
      "Training iteration 76 loss: 0.7873407602310181, ACC:0.515625\n",
      "Training iteration 77 loss: 0.8014696836471558, ACC:0.546875\n",
      "Training iteration 78 loss: 0.6966509819030762, ACC:0.453125\n",
      "Training iteration 79 loss: 0.7522344589233398, ACC:0.515625\n",
      "Training iteration 80 loss: 0.7123861312866211, ACC:0.46875\n",
      "Training iteration 81 loss: 0.8012994527816772, ACC:0.46875\n",
      "Training iteration 82 loss: 0.7057514190673828, ACC:0.5\n",
      "Training iteration 83 loss: 0.8090646266937256, ACC:0.40625\n",
      "Training iteration 84 loss: 0.6889902353286743, ACC:0.546875\n",
      "Training iteration 85 loss: 0.7857398986816406, ACC:0.515625\n",
      "Training iteration 86 loss: 0.6997551321983337, ACC:0.515625\n",
      "Training iteration 87 loss: 0.759069561958313, ACC:0.484375\n",
      "Training iteration 88 loss: 0.7490416169166565, ACC:0.390625\n",
      "Training iteration 89 loss: 0.992968738079071, ACC:0.40625\n",
      "Training iteration 90 loss: 0.7128463387489319, ACC:0.4375\n",
      "Training iteration 91 loss: 0.9944749474525452, ACC:0.46875\n",
      "Training iteration 92 loss: 0.78753262758255, ACC:0.453125\n",
      "Training iteration 93 loss: 1.046528935432434, ACC:0.421875\n",
      "Training iteration 94 loss: 0.7701311707496643, ACC:0.5\n",
      "Training iteration 95 loss: 0.9068894386291504, ACC:0.421875\n",
      "Training iteration 96 loss: 0.7259126901626587, ACC:0.515625\n",
      "Training iteration 97 loss: 0.7284638285636902, ACC:0.53125\n",
      "Training iteration 98 loss: 0.7689212560653687, ACC:0.53125\n",
      "Training iteration 99 loss: 0.7045854926109314, ACC:0.375\n",
      "Training iteration 100 loss: 0.6942956447601318, ACC:0.453125\n",
      "Training iteration 101 loss: 0.7475855946540833, ACC:0.40625\n",
      "Training iteration 102 loss: 0.7256333827972412, ACC:0.5\n",
      "Training iteration 103 loss: 0.7291655540466309, ACC:0.484375\n",
      "Training iteration 104 loss: 0.7523412108421326, ACC:0.421875\n",
      "Training iteration 105 loss: 0.6940469145774841, ACC:0.453125\n",
      "Training iteration 106 loss: 0.7440017461776733, ACC:0.515625\n",
      "Training iteration 107 loss: 0.6859877109527588, ACC:0.5625\n",
      "Training iteration 108 loss: 0.6934982538223267, ACC:0.515625\n",
      "Training iteration 109 loss: 0.7093780636787415, ACC:0.515625\n",
      "Training iteration 110 loss: 0.7000120282173157, ACC:0.421875\n",
      "Training iteration 111 loss: 0.7206668257713318, ACC:0.578125\n",
      "Training iteration 112 loss: 0.7736630439758301, ACC:0.515625\n",
      "Training iteration 113 loss: 0.7428049445152283, ACC:0.4375\n",
      "Training iteration 114 loss: 0.7059749364852905, ACC:0.53125\n",
      "Training iteration 115 loss: 0.6975566744804382, ACC:0.4375\n",
      "Training iteration 116 loss: 0.6925141215324402, ACC:0.546875\n",
      "Training iteration 117 loss: 0.7060146331787109, ACC:0.484375\n",
      "Training iteration 118 loss: 0.6926587820053101, ACC:0.515625\n",
      "Training iteration 119 loss: 0.686693012714386, ACC:0.5625\n",
      "Training iteration 120 loss: 0.7526896595954895, ACC:0.421875\n",
      "Training iteration 121 loss: 0.7626743912696838, ACC:0.5\n",
      "Training iteration 122 loss: 0.7385030388832092, ACC:0.5\n",
      "Training iteration 123 loss: 0.7611673474311829, ACC:0.4375\n",
      "Training iteration 124 loss: 0.6853228211402893, ACC:0.5625\n",
      "Training iteration 125 loss: 0.6932252645492554, ACC:0.46875\n",
      "Training iteration 126 loss: 0.6931509971618652, ACC:0.5\n",
      "Training iteration 127 loss: 0.6931659579277039, ACC:0.484375\n",
      "Training iteration 128 loss: 0.6912596225738525, ACC:0.53125\n",
      "Training iteration 129 loss: 0.7110099792480469, ACC:0.453125\n",
      "Training iteration 130 loss: 0.7158903479576111, ACC:0.484375\n",
      "Training iteration 131 loss: 0.7109771370887756, ACC:0.421875\n",
      "Training iteration 132 loss: 0.765414297580719, ACC:0.515625\n",
      "Training iteration 133 loss: 0.7390674352645874, ACC:0.46875\n",
      "Training iteration 134 loss: 0.6622903943061829, ACC:0.640625\n",
      "Training iteration 135 loss: 1.0026888847351074, ACC:0.5\n",
      "Training iteration 136 loss: 0.6943637728691101, ACC:0.453125\n",
      "Training iteration 137 loss: 0.9721053242683411, ACC:0.546875\n",
      "Training iteration 138 loss: 0.8722241520881653, ACC:0.515625\n",
      "Training iteration 139 loss: 0.8177379965782166, ACC:0.5\n",
      "Training iteration 140 loss: 0.995604395866394, ACC:0.46875\n",
      "Training iteration 141 loss: 0.7736292481422424, ACC:0.390625\n",
      "Training iteration 142 loss: 0.740696132183075, ACC:0.515625\n",
      "Training iteration 143 loss: 0.676734209060669, ACC:0.625\n",
      "Training iteration 144 loss: 0.7898469567298889, ACC:0.609375\n",
      "Training iteration 145 loss: 1.066794514656067, ACC:0.359375\n",
      "Training iteration 146 loss: 1.1538058519363403, ACC:0.453125\n",
      "Training iteration 147 loss: 1.091705322265625, ACC:0.484375\n",
      "Training iteration 148 loss: 0.7538359761238098, ACC:0.515625\n",
      "Training iteration 149 loss: 1.2331123352050781, ACC:0.4375\n",
      "Training iteration 150 loss: 0.6929462552070618, ACC:0.515625\n",
      "Training iteration 151 loss: 1.105263590812683, ACC:0.4375\n",
      "Training iteration 152 loss: 0.7005142569541931, ACC:0.53125\n",
      "Training iteration 153 loss: 0.9728063344955444, ACC:0.40625\n",
      "Training iteration 154 loss: 0.6984109878540039, ACC:0.484375\n",
      "Training iteration 155 loss: 0.7605692744255066, ACC:0.578125\n",
      "Training iteration 156 loss: 0.8154276609420776, ACC:0.515625\n",
      "Training iteration 157 loss: 0.7063500285148621, ACC:0.546875\n",
      "Training iteration 158 loss: 0.9502159357070923, ACC:0.484375\n",
      "Training iteration 159 loss: 0.6933531165122986, ACC:0.484375\n",
      "Training iteration 160 loss: 0.802331268787384, ACC:0.546875\n",
      "Training iteration 161 loss: 0.7244299054145813, ACC:0.546875\n",
      "Training iteration 162 loss: 0.6444982886314392, ACC:0.65625\n",
      "Training iteration 163 loss: 1.1902806758880615, ACC:0.4375\n",
      "Training iteration 164 loss: 0.7001744508743286, ACC:0.4375\n",
      "Training iteration 165 loss: 1.1276472806930542, ACC:0.515625\n",
      "Training iteration 166 loss: 0.9218732714653015, ACC:0.515625\n",
      "Training iteration 167 loss: 0.8066946268081665, ACC:0.53125\n",
      "Training iteration 168 loss: 1.1305944919586182, ACC:0.484375\n",
      "Training iteration 169 loss: 0.6932770609855652, ACC:0.46875\n",
      "Training iteration 170 loss: 1.0309022665023804, ACC:0.453125\n",
      "Training iteration 171 loss: 0.7044017314910889, ACC:0.515625\n",
      "Training iteration 172 loss: 0.8793331384658813, ACC:0.484375\n",
      "Training iteration 173 loss: 0.6846718192100525, ACC:0.59375\n",
      "Training iteration 174 loss: 0.7146486043930054, ACC:0.453125\n",
      "Training iteration 175 loss: 0.7011094093322754, ACC:0.515625\n",
      "Training iteration 176 loss: 0.6926660537719727, ACC:0.515625\n",
      "Training iteration 177 loss: 0.682019829750061, ACC:0.578125\n",
      "Training iteration 178 loss: 0.7141757607460022, ACC:0.53125\n",
      "Training iteration 179 loss: 0.6937793493270874, ACC:0.484375\n",
      "Training iteration 180 loss: 0.7193658947944641, ACC:0.484375\n",
      "Training iteration 181 loss: 0.6890273094177246, ACC:0.578125\n",
      "Training iteration 182 loss: 0.7366124391555786, ACC:0.5625\n",
      "Training iteration 183 loss: 0.7429530620574951, ACC:0.5\n",
      "Training iteration 184 loss: 0.7705579400062561, ACC:0.46875\n",
      "Training iteration 185 loss: 0.7177438735961914, ACC:0.515625\n",
      "Training iteration 186 loss: 0.7002777457237244, ACC:0.53125\n",
      "Training iteration 187 loss: 0.7979968786239624, ACC:0.453125\n",
      "Training iteration 188 loss: 0.7317560315132141, ACC:0.484375\n",
      "Training iteration 189 loss: 0.6749767661094666, ACC:0.609375\n",
      "Training iteration 190 loss: 0.7412044405937195, ACC:0.453125\n",
      "Training iteration 191 loss: 0.7659834027290344, ACC:0.515625\n",
      "Training iteration 192 loss: 0.7138497233390808, ACC:0.5625\n",
      "Training iteration 193 loss: 0.6842887997627258, ACC:0.578125\n",
      "Training iteration 194 loss: 0.8922176957130432, ACC:0.484375\n",
      "Training iteration 195 loss: 0.7064672708511353, ACC:0.453125\n",
      "Training iteration 196 loss: 0.9232325553894043, ACC:0.5\n",
      "Training iteration 197 loss: 0.8153262138366699, ACC:0.421875\n",
      "Training iteration 198 loss: 1.0502647161483765, ACC:0.46875\n",
      "Training iteration 199 loss: 1.0160157680511475, ACC:0.375\n",
      "Training iteration 200 loss: 0.9799193739891052, ACC:0.5625\n",
      "Training iteration 201 loss: 1.4091695547103882, ACC:0.484375\n",
      "Training iteration 202 loss: 0.7050262093544006, ACC:0.390625\n",
      "Training iteration 203 loss: 1.1990803480148315, ACC:0.625\n",
      "Training iteration 204 loss: 1.7888925075531006, ACC:0.515625\n",
      "Training iteration 205 loss: 0.8529955744743347, ACC:0.453125\n",
      "Training iteration 206 loss: 1.7421594858169556, ACC:0.484375\n",
      "Training iteration 207 loss: 1.582714319229126, ACC:0.59375\n",
      "Training iteration 208 loss: 0.9042543172836304, ACC:0.5625\n",
      "Training iteration 209 loss: 1.0678430795669556, ACC:0.5625\n",
      "Training iteration 210 loss: 1.9789254665374756, ACC:0.421875\n",
      "Training iteration 211 loss: 0.7498380541801453, ACC:0.5\n",
      "Training iteration 212 loss: 1.5296189785003662, ACC:0.484375\n",
      "Training iteration 213 loss: 1.4962470531463623, ACC:0.5\n",
      "Training iteration 214 loss: 0.6890357732772827, ACC:0.546875\n",
      "Training iteration 215 loss: 1.3858891725540161, ACC:0.546875\n",
      "Training iteration 216 loss: 1.6792434453964233, ACC:0.40625\n",
      "Training iteration 217 loss: 0.8689586520195007, ACC:0.46875\n",
      "Training iteration 218 loss: 1.4221800565719604, ACC:0.484375\n",
      "Training iteration 219 loss: 0.8811039328575134, ACC:0.421875\n",
      "Training iteration 220 loss: 1.519233226776123, ACC:0.4375\n",
      "Training iteration 221 loss: 1.3366280794143677, ACC:0.484375\n",
      "Training iteration 222 loss: 0.7410907745361328, ACC:0.5\n",
      "Training iteration 223 loss: 1.1577520370483398, ACC:0.546875\n",
      "Training iteration 224 loss: 1.0470757484436035, ACC:0.421875\n",
      "Training iteration 225 loss: 1.272039532661438, ACC:0.421875\n",
      "Training iteration 226 loss: 0.9980601072311401, ACC:0.5625\n",
      "Training iteration 227 loss: 0.6892343163490295, ACC:0.546875\n",
      "Training iteration 228 loss: 1.3120667934417725, ACC:0.4375\n",
      "Training iteration 229 loss: 0.7321562767028809, ACC:0.5625\n",
      "Training iteration 230 loss: 0.8293247818946838, ACC:0.53125\n",
      "Training iteration 231 loss: 1.0665574073791504, ACC:0.421875\n",
      "Training iteration 232 loss: 0.7620744109153748, ACC:0.53125\n",
      "Training iteration 233 loss: 1.2450597286224365, ACC:0.40625\n",
      "Training iteration 234 loss: 0.7353096604347229, ACC:0.421875\n",
      "Training iteration 235 loss: 0.9679188132286072, ACC:0.421875\n",
      "Training iteration 236 loss: 0.7163785099983215, ACC:0.46875\n",
      "Training iteration 237 loss: 0.8086422681808472, ACC:0.5\n",
      "Training iteration 238 loss: 0.695149838924408, ACC:0.46875\n",
      "Training iteration 239 loss: 0.7310473918914795, ACC:0.59375\n",
      "Training iteration 240 loss: 0.8823803067207336, ACC:0.453125\n",
      "Training iteration 241 loss: 0.8014580011367798, ACC:0.484375\n",
      "Training iteration 242 loss: 0.7613102197647095, ACC:0.578125\n",
      "Training iteration 243 loss: 0.6971914768218994, ACC:0.5\n",
      "Training iteration 244 loss: 0.8444775938987732, ACC:0.46875\n",
      "Training iteration 245 loss: 0.7237098813056946, ACC:0.4375\n",
      "Training iteration 246 loss: 1.01261305809021, ACC:0.390625\n",
      "Training iteration 247 loss: 0.6926640868186951, ACC:0.515625\n",
      "Training iteration 248 loss: 0.8776257038116455, ACC:0.4375\n",
      "Training iteration 249 loss: 0.6931686401367188, ACC:0.5\n",
      "Training iteration 250 loss: 0.707906186580658, ACC:0.59375\n",
      "Training iteration 251 loss: 0.9094589352607727, ACC:0.375\n",
      "Training iteration 252 loss: 0.9908322691917419, ACC:0.4375\n",
      "Training iteration 253 loss: 0.8291662335395813, ACC:0.484375\n",
      "Training iteration 254 loss: 0.821494460105896, ACC:0.484375\n",
      "Training iteration 255 loss: 0.9687858819961548, ACC:0.390625\n",
      "Training iteration 256 loss: 0.9284823536872864, ACC:0.4375\n",
      "Training iteration 257 loss: 0.8608539700508118, ACC:0.46875\n",
      "Training iteration 258 loss: 0.812312662601471, ACC:0.46875\n",
      "Training iteration 259 loss: 0.7880479097366333, ACC:0.53125\n",
      "Training iteration 260 loss: 0.6729008555412292, ACC:0.625\n",
      "Training iteration 261 loss: 1.057294249534607, ACC:0.5\n",
      "Training iteration 262 loss: 0.7813273072242737, ACC:0.5\n",
      "Training iteration 263 loss: 0.9501605033874512, ACC:0.46875\n",
      "Training iteration 264 loss: 0.7439813613891602, ACC:0.59375\n",
      "Training iteration 265 loss: 0.6890400648117065, ACC:0.546875\n",
      "Training iteration 266 loss: 0.8090749979019165, ACC:0.578125\n",
      "Training iteration 267 loss: 0.8674244284629822, ACC:0.453125\n",
      "Training iteration 268 loss: 0.7505576610565186, ACC:0.59375\n",
      "Training iteration 269 loss: 1.1074044704437256, ACC:0.515625\n",
      "Training iteration 270 loss: 0.7103027701377869, ACC:0.484375\n",
      "Training iteration 271 loss: 1.0746990442276, ACC:0.5\n",
      "Training iteration 272 loss: 0.8683125376701355, ACC:0.53125\n",
      "Training iteration 273 loss: 0.7148723602294922, ACC:0.578125\n",
      "Training iteration 274 loss: 1.1383684873580933, ACC:0.5\n",
      "Training iteration 275 loss: 0.7345533967018127, ACC:0.484375\n",
      "Training iteration 276 loss: 1.0234942436218262, ACC:0.515625\n",
      "Training iteration 277 loss: 1.1047080755233765, ACC:0.421875\n",
      "Training iteration 278 loss: 1.022633671760559, ACC:0.40625\n",
      "Training iteration 279 loss: 0.9159974455833435, ACC:0.5\n",
      "Training iteration 280 loss: 0.7319127321243286, ACC:0.5\n",
      "Training iteration 281 loss: 1.0522884130477905, ACC:0.40625\n",
      "Training iteration 282 loss: 0.7144570350646973, ACC:0.515625\n",
      "Training iteration 283 loss: 1.0664494037628174, ACC:0.421875\n",
      "Training iteration 284 loss: 0.6959971785545349, ACC:0.515625\n",
      "Training iteration 285 loss: 0.9530355334281921, ACC:0.5\n",
      "Training iteration 286 loss: 0.7367609739303589, ACC:0.484375\n",
      "Training iteration 287 loss: 0.7102756500244141, ACC:0.640625\n",
      "Training iteration 288 loss: 1.155877709388733, ACC:0.484375\n",
      "Training iteration 289 loss: 0.6932742595672607, ACC:0.5\n",
      "Training iteration 290 loss: 0.9980173707008362, ACC:0.53125\n",
      "Training iteration 291 loss: 0.8865408301353455, ACC:0.484375\n",
      "Training iteration 292 loss: 0.8318823575973511, ACC:0.53125\n",
      "Training iteration 293 loss: 1.1703680753707886, ACC:0.4375\n",
      "Training iteration 294 loss: 0.7718496322631836, ACC:0.390625\n",
      "Training iteration 295 loss: 0.8037756681442261, ACC:0.515625\n",
      "Training iteration 296 loss: 0.6888899207115173, ACC:0.578125\n",
      "Training iteration 297 loss: 0.6999477744102478, ACC:0.53125\n",
      "Training iteration 298 loss: 0.6641734838485718, ACC:0.625\n",
      "Training iteration 299 loss: 0.697777271270752, ACC:0.5625\n",
      "Training iteration 300 loss: 0.6930772066116333, ACC:0.53125\n",
      "Training iteration 301 loss: 0.716264545917511, ACC:0.4375\n",
      "Training iteration 302 loss: 0.6890829801559448, ACC:0.546875\n",
      "Training iteration 303 loss: 0.7418383359909058, ACC:0.484375\n",
      "Training iteration 304 loss: 0.6926763653755188, ACC:0.515625\n",
      "Training iteration 305 loss: 0.7424678802490234, ACC:0.5\n",
      "Training iteration 306 loss: 0.6996769905090332, ACC:0.453125\n",
      "Training iteration 307 loss: 0.7562196254730225, ACC:0.53125\n",
      "Training iteration 308 loss: 0.6893301606178284, ACC:0.578125\n",
      "Training iteration 309 loss: 0.6928040385246277, ACC:0.515625\n",
      "Training iteration 310 loss: 0.8057817220687866, ACC:0.40625\n",
      "Training iteration 311 loss: 0.7799488306045532, ACC:0.40625\n",
      "Training iteration 312 loss: 0.698491096496582, ACC:0.453125\n",
      "Training iteration 313 loss: 0.8046401143074036, ACC:0.453125\n",
      "Training iteration 314 loss: 0.6933380961418152, ACC:0.390625\n",
      "Training iteration 315 loss: 0.6962900757789612, ACC:0.40625\n",
      "Training iteration 316 loss: 0.809444785118103, ACC:0.390625\n",
      "Training iteration 317 loss: 0.755195140838623, ACC:0.453125\n",
      "Training iteration 318 loss: 0.6934189796447754, ACC:0.546875\n",
      "Training iteration 319 loss: 0.6927199363708496, ACC:0.515625\n",
      "Training iteration 320 loss: 0.6837363839149475, ACC:0.578125\n",
      "Training iteration 321 loss: 0.7853454351425171, ACC:0.40625\n",
      "Training iteration 322 loss: 0.8230781555175781, ACC:0.46875\n",
      "Training iteration 323 loss: 0.762799084186554, ACC:0.453125\n",
      "Training iteration 324 loss: 0.9361297488212585, ACC:0.375\n",
      "Training iteration 325 loss: 0.6938692331314087, ACC:0.4375\n",
      "Training iteration 326 loss: 0.9163379669189453, ACC:0.484375\n",
      "Training iteration 327 loss: 0.781632125377655, ACC:0.359375\n",
      "Training iteration 328 loss: 1.2977837324142456, ACC:0.421875\n",
      "Training iteration 329 loss: 0.7756274342536926, ACC:0.5625\n",
      "Training iteration 330 loss: 0.9327790141105652, ACC:0.375\n",
      "Training iteration 331 loss: 0.796370804309845, ACC:0.359375\n",
      "Training iteration 332 loss: 0.9145523309707642, ACC:0.5625\n",
      "Training iteration 333 loss: 0.9642231464385986, ACC:0.546875\n",
      "Training iteration 334 loss: 0.7148026823997498, ACC:0.4375\n",
      "Training iteration 335 loss: 0.8349555134773254, ACC:0.53125\n",
      "Training iteration 336 loss: 0.669296383857727, ACC:0.609375\n",
      "Training iteration 337 loss: 0.6733283996582031, ACC:0.609375\n",
      "Training iteration 338 loss: 1.1451447010040283, ACC:0.28125\n",
      "Training iteration 339 loss: 0.9047018885612488, ACC:0.46875\n",
      "Training iteration 340 loss: 0.8932452201843262, ACC:0.515625\n",
      "Training iteration 341 loss: 0.7110715508460999, ACC:0.5\n",
      "Training iteration 342 loss: 0.9004834294319153, ACC:0.5\n",
      "Training iteration 343 loss: 0.702946662902832, ACC:0.484375\n",
      "Training iteration 344 loss: 0.878820538520813, ACC:0.5\n",
      "Training iteration 345 loss: 0.7437000870704651, ACC:0.515625\n",
      "Training iteration 346 loss: 0.7895049452781677, ACC:0.484375\n",
      "Training iteration 347 loss: 0.73113614320755, ACC:0.546875\n",
      "Training iteration 348 loss: 0.6981878876686096, ACC:0.5\n",
      "Training iteration 349 loss: 0.7732031345367432, ACC:0.484375\n",
      "Training iteration 350 loss: 0.6915006041526794, ACC:0.53125\n",
      "Training iteration 351 loss: 0.8195807337760925, ACC:0.46875\n",
      "Training iteration 352 loss: 0.6930831074714661, ACC:0.546875\n",
      "Training iteration 353 loss: 0.73872309923172, ACC:0.46875\n",
      "Training iteration 354 loss: 0.691316545009613, ACC:0.53125\n",
      "Training iteration 355 loss: 0.8125910758972168, ACC:0.40625\n",
      "Training iteration 356 loss: 0.8398142457008362, ACC:0.296875\n",
      "Training iteration 357 loss: 0.6834316253662109, ACC:0.59375\n",
      "Training iteration 358 loss: 0.9085173010826111, ACC:0.484375\n",
      "Training iteration 359 loss: 0.6813717484474182, ACC:0.59375\n",
      "Training iteration 360 loss: 1.3223882913589478, ACC:0.359375\n",
      "Training iteration 361 loss: 0.7006739974021912, ACC:0.40625\n",
      "Training iteration 362 loss: 0.8362711668014526, ACC:0.484375\n",
      "Training iteration 363 loss: 0.7082197666168213, ACC:0.4375\n",
      "Training iteration 364 loss: 0.9362577199935913, ACC:0.46875\n",
      "Training iteration 365 loss: 0.6998448967933655, ACC:0.546875\n",
      "Training iteration 366 loss: 0.6915684342384338, ACC:0.59375\n",
      "Training iteration 367 loss: 0.9472424387931824, ACC:0.453125\n",
      "Training iteration 368 loss: 0.713707685470581, ACC:0.515625\n",
      "Training iteration 369 loss: 0.8699624538421631, ACC:0.53125\n",
      "Training iteration 370 loss: 0.6580924391746521, ACC:0.640625\n",
      "Training iteration 371 loss: 0.710725724697113, ACC:0.4375\n",
      "Training iteration 372 loss: 0.6887527108192444, ACC:0.59375\n",
      "Training iteration 373 loss: 0.7119731307029724, ACC:0.578125\n",
      "Training iteration 374 loss: 0.7916967868804932, ACC:0.453125\n",
      "Training iteration 375 loss: 0.7308516502380371, ACC:0.5625\n",
      "Training iteration 376 loss: 0.8602209091186523, ACC:0.53125\n",
      "Training iteration 377 loss: 0.6932086944580078, ACC:0.5\n",
      "Training iteration 378 loss: 0.8280993700027466, ACC:0.53125\n",
      "Training iteration 379 loss: 0.7546795010566711, ACC:0.484375\n",
      "Training iteration 380 loss: 0.8212876915931702, ACC:0.5\n",
      "Training iteration 381 loss: 0.7240155339241028, ACC:0.578125\n",
      "Training iteration 382 loss: 0.6747574806213379, ACC:0.671875\n",
      "Training iteration 383 loss: 1.0431113243103027, ACC:0.546875\n",
      "Training iteration 384 loss: 0.9573161005973816, ACC:0.484375\n",
      "Training iteration 385 loss: 0.8295917510986328, ACC:0.546875\n",
      "Training iteration 386 loss: 1.3477566242218018, ACC:0.421875\n",
      "Training iteration 387 loss: 0.6962838768959045, ACC:0.53125\n",
      "Training iteration 388 loss: 1.2085456848144531, ACC:0.5\n",
      "Training iteration 389 loss: 0.8364361524581909, ACC:0.515625\n",
      "Training iteration 390 loss: 1.208700180053711, ACC:0.3125\n",
      "Training iteration 391 loss: 0.6756824851036072, ACC:0.59375\n",
      "Training iteration 392 loss: 0.6980139017105103, ACC:0.5625\n",
      "Training iteration 393 loss: 0.7272261381149292, ACC:0.578125\n",
      "Training iteration 394 loss: 0.7247470617294312, ACC:0.4375\n",
      "Training iteration 395 loss: 0.8476442694664001, ACC:0.515625\n",
      "Training iteration 396 loss: 0.819785475730896, ACC:0.453125\n",
      "Training iteration 397 loss: 0.9626734256744385, ACC:0.40625\n",
      "Training iteration 398 loss: 0.6874960064888, ACC:0.578125\n",
      "Training iteration 399 loss: 0.70549076795578, ACC:0.515625\n",
      "Training iteration 400 loss: 0.739966094493866, ACC:0.515625\n",
      "Training iteration 401 loss: 0.6975166201591492, ACC:0.4375\n",
      "Training iteration 402 loss: 0.69509357213974, ACC:0.515625\n",
      "Training iteration 403 loss: 0.6927592754364014, ACC:0.515625\n",
      "Training iteration 404 loss: 0.6908519864082336, ACC:0.546875\n",
      "Training iteration 405 loss: 0.688534140586853, ACC:0.5625\n",
      "Training iteration 406 loss: 0.7487840056419373, ACC:0.4375\n",
      "Training iteration 407 loss: 0.764137327671051, ACC:0.484375\n",
      "Training iteration 408 loss: 0.7683090567588806, ACC:0.40625\n",
      "Training iteration 409 loss: 0.9454259276390076, ACC:0.421875\n",
      "Training iteration 410 loss: 0.7358804941177368, ACC:0.40625\n",
      "Training iteration 411 loss: 1.0188215970993042, ACC:0.484375\n",
      "Training iteration 412 loss: 0.9285117983818054, ACC:0.375\n",
      "Training iteration 413 loss: 1.3223403692245483, ACC:0.40625\n",
      "Training iteration 414 loss: 0.8221736550331116, ACC:0.578125\n",
      "Training iteration 415 loss: 0.7642943263053894, ACC:0.46875\n",
      "Training iteration 416 loss: 0.8382765054702759, ACC:0.53125\n",
      "Training iteration 417 loss: 0.6984314322471619, ACC:0.46875\n",
      "Training iteration 418 loss: 0.7315405011177063, ACC:0.625\n",
      "Training iteration 419 loss: 0.9943689703941345, ACC:0.46875\n",
      "Training iteration 420 loss: 0.7470378279685974, ACC:0.515625\n",
      "Training iteration 421 loss: 1.0484517812728882, ACC:0.46875\n",
      "Training iteration 422 loss: 0.6936232447624207, ACC:0.5\n",
      "Training iteration 423 loss: 1.0176918506622314, ACC:0.453125\n",
      "Training iteration 424 loss: 0.6993404626846313, ACC:0.515625\n",
      "Training iteration 425 loss: 0.8931114673614502, ACC:0.46875\n",
      "Training iteration 426 loss: 0.7589195370674133, ACC:0.421875\n",
      "Training iteration 427 loss: 0.9000310301780701, ACC:0.53125\n",
      "Training iteration 428 loss: 0.8733947277069092, ACC:0.515625\n",
      "Training iteration 429 loss: 0.7759389281272888, ACC:0.46875\n",
      "Training iteration 430 loss: 0.8854891657829285, ACC:0.46875\n",
      "Training iteration 431 loss: 0.6690343618392944, ACC:0.609375\n",
      "Training iteration 432 loss: 0.9930358529090881, ACC:0.546875\n",
      "Training iteration 433 loss: 0.8708661794662476, ACC:0.4375\n",
      "Training iteration 434 loss: 1.0826835632324219, ACC:0.484375\n",
      "Training iteration 435 loss: 1.1195820569992065, ACC:0.453125\n",
      "Training iteration 436 loss: 0.7684116363525391, ACC:0.546875\n",
      "Training iteration 437 loss: 1.3223536014556885, ACC:0.453125\n",
      "Training iteration 438 loss: 0.7019795179367065, ACC:0.5\n",
      "Training iteration 439 loss: 1.140283465385437, ACC:0.484375\n",
      "Training iteration 440 loss: 0.7868258953094482, ACC:0.578125\n",
      "Training iteration 441 loss: 0.7441047430038452, ACC:0.515625\n",
      "Training iteration 442 loss: 1.0284745693206787, ACC:0.4375\n",
      "Training iteration 443 loss: 0.720984935760498, ACC:0.484375\n",
      "Training iteration 444 loss: 0.7483486533164978, ACC:0.609375\n",
      "Training iteration 445 loss: 0.9015371203422546, ACC:0.375\n",
      "Training iteration 446 loss: 0.9792653322219849, ACC:0.53125\n",
      "Training iteration 447 loss: 1.0751359462738037, ACC:0.515625\n",
      "Training iteration 448 loss: 0.6958348751068115, ACC:0.53125\n",
      "Training iteration 449 loss: 1.1389586925506592, ACC:0.5\n",
      "Training iteration 450 loss: 0.7747291922569275, ACC:0.53125\n",
      "Validation iteration 451 loss: 0.913306713104248, ACC: 0.484375\n",
      "Validation iteration 452 loss: 0.8928913474082947, ACC: 0.5\n",
      "Validation iteration 453 loss: 0.9541370868682861, ACC: 0.453125\n",
      "Validation iteration 454 loss: 0.7091543674468994, ACC: 0.640625\n",
      "Validation iteration 455 loss: 0.811230480670929, ACC: 0.5625\n",
      "Validation iteration 456 loss: 0.9337218999862671, ACC: 0.46875\n",
      "Validation iteration 457 loss: 0.9337218999862671, ACC: 0.46875\n",
      "Validation iteration 458 loss: 0.7295695543289185, ACC: 0.625\n",
      "Validation iteration 459 loss: 0.8112304210662842, ACC: 0.5625\n",
      "Validation iteration 460 loss: 0.8724762201309204, ACC: 0.515625\n",
      "Validation iteration 461 loss: 0.7703999876976013, ACC: 0.59375\n",
      "Validation iteration 462 loss: 1.097043752670288, ACC: 0.34375\n",
      "Validation iteration 463 loss: 0.9541370868682861, ACC: 0.453125\n",
      "Validation iteration 464 loss: 0.8724762201309204, ACC: 0.515625\n",
      "Validation iteration 465 loss: 0.8724762201309204, ACC: 0.515625\n",
      "Validation iteration 466 loss: 0.9949675798416138, ACC: 0.421875\n",
      "Validation iteration 467 loss: 0.8928914070129395, ACC: 0.5\n",
      "Validation iteration 468 loss: 0.8928912878036499, ACC: 0.5\n",
      "Validation iteration 469 loss: 0.9541370868682861, ACC: 0.453125\n",
      "Validation iteration 470 loss: 0.7908152341842651, ACC: 0.578125\n",
      "Validation iteration 471 loss: 0.8928914070129395, ACC: 0.5\n",
      "Validation iteration 472 loss: 0.7704000473022461, ACC: 0.59375\n",
      "Validation iteration 473 loss: 0.9745522737503052, ACC: 0.4375\n",
      "Validation iteration 474 loss: 0.8928914070129395, ACC: 0.5\n",
      "Validation iteration 475 loss: 0.811230480670929, ACC: 0.5625\n",
      "Validation iteration 476 loss: 0.7703999876976013, ACC: 0.59375\n",
      "Validation iteration 477 loss: 0.8724761605262756, ACC: 0.515625\n",
      "Validation iteration 478 loss: 0.9949675798416138, ACC: 0.421875\n",
      "Validation iteration 479 loss: 0.9133066534996033, ACC: 0.484375\n",
      "Validation iteration 480 loss: 0.9337218999862671, ACC: 0.46875\n",
      "Validation iteration 481 loss: 1.1174589395523071, ACC: 0.328125\n",
      "Validation iteration 482 loss: 0.9133066534996033, ACC: 0.484375\n",
      "Validation iteration 483 loss: 0.8724762201309204, ACC: 0.515625\n",
      "Validation iteration 484 loss: 0.8520609140396118, ACC: 0.53125\n",
      "Validation iteration 485 loss: 0.9541370868682861, ACC: 0.453125\n",
      "Validation iteration 486 loss: 0.8724761605262756, ACC: 0.515625\n",
      "Validation iteration 487 loss: 0.9745522737503052, ACC: 0.4375\n",
      "Validation iteration 488 loss: 0.9745522737503052, ACC: 0.4375\n",
      "Validation iteration 489 loss: 0.8520609736442566, ACC: 0.53125\n",
      "Validation iteration 490 loss: 0.8724761605262756, ACC: 0.515625\n",
      "Validation iteration 491 loss: 0.8724761605262756, ACC: 0.515625\n",
      "Validation iteration 492 loss: 0.8520609736442566, ACC: 0.53125\n",
      "Validation iteration 493 loss: 0.9337218999862671, ACC: 0.46875\n",
      "Validation iteration 494 loss: 0.9745522737503052, ACC: 0.4375\n",
      "Validation iteration 495 loss: 0.7499847412109375, ACC: 0.609375\n",
      "Validation iteration 496 loss: 0.9541371464729309, ACC: 0.453125\n",
      "Validation iteration 497 loss: 0.8112305402755737, ACC: 0.5625\n",
      "Validation iteration 498 loss: 1.0153827667236328, ACC: 0.40625\n",
      "Validation iteration 499 loss: 1.0153827667236328, ACC: 0.40625\n",
      "Validation iteration 500 loss: 0.9337218999862671, ACC: 0.46875\n",
      "-- Epoch 2 done -- Train loss: 0.8617135654555427, train ACC: 0.49319444444444444, val loss: 0.8969744515419006, val ACC: 0.496875\n",
      "<--- 8821.797569274902 seconds --->\n",
      "Training iteration 1 loss: 0.7908152937889099, ACC:0.578125\n",
      "Training iteration 2 loss: 1.100777506828308, ACC:0.5\n",
      "Training iteration 3 loss: 0.6946994066238403, ACC:0.46875\n",
      "Training iteration 4 loss: 0.866759717464447, ACC:0.5625\n",
      "Training iteration 5 loss: 0.9267613291740417, ACC:0.421875\n",
      "Training iteration 6 loss: 0.8849833607673645, ACC:0.546875\n",
      "Training iteration 7 loss: 1.136246681213379, ACC:0.5\n",
      "Training iteration 8 loss: 0.7075430750846863, ACC:0.390625\n",
      "Training iteration 9 loss: 0.9076353907585144, ACC:0.4375\n",
      "Training iteration 10 loss: 0.6953483819961548, ACC:0.5\n",
      "Training iteration 11 loss: 0.882473349571228, ACC:0.4375\n",
      "Training iteration 12 loss: 0.69185870885849, ACC:0.53125\n",
      "Training iteration 13 loss: 0.8749263882637024, ACC:0.484375\n",
      "Training iteration 14 loss: 0.6743437647819519, ACC:0.640625\n",
      "Training iteration 15 loss: 0.6915605664253235, ACC:0.53125\n",
      "Training iteration 16 loss: 0.7270148992538452, ACC:0.453125\n",
      "Training iteration 17 loss: 0.6619969606399536, ACC:0.640625\n",
      "Training iteration 18 loss: 0.9059367179870605, ACC:0.5\n",
      "Training iteration 19 loss: 0.703742265701294, ACC:0.5\n",
      "Training iteration 20 loss: 0.7432715892791748, ACC:0.59375\n",
      "Training iteration 21 loss: 0.8974815607070923, ACC:0.5\n",
      "Training iteration 22 loss: 0.7006828784942627, ACC:0.546875\n",
      "Training iteration 23 loss: 0.992700457572937, ACC:0.5\n",
      "Training iteration 24 loss: 0.6920551061630249, ACC:0.546875\n",
      "Training iteration 25 loss: 0.7630633115768433, ACC:0.5625\n",
      "Training iteration 26 loss: 0.8913607001304626, ACC:0.453125\n",
      "Training iteration 27 loss: 0.887681245803833, ACC:0.390625\n",
      "Training iteration 28 loss: 0.7022380828857422, ACC:0.546875\n",
      "Training iteration 29 loss: 0.6982545256614685, ACC:0.53125\n",
      "Training iteration 30 loss: 0.8110923767089844, ACC:0.4375\n",
      "Training iteration 31 loss: 0.7302717566490173, ACC:0.5\n",
      "Training iteration 32 loss: 0.7735444903373718, ACC:0.5\n",
      "Training iteration 33 loss: 0.7060958743095398, ACC:0.484375\n",
      "Training iteration 34 loss: 0.7814716100692749, ACC:0.453125\n",
      "Training iteration 35 loss: 0.7178876399993896, ACC:0.484375\n",
      "Training iteration 36 loss: 0.7446481585502625, ACC:0.484375\n",
      "Training iteration 37 loss: 0.6957831382751465, ACC:0.53125\n",
      "Training iteration 38 loss: 0.7596925497055054, ACC:0.515625\n",
      "Training iteration 39 loss: 0.6953285932540894, ACC:0.453125\n",
      "Training iteration 40 loss: 0.7414032220840454, ACC:0.578125\n",
      "Training iteration 41 loss: 0.7080831527709961, ACC:0.59375\n",
      "Training iteration 42 loss: 0.696015477180481, ACC:0.390625\n",
      "Training iteration 43 loss: 0.6903775334358215, ACC:0.546875\n",
      "Training iteration 44 loss: 0.7248798608779907, ACC:0.4375\n",
      "Training iteration 45 loss: 0.737091064453125, ACC:0.453125\n",
      "Training iteration 46 loss: 0.6949635148048401, ACC:0.46875\n",
      "Training iteration 47 loss: 0.7092582583427429, ACC:0.546875\n",
      "Training iteration 48 loss: 0.669426441192627, ACC:0.609375\n",
      "Training iteration 49 loss: 0.7009801268577576, ACC:0.515625\n",
      "Training iteration 50 loss: 0.6854471564292908, ACC:0.5625\n",
      "Training iteration 51 loss: 0.8024779558181763, ACC:0.453125\n",
      "Training iteration 52 loss: 0.6934529542922974, ACC:0.546875\n",
      "Training iteration 53 loss: 0.8018724918365479, ACC:0.53125\n",
      "Training iteration 54 loss: 0.7172425389289856, ACC:0.421875\n",
      "Training iteration 55 loss: 0.923995852470398, ACC:0.515625\n",
      "Training iteration 56 loss: 0.8173596858978271, ACC:0.484375\n",
      "Training iteration 57 loss: 0.8370835781097412, ACC:0.515625\n",
      "Training iteration 58 loss: 0.9106407761573792, ACC:0.5\n",
      "Training iteration 59 loss: 0.7491326928138733, ACC:0.453125\n",
      "Training iteration 60 loss: 0.9109471440315247, ACC:0.40625\n",
      "Training iteration 61 loss: 0.7708510756492615, ACC:0.5\n",
      "Training iteration 62 loss: 0.7891592979431152, ACC:0.546875\n",
      "Training iteration 63 loss: 0.6915996074676514, ACC:0.5625\n",
      "Training iteration 64 loss: 0.843346118927002, ACC:0.5625\n",
      "Training iteration 65 loss: 0.7369191646575928, ACC:0.578125\n",
      "Training iteration 66 loss: 0.6690334677696228, ACC:0.609375\n",
      "Training iteration 67 loss: 0.9831889271736145, ACC:0.53125\n",
      "Training iteration 68 loss: 0.7867105007171631, ACC:0.453125\n",
      "Training iteration 69 loss: 1.0052309036254883, ACC:0.515625\n",
      "Training iteration 70 loss: 0.838387131690979, ACC:0.59375\n",
      "Training iteration 71 loss: 0.6938630938529968, ACC:0.515625\n",
      "Training iteration 72 loss: 0.7079242467880249, ACC:0.671875\n",
      "Training iteration 73 loss: 1.0178611278533936, ACC:0.515625\n",
      "Training iteration 74 loss: 0.7140127420425415, ACC:0.5\n",
      "Training iteration 75 loss: 0.8536055684089661, ACC:0.59375\n",
      "Training iteration 76 loss: 0.8534402251243591, ACC:0.484375\n",
      "Training iteration 77 loss: 0.7830374240875244, ACC:0.578125\n",
      "Training iteration 78 loss: 1.1245230436325073, ACC:0.5\n",
      "Training iteration 79 loss: 0.6935350894927979, ACC:0.46875\n",
      "Training iteration 80 loss: 1.1623085737228394, ACC:0.484375\n",
      "Training iteration 81 loss: 0.8312756419181824, ACC:0.5\n",
      "Training iteration 82 loss: 1.1013789176940918, ACC:0.390625\n",
      "Training iteration 83 loss: 0.8559281229972839, ACC:0.40625\n",
      "Training iteration 84 loss: 1.0884184837341309, ACC:0.46875\n",
      "Training iteration 85 loss: 0.9721086621284485, ACC:0.46875\n",
      "Training iteration 86 loss: 0.8425902724266052, ACC:0.515625\n",
      "Training iteration 87 loss: 1.0004777908325195, ACC:0.53125\n",
      "Training iteration 88 loss: 0.692884624004364, ACC:0.515625\n",
      "Training iteration 89 loss: 0.9530535340309143, ACC:0.5\n",
      "Training iteration 90 loss: 0.7515116930007935, ACC:0.515625\n",
      "Training iteration 91 loss: 0.7932978272438049, ACC:0.53125\n",
      "Training iteration 92 loss: 0.8148621320724487, ACC:0.546875\n",
      "Training iteration 93 loss: 0.6969857215881348, ACC:0.5\n",
      "Training iteration 94 loss: 0.8483977317810059, ACC:0.5\n",
      "Training iteration 95 loss: 0.6997891664505005, ACC:0.5\n",
      "Training iteration 96 loss: 0.6715899705886841, ACC:0.640625\n",
      "Training iteration 97 loss: 0.8979111909866333, ACC:0.53125\n",
      "Training iteration 98 loss: 0.6933405995368958, ACC:0.5\n",
      "Training iteration 99 loss: 0.8389829993247986, ACC:0.546875\n",
      "Training iteration 100 loss: 0.7222006916999817, ACC:0.5625\n",
      "Training iteration 101 loss: 0.7409616708755493, ACC:0.484375\n",
      "Training iteration 102 loss: 0.7831249237060547, ACC:0.46875\n",
      "Training iteration 103 loss: 0.8229357004165649, ACC:0.328125\n",
      "Training iteration 104 loss: 0.6884362101554871, ACC:0.5625\n",
      "Training iteration 105 loss: 0.6727580428123474, ACC:0.640625\n",
      "Training iteration 106 loss: 0.7495847940444946, ACC:0.546875\n",
      "Training iteration 107 loss: 0.6962817907333374, ACC:0.53125\n",
      "Training iteration 108 loss: 0.7667263150215149, ACC:0.5625\n",
      "Training iteration 109 loss: 0.7298352718353271, ACC:0.484375\n",
      "Training iteration 110 loss: 0.7810196876525879, ACC:0.53125\n",
      "Training iteration 111 loss: 0.7861040830612183, ACC:0.515625\n",
      "Training iteration 112 loss: 0.7023504376411438, ACC:0.546875\n",
      "Training iteration 113 loss: 0.8657088875770569, ACC:0.515625\n",
      "Training iteration 114 loss: 0.7045854330062866, ACC:0.40625\n",
      "Training iteration 115 loss: 1.021209955215454, ACC:0.515625\n",
      "Training iteration 116 loss: 0.8251016736030579, ACC:0.515625\n",
      "Training iteration 117 loss: 0.883139431476593, ACC:0.46875\n",
      "Training iteration 118 loss: 0.7083554267883301, ACC:0.625\n",
      "Training iteration 119 loss: 0.6998858451843262, ACC:0.5\n",
      "Training iteration 120 loss: 0.8629882335662842, ACC:0.453125\n",
      "Training iteration 121 loss: 0.6943650841712952, ACC:0.515625\n",
      "Training iteration 122 loss: 0.850206732749939, ACC:0.390625\n",
      "Training iteration 123 loss: 0.6820998191833496, ACC:0.578125\n",
      "Training iteration 124 loss: 0.9570965766906738, ACC:0.453125\n",
      "Training iteration 125 loss: 0.7019374966621399, ACC:0.484375\n",
      "Training iteration 126 loss: 0.8399437069892883, ACC:0.5\n",
      "Training iteration 127 loss: 0.686281144618988, ACC:0.5625\n",
      "Training iteration 128 loss: 0.7709302306175232, ACC:0.421875\n",
      "Training iteration 129 loss: 0.7083773612976074, ACC:0.484375\n",
      "Training iteration 130 loss: 0.7420327067375183, ACC:0.4375\n",
      "Training iteration 131 loss: 0.7409684062004089, ACC:0.5\n",
      "Training iteration 132 loss: 0.7175531983375549, ACC:0.515625\n",
      "Training iteration 133 loss: 0.7143713235855103, ACC:0.484375\n",
      "Training iteration 134 loss: 0.7270821332931519, ACC:0.46875\n",
      "Training iteration 135 loss: 0.7273929119110107, ACC:0.484375\n",
      "Training iteration 136 loss: 0.7296236753463745, ACC:0.4375\n",
      "Training iteration 137 loss: 0.8087568283081055, ACC:0.453125\n",
      "Training iteration 138 loss: 0.7129262685775757, ACC:0.40625\n",
      "Training iteration 139 loss: 0.9484118819236755, ACC:0.46875\n",
      "Training iteration 140 loss: 0.77225261926651, ACC:0.375\n",
      "Training iteration 141 loss: 1.3532708883285522, ACC:0.390625\n",
      "Training iteration 142 loss: 0.8411922454833984, ACC:0.40625\n",
      "Training iteration 143 loss: 1.131776213645935, ACC:0.546875\n",
      "Training iteration 144 loss: 1.4566516876220703, ACC:0.453125\n",
      "Training iteration 145 loss: 0.7312616109848022, ACC:0.5\n",
      "Training iteration 146 loss: 1.5404942035675049, ACC:0.390625\n",
      "Training iteration 147 loss: 0.7105523943901062, ACC:0.5\n",
      "Training iteration 148 loss: 1.0271819829940796, ACC:0.5625\n",
      "Training iteration 149 loss: 1.0385230779647827, ACC:0.546875\n",
      "Training iteration 150 loss: 0.7161200642585754, ACC:0.484375\n",
      "Training iteration 151 loss: 1.0162051916122437, ACC:0.515625\n",
      "Training iteration 152 loss: 0.7072262763977051, ACC:0.5625\n",
      "Training iteration 153 loss: 0.8155888319015503, ACC:0.5\n",
      "Training iteration 154 loss: 0.7506111860275269, ACC:0.546875\n",
      "Training iteration 155 loss: 0.7071456909179688, ACC:0.5\n",
      "Training iteration 156 loss: 0.827936589717865, ACC:0.453125\n",
      "Training iteration 157 loss: 0.6631249189376831, ACC:0.625\n",
      "Training iteration 158 loss: 1.083858609199524, ACC:0.46875\n",
      "Training iteration 159 loss: 0.6765369176864624, ACC:0.59375\n",
      "Training iteration 160 loss: 0.7425961494445801, ACC:0.546875\n",
      "Training iteration 161 loss: 0.8101211190223694, ACC:0.453125\n",
      "Training iteration 162 loss: 0.7210075259208679, ACC:0.578125\n",
      "Training iteration 163 loss: 0.907844603061676, ACC:0.53125\n",
      "Training iteration 164 loss: 0.6960344910621643, ACC:0.453125\n",
      "Training iteration 165 loss: 1.063264012336731, ACC:0.46875\n",
      "Training iteration 166 loss: 0.7621293067932129, ACC:0.46875\n",
      "Training iteration 167 loss: 1.081636667251587, ACC:0.4375\n",
      "Training iteration 168 loss: 0.7419977784156799, ACC:0.546875\n",
      "Training iteration 169 loss: 0.7053067684173584, ACC:0.59375\n",
      "Training iteration 170 loss: 0.8742071986198425, ACC:0.578125\n",
      "Training iteration 171 loss: 0.7277374267578125, ACC:0.5\n",
      "Training iteration 172 loss: 0.938980758190155, ACC:0.484375\n",
      "Training iteration 173 loss: 0.8368145823478699, ACC:0.453125\n",
      "Training iteration 174 loss: 0.8580535650253296, ACC:0.53125\n",
      "Training iteration 175 loss: 1.0127127170562744, ACC:0.46875\n",
      "Training iteration 176 loss: 0.7136355638504028, ACC:0.5625\n",
      "Training iteration 177 loss: 1.0007541179656982, ACC:0.5625\n",
      "Training iteration 178 loss: 0.7372848391532898, ACC:0.5625\n",
      "Training iteration 179 loss: 0.9101864695549011, ACC:0.4375\n",
      "Training iteration 180 loss: 0.6911320090293884, ACC:0.59375\n",
      "Training iteration 181 loss: 0.677914559841156, ACC:0.609375\n",
      "Training iteration 182 loss: 1.0305918455123901, ACC:0.4375\n",
      "Training iteration 183 loss: 0.6922882199287415, ACC:0.5625\n",
      "Training iteration 184 loss: 0.8827743530273438, ACC:0.390625\n",
      "Training iteration 185 loss: 0.7505459189414978, ACC:0.453125\n",
      "Training iteration 186 loss: 0.7504512071609497, ACC:0.46875\n",
      "Training iteration 187 loss: 0.6708320379257202, ACC:0.609375\n",
      "Training iteration 188 loss: 0.8735091686248779, ACC:0.53125\n",
      "Training iteration 189 loss: 0.7087780237197876, ACC:0.46875\n",
      "Training iteration 190 loss: 0.9731622934341431, ACC:0.46875\n",
      "Training iteration 191 loss: 0.6893941760063171, ACC:0.578125\n",
      "Training iteration 192 loss: 0.6921060085296631, ACC:0.578125\n",
      "Training iteration 193 loss: 0.844310998916626, ACC:0.515625\n",
      "Training iteration 194 loss: 0.6926965713500977, ACC:0.515625\n",
      "Training iteration 195 loss: 0.9930778741836548, ACC:0.390625\n",
      "Training iteration 196 loss: 0.7444917559623718, ACC:0.421875\n",
      "Training iteration 197 loss: 0.775626003742218, ACC:0.4375\n",
      "Training iteration 198 loss: 0.7067176103591919, ACC:0.5625\n",
      "Training iteration 199 loss: 0.8439022302627563, ACC:0.5\n",
      "Training iteration 200 loss: 0.7047619819641113, ACC:0.484375\n",
      "Training iteration 201 loss: 0.6827429533004761, ACC:0.625\n",
      "Training iteration 202 loss: 0.8067293763160706, ACC:0.484375\n",
      "Training iteration 203 loss: 0.8134927153587341, ACC:0.421875\n",
      "Training iteration 204 loss: 0.662316620349884, ACC:0.625\n",
      "Training iteration 205 loss: 0.6997017860412598, ACC:0.515625\n",
      "Training iteration 206 loss: 0.7069174647331238, ACC:0.46875\n",
      "Training iteration 207 loss: 0.7050317525863647, ACC:0.375\n",
      "Training iteration 208 loss: 0.7631545662879944, ACC:0.5625\n",
      "Training iteration 209 loss: 0.7517668604850769, ACC:0.53125\n",
      "Training iteration 210 loss: 0.7635723352432251, ACC:0.4375\n",
      "Training iteration 211 loss: 0.7015196084976196, ACC:0.53125\n",
      "Training iteration 212 loss: 0.6887632608413696, ACC:0.546875\n",
      "Training iteration 213 loss: 0.7130030989646912, ACC:0.5625\n",
      "Training iteration 214 loss: 0.662570595741272, ACC:0.625\n",
      "Training iteration 215 loss: 0.7081149816513062, ACC:0.484375\n",
      "Training iteration 216 loss: 0.7105551958084106, ACC:0.515625\n",
      "Training iteration 217 loss: 0.7315678000450134, ACC:0.46875\n",
      "Training iteration 218 loss: 0.7043153643608093, ACC:0.546875\n",
      "Training iteration 219 loss: 0.8414344787597656, ACC:0.421875\n",
      "Training iteration 220 loss: 0.7943305373191833, ACC:0.484375\n",
      "Training iteration 221 loss: 0.8042310476303101, ACC:0.46875\n",
      "Training iteration 222 loss: 0.8191765546798706, ACC:0.4375\n",
      "Training iteration 223 loss: 0.7149965167045593, ACC:0.515625\n",
      "Training iteration 224 loss: 0.7022331357002258, ACC:0.546875\n",
      "Training iteration 225 loss: 0.7561357617378235, ACC:0.53125\n",
      "Training iteration 226 loss: 0.6912761926651001, ACC:0.546875\n",
      "Training iteration 227 loss: 0.7710046172142029, ACC:0.5625\n",
      "Training iteration 228 loss: 0.7239169478416443, ACC:0.546875\n",
      "Training iteration 229 loss: 0.7510830760002136, ACC:0.453125\n",
      "Training iteration 230 loss: 0.68846595287323, ACC:0.5625\n",
      "Training iteration 231 loss: 0.6927388906478882, ACC:0.515625\n",
      "Training iteration 232 loss: 0.7172523736953735, ACC:0.4375\n",
      "Training iteration 233 loss: 0.7164947986602783, ACC:0.484375\n",
      "Training iteration 234 loss: 0.6887705326080322, ACC:0.546875\n",
      "Training iteration 235 loss: 0.693736732006073, ACC:0.46875\n",
      "Training iteration 236 loss: 0.6695790886878967, ACC:0.609375\n",
      "Training iteration 237 loss: 0.7671799063682556, ACC:0.515625\n",
      "Training iteration 238 loss: 0.6952801942825317, ACC:0.453125\n",
      "Training iteration 239 loss: 0.7097888588905334, ACC:0.515625\n",
      "Training iteration 240 loss: 0.6890559196472168, ACC:0.546875\n",
      "Training iteration 241 loss: 0.698420524597168, ACC:0.375\n",
      "Training iteration 242 loss: 0.7522610425949097, ACC:0.515625\n",
      "Training iteration 243 loss: 0.7369498014450073, ACC:0.390625\n",
      "Training iteration 244 loss: 0.7953299880027771, ACC:0.59375\n",
      "Training iteration 245 loss: 0.7112458348274231, ACC:0.65625\n",
      "Training iteration 246 loss: 0.6933326125144958, ACC:0.546875\n",
      "Training iteration 247 loss: 0.7979195713996887, ACC:0.5\n",
      "Training iteration 248 loss: 0.7578898072242737, ACC:0.453125\n",
      "Training iteration 249 loss: 0.7489978075027466, ACC:0.578125\n",
      "Training iteration 250 loss: 0.8009458184242249, ACC:0.578125\n",
      "Training iteration 251 loss: 0.6976343393325806, ACC:0.40625\n",
      "Training iteration 252 loss: 0.9367856383323669, ACC:0.578125\n",
      "Training iteration 253 loss: 0.8213083744049072, ACC:0.59375\n",
      "Training iteration 254 loss: 0.7075949311256409, ACC:0.515625\n",
      "Training iteration 255 loss: 0.9462598562240601, ACC:0.515625\n",
      "Training iteration 256 loss: 0.6986058354377747, ACC:0.53125\n",
      "Training iteration 257 loss: 0.9164158701896667, ACC:0.4375\n",
      "Training iteration 258 loss: 0.6982353329658508, ACC:0.5\n",
      "Training iteration 259 loss: 0.7657949328422546, ACC:0.546875\n",
      "Training iteration 260 loss: 0.7400180101394653, ACC:0.53125\n",
      "Training iteration 261 loss: 0.7112034559249878, ACC:0.53125\n",
      "Training iteration 262 loss: 0.7795088887214661, ACC:0.53125\n",
      "Training iteration 263 loss: 0.6921898722648621, ACC:0.5625\n",
      "Training iteration 264 loss: 0.9024807810783386, ACC:0.5\n",
      "Training iteration 265 loss: 0.6825414896011353, ACC:0.578125\n",
      "Training iteration 266 loss: 0.744983971118927, ACC:0.484375\n",
      "Training iteration 267 loss: 0.7213846445083618, ACC:0.46875\n",
      "Training iteration 268 loss: 0.7547321915626526, ACC:0.5\n",
      "Training iteration 269 loss: 0.7187075018882751, ACC:0.5\n",
      "Training iteration 270 loss: 0.7706022262573242, ACC:0.4375\n",
      "Training iteration 271 loss: 0.6934059262275696, ACC:0.5\n",
      "Training iteration 272 loss: 0.7342167496681213, ACC:0.484375\n",
      "Training iteration 273 loss: 0.6931172609329224, ACC:0.609375\n",
      "Training iteration 274 loss: 0.7996199727058411, ACC:0.5625\n",
      "Training iteration 275 loss: 0.802508533000946, ACC:0.46875\n",
      "Training iteration 276 loss: 0.917384684085846, ACC:0.453125\n",
      "Training iteration 277 loss: 0.7263835072517395, ACC:0.546875\n",
      "Training iteration 278 loss: 0.7003085017204285, ACC:0.5625\n",
      "Training iteration 279 loss: 0.84822016954422, ACC:0.515625\n",
      "Training iteration 280 loss: 0.7013231515884399, ACC:0.40625\n",
      "Training iteration 281 loss: 0.6690747737884521, ACC:0.609375\n",
      "Training iteration 282 loss: 0.7174410223960876, ACC:0.5625\n",
      "Training iteration 283 loss: 0.7178372144699097, ACC:0.40625\n",
      "Training iteration 284 loss: 0.7791760563850403, ACC:0.59375\n",
      "Training iteration 285 loss: 1.0139983892440796, ACC:0.40625\n",
      "Training iteration 286 loss: 1.0397847890853882, ACC:0.4375\n",
      "Training iteration 287 loss: 0.9407025575637817, ACC:0.453125\n",
      "Training iteration 288 loss: 0.8274580240249634, ACC:0.546875\n",
      "Training iteration 289 loss: 1.0522338151931763, ACC:0.515625\n",
      "Training iteration 290 loss: 0.7009953260421753, ACC:0.453125\n",
      "Training iteration 291 loss: 0.9318310022354126, ACC:0.484375\n",
      "Training iteration 292 loss: 0.7071155309677124, ACC:0.46875\n",
      "Training iteration 293 loss: 0.9076429009437561, ACC:0.515625\n",
      "Training iteration 294 loss: 0.7246769666671753, ACC:0.5625\n",
      "Training iteration 295 loss: 0.6965396404266357, ACC:0.578125\n",
      "Training iteration 296 loss: 0.9262493252754211, ACC:0.5\n",
      "Training iteration 297 loss: 0.6947861909866333, ACC:0.5\n",
      "Training iteration 298 loss: 0.9283076524734497, ACC:0.46875\n",
      "Training iteration 299 loss: 0.6904089450836182, ACC:0.546875\n",
      "Training iteration 300 loss: 0.7996733784675598, ACC:0.46875\n",
      "Training iteration 301 loss: 0.6940376162528992, ACC:0.34375\n",
      "Training iteration 302 loss: 0.73687744140625, ACC:0.4375\n",
      "Training iteration 303 loss: 0.6942741870880127, ACC:0.546875\n",
      "Training iteration 304 loss: 0.7946949005126953, ACC:0.453125\n",
      "Training iteration 305 loss: 0.7156766653060913, ACC:0.53125\n",
      "Training iteration 306 loss: 0.7917495965957642, ACC:0.515625\n",
      "Training iteration 307 loss: 0.6916605234146118, ACC:0.53125\n",
      "Training iteration 308 loss: 0.9648275375366211, ACC:0.390625\n",
      "Training iteration 309 loss: 0.7584290504455566, ACC:0.484375\n",
      "Training iteration 310 loss: 0.7194613814353943, ACC:0.59375\n",
      "Training iteration 311 loss: 0.676142692565918, ACC:0.59375\n",
      "Training iteration 312 loss: 0.7065286040306091, ACC:0.453125\n",
      "Training iteration 313 loss: 0.6912051439285278, ACC:0.546875\n",
      "Training iteration 314 loss: 0.7822505235671997, ACC:0.390625\n",
      "Training iteration 315 loss: 0.7656598091125488, ACC:0.5\n",
      "Training iteration 316 loss: 0.7648178339004517, ACC:0.453125\n",
      "Training iteration 317 loss: 0.8320705890655518, ACC:0.46875\n",
      "Training iteration 318 loss: 0.6855206489562988, ACC:0.578125\n",
      "Training iteration 319 loss: 0.6889286041259766, ACC:0.546875\n",
      "Training iteration 320 loss: 0.7367589473724365, ACC:0.546875\n",
      "Training iteration 321 loss: 0.7032002210617065, ACC:0.5\n",
      "Training iteration 322 loss: 0.7442728281021118, ACC:0.515625\n",
      "Training iteration 323 loss: 0.7141658067703247, ACC:0.515625\n",
      "Training iteration 324 loss: 0.6617282032966614, ACC:0.625\n",
      "Training iteration 325 loss: 1.0283364057540894, ACC:0.421875\n",
      "Training iteration 326 loss: 0.6868142485618591, ACC:0.578125\n",
      "Training iteration 327 loss: 1.1456384658813477, ACC:0.46875\n",
      "Training iteration 328 loss: 0.7137614488601685, ACC:0.421875\n",
      "Training iteration 329 loss: 1.1566084623336792, ACC:0.53125\n",
      "Training iteration 330 loss: 1.1152867078781128, ACC:0.453125\n",
      "Training iteration 331 loss: 0.9847687482833862, ACC:0.5\n",
      "Training iteration 332 loss: 1.2407187223434448, ACC:0.484375\n",
      "Training iteration 333 loss: 0.681131899356842, ACC:0.578125\n",
      "Training iteration 334 loss: 1.402153491973877, ACC:0.484375\n",
      "Training iteration 335 loss: 0.9298539757728577, ACC:0.5\n",
      "Training iteration 336 loss: 1.230735421180725, ACC:0.40625\n",
      "Training iteration 337 loss: 0.893825888633728, ACC:0.53125\n",
      "Training iteration 338 loss: 0.8153579831123352, ACC:0.46875\n",
      "Training iteration 339 loss: 0.8433917760848999, ACC:0.53125\n",
      "Training iteration 340 loss: 0.7028341889381409, ACC:0.46875\n",
      "Training iteration 341 loss: 0.7554347515106201, ACC:0.546875\n",
      "Training iteration 342 loss: 0.7407853007316589, ACC:0.421875\n",
      "Training iteration 343 loss: 0.9099057912826538, ACC:0.5\n",
      "Training iteration 344 loss: 0.8195938467979431, ACC:0.4375\n",
      "Training iteration 345 loss: 1.0098028182983398, ACC:0.46875\n",
      "Training iteration 346 loss: 0.7974656224250793, ACC:0.53125\n",
      "Training iteration 347 loss: 0.6910024285316467, ACC:0.609375\n",
      "Training iteration 348 loss: 0.9521498680114746, ACC:0.578125\n",
      "Training iteration 349 loss: 0.8092572689056396, ACC:0.4375\n",
      "Training iteration 350 loss: 1.4406336545944214, ACC:0.359375\n",
      "Training iteration 351 loss: 0.7446926236152649, ACC:0.53125\n",
      "Training iteration 352 loss: 1.061303973197937, ACC:0.421875\n",
      "Training iteration 353 loss: 0.6875852346420288, ACC:0.59375\n",
      "Training iteration 354 loss: 0.7542994618415833, ACC:0.453125\n",
      "Training iteration 355 loss: 0.7102521061897278, ACC:0.5\n",
      "Training iteration 356 loss: 0.7378372550010681, ACC:0.453125\n",
      "Training iteration 357 loss: 0.6918396353721619, ACC:0.53125\n",
      "Training iteration 358 loss: 0.6784223318099976, ACC:0.609375\n",
      "Training iteration 359 loss: 0.781559944152832, ACC:0.5\n",
      "Training iteration 360 loss: 0.6954387426376343, ACC:0.390625\n",
      "Training iteration 361 loss: 0.9450429081916809, ACC:0.515625\n",
      "Training iteration 362 loss: 0.7941201329231262, ACC:0.453125\n",
      "Training iteration 363 loss: 0.9368762373924255, ACC:0.53125\n",
      "Training iteration 364 loss: 0.9892843961715698, ACC:0.484375\n",
      "Training iteration 365 loss: 0.8801230788230896, ACC:0.421875\n",
      "Training iteration 366 loss: 0.7746520638465881, ACC:0.546875\n",
      "Training iteration 367 loss: 0.7362738251686096, ACC:0.375\n",
      "Training iteration 368 loss: 0.6915847063064575, ACC:0.53125\n",
      "Training iteration 369 loss: 0.6931664347648621, ACC:0.5\n",
      "Training iteration 370 loss: 0.6896857023239136, ACC:0.5625\n",
      "Training iteration 371 loss: 0.7247933149337769, ACC:0.484375\n",
      "Training iteration 372 loss: 0.7019473910331726, ACC:0.4375\n",
      "Training iteration 373 loss: 0.6939454674720764, ACC:0.484375\n",
      "Training iteration 374 loss: 0.6931520104408264, ACC:0.5\n",
      "Training iteration 375 loss: 0.6955335140228271, ACC:0.4375\n",
      "Training iteration 376 loss: 0.7330157160758972, ACC:0.46875\n",
      "Training iteration 377 loss: 0.6912194490432739, ACC:0.53125\n",
      "Training iteration 378 loss: 0.7651844620704651, ACC:0.46875\n",
      "Training iteration 379 loss: 0.7133253216743469, ACC:0.453125\n",
      "Training iteration 380 loss: 0.7021694183349609, ACC:0.5\n",
      "Training iteration 381 loss: 0.6983733773231506, ACC:0.5\n",
      "Training iteration 382 loss: 0.7179366946220398, ACC:0.4375\n",
      "Training iteration 383 loss: 0.720375657081604, ACC:0.53125\n",
      "Training iteration 384 loss: 0.7556659579277039, ACC:0.453125\n",
      "Training iteration 385 loss: 0.8146137595176697, ACC:0.453125\n",
      "Training iteration 386 loss: 0.7367970943450928, ACC:0.390625\n",
      "Training iteration 387 loss: 1.072700023651123, ACC:0.421875\n",
      "Training iteration 388 loss: 0.6853147149085999, ACC:0.5625\n",
      "Training iteration 389 loss: 0.8427332043647766, ACC:0.4375\n",
      "Training iteration 390 loss: 0.6888765096664429, ACC:0.578125\n",
      "Training iteration 391 loss: 0.8254051804542542, ACC:0.5625\n",
      "Training iteration 392 loss: 0.7130979895591736, ACC:0.578125\n",
      "Training iteration 393 loss: 0.7335614562034607, ACC:0.484375\n",
      "Training iteration 394 loss: 0.7426072359085083, ACC:0.515625\n",
      "Training iteration 395 loss: 0.6928668022155762, ACC:0.53125\n",
      "Training iteration 396 loss: 0.7184010744094849, ACC:0.578125\n",
      "Training iteration 397 loss: 0.7827772498130798, ACC:0.390625\n",
      "Training iteration 398 loss: 0.8176871538162231, ACC:0.59375\n",
      "Training iteration 399 loss: 1.1007779836654663, ACC:0.453125\n",
      "Training iteration 400 loss: 0.828296422958374, ACC:0.484375\n",
      "Training iteration 401 loss: 0.8947345614433289, ACC:0.5625\n",
      "Training iteration 402 loss: 0.6854729652404785, ACC:0.5625\n",
      "Training iteration 403 loss: 0.7669217586517334, ACC:0.546875\n",
      "Training iteration 404 loss: 0.7036103010177612, ACC:0.578125\n",
      "Training iteration 405 loss: 0.6954996585845947, ACC:0.515625\n",
      "Training iteration 406 loss: 0.7857143878936768, ACC:0.484375\n",
      "Training iteration 407 loss: 0.6993506550788879, ACC:0.484375\n",
      "Training iteration 408 loss: 0.7123227715492249, ACC:0.546875\n",
      "Training iteration 409 loss: 0.689643919467926, ACC:0.546875\n",
      "Training iteration 410 loss: 0.6856290698051453, ACC:0.5625\n",
      "Training iteration 411 loss: 0.7676558494567871, ACC:0.484375\n",
      "Training iteration 412 loss: 0.7095616459846497, ACC:0.4375\n",
      "Training iteration 413 loss: 0.6887784600257874, ACC:0.546875\n",
      "Training iteration 414 loss: 0.7033883333206177, ACC:0.46875\n",
      "Training iteration 415 loss: 0.7191856503486633, ACC:0.484375\n",
      "Training iteration 416 loss: 0.6800452470779419, ACC:0.65625\n",
      "Training iteration 417 loss: 0.8193701505661011, ACC:0.4375\n",
      "Training iteration 418 loss: 0.7146428823471069, ACC:0.5\n",
      "Training iteration 419 loss: 0.6564857959747314, ACC:0.640625\n",
      "Training iteration 420 loss: 0.705138087272644, ACC:0.578125\n",
      "Training iteration 421 loss: 0.6940043568611145, ACC:0.421875\n",
      "Training iteration 422 loss: 0.841722309589386, ACC:0.53125\n",
      "Training iteration 423 loss: 0.6530633568763733, ACC:0.640625\n",
      "Training iteration 424 loss: 0.6883867979049683, ACC:0.625\n",
      "Training iteration 425 loss: 1.0941089391708374, ACC:0.390625\n",
      "Training iteration 426 loss: 0.7612506747245789, ACC:0.359375\n",
      "Training iteration 427 loss: 0.6950222253799438, ACC:0.53125\n",
      "Training iteration 428 loss: 0.6890758275985718, ACC:0.5625\n",
      "Training iteration 429 loss: 0.7478550672531128, ACC:0.515625\n",
      "Training iteration 430 loss: 0.7004712820053101, ACC:0.46875\n",
      "Training iteration 431 loss: 0.7463310360908508, ACC:0.546875\n",
      "Training iteration 432 loss: 0.6925603747367859, ACC:0.578125\n",
      "Training iteration 433 loss: 0.6958655118942261, ACC:0.5\n",
      "Training iteration 434 loss: 0.7427269220352173, ACC:0.484375\n",
      "Training iteration 435 loss: 0.697590172290802, ACC:0.5\n",
      "Training iteration 436 loss: 0.7193459272384644, ACC:0.515625\n",
      "Training iteration 437 loss: 0.6932209134101868, ACC:0.484375\n",
      "Training iteration 438 loss: 0.6811373233795166, ACC:0.578125\n",
      "Training iteration 439 loss: 0.6905689239501953, ACC:0.578125\n",
      "Training iteration 440 loss: 0.6956892013549805, ACC:0.53125\n",
      "Training iteration 441 loss: 0.6899856328964233, ACC:0.546875\n",
      "Training iteration 442 loss: 0.694200873374939, ACC:0.578125\n",
      "Training iteration 443 loss: 0.7120627164840698, ACC:0.5\n",
      "Training iteration 444 loss: 0.7403051257133484, ACC:0.484375\n",
      "Training iteration 445 loss: 0.7033159136772156, ACC:0.5\n",
      "Training iteration 446 loss: 0.6996634602546692, ACC:0.546875\n",
      "Training iteration 447 loss: 0.75409996509552, ACC:0.484375\n",
      "Training iteration 448 loss: 0.7227011919021606, ACC:0.5\n",
      "Training iteration 449 loss: 0.7303102016448975, ACC:0.515625\n",
      "Training iteration 450 loss: 0.7018664479255676, ACC:0.5\n",
      "Validation iteration 451 loss: 0.7418595552444458, ACC: 0.484375\n",
      "Validation iteration 452 loss: 0.7329804301261902, ACC: 0.5\n",
      "Validation iteration 453 loss: 0.6974641680717468, ACC: 0.5625\n",
      "Validation iteration 454 loss: 0.7063432335853577, ACC: 0.546875\n",
      "Validation iteration 455 loss: 0.7329804301261902, ACC: 0.5\n",
      "Validation iteration 456 loss: 0.7241014242172241, ACC: 0.515625\n",
      "Validation iteration 457 loss: 0.7329804301261902, ACC: 0.5\n",
      "Validation iteration 458 loss: 0.8217711448669434, ACC: 0.34375\n",
      "Validation iteration 459 loss: 0.7152222990989685, ACC: 0.53125\n",
      "Validation iteration 460 loss: 0.7241013646125793, ACC: 0.515625\n",
      "Validation iteration 461 loss: 0.6530688405036926, ACC: 0.640625\n",
      "Validation iteration 462 loss: 0.7684967517852783, ACC: 0.4375\n",
      "Validation iteration 463 loss: 0.7773758172988892, ACC: 0.421875\n",
      "Validation iteration 464 loss: 0.6619477868080139, ACC: 0.625\n",
      "Validation iteration 465 loss: 0.8040130734443665, ACC: 0.375\n",
      "Validation iteration 466 loss: 0.697464108467102, ACC: 0.5625\n",
      "Validation iteration 467 loss: 0.7329803705215454, ACC: 0.5\n",
      "Validation iteration 468 loss: 0.7152222990989685, ACC: 0.53125\n",
      "Validation iteration 469 loss: 0.7063432931900024, ACC: 0.546875\n",
      "Validation iteration 470 loss: 0.7773757576942444, ACC: 0.421875\n",
      "Validation iteration 471 loss: 0.7418595552444458, ACC: 0.484375\n",
      "Validation iteration 472 loss: 0.7773758769035339, ACC: 0.421875\n",
      "Validation iteration 473 loss: 0.6885850429534912, ACC: 0.578125\n",
      "Validation iteration 474 loss: 0.7063432335853577, ACC: 0.546875\n",
      "Validation iteration 475 loss: 0.7063431739807129, ACC: 0.546875\n",
      "Validation iteration 476 loss: 0.7418595552444458, ACC: 0.484375\n",
      "Validation iteration 477 loss: 0.7152222990989685, ACC: 0.53125\n",
      "Validation iteration 478 loss: 0.732980489730835, ACC: 0.5\n",
      "Validation iteration 479 loss: 0.7596176266670227, ACC: 0.453125\n",
      "Validation iteration 480 loss: 0.7773758172988892, ACC: 0.421875\n",
      "Validation iteration 481 loss: 0.7951339483261108, ACC: 0.390625\n",
      "Validation iteration 482 loss: 0.7329804301261902, ACC: 0.5\n",
      "Validation iteration 483 loss: 0.741859495639801, ACC: 0.484375\n",
      "Validation iteration 484 loss: 0.7507386207580566, ACC: 0.46875\n",
      "Validation iteration 485 loss: 0.7152222990989685, ACC: 0.53125\n",
      "Validation iteration 486 loss: 0.7329804301261902, ACC: 0.5\n",
      "Validation iteration 487 loss: 0.6797060370445251, ACC: 0.59375\n",
      "Validation iteration 488 loss: 0.741859495639801, ACC: 0.484375\n",
      "Validation iteration 489 loss: 0.7596176266670227, ACC: 0.453125\n",
      "Validation iteration 490 loss: 0.7773758769035339, ACC: 0.421875\n",
      "Validation iteration 491 loss: 0.6708269119262695, ACC: 0.609375\n",
      "Validation iteration 492 loss: 0.7152222394943237, ACC: 0.53125\n",
      "Validation iteration 493 loss: 0.7063432931900024, ACC: 0.546875\n",
      "Validation iteration 494 loss: 0.688585102558136, ACC: 0.578125\n",
      "Validation iteration 495 loss: 0.6974641680717468, ACC: 0.5625\n",
      "Validation iteration 496 loss: 0.6708269715309143, ACC: 0.609375\n",
      "Validation iteration 497 loss: 0.741859495639801, ACC: 0.484375\n",
      "Validation iteration 498 loss: 0.7773757576942444, ACC: 0.421875\n",
      "Validation iteration 499 loss: 0.7241013646125793, ACC: 0.515625\n",
      "Validation iteration 500 loss: 0.7152222990989685, ACC: 0.53125\n",
      "-- Epoch 3 done -- Train loss: 0.7890350225236681, train ACC: 0.5060416666666666, val loss: 0.7301391422748565, val ACC: 0.505\n",
      "<--- 9298.537127256393 seconds --->\n",
      "Training iteration 1 loss: 0.7418594360351562, ACC:0.484375\n",
      "Training iteration 2 loss: 0.6856793165206909, ACC:0.5625\n",
      "Training iteration 3 loss: 0.9048579931259155, ACC:0.40625\n",
      "Training iteration 4 loss: 0.7709484100341797, ACC:0.5\n",
      "Training iteration 5 loss: 0.8907780051231384, ACC:0.421875\n",
      "Training iteration 6 loss: 0.7730708122253418, ACC:0.5625\n",
      "Training iteration 7 loss: 0.8843400478363037, ACC:0.5625\n",
      "Training iteration 8 loss: 0.69216388463974, ACC:0.546875\n",
      "Training iteration 9 loss: 0.9377700090408325, ACC:0.40625\n",
      "Training iteration 10 loss: 0.704736590385437, ACC:0.53125\n",
      "Training iteration 11 loss: 0.8568810224533081, ACC:0.515625\n",
      "Training iteration 12 loss: 0.6933677792549133, ACC:0.484375\n",
      "Training iteration 13 loss: 0.9322890043258667, ACC:0.453125\n",
      "Training iteration 14 loss: 0.6966456174850464, ACC:0.40625\n",
      "Training iteration 15 loss: 0.688747763633728, ACC:0.5625\n",
      "Training iteration 16 loss: 0.6762486696243286, ACC:0.59375\n",
      "Training iteration 17 loss: 0.6976194381713867, ACC:0.53125\n",
      "Training iteration 18 loss: 0.6939086318016052, ACC:0.515625\n",
      "Training iteration 19 loss: 0.710344135761261, ACC:0.515625\n",
      "Training iteration 20 loss: 0.6934653520584106, ACC:0.390625\n",
      "Training iteration 21 loss: 0.7415273189544678, ACC:0.4375\n",
      "Training iteration 22 loss: 0.731102466583252, ACC:0.46875\n",
      "Training iteration 23 loss: 0.6941312551498413, ACC:0.515625\n",
      "Training iteration 24 loss: 0.6724931597709656, ACC:0.609375\n",
      "Training iteration 25 loss: 0.840506911277771, ACC:0.46875\n",
      "Training iteration 26 loss: 0.6909033060073853, ACC:0.546875\n",
      "Training iteration 27 loss: 0.8578110337257385, ACC:0.515625\n",
      "Training iteration 28 loss: 0.6990139484405518, ACC:0.46875\n",
      "Training iteration 29 loss: 0.8654875755310059, ACC:0.53125\n",
      "Training iteration 30 loss: 0.7550079822540283, ACC:0.5\n",
      "Training iteration 31 loss: 0.6886115074157715, ACC:0.640625\n",
      "Training iteration 32 loss: 0.8602865934371948, ACC:0.625\n",
      "Training iteration 33 loss: 0.8258631229400635, ACC:0.421875\n",
      "Training iteration 34 loss: 1.2565150260925293, ACC:0.484375\n",
      "Training iteration 35 loss: 1.1107035875320435, ACC:0.46875\n",
      "Training iteration 36 loss: 0.9769942164421082, ACC:0.484375\n",
      "Training iteration 37 loss: 1.0590934753417969, ACC:0.53125\n",
      "Training iteration 38 loss: 0.6772153973579407, ACC:0.609375\n",
      "Training iteration 39 loss: 1.4991474151611328, ACC:0.453125\n",
      "Training iteration 40 loss: 0.9267117381095886, ACC:0.453125\n",
      "Training iteration 41 loss: 1.289946436882019, ACC:0.5\n",
      "Training iteration 42 loss: 1.676251769065857, ACC:0.375\n",
      "Training iteration 43 loss: 0.8335174322128296, ACC:0.5625\n",
      "Training iteration 44 loss: 1.436234474182129, ACC:0.5625\n",
      "Training iteration 45 loss: 1.0420626401901245, ACC:0.515625\n",
      "Training iteration 46 loss: 1.2233519554138184, ACC:0.4375\n",
      "Training iteration 47 loss: 1.335196852684021, ACC:0.421875\n",
      "Training iteration 48 loss: 0.8621282577514648, ACC:0.5\n",
      "Training iteration 49 loss: 1.2043782472610474, ACC:0.5\n",
      "Training iteration 50 loss: 0.7033417224884033, ACC:0.453125\n",
      "Training iteration 51 loss: 1.3964338302612305, ACC:0.4375\n",
      "Training iteration 52 loss: 0.8107024431228638, ACC:0.515625\n",
      "Training iteration 53 loss: 1.08572256565094, ACC:0.46875\n",
      "Training iteration 54 loss: 0.9134517908096313, ACC:0.515625\n",
      "Training iteration 55 loss: 0.7911248803138733, ACC:0.515625\n",
      "Training iteration 56 loss: 1.1891905069351196, ACC:0.375\n",
      "Training iteration 57 loss: 0.9105239510536194, ACC:0.453125\n",
      "Training iteration 58 loss: 0.899387001991272, ACC:0.515625\n",
      "Training iteration 59 loss: 0.7151361703872681, ACC:0.515625\n",
      "Training iteration 60 loss: 0.9516790509223938, ACC:0.484375\n",
      "Training iteration 61 loss: 0.6887457370758057, ACC:0.5625\n",
      "Training iteration 62 loss: 1.0629063844680786, ACC:0.484375\n",
      "Training iteration 63 loss: 0.7841051816940308, ACC:0.40625\n",
      "Training iteration 64 loss: 1.3065106868743896, ACC:0.453125\n",
      "Training iteration 65 loss: 0.7029308080673218, ACC:0.65625\n",
      "Training iteration 66 loss: 0.716658353805542, ACC:0.484375\n",
      "Training iteration 67 loss: 0.9348952770233154, ACC:0.4375\n",
      "Training iteration 68 loss: 0.7221029996871948, ACC:0.5\n",
      "Training iteration 69 loss: 0.8432920575141907, ACC:0.5\n",
      "Training iteration 70 loss: 0.7039620876312256, ACC:0.453125\n",
      "Training iteration 71 loss: 0.8139879703521729, ACC:0.40625\n",
      "Training iteration 72 loss: 0.7477642297744751, ACC:0.515625\n",
      "Training iteration 73 loss: 0.8047118782997131, ACC:0.46875\n",
      "Training iteration 74 loss: 0.7145641446113586, ACC:0.5625\n",
      "Training iteration 75 loss: 0.9811159372329712, ACC:0.4375\n",
      "Training iteration 76 loss: 0.8042950630187988, ACC:0.4375\n",
      "Training iteration 77 loss: 0.8195646405220032, ACC:0.453125\n",
      "Training iteration 78 loss: 0.7682676911354065, ACC:0.515625\n",
      "Training iteration 79 loss: 0.7871444821357727, ACC:0.53125\n",
      "Training iteration 80 loss: 0.7255308628082275, ACC:0.4375\n",
      "Training iteration 81 loss: 0.7001553773880005, ACC:0.546875\n",
      "Training iteration 82 loss: 0.7032504677772522, ACC:0.421875\n",
      "Training iteration 83 loss: 0.8310425877571106, ACC:0.484375\n",
      "Training iteration 84 loss: 0.7010349631309509, ACC:0.484375\n",
      "Training iteration 85 loss: 0.8151689171791077, ACC:0.5\n",
      "Training iteration 86 loss: 0.7360334992408752, ACC:0.4375\n",
      "Training iteration 87 loss: 0.9830611348152161, ACC:0.4375\n",
      "Training iteration 88 loss: 0.7079208493232727, ACC:0.484375\n",
      "Training iteration 89 loss: 0.6865419149398804, ACC:0.65625\n",
      "Training iteration 90 loss: 1.0985063314437866, ACC:0.46875\n",
      "Training iteration 91 loss: 0.7114002108573914, ACC:0.546875\n",
      "Training iteration 92 loss: 1.2595441341400146, ACC:0.4375\n",
      "Training iteration 93 loss: 0.6926498413085938, ACC:0.53125\n",
      "Training iteration 94 loss: 0.9794954657554626, ACC:0.5\n",
      "Training iteration 95 loss: 0.8548774123191833, ACC:0.3125\n",
      "Training iteration 96 loss: 1.4232611656188965, ACC:0.5\n",
      "Training iteration 97 loss: 1.218225121498108, ACC:0.546875\n",
      "Training iteration 98 loss: 0.7284687757492065, ACC:0.46875\n",
      "Training iteration 99 loss: 0.963502049446106, ACC:0.59375\n",
      "Training iteration 100 loss: 0.8606970906257629, ACC:0.546875\n",
      "Training iteration 101 loss: 0.7938371300697327, ACC:0.53125\n",
      "Training iteration 102 loss: 1.0909337997436523, ACC:0.46875\n",
      "Training iteration 103 loss: 0.740405797958374, ACC:0.4375\n",
      "Training iteration 104 loss: 0.8179161548614502, ACC:0.53125\n",
      "Training iteration 105 loss: 0.7087153792381287, ACC:0.421875\n",
      "Training iteration 106 loss: 0.9107230305671692, ACC:0.546875\n",
      "Training iteration 107 loss: 0.7012514472007751, ACC:0.625\n",
      "Training iteration 108 loss: 0.7058534026145935, ACC:0.484375\n",
      "Training iteration 109 loss: 0.7221636772155762, ACC:0.578125\n",
      "Training iteration 110 loss: 0.7251798510551453, ACC:0.5\n",
      "Training iteration 111 loss: 0.7163025140762329, ACC:0.5625\n",
      "Training iteration 112 loss: 0.9023696184158325, ACC:0.421875\n",
      "Training iteration 113 loss: 0.8029929995536804, ACC:0.515625\n",
      "Training iteration 114 loss: 0.9027010202407837, ACC:0.484375\n",
      "Training iteration 115 loss: 0.7587877511978149, ACC:0.484375\n",
      "Training iteration 116 loss: 0.8261098265647888, ACC:0.515625\n",
      "Training iteration 117 loss: 0.7119874954223633, ACC:0.4375\n",
      "Training iteration 118 loss: 0.6799788475036621, ACC:0.59375\n",
      "Training iteration 119 loss: 0.7148364186286926, ACC:0.53125\n",
      "Training iteration 120 loss: 0.7095750570297241, ACC:0.453125\n",
      "Training iteration 121 loss: 0.6916128396987915, ACC:0.53125\n",
      "Training iteration 122 loss: 0.699553906917572, ACC:0.359375\n",
      "Training iteration 123 loss: 0.7120372653007507, ACC:0.625\n",
      "Training iteration 124 loss: 0.8807501792907715, ACC:0.5\n",
      "Training iteration 125 loss: 0.7675535082817078, ACC:0.484375\n",
      "Training iteration 126 loss: 0.8793796300888062, ACC:0.46875\n",
      "Training iteration 127 loss: 0.7755788564682007, ACC:0.4375\n",
      "Training iteration 128 loss: 0.7224262952804565, ACC:0.53125\n",
      "Training iteration 129 loss: 0.685354471206665, ACC:0.5625\n",
      "Training iteration 130 loss: 0.8237209320068359, ACC:0.5\n",
      "Training iteration 131 loss: 0.6948536038398743, ACC:0.40625\n",
      "Training iteration 132 loss: 1.0182381868362427, ACC:0.484375\n",
      "Training iteration 133 loss: 0.7197383046150208, ACC:0.515625\n",
      "Training iteration 134 loss: 0.6868308186531067, ACC:0.65625\n",
      "Training iteration 135 loss: 1.2178269624710083, ACC:0.453125\n",
      "Training iteration 136 loss: 0.7165395021438599, ACC:0.515625\n",
      "Training iteration 137 loss: 1.266252040863037, ACC:0.421875\n",
      "Training iteration 138 loss: 0.694298267364502, ACC:0.484375\n",
      "Training iteration 139 loss: 0.8995410203933716, ACC:0.5625\n",
      "Training iteration 140 loss: 0.9143375158309937, ACC:0.421875\n",
      "Training iteration 141 loss: 1.0303863286972046, ACC:0.515625\n",
      "Training iteration 142 loss: 0.9698925614356995, ACC:0.5625\n",
      "Training iteration 143 loss: 0.6811218857765198, ACC:0.578125\n",
      "Training iteration 144 loss: 1.3546533584594727, ACC:0.453125\n",
      "Training iteration 145 loss: 0.7840702533721924, ACC:0.484375\n",
      "Training iteration 146 loss: 1.3069828748703003, ACC:0.421875\n",
      "Training iteration 147 loss: 0.8922566771507263, ACC:0.484375\n",
      "Training iteration 148 loss: 0.8115875720977783, ACC:0.609375\n",
      "Training iteration 149 loss: 0.9497096538543701, ACC:0.65625\n",
      "Training iteration 150 loss: 0.769079327583313, ACC:0.609375\n",
      "Training iteration 151 loss: 0.8350406289100647, ACC:0.484375\n",
      "Training iteration 152 loss: 0.9158477783203125, ACC:0.5\n",
      "Training iteration 153 loss: 0.6617438793182373, ACC:0.625\n",
      "Training iteration 154 loss: 1.209366798400879, ACC:0.5\n",
      "Training iteration 155 loss: 0.8359416723251343, ACC:0.4375\n",
      "Training iteration 156 loss: 0.9890778064727783, ACC:0.609375\n",
      "Training iteration 157 loss: 1.8909492492675781, ACC:0.375\n",
      "Training iteration 158 loss: 0.7515294551849365, ACC:0.484375\n",
      "Training iteration 159 loss: 1.46622896194458, ACC:0.46875\n",
      "Training iteration 160 loss: 0.7800126671791077, ACC:0.546875\n",
      "Training iteration 161 loss: 0.874372124671936, ACC:0.59375\n",
      "Training iteration 162 loss: 1.2250210046768188, ACC:0.53125\n",
      "Training iteration 163 loss: 0.6780223846435547, ACC:0.59375\n",
      "Training iteration 164 loss: 0.8552663922309875, ACC:0.5625\n",
      "Training iteration 165 loss: 0.7714079022407532, ACC:0.578125\n",
      "Training iteration 166 loss: 0.736508846282959, ACC:0.453125\n",
      "Training iteration 167 loss: 0.7840483784675598, ACC:0.484375\n",
      "Training iteration 168 loss: 0.7198691964149475, ACC:0.46875\n",
      "Training iteration 169 loss: 0.6867265105247498, ACC:0.578125\n",
      "Training iteration 170 loss: 0.7056743502616882, ACC:0.5\n",
      "Training iteration 171 loss: 0.7602084875106812, ACC:0.40625\n",
      "Training iteration 172 loss: 0.7288337349891663, ACC:0.4375\n",
      "Training iteration 173 loss: 0.6931679844856262, ACC:0.5\n",
      "Training iteration 174 loss: 0.6953182816505432, ACC:0.53125\n",
      "Training iteration 175 loss: 0.7122365832328796, ACC:0.453125\n",
      "Training iteration 176 loss: 0.7180709838867188, ACC:0.53125\n",
      "Training iteration 177 loss: 0.7774158716201782, ACC:0.40625\n",
      "Training iteration 178 loss: 0.7960752248764038, ACC:0.546875\n",
      "Training iteration 179 loss: 0.9636096954345703, ACC:0.390625\n",
      "Training iteration 180 loss: 1.1594698429107666, ACC:0.375\n",
      "Training iteration 181 loss: 0.7075843811035156, ACC:0.5625\n",
      "Training iteration 182 loss: 0.7384816408157349, ACC:0.546875\n",
      "Training iteration 183 loss: 0.8741031885147095, ACC:0.46875\n",
      "Training iteration 184 loss: 0.7417314648628235, ACC:0.515625\n",
      "Training iteration 185 loss: 0.9260748624801636, ACC:0.453125\n",
      "Training iteration 186 loss: 0.7794060707092285, ACC:0.4375\n",
      "Training iteration 187 loss: 0.7821144461631775, ACC:0.46875\n",
      "Training iteration 188 loss: 0.8096801042556763, ACC:0.40625\n",
      "Training iteration 189 loss: 0.6914263963699341, ACC:0.53125\n",
      "Training iteration 190 loss: 0.7153712511062622, ACC:0.484375\n",
      "Training iteration 191 loss: 0.6930335760116577, ACC:0.59375\n",
      "Training iteration 192 loss: 0.8627352118492126, ACC:0.453125\n",
      "Training iteration 193 loss: 0.6840226054191589, ACC:0.59375\n",
      "Training iteration 194 loss: 1.0838751792907715, ACC:0.4375\n",
      "Training iteration 195 loss: 0.6892911791801453, ACC:0.5625\n",
      "Training iteration 196 loss: 0.7935376167297363, ACC:0.515625\n",
      "Training iteration 197 loss: 0.7060754299163818, ACC:0.515625\n",
      "Training iteration 198 loss: 0.7010907530784607, ACC:0.578125\n",
      "Training iteration 199 loss: 0.82181715965271, ACC:0.5\n",
      "Training iteration 200 loss: 0.7017074227333069, ACC:0.53125\n",
      "Training iteration 201 loss: 0.7894617319107056, ACC:0.5625\n",
      "Training iteration 202 loss: 0.7139477729797363, ACC:0.5\n",
      "Training iteration 203 loss: 0.8558042049407959, ACC:0.46875\n",
      "Training iteration 204 loss: 0.6635957956314087, ACC:0.625\n",
      "Training iteration 205 loss: 0.6931599378585815, ACC:0.5\n",
      "Training iteration 206 loss: 0.6809734106063843, ACC:0.578125\n",
      "Training iteration 207 loss: 0.7104873061180115, ACC:0.546875\n",
      "Training iteration 208 loss: 0.6930036544799805, ACC:0.515625\n",
      "Training iteration 209 loss: 0.6890991926193237, ACC:0.5625\n",
      "Training iteration 210 loss: 0.7002966403961182, ACC:0.5625\n",
      "Training iteration 211 loss: 0.6866188049316406, ACC:0.5625\n",
      "Training iteration 212 loss: 0.6950350999832153, ACC:0.484375\n",
      "Training iteration 213 loss: 0.6932246685028076, ACC:0.5\n",
      "Training iteration 214 loss: 0.6988605856895447, ACC:0.421875\n",
      "Training iteration 215 loss: 0.7213366031646729, ACC:0.515625\n",
      "Training iteration 216 loss: 0.7000530362129211, ACC:0.5\n",
      "Training iteration 217 loss: 0.6986973881721497, ACC:0.546875\n",
      "Training iteration 218 loss: 0.7002395391464233, ACC:0.5625\n",
      "Training iteration 219 loss: 0.6814594864845276, ACC:0.625\n",
      "Training iteration 220 loss: 0.7232165932655334, ACC:0.46875\n",
      "Training iteration 221 loss: 0.6991496682167053, ACC:0.515625\n",
      "Training iteration 222 loss: 0.7367933988571167, ACC:0.46875\n",
      "Training iteration 223 loss: 0.7386615872383118, ACC:0.453125\n",
      "Training iteration 224 loss: 0.6831700205802917, ACC:0.59375\n",
      "Training iteration 225 loss: 0.6994864344596863, ACC:0.515625\n",
      "Training iteration 226 loss: 0.6948394775390625, ACC:0.421875\n",
      "Training iteration 227 loss: 0.7003262639045715, ACC:0.59375\n",
      "Training iteration 228 loss: 0.7664297223091125, ACC:0.515625\n",
      "Training iteration 229 loss: 0.7888355255126953, ACC:0.375\n",
      "Training iteration 230 loss: 0.6965645551681519, ACC:0.484375\n",
      "Training iteration 231 loss: 0.6974301934242249, ACC:0.53125\n",
      "Training iteration 232 loss: 0.6912919878959656, ACC:0.53125\n",
      "Training iteration 233 loss: 0.6938562393188477, ACC:0.5\n",
      "Training iteration 234 loss: 0.6934499144554138, ACC:0.515625\n",
      "Training iteration 235 loss: 0.6952649354934692, ACC:0.46875\n",
      "Training iteration 236 loss: 0.7256671786308289, ACC:0.453125\n",
      "Training iteration 237 loss: 0.7279971241950989, ACC:0.40625\n",
      "Training iteration 238 loss: 0.6476194858551025, ACC:0.671875\n",
      "Training iteration 239 loss: 0.868896484375, ACC:0.5625\n",
      "Training iteration 240 loss: 0.7593044638633728, ACC:0.484375\n",
      "Training iteration 241 loss: 0.9513975977897644, ACC:0.484375\n",
      "Training iteration 242 loss: 0.9580087661743164, ACC:0.328125\n",
      "Training iteration 243 loss: 1.0977528095245361, ACC:0.578125\n",
      "Training iteration 244 loss: 1.2266297340393066, ACC:0.59375\n",
      "Training iteration 245 loss: 0.7223564386367798, ACC:0.546875\n",
      "Training iteration 246 loss: 1.054240107536316, ACC:0.5625\n",
      "Training iteration 247 loss: 1.1659750938415527, ACC:0.53125\n",
      "Training iteration 248 loss: 0.7056747674942017, ACC:0.515625\n",
      "Training iteration 249 loss: 1.135643482208252, ACC:0.53125\n",
      "Training iteration 250 loss: 0.821778416633606, ACC:0.515625\n",
      "Training iteration 251 loss: 1.0645838975906372, ACC:0.453125\n",
      "Training iteration 252 loss: 0.8784895539283752, ACC:0.5\n",
      "Training iteration 253 loss: 0.9288681745529175, ACC:0.421875\n",
      "Training iteration 254 loss: 0.8111863732337952, ACC:0.453125\n",
      "Training iteration 255 loss: 0.6977402567863464, ACC:0.640625\n",
      "Training iteration 256 loss: 1.1591618061065674, ACC:0.5\n",
      "Training iteration 257 loss: 0.6933687329292297, ACC:0.46875\n",
      "Training iteration 258 loss: 1.1283135414123535, ACC:0.421875\n",
      "Training iteration 259 loss: 0.6926597952842712, ACC:0.515625\n",
      "Training iteration 260 loss: 0.931124210357666, ACC:0.484375\n",
      "Training iteration 261 loss: 0.6865584850311279, ACC:0.5625\n",
      "Training iteration 262 loss: 0.8065188527107239, ACC:0.4375\n",
      "Training iteration 263 loss: 0.6945115327835083, ACC:0.46875\n",
      "Training iteration 264 loss: 0.7308329343795776, ACC:0.46875\n",
      "Training iteration 265 loss: 0.709995448589325, ACC:0.453125\n",
      "Training iteration 266 loss: 0.6926599740982056, ACC:0.515625\n",
      "Training iteration 267 loss: 0.6979323625564575, ACC:0.453125\n",
      "Training iteration 268 loss: 0.6986264586448669, ACC:0.515625\n",
      "Training iteration 269 loss: 0.7019107937812805, ACC:0.484375\n",
      "Training iteration 270 loss: 0.7311443090438843, ACC:0.4375\n",
      "Training iteration 271 loss: 0.7045748233795166, ACC:0.484375\n",
      "Training iteration 272 loss: 0.6819846630096436, ACC:0.578125\n",
      "Training iteration 273 loss: 0.6870022416114807, ACC:0.5625\n",
      "Training iteration 274 loss: 0.708175003528595, ACC:0.484375\n",
      "Training iteration 275 loss: 0.7227282524108887, ACC:0.484375\n",
      "Training iteration 276 loss: 0.6999320387840271, ACC:0.484375\n",
      "Training iteration 277 loss: 0.7617228627204895, ACC:0.421875\n",
      "Training iteration 278 loss: 0.7401573657989502, ACC:0.421875\n",
      "Training iteration 279 loss: 0.691223680973053, ACC:0.53125\n",
      "Training iteration 280 loss: 0.7790597081184387, ACC:0.421875\n",
      "Training iteration 281 loss: 0.7603533267974854, ACC:0.46875\n",
      "Training iteration 282 loss: 0.697502076625824, ACC:0.53125\n",
      "Training iteration 283 loss: 0.7000648975372314, ACC:0.515625\n",
      "Training iteration 284 loss: 0.7604349851608276, ACC:0.40625\n",
      "Training iteration 285 loss: 0.7702329754829407, ACC:0.53125\n",
      "Training iteration 286 loss: 0.8447621464729309, ACC:0.40625\n",
      "Training iteration 287 loss: 0.9980034232139587, ACC:0.453125\n",
      "Training iteration 288 loss: 0.7601234316825867, ACC:0.515625\n",
      "Training iteration 289 loss: 0.8279991149902344, ACC:0.5\n",
      "Training iteration 290 loss: 0.8147836327552795, ACC:0.484375\n",
      "Training iteration 291 loss: 0.7366166114807129, ACC:0.5625\n",
      "Training iteration 292 loss: 0.8704084157943726, ACC:0.546875\n",
      "Training iteration 293 loss: 0.6911191940307617, ACC:0.6875\n",
      "Training iteration 294 loss: 1.3757719993591309, ACC:0.515625\n",
      "Training iteration 295 loss: 1.0027717351913452, ACC:0.5625\n",
      "Training iteration 296 loss: 0.84222012758255, ACC:0.46875\n",
      "Training iteration 297 loss: 0.8724208474159241, ACC:0.59375\n",
      "Training iteration 298 loss: 0.7108107805252075, ACC:0.53125\n",
      "Training iteration 299 loss: 0.9321863055229187, ACC:0.46875\n",
      "Training iteration 300 loss: 0.6530880928039551, ACC:0.640625\n",
      "Training iteration 301 loss: 0.6915666460990906, ACC:0.53125\n",
      "Training iteration 302 loss: 0.7414830923080444, ACC:0.53125\n",
      "Training iteration 303 loss: 0.7107096314430237, ACC:0.46875\n",
      "Training iteration 304 loss: 0.878693699836731, ACC:0.40625\n",
      "Training iteration 305 loss: 0.714104175567627, ACC:0.46875\n",
      "Training iteration 306 loss: 0.7572557926177979, ACC:0.46875\n",
      "Training iteration 307 loss: 0.7455466389656067, ACC:0.4375\n",
      "Training iteration 308 loss: 0.6947745084762573, ACC:0.5\n",
      "Training iteration 309 loss: 0.7002332806587219, ACC:0.53125\n",
      "Training iteration 310 loss: 0.7488640546798706, ACC:0.390625\n",
      "Training iteration 311 loss: 0.795619547367096, ACC:0.546875\n",
      "Training iteration 312 loss: 0.8550238013267517, ACC:0.4375\n",
      "Training iteration 313 loss: 0.8409502506256104, ACC:0.546875\n",
      "Training iteration 314 loss: 0.9214638471603394, ACC:0.53125\n",
      "Training iteration 315 loss: 0.7172082662582397, ACC:0.5\n",
      "Training iteration 316 loss: 0.9085501432418823, ACC:0.515625\n",
      "Training iteration 317 loss: 0.6938256025314331, ACC:0.515625\n",
      "Training iteration 318 loss: 0.8183915019035339, ACC:0.53125\n",
      "Training iteration 319 loss: 0.7637402415275574, ACC:0.46875\n",
      "Training iteration 320 loss: 0.8683265447616577, ACC:0.5\n",
      "Training iteration 321 loss: 0.7987886667251587, ACC:0.484375\n",
      "Training iteration 322 loss: 0.8835302591323853, ACC:0.453125\n",
      "Training iteration 323 loss: 0.776751697063446, ACC:0.453125\n",
      "Training iteration 324 loss: 0.8581312298774719, ACC:0.515625\n",
      "Training iteration 325 loss: 0.8024575114250183, ACC:0.515625\n",
      "Training iteration 326 loss: 0.8386548757553101, ACC:0.421875\n",
      "Training iteration 327 loss: 0.732805073261261, ACC:0.46875\n",
      "Training iteration 328 loss: 0.7257735133171082, ACC:0.578125\n",
      "Training iteration 329 loss: 0.8845186233520508, ACC:0.46875\n",
      "Training iteration 330 loss: 0.7624080181121826, ACC:0.515625\n",
      "Training iteration 331 loss: 0.899802029132843, ACC:0.484375\n",
      "Training iteration 332 loss: 0.6866798996925354, ACC:0.578125\n",
      "Training iteration 333 loss: 1.0669947862625122, ACC:0.484375\n",
      "Training iteration 334 loss: 0.7028148770332336, ACC:0.4375\n",
      "Training iteration 335 loss: 0.9845895171165466, ACC:0.578125\n",
      "Training iteration 336 loss: 1.0225796699523926, ACC:0.5\n",
      "Training iteration 337 loss: 0.7625535726547241, ACC:0.578125\n",
      "Training iteration 338 loss: 1.5058671236038208, ACC:0.40625\n",
      "Training iteration 339 loss: 0.70335453748703, ACC:0.5\n",
      "Training iteration 340 loss: 1.2600513696670532, ACC:0.46875\n",
      "Training iteration 341 loss: 0.7188974022865295, ACC:0.546875\n",
      "Training iteration 342 loss: 0.8873468041419983, ACC:0.546875\n",
      "Training iteration 343 loss: 1.0852662324905396, ACC:0.40625\n",
      "Training iteration 344 loss: 0.8768644332885742, ACC:0.5625\n",
      "Training iteration 345 loss: 1.4548190832138062, ACC:0.421875\n",
      "Training iteration 346 loss: 0.7055805325508118, ACC:0.546875\n",
      "Training iteration 347 loss: 1.3806977272033691, ACC:0.484375\n",
      "Training iteration 348 loss: 0.7584649324417114, ACC:0.5625\n",
      "Training iteration 349 loss: 0.9330967664718628, ACC:0.53125\n",
      "Training iteration 350 loss: 0.929324209690094, ACC:0.5625\n",
      "Training iteration 351 loss: 0.7034600377082825, ACC:0.484375\n",
      "Training iteration 352 loss: 1.0170669555664062, ACC:0.453125\n",
      "Training iteration 353 loss: 0.6967018246650696, ACC:0.4375\n",
      "Training iteration 354 loss: 0.7874192595481873, ACC:0.5\n",
      "Training iteration 355 loss: 0.6847782135009766, ACC:0.59375\n",
      "Training iteration 356 loss: 0.6897664070129395, ACC:0.546875\n",
      "Training iteration 357 loss: 0.7037591338157654, ACC:0.53125\n",
      "Training iteration 358 loss: 0.7113929390907288, ACC:0.421875\n",
      "Training iteration 359 loss: 0.6998424530029297, ACC:0.609375\n",
      "Training iteration 360 loss: 0.9579643607139587, ACC:0.40625\n",
      "Training iteration 361 loss: 0.8478714823722839, ACC:0.53125\n",
      "Training iteration 362 loss: 1.1427444219589233, ACC:0.40625\n",
      "Training iteration 363 loss: 0.8932399749755859, ACC:0.484375\n",
      "Training iteration 364 loss: 0.9550922513008118, ACC:0.53125\n",
      "Training iteration 365 loss: 0.7278280854225159, ACC:0.375\n",
      "Training iteration 366 loss: 0.7746054530143738, ACC:0.4375\n",
      "Training iteration 367 loss: 0.7183130383491516, ACC:0.53125\n",
      "Training iteration 368 loss: 0.7910379767417908, ACC:0.484375\n",
      "Training iteration 369 loss: 0.7102224826812744, ACC:0.53125\n",
      "Training iteration 370 loss: 0.8211730718612671, ACC:0.5\n",
      "Training iteration 371 loss: 0.701189398765564, ACC:0.5\n",
      "Training iteration 372 loss: 0.7866974472999573, ACC:0.515625\n",
      "Training iteration 373 loss: 0.695493757724762, ACC:0.46875\n",
      "Training iteration 374 loss: 0.9001180529594421, ACC:0.4375\n",
      "Training iteration 375 loss: 0.707617998123169, ACC:0.4375\n",
      "Training iteration 376 loss: 0.8167010545730591, ACC:0.328125\n",
      "Training iteration 377 loss: 0.8905585408210754, ACC:0.515625\n",
      "Training iteration 378 loss: 0.8436957001686096, ACC:0.46875\n",
      "Training iteration 379 loss: 1.0014169216156006, ACC:0.40625\n",
      "Training iteration 380 loss: 0.7391166090965271, ACC:0.484375\n",
      "Training iteration 381 loss: 0.8260236382484436, ACC:0.53125\n",
      "Training iteration 382 loss: 0.7555443644523621, ACC:0.5625\n",
      "Training iteration 383 loss: 0.7652747631072998, ACC:0.375\n",
      "Training iteration 384 loss: 0.6969202160835266, ACC:0.40625\n",
      "Training iteration 385 loss: 0.7939544320106506, ACC:0.53125\n",
      "Training iteration 386 loss: 0.7609031796455383, ACC:0.421875\n",
      "Training iteration 387 loss: 0.9229550957679749, ACC:0.515625\n",
      "Training iteration 388 loss: 0.8415449857711792, ACC:0.484375\n",
      "Training iteration 389 loss: 0.7559861540794373, ACC:0.59375\n",
      "Training iteration 390 loss: 1.2015454769134521, ACC:0.453125\n",
      "Training iteration 391 loss: 0.7405465841293335, ACC:0.46875\n",
      "Training iteration 392 loss: 0.8536943197250366, ACC:0.578125\n",
      "Training iteration 393 loss: 0.8182523250579834, ACC:0.40625\n",
      "Training iteration 394 loss: 1.3239665031433105, ACC:0.40625\n",
      "Training iteration 395 loss: 0.8790703415870667, ACC:0.421875\n",
      "Training iteration 396 loss: 1.1385680437088013, ACC:0.53125\n",
      "Training iteration 397 loss: 1.2990715503692627, ACC:0.484375\n",
      "Training iteration 398 loss: 0.7905133962631226, ACC:0.4375\n",
      "Training iteration 399 loss: 1.0587437152862549, ACC:0.484375\n",
      "Training iteration 400 loss: 0.6931913495063782, ACC:0.5\n",
      "Training iteration 401 loss: 0.8501653671264648, ACC:0.578125\n",
      "Training iteration 402 loss: 0.9093332290649414, ACC:0.453125\n",
      "Training iteration 403 loss: 0.7784286141395569, ACC:0.609375\n",
      "Training iteration 404 loss: 1.2994637489318848, ACC:0.484375\n",
      "Training iteration 405 loss: 0.6933107376098633, ACC:0.484375\n",
      "Training iteration 406 loss: 1.0753190517425537, ACC:0.5625\n",
      "Training iteration 407 loss: 0.8430804014205933, ACC:0.609375\n",
      "Training iteration 408 loss: 0.7325456142425537, ACC:0.46875\n",
      "Training iteration 409 loss: 1.0045439004898071, ACC:0.4375\n",
      "Training iteration 410 loss: 0.7083372473716736, ACC:0.515625\n",
      "Training iteration 411 loss: 0.8788512349128723, ACC:0.53125\n",
      "Training iteration 412 loss: 0.6808943152427673, ACC:0.578125\n",
      "Training iteration 413 loss: 0.8005521893501282, ACC:0.421875\n",
      "Training iteration 414 loss: 0.7076170444488525, ACC:0.453125\n",
      "Training iteration 415 loss: 0.706938624382019, ACC:0.484375\n",
      "Training iteration 416 loss: 0.6811084151268005, ACC:0.578125\n",
      "Training iteration 417 loss: 0.8452271819114685, ACC:0.421875\n",
      "Training iteration 418 loss: 0.717292308807373, ACC:0.546875\n",
      "Training iteration 419 loss: 0.8428449034690857, ACC:0.515625\n",
      "Training iteration 420 loss: 0.7001909017562866, ACC:0.484375\n",
      "Training iteration 421 loss: 0.7615745663642883, ACC:0.546875\n",
      "Training iteration 422 loss: 0.7104648351669312, ACC:0.5\n",
      "Training iteration 423 loss: 0.8191797733306885, ACC:0.453125\n",
      "Training iteration 424 loss: 0.678646981716156, ACC:0.625\n",
      "Training iteration 425 loss: 0.7174387574195862, ACC:0.375\n",
      "Training iteration 426 loss: 0.8515188694000244, ACC:0.46875\n",
      "Training iteration 427 loss: 0.6933538913726807, ACC:0.515625\n",
      "Training iteration 428 loss: 0.8336918950080872, ACC:0.421875\n",
      "Training iteration 429 loss: 0.7417991161346436, ACC:0.390625\n",
      "Training iteration 430 loss: 0.6908972263336182, ACC:0.546875\n",
      "Training iteration 431 loss: 0.733566403388977, ACC:0.515625\n",
      "Training iteration 432 loss: 0.7023957371711731, ACC:0.453125\n",
      "Training iteration 433 loss: 0.7652258276939392, ACC:0.53125\n",
      "Training iteration 434 loss: 0.718653678894043, ACC:0.515625\n",
      "Training iteration 435 loss: 0.7660133838653564, ACC:0.46875\n",
      "Training iteration 436 loss: 0.7159004807472229, ACC:0.46875\n",
      "Training iteration 437 loss: 0.7700464129447937, ACC:0.5\n",
      "Training iteration 438 loss: 0.7137413024902344, ACC:0.5\n",
      "Training iteration 439 loss: 0.7288020849227905, ACC:0.53125\n",
      "Training iteration 440 loss: 0.7823029160499573, ACC:0.453125\n",
      "Training iteration 441 loss: 0.7719470262527466, ACC:0.515625\n",
      "Training iteration 442 loss: 0.785810112953186, ACC:0.5\n",
      "Training iteration 443 loss: 0.7336755990982056, ACC:0.515625\n",
      "Training iteration 444 loss: 0.8386132121086121, ACC:0.46875\n",
      "Training iteration 445 loss: 0.7504835724830627, ACC:0.484375\n",
      "Training iteration 446 loss: 0.7908848524093628, ACC:0.484375\n",
      "Training iteration 447 loss: 0.7298855185508728, ACC:0.5\n",
      "Training iteration 448 loss: 0.7470213770866394, ACC:0.53125\n",
      "Training iteration 449 loss: 0.7022438049316406, ACC:0.453125\n",
      "Training iteration 450 loss: 0.7057237029075623, ACC:0.515625\n",
      "Validation iteration 451 loss: 0.694011390209198, ACC: 0.453125\n",
      "Validation iteration 452 loss: 0.6931859254837036, ACC: 0.5\n",
      "Validation iteration 453 loss: 0.6926355957984924, ACC: 0.53125\n",
      "Validation iteration 454 loss: 0.6931859850883484, ACC: 0.5\n",
      "Validation iteration 455 loss: 0.694286584854126, ACC: 0.4375\n",
      "Validation iteration 456 loss: 0.6929107904434204, ACC: 0.515625\n",
      "Validation iteration 457 loss: 0.6923604607582092, ACC: 0.546875\n",
      "Validation iteration 458 loss: 0.6940114498138428, ACC: 0.453125\n",
      "Validation iteration 459 loss: 0.6926355957984924, ACC: 0.53125\n",
      "Validation iteration 460 loss: 0.6948369145393372, ACC: 0.40625\n",
      "Validation iteration 461 loss: 0.6931859850883484, ACC: 0.5\n",
      "Validation iteration 462 loss: 0.692085325717926, ACC: 0.5625\n",
      "Validation iteration 463 loss: 0.694011390209198, ACC: 0.453125\n",
      "Validation iteration 464 loss: 0.69373619556427, ACC: 0.46875\n",
      "Validation iteration 465 loss: 0.6934610605239868, ACC: 0.484375\n",
      "Validation iteration 466 loss: 0.6951120495796204, ACC: 0.390625\n",
      "Validation iteration 467 loss: 0.6912597417831421, ACC: 0.609375\n",
      "Validation iteration 468 loss: 0.6915349960327148, ACC: 0.59375\n",
      "Validation iteration 469 loss: 0.6945616602897644, ACC: 0.421875\n",
      "Validation iteration 470 loss: 0.6929107904434204, ACC: 0.515625\n",
      "Validation iteration 471 loss: 0.6929107904434204, ACC: 0.515625\n",
      "Validation iteration 472 loss: 0.692360520362854, ACC: 0.546875\n",
      "Validation iteration 473 loss: 0.692085325717926, ACC: 0.5625\n",
      "Validation iteration 474 loss: 0.694011390209198, ACC: 0.453125\n",
      "Validation iteration 475 loss: 0.6940114498138428, ACC: 0.453125\n",
      "Validation iteration 476 loss: 0.6926356554031372, ACC: 0.53125\n",
      "Validation iteration 477 loss: 0.6931859254837036, ACC: 0.5\n",
      "Validation iteration 478 loss: 0.6929107904434204, ACC: 0.515625\n",
      "Validation iteration 479 loss: 0.6931859254837036, ACC: 0.5\n",
      "Validation iteration 480 loss: 0.6937362551689148, ACC: 0.46875\n",
      "Validation iteration 481 loss: 0.6929108500480652, ACC: 0.515625\n",
      "Validation iteration 482 loss: 0.6937363147735596, ACC: 0.46875\n",
      "Validation iteration 483 loss: 0.6923604607582092, ACC: 0.546875\n",
      "Validation iteration 484 loss: 0.6912598609924316, ACC: 0.609375\n",
      "Validation iteration 485 loss: 0.694286584854126, ACC: 0.4375\n",
      "Validation iteration 486 loss: 0.6926355957984924, ACC: 0.53125\n",
      "Validation iteration 487 loss: 0.6934611201286316, ACC: 0.484375\n",
      "Validation iteration 488 loss: 0.694011390209198, ACC: 0.453125\n",
      "Validation iteration 489 loss: 0.694011390209198, ACC: 0.453125\n",
      "Validation iteration 490 loss: 0.6926356554031372, ACC: 0.53125\n",
      "Validation iteration 491 loss: 0.6926356554031372, ACC: 0.53125\n",
      "Validation iteration 492 loss: 0.6934611201286316, ACC: 0.484375\n",
      "Validation iteration 493 loss: 0.6940114498138428, ACC: 0.453125\n",
      "Validation iteration 494 loss: 0.694011390209198, ACC: 0.453125\n",
      "Validation iteration 495 loss: 0.6951120495796204, ACC: 0.390625\n",
      "Validation iteration 496 loss: 0.6937362551689148, ACC: 0.46875\n",
      "Validation iteration 497 loss: 0.6931859850883484, ACC: 0.5\n",
      "Validation iteration 498 loss: 0.6937362551689148, ACC: 0.46875\n",
      "Validation iteration 499 loss: 0.6931859254837036, ACC: 0.5\n",
      "Validation iteration 500 loss: 0.6909847259521484, ACC: 0.625\n",
      "-- Epoch 4 done -- Train loss: 0.8309233303864797, train ACC: 0.49899305555555556, val loss: 0.6932464790344238, val ACC: 0.4965625\n",
      "<--- 9566.719621896744 seconds --->\n",
      "Training iteration 1 loss: 0.6926355361938477, ACC:0.53125\n",
      "Training iteration 2 loss: 0.6839104294776917, ACC:0.59375\n",
      "Training iteration 3 loss: 0.8677080273628235, ACC:0.34375\n",
      "Training iteration 4 loss: 0.8920693397521973, ACC:0.46875\n",
      "Training iteration 5 loss: 0.7731643319129944, ACC:0.46875\n",
      "Training iteration 6 loss: 0.8259822726249695, ACC:0.53125\n",
      "Training iteration 7 loss: 0.95698082447052, ACC:0.40625\n",
      "Training iteration 8 loss: 1.0242904424667358, ACC:0.453125\n",
      "Training iteration 9 loss: 0.6753385663032532, ACC:0.65625\n",
      "Training iteration 10 loss: 0.6870108246803284, ACC:0.578125\n",
      "Training iteration 11 loss: 0.6925157308578491, ACC:0.5625\n",
      "Training iteration 12 loss: 0.726706326007843, ACC:0.515625\n",
      "Training iteration 13 loss: 0.6895597577095032, ACC:0.546875\n",
      "Training iteration 14 loss: 0.7200402617454529, ACC:0.578125\n",
      "Training iteration 15 loss: 0.7520930171012878, ACC:0.4375\n",
      "Training iteration 16 loss: 0.8897755742073059, ACC:0.484375\n",
      "Training iteration 17 loss: 0.8000775575637817, ACC:0.40625\n",
      "Training iteration 18 loss: 1.0284725427627563, ACC:0.5\n",
      "Training iteration 19 loss: 0.8811984658241272, ACC:0.5\n",
      "Training iteration 20 loss: 0.8504728674888611, ACC:0.515625\n",
      "Training iteration 21 loss: 0.8495128154754639, ACC:0.578125\n",
      "Training iteration 22 loss: 0.6923152208328247, ACC:0.53125\n",
      "Training iteration 23 loss: 0.7647358775138855, ACC:0.578125\n",
      "Training iteration 24 loss: 0.7878594994544983, ACC:0.5\n",
      "Training iteration 25 loss: 0.9368434548377991, ACC:0.34375\n",
      "Training iteration 26 loss: 0.6951755285263062, ACC:0.515625\n",
      "Training iteration 27 loss: 0.7277754545211792, ACC:0.578125\n",
      "Training iteration 28 loss: 0.7586872577667236, ACC:0.453125\n",
      "Training iteration 29 loss: 0.9211156964302063, ACC:0.4375\n",
      "Training iteration 30 loss: 0.7290153503417969, ACC:0.421875\n",
      "Training iteration 31 loss: 0.9311800599098206, ACC:0.53125\n",
      "Training iteration 32 loss: 0.884259819984436, ACC:0.453125\n",
      "Training iteration 33 loss: 0.9662661552429199, ACC:0.5\n",
      "Training iteration 34 loss: 1.0511995553970337, ACC:0.4375\n",
      "Training iteration 35 loss: 0.9008336067199707, ACC:0.5\n",
      "Training iteration 36 loss: 1.1980762481689453, ACC:0.40625\n",
      "Training iteration 37 loss: 0.9076430797576904, ACC:0.453125\n",
      "Training iteration 38 loss: 0.8359580039978027, ACC:0.578125\n",
      "Training iteration 39 loss: 0.6875218152999878, ACC:0.625\n",
      "Training iteration 40 loss: 0.7300196290016174, ACC:0.46875\n",
      "Training iteration 41 loss: 0.6865777969360352, ACC:0.5625\n",
      "Training iteration 42 loss: 0.7426331639289856, ACC:0.546875\n",
      "Training iteration 43 loss: 0.6810408234596252, ACC:0.578125\n",
      "Training iteration 44 loss: 0.6941608786582947, ACC:0.515625\n",
      "Training iteration 45 loss: 0.7103725075721741, ACC:0.515625\n",
      "Training iteration 46 loss: 0.6929052472114563, ACC:0.578125\n",
      "Training iteration 47 loss: 0.8379423022270203, ACC:0.46875\n",
      "Training iteration 48 loss: 0.6902095675468445, ACC:0.5625\n",
      "Training iteration 49 loss: 0.9662830829620361, ACC:0.453125\n",
      "Training iteration 50 loss: 0.6932781338691711, ACC:0.484375\n",
      "Training iteration 51 loss: 0.835830569267273, ACC:0.5\n",
      "Training iteration 52 loss: 0.6674339771270752, ACC:0.640625\n",
      "Training iteration 53 loss: 0.6926636099815369, ACC:0.515625\n",
      "Training iteration 54 loss: 0.6995833516120911, ACC:0.4375\n",
      "Training iteration 55 loss: 0.6878906488418579, ACC:0.5625\n",
      "Training iteration 56 loss: 0.7778796553611755, ACC:0.421875\n",
      "Training iteration 57 loss: 0.7458493113517761, ACC:0.546875\n",
      "Training iteration 58 loss: 0.7039743065834045, ACC:0.609375\n",
      "Training iteration 59 loss: 0.7272962331771851, ACC:0.359375\n",
      "Training iteration 60 loss: 1.0714178085327148, ACC:0.515625\n",
      "Training iteration 61 loss: 0.7889856696128845, ACC:0.5625\n",
      "Training iteration 62 loss: 0.8097027540206909, ACC:0.5\n",
      "Training iteration 63 loss: 0.7820725440979004, ACC:0.578125\n",
      "Training iteration 64 loss: 0.6905246376991272, ACC:0.625\n",
      "Training iteration 65 loss: 0.6888068318367004, ACC:0.546875\n",
      "Training iteration 66 loss: 0.6931893825531006, ACC:0.546875\n",
      "Training iteration 67 loss: 0.7174692153930664, ACC:0.390625\n",
      "Training iteration 68 loss: 0.9020366072654724, ACC:0.453125\n",
      "Training iteration 69 loss: 0.698316752910614, ACC:0.4375\n",
      "Training iteration 70 loss: 0.9018858075141907, ACC:0.53125\n",
      "Training iteration 71 loss: 0.7310027480125427, ACC:0.546875\n",
      "Training iteration 72 loss: 0.8308429718017578, ACC:0.453125\n",
      "Training iteration 73 loss: 0.7544217109680176, ACC:0.4375\n",
      "Training iteration 74 loss: 0.9085766673088074, ACC:0.46875\n",
      "Training iteration 75 loss: 0.7500093579292297, ACC:0.453125\n",
      "Training iteration 76 loss: 0.9670102000236511, ACC:0.46875\n",
      "Training iteration 77 loss: 0.7660873532295227, ACC:0.46875\n",
      "Training iteration 78 loss: 0.9293112754821777, ACC:0.5\n",
      "Training iteration 79 loss: 0.88381028175354, ACC:0.4375\n",
      "Training iteration 80 loss: 0.9435023665428162, ACC:0.515625\n",
      "Training iteration 81 loss: 1.0500502586364746, ACC:0.453125\n",
      "Training iteration 82 loss: 0.766025185585022, ACC:0.578125\n",
      "Training iteration 83 loss: 1.4148062467575073, ACC:0.421875\n",
      "Training iteration 84 loss: 0.7362822890281677, ACC:0.40625\n",
      "Training iteration 85 loss: 0.8281528353691101, ACC:0.5625\n",
      "Training iteration 86 loss: 0.7624943256378174, ACC:0.453125\n",
      "Training iteration 87 loss: 0.9845967888832092, ACC:0.484375\n",
      "Training iteration 88 loss: 0.7841430306434631, ACC:0.515625\n",
      "Training iteration 89 loss: 0.8362756967544556, ACC:0.5\n",
      "Training iteration 90 loss: 0.8493198156356812, ACC:0.484375\n",
      "Training iteration 91 loss: 0.7714304327964783, ACC:0.515625\n",
      "Training iteration 92 loss: 0.928534984588623, ACC:0.453125\n",
      "Training iteration 93 loss: 0.7231826782226562, ACC:0.5625\n",
      "Training iteration 94 loss: 0.9350107908248901, ACC:0.546875\n",
      "Training iteration 95 loss: 0.7072759866714478, ACC:0.484375\n",
      "Training iteration 96 loss: 0.9648319482803345, ACC:0.515625\n",
      "Training iteration 97 loss: 0.8065668940544128, ACC:0.5\n",
      "Training iteration 98 loss: 0.8443471193313599, ACC:0.53125\n",
      "Training iteration 99 loss: 1.0934349298477173, ACC:0.390625\n",
      "Training iteration 100 loss: 1.156838059425354, ACC:0.359375\n",
      "Training iteration 101 loss: 0.704900324344635, ACC:0.578125\n",
      "Training iteration 102 loss: 0.6639353632926941, ACC:0.625\n",
      "Training iteration 103 loss: 1.0935603380203247, ACC:0.453125\n",
      "Training iteration 104 loss: 0.685319185256958, ACC:0.5625\n",
      "Training iteration 105 loss: 1.126964807510376, ACC:0.5\n",
      "Training iteration 106 loss: 0.7645149230957031, ACC:0.5\n",
      "Training iteration 107 loss: 1.0406267642974854, ACC:0.484375\n",
      "Training iteration 108 loss: 0.8771435022354126, ACC:0.5\n",
      "Training iteration 109 loss: 0.8219589591026306, ACC:0.53125\n",
      "Training iteration 110 loss: 1.1045112609863281, ACC:0.4375\n",
      "Training iteration 111 loss: 0.7315050363540649, ACC:0.5625\n",
      "Training iteration 112 loss: 1.0664068460464478, ACC:0.546875\n",
      "Training iteration 113 loss: 0.7381370067596436, ACC:0.515625\n",
      "Training iteration 114 loss: 1.0657386779785156, ACC:0.46875\n",
      "Training iteration 115 loss: 0.9420772790908813, ACC:0.390625\n",
      "Training iteration 116 loss: 0.997056782245636, ACC:0.578125\n",
      "Training iteration 117 loss: 1.5493749380111694, ACC:0.4375\n",
      "Training iteration 118 loss: 0.7399304509162903, ACC:0.46875\n",
      "Training iteration 119 loss: 1.3744032382965088, ACC:0.421875\n",
      "Training iteration 120 loss: 0.7172812223434448, ACC:0.421875\n",
      "Training iteration 121 loss: 1.30123770236969, ACC:0.515625\n",
      "Training iteration 122 loss: 1.1935855150222778, ACC:0.46875\n",
      "Training iteration 123 loss: 1.0319010019302368, ACC:0.4375\n",
      "Training iteration 124 loss: 0.9007529020309448, ACC:0.59375\n",
      "Training iteration 125 loss: 0.6966350674629211, ACC:0.515625\n",
      "Training iteration 126 loss: 1.1091015338897705, ACC:0.40625\n",
      "Training iteration 127 loss: 0.6979908347129822, ACC:0.46875\n",
      "Training iteration 128 loss: 0.8855667114257812, ACC:0.5625\n",
      "Training iteration 129 loss: 0.8769429326057434, ACC:0.484375\n",
      "Training iteration 130 loss: 0.8166587352752686, ACC:0.546875\n",
      "Training iteration 131 loss: 0.9282870292663574, ACC:0.5625\n",
      "Training iteration 132 loss: 0.6963915824890137, ACC:0.484375\n",
      "Training iteration 133 loss: 0.9872347116470337, ACC:0.515625\n",
      "Training iteration 134 loss: 0.7663684487342834, ACC:0.53125\n",
      "Training iteration 135 loss: 0.8740882277488708, ACC:0.46875\n",
      "Training iteration 136 loss: 0.7447822690010071, ACC:0.546875\n",
      "Training iteration 137 loss: 0.7306728959083557, ACC:0.484375\n",
      "Training iteration 138 loss: 0.7013883590698242, ACC:0.578125\n",
      "Training iteration 139 loss: 0.7004311084747314, ACC:0.5\n",
      "Training iteration 140 loss: 0.752943217754364, ACC:0.484375\n",
      "Training iteration 141 loss: 0.715719997882843, ACC:0.40625\n",
      "Training iteration 142 loss: 1.009912133216858, ACC:0.390625\n",
      "Training iteration 143 loss: 0.7068471908569336, ACC:0.484375\n",
      "Training iteration 144 loss: 0.8198167085647583, ACC:0.5\n",
      "Training iteration 145 loss: 0.6921321749687195, ACC:0.5625\n",
      "Training iteration 146 loss: 1.0231703519821167, ACC:0.421875\n",
      "Training iteration 147 loss: 0.6976838707923889, ACC:0.484375\n",
      "Training iteration 148 loss: 0.9016926884651184, ACC:0.453125\n",
      "Training iteration 149 loss: 0.702426552772522, ACC:0.46875\n",
      "Training iteration 150 loss: 0.7231361269950867, ACC:0.5625\n",
      "Training iteration 151 loss: 0.6964839100837708, ACC:0.546875\n",
      "Training iteration 152 loss: 0.7296798229217529, ACC:0.4375\n",
      "Training iteration 153 loss: 0.6941090822219849, ACC:0.46875\n",
      "Training iteration 154 loss: 0.691386878490448, ACC:0.53125\n",
      "Training iteration 155 loss: 0.6724163889884949, ACC:0.625\n",
      "Training iteration 156 loss: 0.8348003029823303, ACC:0.4375\n",
      "Training iteration 157 loss: 0.7311570048332214, ACC:0.484375\n",
      "Training iteration 158 loss: 0.7766761779785156, ACC:0.46875\n",
      "Training iteration 159 loss: 0.7154868841171265, ACC:0.53125\n",
      "Training iteration 160 loss: 0.7693864107131958, ACC:0.53125\n",
      "Training iteration 161 loss: 0.7009848952293396, ACC:0.4375\n",
      "Training iteration 162 loss: 0.6938482522964478, ACC:0.546875\n",
      "Training iteration 163 loss: 0.6944057941436768, ACC:0.53125\n",
      "Training iteration 164 loss: 0.6944115161895752, ACC:0.5\n",
      "Training iteration 165 loss: 0.6936073303222656, ACC:0.53125\n",
      "Training iteration 166 loss: 0.6963087320327759, ACC:0.5\n",
      "Training iteration 167 loss: 0.6960381865501404, ACC:0.515625\n",
      "Training iteration 168 loss: 0.6982489824295044, ACC:0.515625\n",
      "Training iteration 169 loss: 0.6980711817741394, ACC:0.4375\n",
      "Training iteration 170 loss: 0.6920421123504639, ACC:0.53125\n",
      "Training iteration 171 loss: 0.7239830493927002, ACC:0.4375\n",
      "Training iteration 172 loss: 0.770291268825531, ACC:0.453125\n",
      "Training iteration 173 loss: 0.693286120891571, ACC:0.46875\n",
      "Training iteration 174 loss: 0.7324231266975403, ACC:0.546875\n",
      "Training iteration 175 loss: 0.6910018920898438, ACC:0.5625\n",
      "Training iteration 176 loss: 0.6789684891700745, ACC:0.59375\n",
      "Training iteration 177 loss: 0.9669369459152222, ACC:0.390625\n",
      "Training iteration 178 loss: 0.7823149561882019, ACC:0.46875\n",
      "Training iteration 179 loss: 0.7601075768470764, ACC:0.53125\n",
      "Training iteration 180 loss: 0.7384475469589233, ACC:0.375\n",
      "Training iteration 181 loss: 0.6962252259254456, ACC:0.484375\n",
      "Training iteration 182 loss: 0.6957935094833374, ACC:0.5\n",
      "Training iteration 183 loss: 0.6886444091796875, ACC:0.5625\n",
      "Training iteration 184 loss: 0.6531009078025818, ACC:0.640625\n",
      "Training iteration 185 loss: 0.8348669409751892, ACC:0.484375\n",
      "Training iteration 186 loss: 0.6690292358398438, ACC:0.609375\n",
      "Training iteration 187 loss: 1.1520434617996216, ACC:0.4375\n",
      "Training iteration 188 loss: 0.6969476938247681, ACC:0.484375\n",
      "Training iteration 189 loss: 1.0852785110473633, ACC:0.40625\n",
      "Training iteration 190 loss: 0.7292282581329346, ACC:0.421875\n",
      "Training iteration 191 loss: 0.743088960647583, ACC:0.53125\n",
      "Training iteration 192 loss: 0.6946606636047363, ACC:0.484375\n",
      "Training iteration 193 loss: 0.6848329901695251, ACC:0.609375\n",
      "Training iteration 194 loss: 0.7794935703277588, ACC:0.53125\n",
      "Training iteration 195 loss: 0.666088342666626, ACC:0.625\n",
      "Training iteration 196 loss: 1.1227648258209229, ACC:0.46875\n",
      "Training iteration 197 loss: 0.6763704419136047, ACC:0.59375\n",
      "Training iteration 198 loss: 0.7409847378730774, ACC:0.5625\n",
      "Training iteration 199 loss: 0.7337671518325806, ACC:0.5625\n",
      "Training iteration 200 loss: 0.6964337825775146, ACC:0.515625\n",
      "Training iteration 201 loss: 0.7836783528327942, ACC:0.515625\n",
      "Training iteration 202 loss: 0.6987078785896301, ACC:0.421875\n",
      "Training iteration 203 loss: 0.8145230412483215, ACC:0.578125\n",
      "Training iteration 204 loss: 0.8801847100257874, ACC:0.453125\n",
      "Training iteration 205 loss: 0.9197983145713806, ACC:0.5\n",
      "Training iteration 206 loss: 0.8726862668991089, ACC:0.53125\n",
      "Training iteration 207 loss: 0.7489566206932068, ACC:0.484375\n",
      "Training iteration 208 loss: 0.8436771035194397, ACC:0.515625\n",
      "Training iteration 209 loss: 0.6686467528343201, ACC:0.6875\n",
      "Training iteration 210 loss: 1.2786693572998047, ACC:0.515625\n",
      "Training iteration 211 loss: 0.8488814234733582, ACC:0.578125\n",
      "Training iteration 212 loss: 0.7164315581321716, ACC:0.609375\n",
      "Training iteration 213 loss: 1.1550660133361816, ACC:0.5625\n",
      "Training iteration 214 loss: 0.9309523105621338, ACC:0.390625\n",
      "Training iteration 215 loss: 1.3306092023849487, ACC:0.546875\n",
      "Training iteration 216 loss: 1.440070629119873, ACC:0.578125\n",
      "Training iteration 217 loss: 0.8605843186378479, ACC:0.4375\n",
      "Training iteration 218 loss: 1.7799848318099976, ACC:0.46875\n",
      "Training iteration 219 loss: 2.0746963024139404, ACC:0.40625\n",
      "Training iteration 220 loss: 0.7458337545394897, ACC:0.46875\n",
      "Training iteration 221 loss: 1.333343744277954, ACC:0.5625\n",
      "Training iteration 222 loss: 1.1882280111312866, ACC:0.515625\n",
      "Training iteration 223 loss: 0.7717720866203308, ACC:0.578125\n",
      "Training iteration 224 loss: 1.1915900707244873, ACC:0.609375\n",
      "Training iteration 225 loss: 1.1453392505645752, ACC:0.484375\n",
      "Training iteration 226 loss: 0.9546626210212708, ACC:0.546875\n",
      "Training iteration 227 loss: 1.6255680322647095, ACC:0.453125\n",
      "Training iteration 228 loss: 0.7029730677604675, ACC:0.5\n",
      "Training iteration 229 loss: 1.554829478263855, ACC:0.421875\n",
      "Training iteration 230 loss: 0.8845564723014832, ACC:0.53125\n",
      "Training iteration 231 loss: 0.8820165991783142, ACC:0.5625\n",
      "Training iteration 232 loss: 1.1594414710998535, ACC:0.546875\n",
      "Training iteration 233 loss: 0.6934628486633301, ACC:0.546875\n",
      "Training iteration 234 loss: 1.0329927206039429, ACC:0.5\n",
      "Training iteration 235 loss: 0.7870662212371826, ACC:0.546875\n",
      "Training iteration 236 loss: 0.7580388188362122, ACC:0.546875\n",
      "Training iteration 237 loss: 0.9871309399604797, ACC:0.484375\n",
      "Training iteration 238 loss: 0.7190299034118652, ACC:0.484375\n",
      "Training iteration 239 loss: 0.8536614179611206, ACC:0.53125\n",
      "Training iteration 240 loss: 0.6895974278450012, ACC:0.546875\n",
      "Training iteration 241 loss: 0.6976585388183594, ACC:0.59375\n",
      "Training iteration 242 loss: 0.8466866612434387, ACC:0.484375\n",
      "Training iteration 243 loss: 0.7736072540283203, ACC:0.4375\n",
      "Training iteration 244 loss: 0.7449598908424377, ACC:0.484375\n",
      "Training iteration 245 loss: 0.7154077291488647, ACC:0.53125\n",
      "Training iteration 246 loss: 0.6861427426338196, ACC:0.609375\n",
      "Training iteration 247 loss: 0.7229152917861938, ACC:0.484375\n",
      "Training iteration 248 loss: 0.6848600506782532, ACC:0.609375\n",
      "Training iteration 249 loss: 0.981473982334137, ACC:0.4375\n",
      "Training iteration 250 loss: 0.8290473818778992, ACC:0.421875\n",
      "Training iteration 251 loss: 0.7274411916732788, ACC:0.546875\n",
      "Training iteration 252 loss: 0.7042021751403809, ACC:0.484375\n",
      "Training iteration 253 loss: 0.7081452012062073, ACC:0.546875\n",
      "Training iteration 254 loss: 0.6981180906295776, ACC:0.5\n",
      "Training iteration 255 loss: 0.7176214456558228, ACC:0.515625\n",
      "Training iteration 256 loss: 0.725125253200531, ACC:0.453125\n",
      "Training iteration 257 loss: 0.7174559235572815, ACC:0.5625\n",
      "Training iteration 258 loss: 0.7204329371452332, ACC:0.578125\n",
      "Training iteration 259 loss: 0.694790244102478, ACC:0.46875\n",
      "Training iteration 260 loss: 0.7564125061035156, ACC:0.5625\n",
      "Training iteration 261 loss: 0.7345567345619202, ACC:0.53125\n",
      "Training iteration 262 loss: 0.7181570529937744, ACC:0.53125\n",
      "Training iteration 263 loss: 0.9068470597267151, ACC:0.390625\n",
      "Training iteration 264 loss: 0.9388244152069092, ACC:0.421875\n",
      "Training iteration 265 loss: 0.6977123618125916, ACC:0.5625\n",
      "Training iteration 266 loss: 0.7407493591308594, ACC:0.46875\n",
      "Training iteration 267 loss: 0.7343107461929321, ACC:0.421875\n",
      "Training iteration 268 loss: 0.8439456224441528, ACC:0.46875\n",
      "Training iteration 269 loss: 0.7033867835998535, ACC:0.5\n",
      "Training iteration 270 loss: 0.7722379565238953, ACC:0.515625\n",
      "Training iteration 271 loss: 0.7028689384460449, ACC:0.546875\n",
      "Training iteration 272 loss: 0.6453129649162292, ACC:0.671875\n",
      "Training iteration 273 loss: 1.153071403503418, ACC:0.4375\n",
      "Training iteration 274 loss: 0.6945105791091919, ACC:0.46875\n",
      "Training iteration 275 loss: 0.8680726885795593, ACC:0.53125\n",
      "Training iteration 276 loss: 0.7885066866874695, ACC:0.421875\n",
      "Training iteration 277 loss: 0.9188798069953918, ACC:0.5625\n",
      "Training iteration 278 loss: 1.1397716999053955, ACC:0.4375\n",
      "Training iteration 279 loss: 0.8229976892471313, ACC:0.546875\n",
      "Training iteration 280 loss: 1.055152177810669, ACC:0.578125\n",
      "Training iteration 281 loss: 0.6810959577560425, ACC:0.609375\n",
      "Training iteration 282 loss: 0.7726377248764038, ACC:0.5625\n",
      "Training iteration 283 loss: 1.1037647724151611, ACC:0.375\n",
      "Training iteration 284 loss: 0.8917161822319031, ACC:0.53125\n",
      "Training iteration 285 loss: 1.189581274986267, ACC:0.46875\n",
      "Training iteration 286 loss: 0.790084719657898, ACC:0.375\n",
      "Training iteration 287 loss: 0.7867961525917053, ACC:0.515625\n",
      "Training iteration 288 loss: 0.6790668964385986, ACC:0.609375\n",
      "Training iteration 289 loss: 0.9536318778991699, ACC:0.53125\n",
      "Training iteration 290 loss: 0.8119351267814636, ACC:0.4375\n",
      "Training iteration 291 loss: 0.9212243556976318, ACC:0.578125\n",
      "Training iteration 292 loss: 1.1642705202102661, ACC:0.5\n",
      "Training iteration 293 loss: 0.7172743678092957, ACC:0.515625\n",
      "Training iteration 294 loss: 1.2149702310562134, ACC:0.46875\n",
      "Training iteration 295 loss: 0.7593337297439575, ACC:0.390625\n",
      "Training iteration 296 loss: 1.4157980680465698, ACC:0.484375\n",
      "Training iteration 297 loss: 1.2524656057357788, ACC:0.453125\n",
      "Training iteration 298 loss: 1.1677788496017456, ACC:0.375\n",
      "Training iteration 299 loss: 1.0677919387817383, ACC:0.4375\n",
      "Training iteration 300 loss: 0.9583534002304077, ACC:0.4375\n",
      "Training iteration 301 loss: 0.9188073873519897, ACC:0.484375\n",
      "Training iteration 302 loss: 0.7374672889709473, ACC:0.546875\n",
      "Training iteration 303 loss: 1.0134172439575195, ACC:0.5\n",
      "Training iteration 304 loss: 0.6928426623344421, ACC:0.515625\n",
      "Training iteration 305 loss: 0.9444217681884766, ACC:0.53125\n",
      "Training iteration 306 loss: 0.7721647024154663, ACC:0.515625\n",
      "Training iteration 307 loss: 0.7850508093833923, ACC:0.5625\n",
      "Training iteration 308 loss: 0.8507494926452637, ACC:0.578125\n",
      "Training iteration 309 loss: 0.6946484446525574, ACC:0.5\n",
      "Training iteration 310 loss: 1.0960183143615723, ACC:0.375\n",
      "Training iteration 311 loss: 0.6911981105804443, ACC:0.546875\n",
      "Training iteration 312 loss: 0.7916074991226196, ACC:0.625\n",
      "Training iteration 313 loss: 0.8653527498245239, ACC:0.515625\n",
      "Training iteration 314 loss: 0.9125561714172363, ACC:0.390625\n",
      "Training iteration 315 loss: 0.7251793146133423, ACC:0.515625\n",
      "Training iteration 316 loss: 0.696636974811554, ACC:0.578125\n",
      "Training iteration 317 loss: 0.9729957580566406, ACC:0.40625\n",
      "Training iteration 318 loss: 0.8177610635757446, ACC:0.484375\n",
      "Training iteration 319 loss: 0.855044960975647, ACC:0.5\n",
      "Training iteration 320 loss: 0.728856086730957, ACC:0.5\n",
      "Training iteration 321 loss: 0.7206597328186035, ACC:0.609375\n",
      "Training iteration 322 loss: 0.6907209753990173, ACC:0.578125\n",
      "Training iteration 323 loss: 0.726309597492218, ACC:0.453125\n",
      "Training iteration 324 loss: 0.7004653811454773, ACC:0.5\n",
      "Training iteration 325 loss: 0.6997476816177368, ACC:0.515625\n",
      "Training iteration 326 loss: 0.73211270570755, ACC:0.4375\n",
      "Training iteration 327 loss: 0.7828343510627747, ACC:0.453125\n",
      "Training iteration 328 loss: 0.7000443935394287, ACC:0.4375\n",
      "Training iteration 329 loss: 0.7781698107719421, ACC:0.546875\n",
      "Training iteration 330 loss: 0.7602468132972717, ACC:0.484375\n",
      "Training iteration 331 loss: 0.8204893469810486, ACC:0.484375\n",
      "Training iteration 332 loss: 0.7981284856796265, ACC:0.4375\n",
      "Training iteration 333 loss: 0.9040961265563965, ACC:0.46875\n",
      "Training iteration 334 loss: 0.8079657554626465, ACC:0.4375\n",
      "Training iteration 335 loss: 0.8595556616783142, ACC:0.546875\n",
      "Training iteration 336 loss: 0.9964202046394348, ACC:0.46875\n",
      "Training iteration 337 loss: 0.9237310290336609, ACC:0.375\n",
      "Training iteration 338 loss: 0.7343788146972656, ACC:0.515625\n",
      "Training iteration 339 loss: 0.7965644598007202, ACC:0.421875\n",
      "Training iteration 340 loss: 0.7019186019897461, ACC:0.4375\n",
      "Training iteration 341 loss: 0.852647066116333, ACC:0.46875\n",
      "Training iteration 342 loss: 0.6930274367332458, ACC:0.515625\n",
      "Training iteration 343 loss: 0.7970572113990784, ACC:0.46875\n",
      "Training iteration 344 loss: 0.6942787766456604, ACC:0.453125\n",
      "Training iteration 345 loss: 0.7948459982872009, ACC:0.53125\n",
      "Training iteration 346 loss: 0.7870146036148071, ACC:0.375\n",
      "Training iteration 347 loss: 0.9688975811004639, ACC:0.546875\n",
      "Training iteration 348 loss: 0.9130397439002991, ACC:0.546875\n",
      "Training iteration 349 loss: 0.7824194431304932, ACC:0.4375\n",
      "Training iteration 350 loss: 0.9379898905754089, ACC:0.40625\n",
      "Training iteration 351 loss: 0.7012760639190674, ACC:0.609375\n",
      "Training iteration 352 loss: 1.155043125152588, ACC:0.484375\n",
      "Training iteration 353 loss: 0.6914998888969421, ACC:0.546875\n",
      "Training iteration 354 loss: 0.8906186819076538, ACC:0.53125\n",
      "Training iteration 355 loss: 0.801953136920929, ACC:0.453125\n",
      "Training iteration 356 loss: 0.9076964855194092, ACC:0.53125\n",
      "Training iteration 357 loss: 0.9449435472488403, ACC:0.5\n",
      "Training iteration 358 loss: 0.724373996257782, ACC:0.5625\n",
      "Training iteration 359 loss: 1.0430892705917358, ACC:0.53125\n",
      "Training iteration 360 loss: 0.7406392693519592, ACC:0.453125\n",
      "Training iteration 361 loss: 1.1061697006225586, ACC:0.515625\n",
      "Training iteration 362 loss: 0.9846696853637695, ACC:0.5\n",
      "Training iteration 363 loss: 0.9143279194831848, ACC:0.453125\n",
      "Training iteration 364 loss: 0.9749414324760437, ACC:0.46875\n",
      "Training iteration 365 loss: 0.8058568835258484, ACC:0.453125\n",
      "Training iteration 366 loss: 0.8872681260108948, ACC:0.453125\n",
      "Training iteration 367 loss: 0.7410538196563721, ACC:0.53125\n",
      "Training iteration 368 loss: 0.9260525703430176, ACC:0.484375\n",
      "Training iteration 369 loss: 0.692514181137085, ACC:0.546875\n",
      "Training iteration 370 loss: 1.006883144378662, ACC:0.484375\n",
      "Training iteration 371 loss: 0.7166582345962524, ACC:0.40625\n",
      "Training iteration 372 loss: 1.1624079942703247, ACC:0.5\n",
      "Training iteration 373 loss: 0.8600640892982483, ACC:0.53125\n",
      "Training iteration 374 loss: 0.9075605273246765, ACC:0.46875\n",
      "Training iteration 375 loss: 0.8178877234458923, ACC:0.5625\n",
      "Training iteration 376 loss: 0.6821131706237793, ACC:0.578125\n",
      "Training iteration 377 loss: 0.8813092708587646, ACC:0.59375\n",
      "Training iteration 378 loss: 0.7911062836647034, ACC:0.578125\n",
      "Training iteration 379 loss: 0.7621704339981079, ACC:0.46875\n",
      "Training iteration 380 loss: 0.7757778763771057, ACC:0.546875\n",
      "Training iteration 381 loss: 0.6936415433883667, ACC:0.484375\n",
      "Training iteration 382 loss: 0.76042640209198, ACC:0.5625\n",
      "Training iteration 383 loss: 0.7820071578025818, ACC:0.46875\n",
      "Training iteration 384 loss: 0.7430242896080017, ACC:0.578125\n",
      "Training iteration 385 loss: 1.0380117893218994, ACC:0.4375\n",
      "Training iteration 386 loss: 0.8618177175521851, ACC:0.40625\n",
      "Training iteration 387 loss: 0.8404162526130676, ACC:0.421875\n",
      "Training iteration 388 loss: 0.8380041718482971, ACC:0.515625\n",
      "Training iteration 389 loss: 0.8954834938049316, ACC:0.46875\n",
      "Training iteration 390 loss: 0.8908131718635559, ACC:0.40625\n",
      "Training iteration 391 loss: 0.7745933532714844, ACC:0.4375\n",
      "Training iteration 392 loss: 0.9054960012435913, ACC:0.46875\n",
      "Training iteration 393 loss: 0.7060678601264954, ACC:0.5625\n",
      "Training iteration 394 loss: 0.745390772819519, ACC:0.46875\n",
      "Training iteration 395 loss: 0.725234866142273, ACC:0.484375\n",
      "Training iteration 396 loss: 0.6448231935501099, ACC:0.65625\n",
      "Training iteration 397 loss: 1.1872313022613525, ACC:0.375\n",
      "Training iteration 398 loss: 0.7475523948669434, ACC:0.515625\n",
      "Training iteration 399 loss: 1.066288709640503, ACC:0.46875\n",
      "Training iteration 400 loss: 0.6913222670555115, ACC:0.53125\n",
      "Training iteration 401 loss: 0.956595778465271, ACC:0.546875\n",
      "Training iteration 402 loss: 0.7033588290214539, ACC:0.609375\n",
      "Training iteration 403 loss: 0.7765703201293945, ACC:0.421875\n",
      "Training iteration 404 loss: 0.7403573989868164, ACC:0.4375\n",
      "Training iteration 405 loss: 0.7644752264022827, ACC:0.53125\n",
      "Training iteration 406 loss: 0.8063492178916931, ACC:0.453125\n",
      "Training iteration 407 loss: 0.7049819231033325, ACC:0.609375\n",
      "Training iteration 408 loss: 1.0582983493804932, ACC:0.484375\n",
      "Training iteration 409 loss: 0.6891512274742126, ACC:0.546875\n",
      "Training iteration 410 loss: 1.0099198818206787, ACC:0.546875\n",
      "Training iteration 411 loss: 0.9205583930015564, ACC:0.421875\n",
      "Training iteration 412 loss: 1.291121482849121, ACC:0.421875\n",
      "Training iteration 413 loss: 0.9033784866333008, ACC:0.53125\n",
      "Training iteration 414 loss: 0.9163181185722351, ACC:0.40625\n",
      "Training iteration 415 loss: 0.7385263442993164, ACC:0.5625\n",
      "Training iteration 416 loss: 0.689635694026947, ACC:0.546875\n",
      "Training iteration 417 loss: 0.9570702910423279, ACC:0.421875\n",
      "Training iteration 418 loss: 0.7335866093635559, ACC:0.46875\n",
      "Training iteration 419 loss: 0.7517212629318237, ACC:0.546875\n",
      "Training iteration 420 loss: 0.6931233406066895, ACC:0.515625\n",
      "Training iteration 421 loss: 0.7860450148582458, ACC:0.453125\n",
      "Training iteration 422 loss: 0.6795041561126709, ACC:0.640625\n",
      "Training iteration 423 loss: 1.0279884338378906, ACC:0.484375\n",
      "Training iteration 424 loss: 0.7075114250183105, ACC:0.53125\n",
      "Training iteration 425 loss: 0.8628591895103455, ACC:0.5\n",
      "Training iteration 426 loss: 0.817534327507019, ACC:0.4375\n",
      "Training iteration 427 loss: 0.9148537516593933, ACC:0.5\n",
      "Training iteration 428 loss: 0.8757051229476929, ACC:0.484375\n",
      "Training iteration 429 loss: 0.786420464515686, ACC:0.53125\n",
      "Training iteration 430 loss: 0.9858509302139282, ACC:0.484375\n",
      "Training iteration 431 loss: 0.7376468777656555, ACC:0.453125\n",
      "Training iteration 432 loss: 0.8372529149055481, ACC:0.5\n",
      "Training iteration 433 loss: 0.6978650093078613, ACC:0.46875\n",
      "Training iteration 434 loss: 0.7787557244300842, ACC:0.484375\n",
      "Training iteration 435 loss: 0.6966597437858582, ACC:0.453125\n",
      "Training iteration 436 loss: 0.7156919836997986, ACC:0.484375\n",
      "Training iteration 437 loss: 0.6852595210075378, ACC:0.578125\n",
      "Training iteration 438 loss: 0.8068230748176575, ACC:0.484375\n",
      "Training iteration 439 loss: 0.6938401460647583, ACC:0.484375\n",
      "Training iteration 440 loss: 0.7323372960090637, ACC:0.53125\n",
      "Training iteration 441 loss: 0.7444446682929993, ACC:0.375\n",
      "Training iteration 442 loss: 0.8823661208152771, ACC:0.546875\n",
      "Training iteration 443 loss: 0.7660325169563293, ACC:0.578125\n",
      "Training iteration 444 loss: 0.7193669080734253, ACC:0.5\n",
      "Training iteration 445 loss: 0.9479978680610657, ACC:0.40625\n",
      "Training iteration 446 loss: 0.6956244111061096, ACC:0.59375\n",
      "Training iteration 447 loss: 0.9792156219482422, ACC:0.546875\n",
      "Training iteration 448 loss: 0.7000795602798462, ACC:0.546875\n",
      "Training iteration 449 loss: 0.7789512872695923, ACC:0.578125\n",
      "Training iteration 450 loss: 0.9141128659248352, ACC:0.5\n",
      "Validation iteration 451 loss: 0.7054720520973206, ACC: 0.5625\n",
      "Validation iteration 452 loss: 0.7571032643318176, ACC: 0.484375\n",
      "Validation iteration 453 loss: 0.7674294710159302, ACC: 0.46875\n",
      "Validation iteration 454 loss: 0.6744931936264038, ACC: 0.609375\n",
      "Validation iteration 455 loss: 0.7571032643318176, ACC: 0.484375\n",
      "Validation iteration 456 loss: 0.7571033239364624, ACC: 0.484375\n",
      "Validation iteration 457 loss: 0.7261244654655457, ACC: 0.53125\n",
      "Validation iteration 458 loss: 0.7571033239364624, ACC: 0.484375\n",
      "Validation iteration 459 loss: 0.7364507913589478, ACC: 0.515625\n",
      "Validation iteration 460 loss: 0.7364507913589478, ACC: 0.515625\n",
      "Validation iteration 461 loss: 0.7777557969093323, ACC: 0.453125\n",
      "Validation iteration 462 loss: 0.7467769980430603, ACC: 0.5\n",
      "Validation iteration 463 loss: 0.7157981991767883, ACC: 0.546875\n",
      "Validation iteration 464 loss: 0.7984083294868469, ACC: 0.421875\n",
      "Validation iteration 465 loss: 0.6951457262039185, ACC: 0.578125\n",
      "Validation iteration 466 loss: 0.767429530620575, ACC: 0.46875\n",
      "Validation iteration 467 loss: 0.767429530620575, ACC: 0.46875\n",
      "Validation iteration 468 loss: 0.7054719924926758, ACC: 0.5625\n",
      "Validation iteration 469 loss: 0.7467770576477051, ACC: 0.5\n",
      "Validation iteration 470 loss: 0.7777557969093323, ACC: 0.453125\n",
      "Validation iteration 471 loss: 0.8087344765663147, ACC: 0.40625\n",
      "Validation iteration 472 loss: 0.6435144543647766, ACC: 0.65625\n",
      "Validation iteration 473 loss: 0.7984082698822021, ACC: 0.421875\n",
      "Validation iteration 474 loss: 0.7571032643318176, ACC: 0.484375\n",
      "Validation iteration 475 loss: 0.7880820035934448, ACC: 0.4375\n",
      "Validation iteration 476 loss: 0.8190608024597168, ACC: 0.390625\n",
      "Validation iteration 477 loss: 0.7467769980430603, ACC: 0.5\n",
      "Validation iteration 478 loss: 0.7261245250701904, ACC: 0.53125\n",
      "Validation iteration 479 loss: 0.7054719924926758, ACC: 0.5625\n",
      "Validation iteration 480 loss: 0.736450731754303, ACC: 0.515625\n",
      "Validation iteration 481 loss: 0.767429530620575, ACC: 0.46875\n",
      "Validation iteration 482 loss: 0.7467770576477051, ACC: 0.5\n",
      "Validation iteration 483 loss: 0.7467769980430603, ACC: 0.5\n",
      "Validation iteration 484 loss: 0.7777557373046875, ACC: 0.453125\n",
      "Validation iteration 485 loss: 0.7777557373046875, ACC: 0.453125\n",
      "Validation iteration 486 loss: 0.6641669869422913, ACC: 0.625\n",
      "Validation iteration 487 loss: 0.7880820631980896, ACC: 0.4375\n",
      "Validation iteration 488 loss: 0.7571032643318176, ACC: 0.484375\n",
      "Validation iteration 489 loss: 0.7467770576477051, ACC: 0.5\n",
      "Validation iteration 490 loss: 0.6951457262039185, ACC: 0.578125\n",
      "Validation iteration 491 loss: 0.7571032047271729, ACC: 0.484375\n",
      "Validation iteration 492 loss: 0.7571032643318176, ACC: 0.484375\n",
      "Validation iteration 493 loss: 0.7880820035934448, ACC: 0.4375\n",
      "Validation iteration 494 loss: 0.7467770576477051, ACC: 0.5\n",
      "Validation iteration 495 loss: 0.7157981991767883, ACC: 0.546875\n",
      "Validation iteration 496 loss: 0.7777557969093323, ACC: 0.453125\n",
      "Validation iteration 497 loss: 0.736450731754303, ACC: 0.515625\n",
      "Validation iteration 498 loss: 0.6848195195198059, ACC: 0.59375\n",
      "Validation iteration 499 loss: 0.7571032643318176, ACC: 0.484375\n",
      "Validation iteration 500 loss: 0.7157981991767883, ACC: 0.546875\n",
      "-- Epoch 5 done -- Train loss: 0.8407642153898874, train ACC: 0.5017708333333334, val loss: 0.7461574363708496, val ACC: 0.5009375\n",
      "<--- 9789.616718530655 seconds --->\n",
      "Training iteration 1 loss: 0.7984083294868469, ACC:0.421875\n",
      "Training iteration 2 loss: 0.7309976816177368, ACC:0.546875\n",
      "Training iteration 3 loss: 0.6965819597244263, ACC:0.484375\n",
      "Training iteration 4 loss: 0.7189536094665527, ACC:0.53125\n",
      "Training iteration 5 loss: 0.6789458394050598, ACC:0.59375\n",
      "Training iteration 6 loss: 0.6962217688560486, ACC:0.5\n",
      "Training iteration 7 loss: 0.7133156657218933, ACC:0.40625\n",
      "Training iteration 8 loss: 0.7185429334640503, ACC:0.515625\n",
      "Training iteration 9 loss: 0.723180890083313, ACC:0.453125\n",
      "Training iteration 10 loss: 0.7901549339294434, ACC:0.46875\n",
      "Training iteration 11 loss: 0.7219097018241882, ACC:0.390625\n",
      "Training iteration 12 loss: 1.0102670192718506, ACC:0.453125\n",
      "Training iteration 13 loss: 0.6988592743873596, ACC:0.53125\n",
      "Training iteration 14 loss: 0.8426713943481445, ACC:0.484375\n",
      "Training iteration 15 loss: 0.6542612314224243, ACC:0.640625\n",
      "Training iteration 16 loss: 0.6911935806274414, ACC:0.53125\n",
      "Training iteration 17 loss: 0.7096152305603027, ACC:0.4375\n",
      "Training iteration 18 loss: 0.7360908389091492, ACC:0.40625\n",
      "Training iteration 19 loss: 0.7286854982376099, ACC:0.484375\n",
      "Training iteration 20 loss: 0.6889865398406982, ACC:0.546875\n",
      "Training iteration 21 loss: 0.6905692219734192, ACC:0.546875\n",
      "Training iteration 22 loss: 0.7138643860816956, ACC:0.53125\n",
      "Training iteration 23 loss: 0.7113500833511353, ACC:0.453125\n",
      "Training iteration 24 loss: 0.851748526096344, ACC:0.40625\n",
      "Training iteration 25 loss: 0.7212837338447571, ACC:0.453125\n",
      "Training iteration 26 loss: 0.7158999443054199, ACC:0.5\n",
      "Training iteration 27 loss: 0.6891441345214844, ACC:0.546875\n",
      "Training iteration 28 loss: 0.7382498383522034, ACC:0.53125\n",
      "Training iteration 29 loss: 0.6951488852500916, ACC:0.5\n",
      "Training iteration 30 loss: 0.7887940406799316, ACC:0.4375\n",
      "Training iteration 31 loss: 0.7046728134155273, ACC:0.484375\n",
      "Training iteration 32 loss: 0.7721083164215088, ACC:0.40625\n",
      "Training iteration 33 loss: 0.767439067363739, ACC:0.515625\n",
      "Training iteration 34 loss: 0.7453975677490234, ACC:0.515625\n",
      "Training iteration 35 loss: 0.7548316121101379, ACC:0.453125\n",
      "Training iteration 36 loss: 0.6928409934043884, ACC:0.546875\n",
      "Training iteration 37 loss: 0.6929455995559692, ACC:0.515625\n",
      "Training iteration 38 loss: 0.726059079170227, ACC:0.484375\n",
      "Training iteration 39 loss: 0.6966056227684021, ACC:0.5\n",
      "Training iteration 40 loss: 0.6756962537765503, ACC:0.59375\n",
      "Training iteration 41 loss: 0.7072081565856934, ACC:0.546875\n",
      "Training iteration 42 loss: 0.6966320872306824, ACC:0.390625\n",
      "Training iteration 43 loss: 0.9171307682991028, ACC:0.484375\n",
      "Training iteration 44 loss: 0.7140116095542908, ACC:0.46875\n",
      "Training iteration 45 loss: 0.8961564898490906, ACC:0.515625\n",
      "Training iteration 46 loss: 0.7504844665527344, ACC:0.53125\n",
      "Training iteration 47 loss: 0.802101731300354, ACC:0.484375\n",
      "Training iteration 48 loss: 0.7937166690826416, ACC:0.46875\n",
      "Training iteration 49 loss: 0.7915635108947754, ACC:0.5\n",
      "Training iteration 50 loss: 0.7586846351623535, ACC:0.53125\n",
      "Training iteration 51 loss: 0.689132809638977, ACC:0.5625\n",
      "Training iteration 52 loss: 0.9276021718978882, ACC:0.46875\n",
      "Training iteration 53 loss: 0.7008340954780579, ACC:0.5\n",
      "Training iteration 54 loss: 0.9310801029205322, ACC:0.4375\n",
      "Training iteration 55 loss: 0.7412905097007751, ACC:0.421875\n",
      "Training iteration 56 loss: 0.7350003123283386, ACC:0.484375\n",
      "Training iteration 57 loss: 0.7259431481361389, ACC:0.46875\n",
      "Training iteration 58 loss: 0.69406658411026, ACC:0.53125\n",
      "Training iteration 59 loss: 0.6877009868621826, ACC:0.5625\n",
      "Training iteration 60 loss: 0.6888015270233154, ACC:0.59375\n",
      "Training iteration 61 loss: 0.7938733100891113, ACC:0.4375\n",
      "Training iteration 62 loss: 0.8005245923995972, ACC:0.515625\n",
      "Training iteration 63 loss: 0.7705820798873901, ACC:0.53125\n",
      "Training iteration 64 loss: 0.732755184173584, ACC:0.484375\n",
      "Training iteration 65 loss: 0.6210892796516418, ACC:0.6875\n",
      "Training iteration 66 loss: 0.9042069911956787, ACC:0.421875\n",
      "Training iteration 67 loss: 0.8273043036460876, ACC:0.5\n",
      "Training iteration 68 loss: 0.8724709153175354, ACC:0.484375\n",
      "Training iteration 69 loss: 0.7799033522605896, ACC:0.484375\n",
      "Training iteration 70 loss: 0.8584870100021362, ACC:0.46875\n",
      "Training iteration 71 loss: 0.7654212713241577, ACC:0.484375\n",
      "Training iteration 72 loss: 0.8696237206459045, ACC:0.421875\n",
      "Training iteration 73 loss: 0.8063391447067261, ACC:0.515625\n",
      "Training iteration 74 loss: 0.8188616633415222, ACC:0.53125\n",
      "Training iteration 75 loss: 0.6886528134346008, ACC:0.5625\n",
      "Training iteration 76 loss: 1.06450617313385, ACC:0.4375\n",
      "Training iteration 77 loss: 0.7026816010475159, ACC:0.5\n",
      "Training iteration 78 loss: 0.9476286768913269, ACC:0.484375\n",
      "Training iteration 79 loss: 0.7010210156440735, ACC:0.421875\n",
      "Training iteration 80 loss: 1.0688893795013428, ACC:0.5\n",
      "Training iteration 81 loss: 0.8367334604263306, ACC:0.453125\n",
      "Training iteration 82 loss: 1.0664482116699219, ACC:0.5\n",
      "Training iteration 83 loss: 1.0265986919403076, ACC:0.484375\n",
      "Training iteration 84 loss: 0.8067176938056946, ACC:0.53125\n",
      "Training iteration 85 loss: 1.118935227394104, ACC:0.5\n",
      "Training iteration 86 loss: 0.6937797665596008, ACC:0.46875\n",
      "Training iteration 87 loss: 0.9931591153144836, ACC:0.484375\n",
      "Training iteration 88 loss: 0.7304610013961792, ACC:0.46875\n",
      "Training iteration 89 loss: 0.92964106798172, ACC:0.53125\n",
      "Training iteration 90 loss: 0.7918907403945923, ACC:0.5625\n",
      "Training iteration 91 loss: 0.7141212224960327, ACC:0.546875\n",
      "Training iteration 92 loss: 0.9588527679443359, ACC:0.5\n",
      "Training iteration 93 loss: 0.6951327919960022, ACC:0.453125\n",
      "Training iteration 94 loss: 0.8043749928474426, ACC:0.515625\n",
      "Training iteration 95 loss: 0.6903275847434998, ACC:0.546875\n",
      "Training iteration 96 loss: 0.7056049108505249, ACC:0.546875\n",
      "Training iteration 97 loss: 0.7496098875999451, ACC:0.5\n",
      "Training iteration 98 loss: 0.7532138824462891, ACC:0.40625\n",
      "Training iteration 99 loss: 0.6970502734184265, ACC:0.40625\n",
      "Training iteration 100 loss: 0.6955644488334656, ACC:0.53125\n",
      "Training iteration 101 loss: 0.6900257468223572, ACC:0.546875\n",
      "Training iteration 102 loss: 0.6953634023666382, ACC:0.484375\n",
      "Training iteration 103 loss: 0.7391669154167175, ACC:0.421875\n",
      "Training iteration 104 loss: 0.7422289252281189, ACC:0.453125\n",
      "Training iteration 105 loss: 0.6922616362571716, ACC:0.53125\n",
      "Training iteration 106 loss: 0.7005892395973206, ACC:0.484375\n",
      "Training iteration 107 loss: 0.6949710845947266, ACC:0.484375\n",
      "Training iteration 108 loss: 0.6935943365097046, ACC:0.5\n",
      "Training iteration 109 loss: 0.695600152015686, ACC:0.46875\n",
      "Training iteration 110 loss: 0.705004096031189, ACC:0.453125\n",
      "Training iteration 111 loss: 0.6941320300102234, ACC:0.53125\n",
      "Training iteration 112 loss: 0.7071954011917114, ACC:0.515625\n",
      "Training iteration 113 loss: 0.6977607011795044, ACC:0.46875\n",
      "Training iteration 114 loss: 0.6868604421615601, ACC:0.5625\n",
      "Training iteration 115 loss: 0.6930916905403137, ACC:0.546875\n",
      "Training iteration 116 loss: 0.7008378505706787, ACC:0.5\n",
      "Training iteration 117 loss: 0.6960846781730652, ACC:0.53125\n",
      "Training iteration 118 loss: 0.6914583444595337, ACC:0.5625\n",
      "Training iteration 119 loss: 0.6986730098724365, ACC:0.5\n",
      "Training iteration 120 loss: 0.6952049136161804, ACC:0.546875\n",
      "Training iteration 121 loss: 0.75058913230896, ACC:0.46875\n",
      "Training iteration 122 loss: 0.7398165464401245, ACC:0.484375\n",
      "Training iteration 123 loss: 0.7107515335083008, ACC:0.515625\n",
      "Training iteration 124 loss: 0.730368435382843, ACC:0.4375\n",
      "Training iteration 125 loss: 0.6900871396064758, ACC:0.578125\n",
      "Training iteration 126 loss: 0.8048369288444519, ACC:0.484375\n",
      "Training iteration 127 loss: 0.6933633685112, ACC:0.484375\n",
      "Training iteration 128 loss: 0.7412229776382446, ACC:0.515625\n",
      "Training iteration 129 loss: 0.6915971636772156, ACC:0.53125\n",
      "Training iteration 130 loss: 0.717411994934082, ACC:0.484375\n",
      "Training iteration 131 loss: 0.692668080329895, ACC:0.515625\n",
      "Training iteration 132 loss: 0.6853920817375183, ACC:0.5625\n",
      "Training iteration 133 loss: 0.7376154661178589, ACC:0.484375\n",
      "Training iteration 134 loss: 0.6853950023651123, ACC:0.5625\n",
      "Training iteration 135 loss: 0.7903651595115662, ACC:0.515625\n",
      "Training iteration 136 loss: 0.6936997771263123, ACC:0.484375\n",
      "Training iteration 137 loss: 0.8406749367713928, ACC:0.46875\n",
      "Training iteration 138 loss: 0.6903112530708313, ACC:0.578125\n",
      "Training iteration 139 loss: 0.8965794444084167, ACC:0.53125\n",
      "Training iteration 140 loss: 0.715161919593811, ACC:0.546875\n",
      "Training iteration 141 loss: 0.6796690225601196, ACC:0.625\n",
      "Training iteration 142 loss: 1.0308295488357544, ACC:0.484375\n",
      "Training iteration 143 loss: 0.7211779356002808, ACC:0.453125\n",
      "Training iteration 144 loss: 0.8532657623291016, ACC:0.5\n",
      "Training iteration 145 loss: 0.6933162808418274, ACC:0.5\n",
      "Training iteration 146 loss: 0.8408933281898499, ACC:0.484375\n",
      "Training iteration 147 loss: 0.6934000849723816, ACC:0.5\n",
      "Training iteration 148 loss: 0.8149306178092957, ACC:0.484375\n",
      "Training iteration 149 loss: 0.6926713585853577, ACC:0.515625\n",
      "Training iteration 150 loss: 0.7695828080177307, ACC:0.484375\n",
      "Training iteration 151 loss: 0.6919474005699158, ACC:0.546875\n",
      "Training iteration 152 loss: 0.7094919085502625, ACC:0.484375\n",
      "Training iteration 153 loss: 0.7013449668884277, ACC:0.40625\n",
      "Training iteration 154 loss: 0.7156092524528503, ACC:0.5\n",
      "Training iteration 155 loss: 0.6868852972984314, ACC:0.578125\n",
      "Training iteration 156 loss: 0.6984038949012756, ACC:0.5\n",
      "Training iteration 157 loss: 0.6902357339859009, ACC:0.546875\n",
      "Training iteration 158 loss: 0.6953190565109253, ACC:0.5625\n",
      "Training iteration 159 loss: 0.6758313775062561, ACC:0.59375\n",
      "Training iteration 160 loss: 0.6891003847122192, ACC:0.546875\n",
      "Training iteration 161 loss: 0.6947649717330933, ACC:0.453125\n",
      "Training iteration 162 loss: 0.689102828502655, ACC:0.546875\n",
      "Training iteration 163 loss: 0.7054960131645203, ACC:0.515625\n",
      "Training iteration 164 loss: 0.694889485836029, ACC:0.359375\n",
      "Training iteration 165 loss: 0.9439172148704529, ACC:0.484375\n",
      "Training iteration 166 loss: 0.681029200553894, ACC:0.578125\n",
      "Training iteration 167 loss: 0.7375471591949463, ACC:0.515625\n",
      "Training iteration 168 loss: 0.7157610654830933, ACC:0.5\n",
      "Training iteration 169 loss: 0.7077277898788452, ACC:0.546875\n",
      "Training iteration 170 loss: 0.7667516469955444, ACC:0.5\n",
      "Training iteration 171 loss: 0.710674524307251, ACC:0.515625\n",
      "Training iteration 172 loss: 0.804431140422821, ACC:0.46875\n",
      "Training iteration 173 loss: 0.7205233573913574, ACC:0.515625\n",
      "Training iteration 174 loss: 0.8057620525360107, ACC:0.484375\n",
      "Training iteration 175 loss: 0.6994656324386597, ACC:0.546875\n",
      "Training iteration 176 loss: 0.8420009016990662, ACC:0.515625\n",
      "Training iteration 177 loss: 0.6937808394432068, ACC:0.5\n",
      "Training iteration 178 loss: 0.8472602963447571, ACC:0.484375\n",
      "Training iteration 179 loss: 0.6933348178863525, ACC:0.453125\n",
      "Training iteration 180 loss: 0.8554955720901489, ACC:0.53125\n",
      "Training iteration 181 loss: 0.6443053483963013, ACC:0.65625\n",
      "Training iteration 182 loss: 0.6944334506988525, ACC:0.453125\n",
      "Training iteration 183 loss: 0.7023097276687622, ACC:0.59375\n",
      "Training iteration 184 loss: 0.7645259499549866, ACC:0.515625\n",
      "Training iteration 185 loss: 0.7589377164840698, ACC:0.453125\n",
      "Training iteration 186 loss: 0.7224205136299133, ACC:0.484375\n",
      "Training iteration 187 loss: 0.7870030403137207, ACC:0.421875\n",
      "Training iteration 188 loss: 0.6887920498847961, ACC:0.546875\n",
      "Training iteration 189 loss: 0.863740861415863, ACC:0.421875\n",
      "Training iteration 190 loss: 0.771368145942688, ACC:0.453125\n",
      "Training iteration 191 loss: 0.756000816822052, ACC:0.4375\n",
      "Training iteration 192 loss: 0.976359486579895, ACC:0.34375\n",
      "Training iteration 193 loss: 0.8230401277542114, ACC:0.34375\n",
      "Training iteration 194 loss: 0.7145397663116455, ACC:0.4375\n",
      "Training iteration 195 loss: 0.6926916241645813, ACC:0.515625\n",
      "Training iteration 196 loss: 0.6899023652076721, ACC:0.546875\n",
      "Training iteration 197 loss: 0.6755040884017944, ACC:0.59375\n",
      "Training iteration 198 loss: 0.693504810333252, ACC:0.578125\n",
      "Training iteration 199 loss: 0.6983830332756042, ACC:0.515625\n",
      "Training iteration 200 loss: 0.6902987957000732, ACC:0.5625\n",
      "Training iteration 201 loss: 0.8293330669403076, ACC:0.40625\n",
      "Training iteration 202 loss: 0.859978437423706, ACC:0.46875\n",
      "Training iteration 203 loss: 0.7279971241950989, ACC:0.515625\n",
      "Training iteration 204 loss: 0.7709951996803284, ACC:0.5\n",
      "Training iteration 205 loss: 0.7399572134017944, ACC:0.5\n",
      "Training iteration 206 loss: 0.7397003173828125, ACC:0.515625\n",
      "Training iteration 207 loss: 0.7570637464523315, ACC:0.5\n",
      "Training iteration 208 loss: 0.6966885924339294, ACC:0.5625\n",
      "Training iteration 209 loss: 0.8327064514160156, ACC:0.515625\n",
      "Training iteration 210 loss: 0.6914427280426025, ACC:0.53125\n",
      "Training iteration 211 loss: 0.8842095732688904, ACC:0.5\n",
      "Training iteration 212 loss: 0.6911941766738892, ACC:0.53125\n",
      "Training iteration 213 loss: 0.7935242652893066, ACC:0.5\n",
      "Training iteration 214 loss: 0.6959263682365417, ACC:0.515625\n",
      "Training iteration 215 loss: 0.7204172611236572, ACC:0.546875\n",
      "Training iteration 216 loss: 0.7197291254997253, ACC:0.53125\n",
      "Training iteration 217 loss: 0.7318516969680786, ACC:0.4375\n",
      "Training iteration 218 loss: 0.7023305892944336, ACC:0.390625\n",
      "Training iteration 219 loss: 0.798062264919281, ACC:0.546875\n",
      "Training iteration 220 loss: 0.7631052136421204, ACC:0.46875\n",
      "Training iteration 221 loss: 0.9536013603210449, ACC:0.4375\n",
      "Training iteration 222 loss: 0.731191098690033, ACC:0.4375\n",
      "Training iteration 223 loss: 0.88191819190979, ACC:0.5625\n",
      "Training iteration 224 loss: 0.8707717657089233, ACC:0.515625\n",
      "Training iteration 225 loss: 0.8342689871788025, ACC:0.484375\n",
      "Training iteration 226 loss: 0.9194727540016174, ACC:0.453125\n",
      "Training iteration 227 loss: 0.8716323375701904, ACC:0.453125\n",
      "Training iteration 228 loss: 0.8009523153305054, ACC:0.484375\n",
      "Training iteration 229 loss: 0.8035679459571838, ACC:0.5\n",
      "Training iteration 230 loss: 0.7806128859519958, ACC:0.515625\n",
      "Training iteration 231 loss: 0.771908700466156, ACC:0.453125\n",
      "Training iteration 232 loss: 0.7169123888015747, ACC:0.515625\n",
      "Training iteration 233 loss: 0.7432656288146973, ACC:0.4375\n",
      "Training iteration 234 loss: 0.6933111548423767, ACC:0.484375\n",
      "Training iteration 235 loss: 0.7275002002716064, ACC:0.40625\n",
      "Training iteration 236 loss: 0.8358601927757263, ACC:0.390625\n",
      "Training iteration 237 loss: 0.6784624457359314, ACC:0.59375\n",
      "Training iteration 238 loss: 0.8295260667800903, ACC:0.5625\n",
      "Training iteration 239 loss: 0.7074128985404968, ACC:0.5\n",
      "Training iteration 240 loss: 0.7974943518638611, ACC:0.5625\n",
      "Training iteration 241 loss: 0.838352620601654, ACC:0.484375\n",
      "Training iteration 242 loss: 0.9014838337898254, ACC:0.4375\n",
      "Training iteration 243 loss: 0.7817726135253906, ACC:0.4375\n",
      "Training iteration 244 loss: 0.9259624481201172, ACC:0.5\n",
      "Training iteration 245 loss: 0.9019746780395508, ACC:0.390625\n",
      "Training iteration 246 loss: 1.248408555984497, ACC:0.4375\n",
      "Training iteration 247 loss: 0.8406425714492798, ACC:0.53125\n",
      "Training iteration 248 loss: 0.9458317756652832, ACC:0.453125\n",
      "Training iteration 249 loss: 0.9617350101470947, ACC:0.375\n",
      "Training iteration 250 loss: 1.0093179941177368, ACC:0.546875\n",
      "Training iteration 251 loss: 1.2840440273284912, ACC:0.453125\n",
      "Training iteration 252 loss: 0.7991231083869934, ACC:0.515625\n",
      "Training iteration 253 loss: 1.2060693502426147, ACC:0.5\n",
      "Training iteration 254 loss: 0.6930616497993469, ACC:0.53125\n",
      "Training iteration 255 loss: 0.9508487582206726, ACC:0.546875\n",
      "Training iteration 256 loss: 0.9297403693199158, ACC:0.453125\n",
      "Training iteration 257 loss: 1.0021361112594604, ACC:0.5\n",
      "Training iteration 258 loss: 1.0127394199371338, ACC:0.5\n",
      "Training iteration 259 loss: 0.6908255815505981, ACC:0.609375\n",
      "Training iteration 260 loss: 1.4245433807373047, ACC:0.453125\n",
      "Training iteration 261 loss: 0.7395327687263489, ACC:0.40625\n",
      "Training iteration 262 loss: 1.2788348197937012, ACC:0.578125\n",
      "Training iteration 263 loss: 1.5174365043640137, ACC:0.515625\n",
      "Training iteration 264 loss: 0.6866952180862427, ACC:0.5625\n",
      "Training iteration 265 loss: 1.8939672708511353, ACC:0.421875\n",
      "Training iteration 266 loss: 1.0325713157653809, ACC:0.53125\n",
      "Training iteration 267 loss: 0.8582764267921448, ACC:0.609375\n",
      "Training iteration 268 loss: 1.9325084686279297, ACC:0.421875\n",
      "Training iteration 269 loss: 0.7467080950737, ACC:0.46875\n",
      "Training iteration 270 loss: 1.9096928834915161, ACC:0.40625\n",
      "Training iteration 271 loss: 1.259033203125, ACC:0.5\n",
      "Training iteration 272 loss: 0.871953547000885, ACC:0.546875\n",
      "Training iteration 273 loss: 1.6417831182479858, ACC:0.453125\n",
      "Training iteration 274 loss: 0.7741890549659729, ACC:0.40625\n",
      "Training iteration 275 loss: 1.876900315284729, ACC:0.4375\n",
      "Training iteration 276 loss: 1.2711107730865479, ACC:0.5625\n",
      "Training iteration 277 loss: 0.7386189103126526, ACC:0.46875\n",
      "Training iteration 278 loss: 1.2451337575912476, ACC:0.5\n",
      "Training iteration 279 loss: 0.7060695290565491, ACC:0.59375\n",
      "Training iteration 280 loss: 0.948472797870636, ACC:0.4375\n",
      "Training iteration 281 loss: 0.7145336866378784, ACC:0.546875\n",
      "Training iteration 282 loss: 0.7513381838798523, ACC:0.5\n",
      "Training iteration 283 loss: 0.6747245192527771, ACC:0.609375\n",
      "Training iteration 284 loss: 0.6770480871200562, ACC:0.59375\n",
      "Training iteration 285 loss: 0.6877377033233643, ACC:0.5625\n",
      "Training iteration 286 loss: 0.7141529321670532, ACC:0.4375\n",
      "Training iteration 287 loss: 0.6790303587913513, ACC:0.59375\n",
      "Training iteration 288 loss: 0.8026121258735657, ACC:0.5\n",
      "Training iteration 289 loss: 0.6704936027526855, ACC:0.609375\n",
      "Training iteration 290 loss: 0.908988356590271, ACC:0.5625\n",
      "Training iteration 291 loss: 0.6849915981292725, ACC:0.609375\n",
      "Training iteration 292 loss: 0.7369511723518372, ACC:0.5\n",
      "Training iteration 293 loss: 0.7200100421905518, ACC:0.5625\n",
      "Training iteration 294 loss: 0.6922714710235596, ACC:0.53125\n",
      "Training iteration 295 loss: 0.7364801168441772, ACC:0.46875\n",
      "Training iteration 296 loss: 0.6914297938346863, ACC:0.53125\n",
      "Training iteration 297 loss: 0.7693718671798706, ACC:0.46875\n",
      "Training iteration 298 loss: 0.7350687980651855, ACC:0.40625\n",
      "Training iteration 299 loss: 0.7012778520584106, ACC:0.40625\n",
      "Training iteration 300 loss: 0.7078187465667725, ACC:0.515625\n",
      "Training iteration 301 loss: 0.71869957447052, ACC:0.40625\n",
      "Training iteration 302 loss: 0.8446229696273804, ACC:0.484375\n",
      "Training iteration 303 loss: 0.6984567642211914, ACC:0.515625\n",
      "Training iteration 304 loss: 0.6858627796173096, ACC:0.609375\n",
      "Training iteration 305 loss: 0.8388665318489075, ACC:0.515625\n",
      "Training iteration 306 loss: 0.7155227661132812, ACC:0.484375\n",
      "Training iteration 307 loss: 0.8499195575714111, ACC:0.453125\n",
      "Training iteration 308 loss: 0.7023150324821472, ACC:0.546875\n",
      "Training iteration 309 loss: 0.7803438901901245, ACC:0.578125\n",
      "Training iteration 310 loss: 0.7084932923316956, ACC:0.515625\n",
      "Training iteration 311 loss: 0.7798630595207214, ACC:0.53125\n",
      "Training iteration 312 loss: 0.852454662322998, ACC:0.390625\n",
      "Training iteration 313 loss: 1.0895994901657104, ACC:0.4375\n",
      "Training iteration 314 loss: 0.8097903728485107, ACC:0.453125\n",
      "Training iteration 315 loss: 1.0444178581237793, ACC:0.484375\n",
      "Training iteration 316 loss: 0.7808555364608765, ACC:0.578125\n",
      "Training iteration 317 loss: 0.7429987788200378, ACC:0.5\n",
      "Training iteration 318 loss: 0.8986048698425293, ACC:0.46875\n",
      "Training iteration 319 loss: 0.7451248168945312, ACC:0.46875\n",
      "Training iteration 320 loss: 0.8149155378341675, ACC:0.46875\n",
      "Training iteration 321 loss: 0.7262129783630371, ACC:0.515625\n",
      "Training iteration 322 loss: 0.8343038558959961, ACC:0.46875\n",
      "Training iteration 323 loss: 0.8035013675689697, ACC:0.390625\n",
      "Training iteration 324 loss: 0.6912662386894226, ACC:0.53125\n",
      "Training iteration 325 loss: 0.7189962863922119, ACC:0.453125\n",
      "Training iteration 326 loss: 0.6929787993431091, ACC:0.53125\n",
      "Training iteration 327 loss: 0.7829894423484802, ACC:0.390625\n",
      "Training iteration 328 loss: 0.9073082208633423, ACC:0.40625\n",
      "Training iteration 329 loss: 0.6964527368545532, ACC:0.453125\n",
      "Training iteration 330 loss: 0.7395170331001282, ACC:0.484375\n",
      "Training iteration 331 loss: 0.6992408633232117, ACC:0.46875\n",
      "Training iteration 332 loss: 0.685349702835083, ACC:0.5625\n",
      "Training iteration 333 loss: 0.7491751909255981, ACC:0.390625\n",
      "Training iteration 334 loss: 0.8232491612434387, ACC:0.5\n",
      "Training iteration 335 loss: 0.7018537521362305, ACC:0.546875\n",
      "Training iteration 336 loss: 0.7566750049591064, ACC:0.453125\n",
      "Training iteration 337 loss: 0.7009127736091614, ACC:0.46875\n",
      "Training iteration 338 loss: 0.7691089510917664, ACC:0.484375\n",
      "Training iteration 339 loss: 0.6912879347801208, ACC:0.53125\n",
      "Training iteration 340 loss: 0.7017235159873962, ACC:0.53125\n",
      "Training iteration 341 loss: 0.6809902787208557, ACC:0.578125\n",
      "Training iteration 342 loss: 0.6959496140480042, ACC:0.515625\n",
      "Training iteration 343 loss: 0.6916422843933105, ACC:0.53125\n",
      "Training iteration 344 loss: 0.7036014795303345, ACC:0.53125\n",
      "Training iteration 345 loss: 0.6885083317756653, ACC:0.5625\n",
      "Training iteration 346 loss: 0.686927318572998, ACC:0.59375\n",
      "Training iteration 347 loss: 0.7608012557029724, ACC:0.46875\n",
      "Training iteration 348 loss: 0.6971762180328369, ACC:0.515625\n",
      "Training iteration 349 loss: 0.7618989944458008, ACC:0.484375\n",
      "Training iteration 350 loss: 0.7077206969261169, ACC:0.484375\n",
      "Training iteration 351 loss: 0.7730907797813416, ACC:0.40625\n",
      "Training iteration 352 loss: 0.7693767547607422, ACC:0.53125\n",
      "Training iteration 353 loss: 0.7892568111419678, ACC:0.484375\n",
      "Training iteration 354 loss: 0.7430344223976135, ACC:0.546875\n",
      "Training iteration 355 loss: 0.9023309946060181, ACC:0.46875\n",
      "Training iteration 356 loss: 0.7860114574432373, ACC:0.46875\n",
      "Training iteration 357 loss: 0.8541668057441711, ACC:0.4375\n",
      "Training iteration 358 loss: 0.8497405052185059, ACC:0.46875\n",
      "Training iteration 359 loss: 0.6927690505981445, ACC:0.59375\n",
      "Training iteration 360 loss: 0.712007999420166, ACC:0.375\n",
      "Training iteration 361 loss: 0.7050230503082275, ACC:0.484375\n",
      "Training iteration 362 loss: 0.6935430765151978, ACC:0.453125\n",
      "Training iteration 363 loss: 0.7165839076042175, ACC:0.53125\n",
      "Training iteration 364 loss: 0.7161192893981934, ACC:0.453125\n",
      "Training iteration 365 loss: 0.7713455557823181, ACC:0.515625\n",
      "Training iteration 366 loss: 0.7586235404014587, ACC:0.421875\n",
      "Training iteration 367 loss: 0.8003220558166504, ACC:0.578125\n",
      "Training iteration 368 loss: 0.84555584192276, ACC:0.546875\n",
      "Training iteration 369 loss: 0.7253226041793823, ACC:0.5\n",
      "Training iteration 370 loss: 0.7995579242706299, ACC:0.5625\n",
      "Training iteration 371 loss: 0.671732485294342, ACC:0.609375\n",
      "Training iteration 372 loss: 0.6947561502456665, ACC:0.53125\n",
      "Training iteration 373 loss: 0.701285719871521, ACC:0.546875\n",
      "Training iteration 374 loss: 0.701874315738678, ACC:0.4375\n",
      "Training iteration 375 loss: 0.8411749601364136, ACC:0.453125\n",
      "Training iteration 376 loss: 0.6831225156784058, ACC:0.59375\n",
      "Training iteration 377 loss: 1.063881278038025, ACC:0.4375\n",
      "Training iteration 378 loss: 0.6896023750305176, ACC:0.546875\n",
      "Training iteration 379 loss: 0.9260034561157227, ACC:0.5625\n",
      "Training iteration 380 loss: 0.7901327610015869, ACC:0.53125\n",
      "Training iteration 381 loss: 0.7640557289123535, ACC:0.578125\n",
      "Training iteration 382 loss: 1.12088143825531, ACC:0.453125\n",
      "Training iteration 383 loss: 0.7249261140823364, ACC:0.546875\n",
      "Training iteration 384 loss: 1.034805417060852, ACC:0.546875\n",
      "Training iteration 385 loss: 0.7111652493476868, ACC:0.546875\n",
      "Training iteration 386 loss: 0.9891170859336853, ACC:0.453125\n",
      "Training iteration 387 loss: 0.71685791015625, ACC:0.53125\n",
      "Training iteration 388 loss: 0.8434213399887085, ACC:0.46875\n",
      "Training iteration 389 loss: 0.7103219032287598, ACC:0.515625\n",
      "Training iteration 390 loss: 0.7281653881072998, ACC:0.546875\n",
      "Training iteration 391 loss: 0.8271755576133728, ACC:0.4375\n",
      "Training iteration 392 loss: 0.7125122547149658, ACC:0.609375\n",
      "Training iteration 393 loss: 1.1449220180511475, ACC:0.4375\n",
      "Training iteration 394 loss: 0.7409788370132446, ACC:0.53125\n",
      "Training iteration 395 loss: 0.966880202293396, ACC:0.5625\n",
      "Training iteration 396 loss: 0.7261092066764832, ACC:0.515625\n",
      "Training iteration 397 loss: 0.989883542060852, ACC:0.484375\n",
      "Training iteration 398 loss: 0.8177382946014404, ACC:0.46875\n",
      "Training iteration 399 loss: 0.9077773094177246, ACC:0.53125\n",
      "Training iteration 400 loss: 0.8344478607177734, ACC:0.578125\n",
      "Training iteration 401 loss: 0.666038990020752, ACC:0.625\n",
      "Training iteration 402 loss: 1.3193731307983398, ACC:0.46875\n",
      "Training iteration 403 loss: 0.7893450260162354, ACC:0.46875\n",
      "Training iteration 404 loss: 1.2872122526168823, ACC:0.46875\n",
      "Training iteration 405 loss: 1.028965711593628, ACC:0.484375\n",
      "Training iteration 406 loss: 0.8662963509559631, ACC:0.5625\n",
      "Training iteration 407 loss: 1.2688825130462646, ACC:0.515625\n",
      "Training iteration 408 loss: 0.6997013688087463, ACC:0.5\n",
      "Training iteration 409 loss: 1.1699599027633667, ACC:0.515625\n",
      "Training iteration 410 loss: 0.9492605328559875, ACC:0.5\n",
      "Training iteration 411 loss: 0.885707437992096, ACC:0.546875\n",
      "Training iteration 412 loss: 1.1951704025268555, ACC:0.5\n",
      "Training iteration 413 loss: 0.7057530879974365, ACC:0.46875\n",
      "Training iteration 414 loss: 1.1098248958587646, ACC:0.453125\n",
      "Training iteration 415 loss: 0.695048451423645, ACC:0.5\n",
      "Training iteration 416 loss: 0.9692373871803284, ACC:0.5\n",
      "Training iteration 417 loss: 0.7413192391395569, ACC:0.5\n",
      "Training iteration 418 loss: 0.9044961333274841, ACC:0.5\n",
      "Training iteration 419 loss: 0.781467616558075, ACC:0.515625\n",
      "Training iteration 420 loss: 0.6988369822502136, ACC:0.609375\n",
      "Training iteration 421 loss: 0.8000670075416565, ACC:0.640625\n",
      "Training iteration 422 loss: 0.7684164047241211, ACC:0.53125\n",
      "Training iteration 423 loss: 0.9345919489860535, ACC:0.453125\n",
      "Training iteration 424 loss: 0.7795566916465759, ACC:0.484375\n",
      "Training iteration 425 loss: 0.9753815531730652, ACC:0.390625\n",
      "Training iteration 426 loss: 0.6757608652114868, ACC:0.65625\n",
      "Training iteration 427 loss: 0.7021300196647644, ACC:0.453125\n",
      "Training iteration 428 loss: 0.6976801753044128, ACC:0.546875\n",
      "Training iteration 429 loss: 0.7166895270347595, ACC:0.515625\n",
      "Training iteration 430 loss: 0.6853306293487549, ACC:0.5625\n",
      "Training iteration 431 loss: 0.7980237603187561, ACC:0.5\n",
      "Training iteration 432 loss: 0.700435221195221, ACC:0.4375\n",
      "Training iteration 433 loss: 0.7220370173454285, ACC:0.46875\n",
      "Training iteration 434 loss: 0.6904625296592712, ACC:0.546875\n",
      "Training iteration 435 loss: 0.7311545014381409, ACC:0.53125\n",
      "Training iteration 436 loss: 0.6951963901519775, ACC:0.421875\n",
      "Training iteration 437 loss: 1.0196847915649414, ACC:0.375\n",
      "Training iteration 438 loss: 0.7443006634712219, ACC:0.5\n",
      "Training iteration 439 loss: 0.8319637179374695, ACC:0.515625\n",
      "Training iteration 440 loss: 0.6889461874961853, ACC:0.546875\n",
      "Training iteration 441 loss: 0.8097160458564758, ACC:0.578125\n",
      "Training iteration 442 loss: 0.7647984623908997, ACC:0.5\n",
      "Training iteration 443 loss: 0.9109464287757874, ACC:0.453125\n",
      "Training iteration 444 loss: 0.7351049780845642, ACC:0.5\n",
      "Training iteration 445 loss: 0.7846892476081848, ACC:0.53125\n",
      "Training iteration 446 loss: 0.826330840587616, ACC:0.46875\n",
      "Training iteration 447 loss: 0.8793354034423828, ACC:0.4375\n",
      "Training iteration 448 loss: 0.770205020904541, ACC:0.421875\n",
      "Training iteration 449 loss: 0.9856273531913757, ACC:0.46875\n",
      "Training iteration 450 loss: 0.760892391204834, ACC:0.484375\n",
      "Validation iteration 451 loss: 0.9977428317070007, ACC: 0.4375\n",
      "Validation iteration 452 loss: 0.9119973182678223, ACC: 0.5\n",
      "Validation iteration 453 loss: 0.9763064980506897, ACC: 0.453125\n",
      "Validation iteration 454 loss: 0.8476881980895996, ACC: 0.546875\n",
      "Validation iteration 455 loss: 0.997742772102356, ACC: 0.4375\n",
      "Validation iteration 456 loss: 0.9763064384460449, ACC: 0.453125\n",
      "Validation iteration 457 loss: 0.7619426846504211, ACC: 0.609375\n",
      "Validation iteration 458 loss: 0.9119973182678223, ACC: 0.5\n",
      "Validation iteration 459 loss: 0.9119973182678223, ACC: 0.5\n",
      "Validation iteration 460 loss: 0.8905608654022217, ACC: 0.515625\n",
      "Validation iteration 461 loss: 0.9548700451850891, ACC: 0.46875\n",
      "Validation iteration 462 loss: 0.8476881980895996, ACC: 0.546875\n",
      "Validation iteration 463 loss: 0.8476881384849548, ACC: 0.546875\n",
      "Validation iteration 464 loss: 0.9334336519241333, ACC: 0.484375\n",
      "Validation iteration 465 loss: 0.997742772102356, ACC: 0.4375\n",
      "Validation iteration 466 loss: 0.826251745223999, ACC: 0.5625\n",
      "Validation iteration 467 loss: 0.7619426250457764, ACC: 0.609375\n",
      "Validation iteration 468 loss: 0.7833790183067322, ACC: 0.59375\n",
      "Validation iteration 469 loss: 0.8262518644332886, ACC: 0.5625\n",
      "Validation iteration 470 loss: 0.8905609250068665, ACC: 0.515625\n",
      "Validation iteration 471 loss: 0.9977428913116455, ACC: 0.4375\n",
      "Validation iteration 472 loss: 0.9763064384460449, ACC: 0.453125\n",
      "Validation iteration 473 loss: 0.8476881980895996, ACC: 0.546875\n",
      "Validation iteration 474 loss: 1.019179105758667, ACC: 0.421875\n",
      "Validation iteration 475 loss: 0.9548699855804443, ACC: 0.46875\n",
      "Validation iteration 476 loss: 0.8905609250068665, ACC: 0.515625\n",
      "Validation iteration 477 loss: 0.8905609250068665, ACC: 0.515625\n",
      "Validation iteration 478 loss: 0.997742772102356, ACC: 0.4375\n",
      "Validation iteration 479 loss: 0.9119972586631775, ACC: 0.5\n",
      "Validation iteration 480 loss: 1.0620520114898682, ACC: 0.390625\n",
      "Validation iteration 481 loss: 0.9119973182678223, ACC: 0.5\n",
      "Validation iteration 482 loss: 1.190670132637024, ACC: 0.296875\n",
      "Validation iteration 483 loss: 0.8691245913505554, ACC: 0.53125\n",
      "Validation iteration 484 loss: 0.9119972586631775, ACC: 0.5\n",
      "Validation iteration 485 loss: 0.9119973182678223, ACC: 0.5\n",
      "Validation iteration 486 loss: 0.8476881384849548, ACC: 0.546875\n",
      "Validation iteration 487 loss: 0.9548700451850891, ACC: 0.46875\n",
      "Validation iteration 488 loss: 0.8691245913505554, ACC: 0.53125\n",
      "Validation iteration 489 loss: 0.8905609250068665, ACC: 0.515625\n",
      "Validation iteration 490 loss: 0.8905609250068665, ACC: 0.515625\n",
      "Validation iteration 491 loss: 0.9763064384460449, ACC: 0.453125\n",
      "Validation iteration 492 loss: 0.9977428317070007, ACC: 0.4375\n",
      "Validation iteration 493 loss: 1.0406155586242676, ACC: 0.40625\n",
      "Validation iteration 494 loss: 0.804815411567688, ACC: 0.578125\n",
      "Validation iteration 495 loss: 0.9763064980506897, ACC: 0.453125\n",
      "Validation iteration 496 loss: 0.8476881980895996, ACC: 0.546875\n",
      "Validation iteration 497 loss: 0.9977428317070007, ACC: 0.4375\n",
      "Validation iteration 498 loss: 0.9548700451850891, ACC: 0.46875\n",
      "Validation iteration 499 loss: 0.9977428317070007, ACC: 0.4375\n",
      "Validation iteration 500 loss: 0.8905609250068665, ACC: 0.515625\n",
      "-- Epoch 6 done -- Train loss: 0.8003123486042023, train ACC: 0.5000694444444445, val loss: 0.9227154910564422, val ACC: 0.4921875\n",
      "<--- 10014.000672578812 seconds --->\n"
     ]
    }
   ],
   "source": [
    "# for timing model training purposes\n",
    "start_time = time.time()\n",
    "\n",
    "# input variables\n",
    "epoch_num = 6\n",
    "learning_rate = 0.1\n",
    "\n",
    "# train model\n",
    "model = LeNet5()\n",
    "cost_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# to store loss for training & validation set\n",
    "jw_train_epoch = []  ## to store loss for training set by epoch (average loss of iterations)\n",
    "jw_val_epoch = []    ## to store loss for validation set by epoch (average loss of iterations)\n",
    "\n",
    "# to store accuracy of training & validation set\n",
    "acc_train_epoch = []\n",
    "acc_val_epoch = []\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "\n",
    "    # track train & validation loss & accuracy by iteration for each epoch\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    test_counter = 1\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ''' include below IF statement if want to limit number of batches '''\n",
    "        if i+1 == 501:  # should have total of 500 batches after train & val sets\n",
    "            break\n",
    "        \n",
    "        # use 80% for training, 20% for testing, and 10% for validation of the 40k training samples\n",
    "        ## batch size=64 so 625 batches total: 450 train batches, 50 val batches, and 125 test batches\n",
    "\n",
    "        if i+1 > 450:  # validate model with validation set (10% of total train data)\n",
    "            inputs, labels = data\n",
    "            \n",
    "            logits, outputs = model(inputs)\n",
    "            cost = cost_fn(logits, labels)\n",
    "            \n",
    "            jw_val = float(cost)\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "            correct = pred == labels\n",
    "            acc = sum(np.array(correct))/len(np.array(correct))\n",
    "\n",
    "            acc_val = acc\n",
    "\n",
    "            val_loss.append(jw_val)\n",
    "            val_acc.append(acc_val)\n",
    "            \n",
    "            print(f'Validation iteration {i+1} loss: {jw_val}, ACC: {acc_val}')\n",
    "            \n",
    "        else:  # train model with training set\n",
    "\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()            # zero the parameter gradients\n",
    "            logits, outputs = model(inputs)  # forward\n",
    "            cost = cost_fn(logits, labels)   # input logits prior to softmax activation into cost function\n",
    "            cost.backward()                  # backward\n",
    "            optimizer.step()                 # optimize\n",
    "\n",
    "            jw_train = float(cost)\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred = torch.Tensor.argmax(outputs, dim=1)            # get labels of prediction with highest probability\n",
    "            correct = pred == labels                              # compare to actual labels and see which was predicted correctly\n",
    "\n",
    "            acc = sum(np.array(correct))/len(np.array(correct))   # calculate accuracy\n",
    "\n",
    "            acc_train = acc\n",
    "\n",
    "            train_loss.append(jw_train)\n",
    "            train_acc.append(acc_train)\n",
    "            \n",
    "            print(f'Training iteration {i+1} loss: {jw_train}, ACC:{acc_train}')\n",
    "\n",
    "    # to save time, epoch loss = the lowest loss, epoch acc = highest acc in training\n",
    "    epoch_jw = np.mean(np.array(train_loss))\n",
    "    epoch_acc = np.mean(np.array(train_acc))\n",
    "\n",
    "    jw_train_epoch.append(epoch_jw)\n",
    "    jw_val_epoch.append(np.mean(val_loss))\n",
    "    acc_train_epoch.append(epoch_acc)\n",
    "    acc_val_epoch.append(np.mean(val_acc))\n",
    "    \n",
    "    print(f'-- Epoch {epoch+1} done -- Train loss: {epoch_jw}, train ACC: {epoch_acc}, val loss: {np.mean(val_loss)}, val ACC: {np.mean(val_acc)}')\n",
    "    \n",
    "    print(\"<--- %s seconds --->\" % (time.time() - start_time))\n",
    "\n",
    "    # save model at every epoch\n",
    "    path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.1/lenet5_lr0.1_cpu_epoch{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwdS7oqJweUo"
   },
   "source": [
    "Plot loss and accuracy over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1636915057701,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "hZPUGYuZYBLH",
    "outputId": "48ed3d54-e9b7-4d29-f9c1-654ac7f3a287"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss over epoch\n",
    "plt.plot([i+1 for i in range(len(jw_train_epoch))],jw_train_epoch)\n",
    "plt.plot([i+1 for i in range(len(jw_val_epoch))],jw_val_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Loss with learning rate {learning_rate}')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1636915057937,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "j8coUlN-YBLI",
    "outputId": "42d1e992-a8b2-4b8c-8277-38b4fa3def7e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracy over epoch\n",
    "plt.plot([i+1 for i in range(len(acc_train_epoch))],acc_train_epoch)\n",
    "plt.plot([i+1 for i in range(len(acc_val_epoch))],acc_val_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Accuracy with learning rate {learning_rate}')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJQvqXWnYBLI"
   },
   "source": [
    "**Calculate test accuracy for model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 116349,
     "status": "ok",
     "timestamp": 1636915899124,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "EoO1l_KoBvaf",
    "outputId": "9ea2644d-3adf-415c-aed6-af85e31145e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=5880, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing accuracy = 50.2%\n"
     ]
    }
   ],
   "source": [
    "# load saved model \n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.1/lenet5_lr0.1_cpu_epoch{3}.pth\"\n",
    "\n",
    "model = LeNet5()\n",
    "model.load_state_dict(torch.load(path))\n",
    "print(model.eval())\n",
    "\n",
    "# or use model in current session\n",
    "# model = model\n",
    "\n",
    "# calculate test accuracy of model at a given epoch\n",
    "\n",
    "testset_path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_testset.pth\"\n",
    "inputs, labels = torch.load(testset_path)\n",
    "\n",
    "logits, outputs = model(inputs)\n",
    "\n",
    "# calculate overall accuracy across all classes\n",
    "pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "correct = pred == labels\n",
    "\n",
    "correct_cnt = sum(np.array(correct))\n",
    "data_cnt = len(np.array(correct))\n",
    "\n",
    "print(f'\\n Testing accuracy = {(correct_cnt/data_cnt)*100:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LeNet5_model_noAugmentation_lr0.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
