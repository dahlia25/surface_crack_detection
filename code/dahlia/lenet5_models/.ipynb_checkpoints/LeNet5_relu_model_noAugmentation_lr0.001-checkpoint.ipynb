{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLEh-LthYBKv"
   },
   "source": [
    "# Surface Crack Images - LeNet5 model\n",
    "This notebook contains the code training the LeNet5 model (without augmented data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEb3XLxjYBK4"
   },
   "source": [
    "**Load packages/modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24315,
     "status": "ok",
     "timestamp": 1638249827715,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "H-oXyKDyYEED",
    "outputId": "6d07c249-2ec3-4eb5-b365-aed35ad0bc0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6178,
     "status": "ok",
     "timestamp": 1638249834434,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "9AKkpfhtYBK5",
    "outputId": "0990c513-f22b-459a-ca35-7a8c44db5c49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.10.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print(f'Torch version: {torch .__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3700,
     "status": "ok",
     "timestamp": 1638249838124,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "i_OfDGEmYBK8",
    "outputId": "9fde544d-ee71-4cf5-eb74-8a98dcb05016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "%pip install torchsummary\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysKmpc9cYBK8"
   },
   "source": [
    "**Load data into tensors first**<br>\n",
    "Then concatenate the tensors with original and augmented data into one for ease of model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syPC_CMNYBK_"
   },
   "outputs": [],
   "source": [
    "# load data into tensors first\n",
    "data_dir = '/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/archive/original'\n",
    "# aug_data_dir = '/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/archive/augmented'\n",
    "batch_size = 64 \n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "data = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(data, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True, \n",
    "                                         pin_memory=True)\n",
    "\n",
    "# aug_dataloader = torch.utils.data.DataLoader(aug_data,\n",
    "#                                              batch_size=batch_size*9,  # multiply by 9 since augmented x9 images for each original\n",
    "#                                              shuffle=True,\n",
    "#                                              pin_memory=True)\n",
    "\n",
    "# images, labels = next(iter(dataloader))\n",
    "# aug_images, aug_labels = next(iter(aug_dataloader))\n",
    "\n",
    "# print(f'Original images shape: {images.shape}')\n",
    "# print(f'Augmented images shape: {aug_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9C4r55Yj-Pf"
   },
   "outputs": [],
   "source": [
    "# generate and save test set\n",
    "test_counter = 1\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    if i+1 > 500:\n",
    "        if test_counter == 1:\n",
    "            test_inputs, test_labels = data \n",
    "            test_counter += 1\n",
    "        else:\n",
    "            new_inputs, new_labels = data\n",
    "            test_inputs = torch.concat((test_inputs, new_inputs), 0)\n",
    "            test_labels = torch.concat((test_labels, new_labels), 0)\n",
    "            test_counter += 1\n",
    "\n",
    "        # print(f'Batch {i+1} for test set complete.')\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# print(f'Saving test inputs {test_inputs.shape}, test labels {test_labels.shape}')\n",
    "\n",
    "# save test set at the end of training\n",
    "testset = [test_inputs, test_labels]\n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_testset.pth\"\n",
    "torch.save(testset, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZaeGsiNYBLC"
   },
   "source": [
    "## Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 160,
     "status": "ok",
     "timestamp": 1638249850378,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "-0gG2OoGYBLD"
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, padding=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=2, stride=2)       \n",
    "        self.conv3 = nn.Conv2d(16, 120, 5, padding=2, stride=2)\n",
    "\n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(120*49, 84)\n",
    "        self.fc2 = nn.Linear(84, 2)   # have two classes (has crack/no crack)\n",
    "        \n",
    "        # softmax layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # pooling layer\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2)  # Average pool 2x2\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = F.relu(self.conv1(x)) # layer 1\n",
    "        x = self.pool(x)          # layer 2\n",
    "        \n",
    "        x = F.relu(self.conv2(x)) # layer 3\n",
    "        x = self.pool(x)          # layer 4\n",
    "        \n",
    "        x = F.relu(self.conv3(x)) # layer 5\n",
    "        \n",
    "        x = torch.flatten(x, 1)   # flatten\n",
    "        \n",
    "        x = F.relu(self.fc1(x))   # reshape layer\n",
    "        logit = self.fc2(x)       # layer 6\n",
    "        \n",
    "        output = self.softmax(logit) # layer 7\n",
    "\n",
    "        return logit, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 160,
     "status": "ok",
     "timestamp": 1638249855900,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "0gbcU53BYBLE",
    "outputId": "12a5f119-a20f-4919-f8e3-dd2b19c4c0c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 114, 114]             456\n",
      "         AvgPool2d-2            [-1, 6, 57, 57]               0\n",
      "            Conv2d-3           [-1, 16, 29, 29]           2,416\n",
      "         AvgPool2d-4           [-1, 16, 14, 14]               0\n",
      "            Conv2d-5            [-1, 120, 7, 7]          48,120\n",
      "            Linear-6                   [-1, 84]         494,004\n",
      "            Linear-7                    [-1, 2]             170\n",
      "           Softmax-8                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 545,166\n",
      "Trainable params: 545,166\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 0.92\n",
      "Params size (MB): 2.08\n",
      "Estimated Total Size (MB): 3.59\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print model summary\n",
    "\n",
    "model = LeNet5()\n",
    "\n",
    "channels = 3\n",
    "H = 227\n",
    "W = 227\n",
    "\n",
    "summary(model, (channels, H, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp89_6KnjowJ"
   },
   "source": [
    "### Find optimal learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiN5vfU2U_-j"
   },
   "source": [
    "**Train model with learning rates by starting with small number of batches**, so can confirm that architecture is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10481796,
     "status": "ok",
     "timestamp": 1636927084450,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "0acPeyXXsDdk",
    "outputId": "67b3fe57-7002-4f05-ad25-365c8a00f7d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Training iteration 21 loss: 0.68706876039505, ACC:0.59375\n",
      "Training iteration 22 loss: 0.6868566274642944, ACC:0.59375\n",
      "Training iteration 23 loss: 0.6895932555198669, ACC:0.53125\n",
      "Training iteration 24 loss: 0.6885916590690613, ACC:0.546875\n",
      "Training iteration 25 loss: 0.6926044225692749, ACC:0.484375\n",
      "Training iteration 26 loss: 0.6852153539657593, ACC:0.59375\n",
      "Training iteration 27 loss: 0.6967850923538208, ACC:0.421875\n",
      "Training iteration 28 loss: 0.6948182582855225, ACC:0.421875\n",
      "Training iteration 29 loss: 0.6860550045967102, ACC:0.5625\n",
      "Training iteration 30 loss: 0.687808632850647, ACC:0.75\n",
      "Training iteration 31 loss: 0.6865770220756531, ACC:0.734375\n",
      "Training iteration 32 loss: 0.6844711899757385, ACC:0.625\n",
      "Training iteration 33 loss: 0.6829376816749573, ACC:0.59375\n",
      "Training iteration 34 loss: 0.680153489112854, ACC:0.6875\n",
      "Training iteration 35 loss: 0.6813803315162659, ACC:0.734375\n",
      "Training iteration 36 loss: 0.6652751564979553, ACC:0.8125\n",
      "Training iteration 37 loss: 0.6729304194450378, ACC:0.578125\n",
      "Training iteration 38 loss: 0.663058876991272, ACC:0.703125\n",
      "Training iteration 39 loss: 0.6828747391700745, ACC:0.578125\n",
      "Training iteration 40 loss: 0.6432594060897827, ACC:0.703125\n",
      "Training iteration 41 loss: 0.6595215201377869, ACC:0.515625\n",
      "Training iteration 42 loss: 0.6082711815834045, ACC:0.65625\n",
      "Training iteration 43 loss: 0.6498780846595764, ACC:0.5625\n",
      "Training iteration 44 loss: 0.6318445205688477, ACC:0.640625\n",
      "Training iteration 45 loss: 0.6252822875976562, ACC:0.84375\n",
      "Training iteration 46 loss: 0.6743173003196716, ACC:0.5\n",
      "Training iteration 47 loss: 0.5690808892250061, ACC:0.921875\n",
      "Training iteration 48 loss: 0.5555682182312012, ACC:0.734375\n",
      "Training iteration 49 loss: 0.543191134929657, ACC:0.828125\n",
      "Training iteration 50 loss: 0.47467583417892456, ACC:0.765625\n",
      "Training iteration 51 loss: 0.4765062928199768, ACC:0.796875\n",
      "Training iteration 52 loss: 0.7044181227684021, ACC:0.65625\n",
      "Training iteration 53 loss: 0.41531509160995483, ACC:0.84375\n",
      "Training iteration 54 loss: 0.48832380771636963, ACC:0.6875\n",
      "Training iteration 55 loss: 0.481334388256073, ACC:0.78125\n",
      "Training iteration 56 loss: 0.9700513482093811, ACC:0.4375\n",
      "Training iteration 57 loss: 0.5500187873840332, ACC:0.625\n",
      "Training iteration 58 loss: 0.47545430064201355, ACC:0.765625\n",
      "Training iteration 59 loss: 0.5215383768081665, ACC:0.765625\n",
      "Training iteration 60 loss: 0.4309237599372864, ACC:0.8125\n",
      "Training iteration 61 loss: 0.46013665199279785, ACC:0.796875\n",
      "Training iteration 62 loss: 0.5213523507118225, ACC:0.8125\n",
      "Training iteration 63 loss: 0.4193727672100067, ACC:0.90625\n",
      "Training iteration 64 loss: 0.4390464425086975, ACC:0.90625\n",
      "Training iteration 65 loss: 0.3538506329059601, ACC:0.953125\n",
      "Training iteration 66 loss: 0.36951279640197754, ACC:0.875\n",
      "Training iteration 67 loss: 0.3127395212650299, ACC:0.921875\n",
      "Training iteration 68 loss: 0.33236339688301086, ACC:0.890625\n",
      "Training iteration 69 loss: 0.36764419078826904, ACC:0.90625\n",
      "Training iteration 70 loss: 0.29121890664100647, ACC:0.921875\n",
      "Training iteration 71 loss: 0.3038235604763031, ACC:0.9375\n",
      "Training iteration 72 loss: 0.23752373456954956, ACC:0.921875\n",
      "Training iteration 73 loss: 0.20916244387626648, ACC:0.921875\n",
      "Training iteration 74 loss: 0.24796389043331146, ACC:0.953125\n",
      "Training iteration 75 loss: 0.3383721709251404, ACC:0.890625\n",
      "Training iteration 76 loss: 0.2886645197868347, ACC:0.875\n",
      "Training iteration 77 loss: 0.23099905252456665, ACC:0.890625\n",
      "Training iteration 78 loss: 0.31047043204307556, ACC:0.890625\n",
      "Training iteration 79 loss: 0.23706947267055511, ACC:0.921875\n",
      "Training iteration 80 loss: 0.30340781807899475, ACC:0.984375\n",
      "Training iteration 81 loss: 0.18345597386360168, ACC:0.984375\n",
      "Training iteration 82 loss: 0.3866070508956909, ACC:0.828125\n",
      "Training iteration 83 loss: 0.42289575934410095, ACC:0.828125\n",
      "Training iteration 84 loss: 0.29301464557647705, ACC:0.875\n",
      "Training iteration 85 loss: 0.6386300921440125, ACC:0.5\n",
      "Training iteration 86 loss: 0.32300934195518494, ACC:0.84375\n",
      "Training iteration 87 loss: 0.2545042335987091, ACC:0.90625\n",
      "Training iteration 88 loss: 0.3204089403152466, ACC:0.875\n",
      "Training iteration 89 loss: 0.26341795921325684, ACC:0.875\n",
      "Training iteration 90 loss: 0.12037869542837143, ACC:0.984375\n",
      "Training iteration 91 loss: 0.9232913255691528, ACC:0.65625\n",
      "Training iteration 92 loss: 0.474635511636734, ACC:0.9375\n",
      "Training iteration 93 loss: 0.20981451869010925, ACC:0.890625\n",
      "Training iteration 94 loss: 0.073980413377285, ACC:1.0\n",
      "Training iteration 95 loss: 0.3650462329387665, ACC:0.828125\n",
      "Training iteration 96 loss: 0.19712743163108826, ACC:0.90625\n",
      "Training iteration 97 loss: 0.15268971025943756, ACC:0.9375\n",
      "Training iteration 98 loss: 0.2853039503097534, ACC:0.859375\n",
      "Training iteration 99 loss: 0.1555800437927246, ACC:0.96875\n",
      "Training iteration 100 loss: 0.19626931846141815, ACC:0.9375\n",
      "Training iteration 101 loss: 0.23258396983146667, ACC:0.90625\n",
      "Training iteration 102 loss: 0.18088003993034363, ACC:0.9375\n",
      "Training iteration 103 loss: 0.18491356074810028, ACC:0.921875\n",
      "Training iteration 104 loss: 0.36635586619377136, ACC:0.828125\n",
      "Training iteration 105 loss: 0.21304744482040405, ACC:0.921875\n",
      "Training iteration 106 loss: 0.2175789624452591, ACC:0.921875\n",
      "Training iteration 107 loss: 0.24380287528038025, ACC:0.890625\n",
      "Training iteration 108 loss: 0.25498104095458984, ACC:0.921875\n",
      "Training iteration 109 loss: 0.21345162391662598, ACC:0.96875\n",
      "Training iteration 110 loss: 0.3159853518009186, ACC:0.890625\n",
      "Training iteration 111 loss: 0.3315090239048004, ACC:0.8125\n",
      "Training iteration 112 loss: 0.15701442956924438, ACC:0.953125\n",
      "Training iteration 113 loss: 0.11886133253574371, ACC:0.96875\n",
      "Training iteration 114 loss: 0.24438564479351044, ACC:0.890625\n",
      "Training iteration 115 loss: 0.1412973254919052, ACC:0.9375\n",
      "Training iteration 116 loss: 0.14490848779678345, ACC:0.953125\n",
      "Training iteration 117 loss: 0.16820946335792542, ACC:0.921875\n",
      "Training iteration 118 loss: 0.16161033511161804, ACC:0.921875\n",
      "Training iteration 119 loss: 0.14382526278495789, ACC:0.921875\n",
      "Training iteration 120 loss: 0.3192133605480194, ACC:0.859375\n",
      "Training iteration 121 loss: 0.137671560049057, ACC:0.953125\n",
      "Training iteration 122 loss: 0.18343134224414825, ACC:0.921875\n",
      "Training iteration 123 loss: 0.2015428990125656, ACC:0.921875\n",
      "Training iteration 124 loss: 0.13138830661773682, ACC:0.96875\n",
      "Training iteration 125 loss: 0.19610793888568878, ACC:0.90625\n",
      "Training iteration 126 loss: 0.13154113292694092, ACC:0.96875\n",
      "Training iteration 127 loss: 0.08289021253585815, ACC:0.953125\n",
      "Training iteration 128 loss: 0.16508494317531586, ACC:0.9375\n",
      "Training iteration 129 loss: 0.14749304950237274, ACC:0.9375\n",
      "Training iteration 130 loss: 0.11146517843008041, ACC:0.96875\n",
      "Training iteration 131 loss: 0.10351919382810593, ACC:0.96875\n",
      "Training iteration 132 loss: 0.13784736394882202, ACC:0.953125\n",
      "Training iteration 133 loss: 0.05714217945933342, ACC:1.0\n",
      "Training iteration 134 loss: 0.1586727499961853, ACC:0.921875\n",
      "Training iteration 135 loss: 0.1334761530160904, ACC:0.953125\n",
      "Training iteration 136 loss: 0.06308796256780624, ACC:0.984375\n",
      "Training iteration 137 loss: 0.07463426142930984, ACC:0.96875\n",
      "Training iteration 138 loss: 0.08976267278194427, ACC:0.96875\n",
      "Training iteration 139 loss: 0.2082066833972931, ACC:0.921875\n",
      "Training iteration 140 loss: 0.026393448933959007, ACC:1.0\n",
      "Training iteration 141 loss: 0.19153019785881042, ACC:0.921875\n",
      "Training iteration 142 loss: 0.20686456561088562, ACC:0.890625\n",
      "Training iteration 143 loss: 0.106819286942482, ACC:0.96875\n",
      "Training iteration 144 loss: 0.16434705257415771, ACC:0.90625\n",
      "Training iteration 145 loss: 0.08625999838113785, ACC:0.953125\n",
      "Training iteration 146 loss: 0.12453816086053848, ACC:0.953125\n",
      "Training iteration 147 loss: 0.04274572804570198, ACC:1.0\n",
      "Training iteration 148 loss: 0.08939160406589508, ACC:0.96875\n",
      "Training iteration 149 loss: 0.11015375703573227, ACC:0.984375\n",
      "Training iteration 150 loss: 0.06672865152359009, ACC:0.984375\n",
      "Training iteration 151 loss: 0.2972572147846222, ACC:0.90625\n",
      "Training iteration 152 loss: 0.09885729104280472, ACC:0.984375\n",
      "Training iteration 153 loss: 0.17047981917858124, ACC:0.921875\n",
      "Training iteration 154 loss: 0.12276782840490341, ACC:0.9375\n",
      "Training iteration 155 loss: 0.07931306958198547, ACC:0.96875\n",
      "Training iteration 156 loss: 0.2766505181789398, ACC:0.9375\n",
      "Training iteration 157 loss: 0.02108202502131462, ACC:1.0\n",
      "Training iteration 158 loss: 0.45959335565567017, ACC:0.890625\n",
      "Training iteration 159 loss: 0.24787265062332153, ACC:0.90625\n",
      "Training iteration 160 loss: 0.6220690608024597, ACC:0.578125\n",
      "Training iteration 161 loss: 0.18959900736808777, ACC:0.921875\n",
      "Training iteration 162 loss: 0.43215101957321167, ACC:0.765625\n",
      "Training iteration 163 loss: 0.12137404084205627, ACC:0.953125\n",
      "Training iteration 164 loss: 0.31197458505630493, ACC:0.90625\n",
      "Training iteration 165 loss: 0.2753349244594574, ACC:0.859375\n",
      "Training iteration 166 loss: 0.10410252213478088, ACC:0.96875\n",
      "Training iteration 167 loss: 0.07959546893835068, ACC:0.96875\n",
      "Training iteration 168 loss: 0.1805780678987503, ACC:0.90625\n",
      "Training iteration 169 loss: 0.16787728667259216, ACC:0.96875\n",
      "Training iteration 170 loss: 0.1921456903219223, ACC:0.9375\n",
      "Training iteration 171 loss: 0.16734173893928528, ACC:0.953125\n",
      "Training iteration 172 loss: 0.14316684007644653, ACC:0.953125\n",
      "Training iteration 173 loss: 0.07200974971055984, ACC:0.96875\n",
      "Training iteration 174 loss: 0.08556436002254486, ACC:0.984375\n",
      "Training iteration 175 loss: 0.08250833302736282, ACC:0.96875\n",
      "Training iteration 176 loss: 0.10598915070295334, ACC:0.953125\n",
      "Training iteration 177 loss: 0.09099119901657104, ACC:0.953125\n",
      "Training iteration 178 loss: 0.06623612344264984, ACC:0.984375\n",
      "Training iteration 179 loss: 0.17097681760787964, ACC:0.953125\n",
      "Training iteration 180 loss: 0.11175896227359772, ACC:0.953125\n",
      "Training iteration 181 loss: 0.14771130681037903, ACC:0.953125\n",
      "Training iteration 182 loss: 0.04224393889307976, ACC:1.0\n",
      "Training iteration 183 loss: 0.04923155531287193, ACC:1.0\n",
      "Training iteration 184 loss: 0.12288521975278854, ACC:0.953125\n",
      "Training iteration 185 loss: 0.0625084638595581, ACC:0.984375\n",
      "Training iteration 186 loss: 0.12016819417476654, ACC:0.9375\n",
      "Training iteration 187 loss: 0.0882720872759819, ACC:0.984375\n",
      "Training iteration 188 loss: 0.032293155789375305, ACC:1.0\n",
      "Training iteration 189 loss: 0.15098357200622559, ACC:0.953125\n",
      "Training iteration 190 loss: 0.07439219951629639, ACC:0.984375\n",
      "Training iteration 191 loss: 0.09204218536615372, ACC:0.96875\n",
      "Training iteration 192 loss: 0.061581142246723175, ACC:1.0\n",
      "Training iteration 193 loss: 0.10797707736492157, ACC:0.96875\n",
      "Training iteration 194 loss: 0.06938649713993073, ACC:0.984375\n",
      "Training iteration 195 loss: 0.15271472930908203, ACC:0.9375\n",
      "Training iteration 196 loss: 0.09862431138753891, ACC:0.953125\n",
      "Training iteration 197 loss: 0.11203654110431671, ACC:0.953125\n",
      "Training iteration 198 loss: 0.14810214936733246, ACC:0.921875\n",
      "Training iteration 199 loss: 0.11588925868272781, ACC:0.9375\n",
      "Training iteration 200 loss: 0.11023423820734024, ACC:0.953125\n",
      "Training iteration 201 loss: 0.16073910892009735, ACC:0.9375\n",
      "Training iteration 202 loss: 0.04663883149623871, ACC:0.984375\n",
      "Training iteration 203 loss: 0.08812173455953598, ACC:0.953125\n",
      "Training iteration 204 loss: 0.05091390386223793, ACC:0.96875\n",
      "Training iteration 205 loss: 0.05898750200867653, ACC:0.96875\n",
      "Training iteration 206 loss: 0.04587636515498161, ACC:0.984375\n",
      "Training iteration 207 loss: 0.03514232486486435, ACC:1.0\n",
      "Training iteration 208 loss: 0.04294470697641373, ACC:0.984375\n",
      "Training iteration 209 loss: 0.10615436732769012, ACC:0.953125\n",
      "Training iteration 210 loss: 0.03862108290195465, ACC:0.984375\n",
      "Training iteration 211 loss: 0.07713478058576584, ACC:0.96875\n",
      "Training iteration 212 loss: 0.020791811868548393, ACC:1.0\n",
      "Training iteration 213 loss: 0.049542758613824844, ACC:0.984375\n",
      "Training iteration 214 loss: 0.0469631664454937, ACC:0.984375\n",
      "Training iteration 215 loss: 0.04054976999759674, ACC:0.984375\n",
      "Training iteration 216 loss: 0.08810461312532425, ACC:0.96875\n",
      "Training iteration 217 loss: 0.07056780904531479, ACC:0.96875\n",
      "Training iteration 218 loss: 0.06514225155115128, ACC:0.984375\n",
      "Training iteration 219 loss: 0.06564459204673767, ACC:0.953125\n",
      "Training iteration 220 loss: 0.01884322240948677, ACC:1.0\n",
      "Training iteration 221 loss: 0.11577532440423965, ACC:0.96875\n",
      "Training iteration 222 loss: 0.07865849882364273, ACC:0.96875\n",
      "Training iteration 223 loss: 0.018922073766589165, ACC:1.0\n",
      "Training iteration 224 loss: 0.19790472090244293, ACC:0.9375\n",
      "Training iteration 225 loss: 0.1768157184123993, ACC:0.9375\n",
      "Training iteration 226 loss: 0.0871865451335907, ACC:0.984375\n",
      "Training iteration 227 loss: 0.049636490643024445, ACC:0.96875\n",
      "Training iteration 228 loss: 0.1495763659477234, ACC:0.96875\n",
      "Training iteration 229 loss: 0.12481985241174698, ACC:0.953125\n",
      "Training iteration 230 loss: 0.048808518797159195, ACC:0.984375\n",
      "Training iteration 231 loss: 0.07685307413339615, ACC:0.96875\n",
      "Training iteration 232 loss: 0.13762794435024261, ACC:0.953125\n",
      "Training iteration 233 loss: 0.08568710088729858, ACC:0.953125\n",
      "Training iteration 234 loss: 0.2162337601184845, ACC:0.9375\n",
      "Training iteration 235 loss: 0.17617425322532654, ACC:0.953125\n",
      "Training iteration 236 loss: 0.11989188939332962, ACC:0.953125\n",
      "Training iteration 237 loss: 0.16183917224407196, ACC:0.953125\n",
      "Training iteration 238 loss: 0.06993885338306427, ACC:0.96875\n",
      "Training iteration 239 loss: 0.03377700597047806, ACC:0.984375\n",
      "Training iteration 240 loss: 0.05929071083664894, ACC:0.96875\n",
      "Training iteration 241 loss: 0.13739225268363953, ACC:0.953125\n",
      "Training iteration 242 loss: 0.13164125382900238, ACC:0.921875\n",
      "Training iteration 243 loss: 0.08932347595691681, ACC:0.96875\n",
      "Training iteration 244 loss: 0.07272259891033173, ACC:0.984375\n",
      "Training iteration 245 loss: 0.13876351714134216, ACC:0.953125\n",
      "Training iteration 246 loss: 0.06278982013463974, ACC:0.984375\n",
      "Training iteration 247 loss: 0.1483897864818573, ACC:0.953125\n",
      "Training iteration 248 loss: 0.11948565393686295, ACC:0.984375\n",
      "Training iteration 249 loss: 0.21135304868221283, ACC:0.921875\n",
      "Training iteration 250 loss: 0.07543694972991943, ACC:0.953125\n",
      "Training iteration 251 loss: 0.1101066917181015, ACC:0.96875\n",
      "Training iteration 252 loss: 0.04656563326716423, ACC:1.0\n",
      "Training iteration 253 loss: 0.15775789320468903, ACC:0.953125\n",
      "Training iteration 254 loss: 0.13339674472808838, ACC:0.9375\n",
      "Training iteration 255 loss: 0.10854966938495636, ACC:0.953125\n",
      "Training iteration 256 loss: 0.10402543842792511, ACC:0.96875\n",
      "Training iteration 257 loss: 0.06855850666761398, ACC:0.984375\n",
      "Training iteration 258 loss: 0.14645670354366302, ACC:0.953125\n",
      "Training iteration 259 loss: 0.0724765956401825, ACC:0.984375\n",
      "Training iteration 260 loss: 0.028459152206778526, ACC:1.0\n",
      "Training iteration 261 loss: 0.12685132026672363, ACC:0.953125\n",
      "Training iteration 262 loss: 0.09748059511184692, ACC:0.96875\n",
      "Training iteration 263 loss: 0.02525465562939644, ACC:1.0\n",
      "Training iteration 264 loss: 0.07295890152454376, ACC:0.984375\n",
      "Training iteration 265 loss: 0.08785673975944519, ACC:0.953125\n",
      "Training iteration 266 loss: 0.16473589837551117, ACC:0.9375\n",
      "Training iteration 267 loss: 0.07194183766841888, ACC:0.96875\n",
      "Training iteration 268 loss: 0.03088906593620777, ACC:0.984375\n",
      "Training iteration 269 loss: 0.049457844346761703, ACC:0.96875\n",
      "Training iteration 270 loss: 0.015915893018245697, ACC:1.0\n",
      "Training iteration 271 loss: 0.08862164616584778, ACC:0.96875\n",
      "Training iteration 272 loss: 0.18456807732582092, ACC:0.953125\n",
      "Training iteration 273 loss: 0.13819071650505066, ACC:0.921875\n",
      "Training iteration 274 loss: 0.03844372555613518, ACC:0.984375\n",
      "Training iteration 275 loss: 0.04649043083190918, ACC:0.984375\n",
      "Training iteration 276 loss: 0.08444912731647491, ACC:0.953125\n",
      "Training iteration 277 loss: 0.026205049827694893, ACC:1.0\n",
      "Training iteration 278 loss: 0.0730087012052536, ACC:0.984375\n",
      "Training iteration 279 loss: 0.0873834490776062, ACC:0.984375\n",
      "Training iteration 280 loss: 0.11155501753091812, ACC:0.96875\n",
      "Training iteration 281 loss: 0.040048129856586456, ACC:0.984375\n",
      "Training iteration 282 loss: 0.070674367249012, ACC:0.96875\n",
      "Training iteration 283 loss: 0.02672278881072998, ACC:1.0\n",
      "Training iteration 284 loss: 0.04518594965338707, ACC:0.984375\n",
      "Training iteration 285 loss: 0.04832357540726662, ACC:0.984375\n",
      "Training iteration 286 loss: 0.08936269581317902, ACC:0.96875\n",
      "Training iteration 287 loss: 0.06211652234196663, ACC:0.984375\n",
      "Training iteration 288 loss: 0.04413379356265068, ACC:1.0\n",
      "Training iteration 289 loss: 0.10313670337200165, ACC:0.9375\n",
      "Training iteration 290 loss: 0.07690371572971344, ACC:0.984375\n",
      "Training iteration 291 loss: 0.01529000885784626, ACC:1.0\n",
      "Training iteration 292 loss: 0.07400474697351456, ACC:0.984375\n",
      "Training iteration 293 loss: 0.042328447103500366, ACC:0.984375\n",
      "Training iteration 294 loss: 0.0176077950745821, ACC:1.0\n",
      "Training iteration 295 loss: 0.1103423461318016, ACC:0.9375\n",
      "Training iteration 296 loss: 0.034909285604953766, ACC:0.984375\n",
      "Training iteration 297 loss: 0.05546123534440994, ACC:0.984375\n",
      "Training iteration 298 loss: 0.015881510451436043, ACC:1.0\n",
      "Training iteration 299 loss: 0.05546828359365463, ACC:0.984375\n",
      "Training iteration 300 loss: 0.06654549390077591, ACC:0.984375\n",
      "Training iteration 301 loss: 0.017455512657761574, ACC:1.0\n",
      "Training iteration 302 loss: 0.008524070493876934, ACC:1.0\n",
      "Training iteration 303 loss: 0.03069182299077511, ACC:0.984375\n",
      "Training iteration 304 loss: 0.19158770143985748, ACC:0.9375\n",
      "Training iteration 305 loss: 0.015199447050690651, ACC:1.0\n",
      "Training iteration 306 loss: 0.13486787676811218, ACC:0.953125\n",
      "Training iteration 307 loss: 0.08253110200166702, ACC:0.953125\n",
      "Training iteration 308 loss: 0.1137201264500618, ACC:0.953125\n",
      "Training iteration 309 loss: 0.08715060353279114, ACC:0.96875\n",
      "Training iteration 310 loss: 0.0837484747171402, ACC:0.96875\n",
      "Training iteration 311 loss: 0.05924343317747116, ACC:0.984375\n",
      "Training iteration 312 loss: 0.06681233644485474, ACC:0.984375\n",
      "Training iteration 313 loss: 0.01741049811244011, ACC:1.0\n",
      "Training iteration 314 loss: 0.05781437084078789, ACC:0.984375\n",
      "Training iteration 315 loss: 0.05474055930972099, ACC:0.984375\n",
      "Training iteration 316 loss: 0.04084259644150734, ACC:0.984375\n",
      "Training iteration 317 loss: 0.03547091409564018, ACC:0.984375\n",
      "Training iteration 318 loss: 0.08967956155538559, ACC:0.96875\n",
      "Training iteration 319 loss: 0.009020598605275154, ACC:1.0\n",
      "Training iteration 320 loss: 0.043423619121313095, ACC:0.96875\n",
      "Training iteration 321 loss: 0.0650792270898819, ACC:0.96875\n",
      "Training iteration 322 loss: 0.04965555667877197, ACC:0.984375\n",
      "Training iteration 323 loss: 0.015689807012677193, ACC:1.0\n",
      "Training iteration 324 loss: 0.021356549113988876, ACC:1.0\n",
      "Training iteration 325 loss: 0.16943959891796112, ACC:0.90625\n",
      "Training iteration 326 loss: 0.08441830426454544, ACC:0.96875\n",
      "Training iteration 327 loss: 0.05379436910152435, ACC:0.96875\n",
      "Training iteration 328 loss: 0.03257552906870842, ACC:0.984375\n",
      "Training iteration 329 loss: 0.035582881420850754, ACC:0.984375\n",
      "Training iteration 330 loss: 0.02198980562388897, ACC:0.984375\n",
      "Training iteration 331 loss: 0.11153273284435272, ACC:0.953125\n",
      "Training iteration 332 loss: 0.04167473316192627, ACC:0.984375\n",
      "Training iteration 333 loss: 0.08702900260686874, ACC:0.96875\n",
      "Training iteration 334 loss: 0.04251496493816376, ACC:0.984375\n",
      "Training iteration 335 loss: 0.09985456615686417, ACC:0.96875\n",
      "Training iteration 336 loss: 0.008872929960489273, ACC:1.0\n",
      "Training iteration 337 loss: 0.14658483862876892, ACC:0.890625\n",
      "Training iteration 338 loss: 0.049618322402238846, ACC:0.96875\n",
      "Training iteration 339 loss: 0.033817190676927567, ACC:1.0\n",
      "Training iteration 340 loss: 0.03263189643621445, ACC:0.984375\n",
      "Training iteration 341 loss: 0.037681061774492264, ACC:0.984375\n",
      "Training iteration 342 loss: 0.0522681288421154, ACC:0.96875\n",
      "Training iteration 343 loss: 0.09849955141544342, ACC:0.9375\n",
      "Training iteration 344 loss: 0.07292836159467697, ACC:0.953125\n",
      "Training iteration 345 loss: 0.03916093334555626, ACC:0.984375\n",
      "Training iteration 346 loss: 0.056339848786592484, ACC:0.984375\n",
      "Training iteration 347 loss: 0.04307987168431282, ACC:0.984375\n",
      "Training iteration 348 loss: 0.012212508358061314, ACC:1.0\n",
      "Training iteration 349 loss: 0.052837252616882324, ACC:0.984375\n",
      "Training iteration 350 loss: 0.07574690133333206, ACC:0.984375\n",
      "Training iteration 351 loss: 0.06349571794271469, ACC:0.96875\n",
      "Training iteration 352 loss: 0.011350482702255249, ACC:1.0\n",
      "Training iteration 353 loss: 0.09371839463710785, ACC:0.953125\n",
      "Training iteration 354 loss: 0.08120782673358917, ACC:0.984375\n",
      "Training iteration 355 loss: 0.08641298115253448, ACC:0.953125\n",
      "Training iteration 356 loss: 0.017177416011691093, ACC:1.0\n",
      "Training iteration 357 loss: 0.10496043413877487, ACC:0.96875\n",
      "Training iteration 358 loss: 0.030037540942430496, ACC:1.0\n",
      "Training iteration 359 loss: 0.02236849255859852, ACC:1.0\n",
      "Training iteration 360 loss: 0.13543017208576202, ACC:0.953125\n",
      "Training iteration 361 loss: 0.057475049048662186, ACC:0.984375\n",
      "Training iteration 362 loss: 0.10326360911130905, ACC:0.96875\n",
      "Training iteration 363 loss: 0.03100782260298729, ACC:0.984375\n",
      "Training iteration 364 loss: 0.02964935638010502, ACC:1.0\n",
      "Training iteration 365 loss: 0.0954199805855751, ACC:0.9375\n",
      "Training iteration 366 loss: 0.010970606468617916, ACC:1.0\n",
      "Training iteration 367 loss: 0.02051013894379139, ACC:0.984375\n",
      "Training iteration 368 loss: 0.045382432639598846, ACC:0.984375\n",
      "Training iteration 369 loss: 0.03477049991488457, ACC:0.96875\n",
      "Training iteration 370 loss: 0.10248813033103943, ACC:0.953125\n",
      "Training iteration 371 loss: 0.013620891608297825, ACC:1.0\n",
      "Training iteration 372 loss: 0.10111262649297714, ACC:0.984375\n",
      "Training iteration 373 loss: 0.03611914440989494, ACC:0.96875\n",
      "Training iteration 374 loss: 0.02687050960958004, ACC:0.984375\n",
      "Training iteration 375 loss: 0.04210241138935089, ACC:0.984375\n",
      "Training iteration 376 loss: 0.10810690373182297, ACC:0.953125\n",
      "Training iteration 377 loss: 0.027757631614804268, ACC:0.984375\n",
      "Training iteration 378 loss: 0.05201426520943642, ACC:0.984375\n",
      "Training iteration 379 loss: 0.016049174591898918, ACC:1.0\n",
      "Training iteration 380 loss: 0.03823409602046013, ACC:0.984375\n",
      "Training iteration 381 loss: 0.016943173483014107, ACC:0.984375\n",
      "Training iteration 382 loss: 0.016232581809163094, ACC:1.0\n",
      "Training iteration 383 loss: 0.047125332057476044, ACC:0.984375\n",
      "Training iteration 384 loss: 0.04745326191186905, ACC:0.984375\n",
      "Training iteration 385 loss: 0.1506873518228531, ACC:0.9375\n",
      "Training iteration 386 loss: 0.018433135002851486, ACC:1.0\n",
      "Training iteration 387 loss: 0.027233295142650604, ACC:1.0\n",
      "Training iteration 388 loss: 0.05146408453583717, ACC:0.96875\n",
      "Training iteration 389 loss: 0.032898928970098495, ACC:0.96875\n",
      "Training iteration 390 loss: 0.08021758496761322, ACC:0.96875\n",
      "Training iteration 391 loss: 0.02853536233305931, ACC:1.0\n",
      "Training iteration 392 loss: 0.06939499080181122, ACC:0.984375\n",
      "Training iteration 393 loss: 0.06352275609970093, ACC:0.96875\n",
      "Training iteration 394 loss: 0.09708382189273834, ACC:0.921875\n",
      "Training iteration 395 loss: 0.007600240875035524, ACC:1.0\n",
      "Training iteration 396 loss: 0.026899224147200584, ACC:0.984375\n",
      "Training iteration 397 loss: 0.30616524815559387, ACC:0.890625\n",
      "Training iteration 398 loss: 0.05918089300394058, ACC:0.96875\n",
      "Training iteration 399 loss: 0.1640237718820572, ACC:0.953125\n",
      "Training iteration 400 loss: 0.05372866615653038, ACC:1.0\n",
      "Training iteration 401 loss: 0.12120601534843445, ACC:0.96875\n",
      "Training iteration 402 loss: 0.11537694931030273, ACC:0.96875\n",
      "Training iteration 403 loss: 0.09141979366540909, ACC:1.0\n",
      "Training iteration 404 loss: 0.1100228875875473, ACC:0.953125\n",
      "Training iteration 405 loss: 0.11642242968082428, ACC:0.96875\n",
      "Training iteration 406 loss: 0.09575313329696655, ACC:0.96875\n",
      "Training iteration 407 loss: 0.10408339649438858, ACC:0.96875\n",
      "Training iteration 408 loss: 0.04558248072862625, ACC:0.984375\n",
      "Training iteration 409 loss: 0.049266256392002106, ACC:0.984375\n",
      "Training iteration 410 loss: 0.08524446189403534, ACC:0.96875\n",
      "Training iteration 411 loss: 0.19786794483661652, ACC:0.9375\n",
      "Training iteration 412 loss: 0.9095444083213806, ACC:0.421875\n",
      "Training iteration 413 loss: 0.38151121139526367, ACC:0.84375\n",
      "Training iteration 414 loss: 0.3231101632118225, ACC:0.890625\n",
      "Training iteration 415 loss: 0.3154210150241852, ACC:0.890625\n",
      "Training iteration 416 loss: 0.39914292097091675, ACC:0.796875\n",
      "Training iteration 417 loss: 0.38065487146377563, ACC:0.703125\n",
      "Training iteration 418 loss: 0.2273818403482437, ACC:0.96875\n",
      "Training iteration 419 loss: 0.14431285858154297, ACC:0.96875\n",
      "Training iteration 420 loss: 0.23286868631839752, ACC:0.921875\n",
      "Training iteration 421 loss: 0.17291589081287384, ACC:0.9375\n",
      "Training iteration 422 loss: 0.09192292392253876, ACC:0.96875\n",
      "Training iteration 423 loss: 0.25797343254089355, ACC:0.953125\n",
      "Training iteration 424 loss: 0.2411891371011734, ACC:0.90625\n",
      "Training iteration 425 loss: 0.24342657625675201, ACC:0.90625\n",
      "Training iteration 426 loss: 0.15853556990623474, ACC:0.921875\n",
      "Training iteration 427 loss: 0.16132985055446625, ACC:0.9375\n",
      "Training iteration 428 loss: 0.2327633947134018, ACC:0.96875\n",
      "Training iteration 429 loss: 0.19215765595436096, ACC:0.9375\n",
      "Training iteration 430 loss: 0.19595974683761597, ACC:0.921875\n",
      "Training iteration 431 loss: 0.26324641704559326, ACC:0.875\n",
      "Training iteration 432 loss: 0.1534959226846695, ACC:0.953125\n",
      "Training iteration 433 loss: 0.16531801223754883, ACC:0.953125\n",
      "Training iteration 434 loss: 0.2389947772026062, ACC:0.953125\n",
      "Training iteration 435 loss: 0.12789277732372284, ACC:0.953125\n",
      "Training iteration 436 loss: 0.3008025288581848, ACC:0.921875\n",
      "Training iteration 437 loss: 0.1719275414943695, ACC:0.9375\n",
      "Training iteration 438 loss: 0.11001183092594147, ACC:0.96875\n",
      "Training iteration 439 loss: 0.557823657989502, ACC:0.921875\n",
      "Training iteration 440 loss: 0.09787622839212418, ACC:0.96875\n",
      "Training iteration 441 loss: 0.3591320514678955, ACC:0.859375\n",
      "Training iteration 442 loss: 0.2816014289855957, ACC:0.875\n",
      "Training iteration 443 loss: 0.3129670321941376, ACC:0.859375\n",
      "Training iteration 444 loss: 0.42641526460647583, ACC:0.859375\n",
      "Training iteration 445 loss: 0.43780025839805603, ACC:0.953125\n",
      "Training iteration 446 loss: 0.36988183856010437, ACC:0.859375\n",
      "Training iteration 447 loss: 0.3369459807872772, ACC:0.875\n",
      "Training iteration 448 loss: 0.334673672914505, ACC:0.828125\n",
      "Training iteration 449 loss: 0.3845427334308624, ACC:0.8125\n",
      "Training iteration 450 loss: 0.286901593208313, ACC:0.890625\n",
      "Validation iteration 451 loss: 0.2572579085826874, ACC: 0.953125\n",
      "Validation iteration 452 loss: 0.2779410481452942, ACC: 0.953125\n",
      "Validation iteration 453 loss: 0.28412726521492004, ACC: 0.953125\n",
      "Validation iteration 454 loss: 0.3124215304851532, ACC: 0.890625\n",
      "Validation iteration 455 loss: 0.3179495930671692, ACC: 0.890625\n",
      "Validation iteration 456 loss: 0.4931589365005493, ACC: 0.890625\n",
      "Validation iteration 457 loss: 0.30157899856567383, ACC: 0.890625\n",
      "Validation iteration 458 loss: 0.2966937720775604, ACC: 0.890625\n",
      "Validation iteration 459 loss: 0.36692485213279724, ACC: 0.8125\n",
      "Validation iteration 460 loss: 0.3024357855319977, ACC: 0.890625\n",
      "Validation iteration 461 loss: 0.3100970685482025, ACC: 0.953125\n",
      "Validation iteration 462 loss: 0.2815842926502228, ACC: 0.9375\n",
      "Validation iteration 463 loss: 0.29257649183273315, ACC: 0.890625\n",
      "Validation iteration 464 loss: 0.3276234567165375, ACC: 0.890625\n",
      "Validation iteration 465 loss: 0.32439592480659485, ACC: 0.90625\n",
      "Validation iteration 466 loss: 0.2779315412044525, ACC: 0.90625\n",
      "Validation iteration 467 loss: 0.29963454604148865, ACC: 0.921875\n",
      "Validation iteration 468 loss: 0.27690258622169495, ACC: 0.9375\n",
      "Validation iteration 469 loss: 0.35990920662879944, ACC: 0.859375\n",
      "Validation iteration 470 loss: 0.3078102171421051, ACC: 0.921875\n",
      "Validation iteration 471 loss: 0.34091219305992126, ACC: 0.90625\n",
      "Validation iteration 472 loss: 0.3201175332069397, ACC: 0.859375\n",
      "Validation iteration 473 loss: 0.27204594016075134, ACC: 0.96875\n",
      "Validation iteration 474 loss: 0.31602704524993896, ACC: 0.953125\n",
      "Validation iteration 475 loss: 0.363025039434433, ACC: 0.84375\n",
      "Validation iteration 476 loss: 0.32816797494888306, ACC: 0.875\n",
      "Validation iteration 477 loss: 0.4601377248764038, ACC: 0.90625\n",
      "Validation iteration 478 loss: 0.3062031865119934, ACC: 0.859375\n",
      "Validation iteration 479 loss: 0.2606384754180908, ACC: 0.90625\n",
      "Validation iteration 480 loss: 0.2929699420928955, ACC: 0.953125\n",
      "Validation iteration 481 loss: 0.3425939381122589, ACC: 0.890625\n",
      "Validation iteration 482 loss: 0.31584271788597107, ACC: 0.90625\n",
      "Validation iteration 483 loss: 0.3168911039829254, ACC: 0.921875\n",
      "Validation iteration 484 loss: 0.37004348635673523, ACC: 0.890625\n",
      "Validation iteration 485 loss: 0.34013038873672485, ACC: 0.875\n",
      "Validation iteration 486 loss: 0.3793341815471649, ACC: 0.90625\n",
      "Validation iteration 487 loss: 0.2729319930076599, ACC: 0.921875\n",
      "Validation iteration 488 loss: 0.2632029056549072, ACC: 0.984375\n",
      "Validation iteration 489 loss: 0.2844078838825226, ACC: 0.890625\n",
      "Validation iteration 490 loss: 0.3883814215660095, ACC: 0.84375\n",
      "Validation iteration 491 loss: 0.2548971474170685, ACC: 0.9375\n",
      "Validation iteration 492 loss: 0.3099502623081207, ACC: 0.9375\n",
      "Validation iteration 493 loss: 0.3036946952342987, ACC: 0.9375\n",
      "Validation iteration 494 loss: 0.273661732673645, ACC: 0.953125\n",
      "Validation iteration 495 loss: 0.3049532175064087, ACC: 0.921875\n",
      "Validation iteration 496 loss: 0.3749658465385437, ACC: 0.9375\n",
      "Validation iteration 497 loss: 0.263173371553421, ACC: 0.953125\n",
      "Validation iteration 498 loss: 0.33600395917892456, ACC: 0.921875\n",
      "Validation iteration 499 loss: 0.3137222230434418, ACC: 0.890625\n",
      "Validation iteration 500 loss: 0.25126102566719055, ACC: 0.953125\n",
      "-- Epoch 1 done -- Train loss: 0.20854091112501918, train ACC: 0.9030555555555555, val loss: 0.31578487157821655, val ACC: 0.9109375\n",
      "<--- 15107.136325597763 seconds --->\n",
      "Training iteration 1 loss: 0.3486545979976654, ACC:0.921875\n",
      "Training iteration 2 loss: 0.27131524682044983, ACC:0.953125\n",
      "Training iteration 3 loss: 0.23867379128932953, ACC:0.9375\n",
      "Training iteration 4 loss: 0.1282990574836731, ACC:0.984375\n",
      "Training iteration 5 loss: 0.16314074397087097, ACC:0.953125\n",
      "Training iteration 6 loss: 0.2263241559267044, ACC:0.921875\n",
      "Training iteration 7 loss: 0.14857345819473267, ACC:0.953125\n",
      "Training iteration 8 loss: 0.16719193756580353, ACC:0.9375\n",
      "Training iteration 9 loss: 0.08547056466341019, ACC:0.96875\n",
      "Training iteration 10 loss: 0.10697068274021149, ACC:0.984375\n",
      "Training iteration 11 loss: 1.597259521484375, ACC:0.921875\n",
      "Training iteration 12 loss: 0.09352654218673706, ACC:0.984375\n",
      "Training iteration 13 loss: 0.06938103586435318, ACC:0.984375\n",
      "Training iteration 14 loss: 0.17293846607208252, ACC:0.953125\n",
      "Training iteration 15 loss: 0.44755271077156067, ACC:0.84375\n",
      "Training iteration 16 loss: 0.23381522297859192, ACC:0.90625\n",
      "Training iteration 17 loss: 0.37803468108177185, ACC:0.859375\n",
      "Training iteration 18 loss: 0.2043611854314804, ACC:0.921875\n",
      "Training iteration 19 loss: 0.288402259349823, ACC:0.921875\n",
      "Training iteration 20 loss: 0.23809802532196045, ACC:0.96875\n",
      "Training iteration 21 loss: 0.2930581867694855, ACC:0.96875\n",
      "Training iteration 22 loss: 0.21409671008586884, ACC:0.984375\n",
      "Training iteration 23 loss: 0.16370511054992676, ACC:0.984375\n",
      "Training iteration 24 loss: 0.22478562593460083, ACC:0.875\n",
      "Training iteration 25 loss: 0.09605808556079865, ACC:0.96875\n",
      "Training iteration 26 loss: 0.13190387189388275, ACC:0.9375\n",
      "Training iteration 27 loss: 0.07574761658906937, ACC:0.96875\n",
      "Training iteration 28 loss: 0.2437307983636856, ACC:0.984375\n",
      "Training iteration 29 loss: 0.05215003341436386, ACC:0.984375\n",
      "Training iteration 30 loss: 0.07415232062339783, ACC:0.984375\n",
      "Training iteration 31 loss: 0.11558229476213455, ACC:0.96875\n",
      "Training iteration 32 loss: 0.11979365348815918, ACC:0.953125\n",
      "Training iteration 33 loss: 0.06512804329395294, ACC:0.984375\n",
      "Training iteration 34 loss: 0.09144186228513718, ACC:0.96875\n",
      "Training iteration 35 loss: 0.04775909706950188, ACC:0.984375\n",
      "Training iteration 36 loss: 0.04703926667571068, ACC:0.984375\n",
      "Training iteration 37 loss: 0.11793220788240433, ACC:0.96875\n",
      "Training iteration 38 loss: 0.0477704219520092, ACC:0.984375\n",
      "Training iteration 39 loss: 0.08825449645519257, ACC:0.953125\n",
      "Training iteration 40 loss: 0.28811004757881165, ACC:0.9375\n",
      "Training iteration 41 loss: 0.051558032631874084, ACC:0.984375\n",
      "Training iteration 42 loss: 0.08868260681629181, ACC:0.96875\n",
      "Training iteration 43 loss: 0.2570430338382721, ACC:0.953125\n",
      "Training iteration 44 loss: 0.166543111205101, ACC:0.9375\n",
      "Training iteration 45 loss: 0.10081936419010162, ACC:0.984375\n",
      "Training iteration 46 loss: 0.09207254648208618, ACC:0.96875\n",
      "Training iteration 47 loss: 0.075212761759758, ACC:0.984375\n",
      "Training iteration 48 loss: 0.10414865612983704, ACC:0.984375\n",
      "Training iteration 49 loss: 0.07071690261363983, ACC:0.984375\n",
      "Training iteration 50 loss: 0.14783097803592682, ACC:0.953125\n",
      "Training iteration 51 loss: 0.03436160087585449, ACC:1.0\n",
      "Training iteration 52 loss: 0.07057087123394012, ACC:0.96875\n",
      "Training iteration 53 loss: 0.10938549041748047, ACC:0.953125\n",
      "Training iteration 54 loss: 0.15413734316825867, ACC:0.984375\n",
      "Training iteration 55 loss: 0.06722172349691391, ACC:0.984375\n",
      "Training iteration 56 loss: 0.12823742628097534, ACC:0.96875\n",
      "Training iteration 57 loss: 0.1255130171775818, ACC:0.953125\n",
      "Training iteration 58 loss: 0.1333433836698532, ACC:0.953125\n",
      "Training iteration 59 loss: 0.10550815612077713, ACC:0.953125\n",
      "Training iteration 60 loss: 0.09854834526777267, ACC:0.96875\n",
      "Training iteration 61 loss: 0.08820068091154099, ACC:0.984375\n",
      "Training iteration 62 loss: 0.10948321968317032, ACC:0.96875\n",
      "Training iteration 63 loss: 0.05933832749724388, ACC:0.984375\n",
      "Training iteration 64 loss: 0.18136504292488098, ACC:0.921875\n",
      "Training iteration 65 loss: 0.08374246209859848, ACC:0.96875\n",
      "Training iteration 66 loss: 0.053892988711595535, ACC:0.984375\n",
      "Training iteration 67 loss: 0.042913466691970825, ACC:1.0\n",
      "Training iteration 68 loss: 0.23398306965827942, ACC:0.921875\n",
      "Training iteration 69 loss: 0.030822325497865677, ACC:1.0\n",
      "Training iteration 70 loss: 0.08579376339912415, ACC:0.953125\n",
      "Training iteration 71 loss: 0.03925013542175293, ACC:0.984375\n",
      "Training iteration 72 loss: 0.029536563903093338, ACC:0.984375\n",
      "Training iteration 73 loss: 0.09091391414403915, ACC:0.96875\n",
      "Training iteration 74 loss: 0.07363414764404297, ACC:0.953125\n",
      "Training iteration 75 loss: 0.08013422787189484, ACC:0.984375\n",
      "Training iteration 76 loss: 0.11098983883857727, ACC:0.953125\n",
      "Training iteration 77 loss: 0.035732392221689224, ACC:0.984375\n",
      "Training iteration 78 loss: 0.023064980283379555, ACC:1.0\n",
      "Training iteration 79 loss: 0.07204548269510269, ACC:0.984375\n",
      "Training iteration 80 loss: 0.020675858482718468, ACC:1.0\n",
      "Training iteration 81 loss: 0.012282944284379482, ACC:1.0\n",
      "Training iteration 82 loss: 0.10695751756429672, ACC:0.96875\n",
      "Training iteration 83 loss: 0.05876174569129944, ACC:0.96875\n",
      "Training iteration 84 loss: 0.05965167656540871, ACC:0.984375\n",
      "Training iteration 85 loss: 0.14007289707660675, ACC:0.953125\n",
      "Training iteration 86 loss: 0.0821743980050087, ACC:0.96875\n",
      "Training iteration 87 loss: 0.07598096877336502, ACC:0.96875\n",
      "Training iteration 88 loss: 0.03226654976606369, ACC:0.984375\n",
      "Training iteration 89 loss: 0.1281527876853943, ACC:0.96875\n",
      "Training iteration 90 loss: 0.10531742870807648, ACC:0.953125\n",
      "Training iteration 91 loss: 0.090433768928051, ACC:0.984375\n",
      "Training iteration 92 loss: 0.07317451387643814, ACC:1.0\n",
      "Training iteration 93 loss: 0.10012173652648926, ACC:0.96875\n",
      "Training iteration 94 loss: 0.06816189736127853, ACC:1.0\n",
      "Training iteration 95 loss: 0.1451898217201233, ACC:0.96875\n",
      "Training iteration 96 loss: 0.08352843672037125, ACC:0.96875\n",
      "Training iteration 97 loss: 0.01667901501059532, ACC:1.0\n",
      "Training iteration 98 loss: 0.030326278880238533, ACC:1.0\n",
      "Training iteration 99 loss: 0.06882181018590927, ACC:0.984375\n",
      "Training iteration 100 loss: 0.1699235886335373, ACC:0.921875\n",
      "Training iteration 101 loss: 0.024700943380594254, ACC:0.984375\n",
      "Training iteration 102 loss: 0.09337713569402695, ACC:0.953125\n",
      "Training iteration 103 loss: 0.016007790341973305, ACC:1.0\n",
      "Training iteration 104 loss: 0.007099083159118891, ACC:1.0\n",
      "Training iteration 105 loss: 0.3071726858615875, ACC:0.96875\n",
      "Training iteration 106 loss: 0.10868605971336365, ACC:0.96875\n",
      "Training iteration 107 loss: 0.05328362062573433, ACC:0.984375\n",
      "Training iteration 108 loss: 0.04079873487353325, ACC:1.0\n",
      "Training iteration 109 loss: 0.10775347799062729, ACC:0.953125\n",
      "Training iteration 110 loss: 0.056022658944129944, ACC:1.0\n",
      "Training iteration 111 loss: 0.056839343160390854, ACC:0.984375\n",
      "Training iteration 112 loss: 0.06391280889511108, ACC:1.0\n",
      "Training iteration 113 loss: 0.10842147469520569, ACC:0.953125\n",
      "Training iteration 114 loss: 0.09618624299764633, ACC:0.984375\n",
      "Training iteration 115 loss: 0.05284000188112259, ACC:1.0\n",
      "Training iteration 116 loss: 0.1205388680100441, ACC:0.96875\n",
      "Training iteration 117 loss: 0.07713078707456589, ACC:0.984375\n",
      "Training iteration 118 loss: 0.15201137959957123, ACC:0.9375\n",
      "Training iteration 119 loss: 0.24314619600772858, ACC:0.90625\n",
      "Training iteration 120 loss: 0.058269891887903214, ACC:0.984375\n",
      "Training iteration 121 loss: 0.10522551089525223, ACC:0.953125\n",
      "Training iteration 122 loss: 0.05710076168179512, ACC:1.0\n",
      "Training iteration 123 loss: 0.04995644837617874, ACC:0.984375\n",
      "Training iteration 124 loss: 0.11641201376914978, ACC:0.953125\n",
      "Training iteration 125 loss: 0.0722292810678482, ACC:0.953125\n",
      "Training iteration 126 loss: 0.04685839265584946, ACC:0.984375\n",
      "Training iteration 127 loss: 0.08439600467681885, ACC:0.96875\n",
      "Training iteration 128 loss: 0.10913333296775818, ACC:0.953125\n",
      "Training iteration 129 loss: 0.044322382658720016, ACC:0.984375\n",
      "Training iteration 130 loss: 0.18085768818855286, ACC:0.9375\n",
      "Training iteration 131 loss: 0.5865691900253296, ACC:0.953125\n",
      "Training iteration 132 loss: 0.12861591577529907, ACC:0.953125\n",
      "Training iteration 133 loss: 0.08170581609010696, ACC:0.953125\n",
      "Training iteration 134 loss: 0.300990492105484, ACC:0.90625\n",
      "Training iteration 135 loss: 0.2016085535287857, ACC:0.90625\n",
      "Training iteration 136 loss: 0.15674422681331635, ACC:0.9375\n",
      "Training iteration 137 loss: 0.09958790242671967, ACC:0.96875\n",
      "Training iteration 138 loss: 0.06376296281814575, ACC:1.0\n",
      "Training iteration 139 loss: 0.09461352974176407, ACC:0.984375\n",
      "Training iteration 140 loss: 0.08635704964399338, ACC:1.0\n",
      "Training iteration 141 loss: 0.07667285948991776, ACC:1.0\n",
      "Training iteration 142 loss: 0.06193774566054344, ACC:1.0\n",
      "Training iteration 143 loss: 0.21151667833328247, ACC:0.96875\n",
      "Training iteration 144 loss: 0.037944190204143524, ACC:1.0\n",
      "Training iteration 145 loss: 0.018534917384386063, ACC:1.0\n",
      "Training iteration 146 loss: 0.017605029046535492, ACC:1.0\n",
      "Training iteration 147 loss: 0.0265393927693367, ACC:0.984375\n",
      "Training iteration 148 loss: 0.029862618073821068, ACC:0.984375\n",
      "Training iteration 149 loss: 0.025301149114966393, ACC:0.984375\n",
      "Training iteration 150 loss: 0.006511314306408167, ACC:1.0\n",
      "Training iteration 151 loss: 0.06823094189167023, ACC:0.984375\n",
      "Training iteration 152 loss: 0.10653271526098251, ACC:0.96875\n",
      "Training iteration 153 loss: 0.033520981669425964, ACC:0.984375\n",
      "Training iteration 154 loss: 0.14503496885299683, ACC:0.984375\n",
      "Training iteration 155 loss: 0.05289300158619881, ACC:0.984375\n",
      "Training iteration 156 loss: 0.020844992250204086, ACC:1.0\n",
      "Training iteration 157 loss: 0.06524989008903503, ACC:0.984375\n",
      "Training iteration 158 loss: 0.030265668407082558, ACC:1.0\n",
      "Training iteration 159 loss: 0.04482315108180046, ACC:1.0\n",
      "Training iteration 160 loss: 0.06366114318370819, ACC:0.984375\n",
      "Training iteration 161 loss: 0.11907538771629333, ACC:0.953125\n",
      "Training iteration 162 loss: 0.03949910029768944, ACC:1.0\n",
      "Training iteration 163 loss: 0.05663160979747772, ACC:0.984375\n",
      "Training iteration 164 loss: 0.17695628106594086, ACC:0.921875\n",
      "Training iteration 165 loss: 0.043935734778642654, ACC:0.984375\n",
      "Training iteration 166 loss: 0.10681185126304626, ACC:0.953125\n",
      "Training iteration 167 loss: 0.14632712304592133, ACC:0.9375\n",
      "Training iteration 168 loss: 0.062396157532930374, ACC:0.984375\n",
      "Training iteration 169 loss: 0.08490993827581406, ACC:0.96875\n",
      "Training iteration 170 loss: 0.1007569208741188, ACC:0.96875\n",
      "Training iteration 171 loss: 0.07610414177179337, ACC:0.984375\n",
      "Training iteration 172 loss: 0.08021259307861328, ACC:0.96875\n",
      "Training iteration 173 loss: 0.12183858454227448, ACC:0.9375\n",
      "Training iteration 174 loss: 0.05306803435087204, ACC:0.96875\n",
      "Training iteration 175 loss: 0.026395147666335106, ACC:1.0\n",
      "Training iteration 176 loss: 0.06339099258184433, ACC:0.984375\n",
      "Training iteration 177 loss: 0.04059504717588425, ACC:0.984375\n",
      "Training iteration 178 loss: 0.07337261736392975, ACC:0.984375\n",
      "Training iteration 179 loss: 0.025144796818494797, ACC:1.0\n",
      "Training iteration 180 loss: 0.011864087544381618, ACC:1.0\n",
      "Training iteration 181 loss: 0.04491350054740906, ACC:0.984375\n",
      "Training iteration 182 loss: 0.023455895483493805, ACC:0.984375\n",
      "Training iteration 183 loss: 0.13201501965522766, ACC:0.96875\n",
      "Training iteration 184 loss: 0.07963792234659195, ACC:0.953125\n",
      "Training iteration 185 loss: 0.008227311074733734, ACC:1.0\n",
      "Training iteration 186 loss: 0.059207018464803696, ACC:0.96875\n",
      "Training iteration 187 loss: 0.09803066402673721, ACC:0.96875\n",
      "Training iteration 188 loss: 0.011059117503464222, ACC:1.0\n",
      "Training iteration 189 loss: 0.04830598458647728, ACC:0.984375\n",
      "Training iteration 190 loss: 0.03885848447680473, ACC:0.984375\n",
      "Training iteration 191 loss: 0.034568995237350464, ACC:0.984375\n",
      "Training iteration 192 loss: 0.01092124916613102, ACC:1.0\n",
      "Training iteration 193 loss: 0.022526239976286888, ACC:0.984375\n",
      "Training iteration 194 loss: 0.0040750205516815186, ACC:1.0\n",
      "Training iteration 195 loss: 0.06891228258609772, ACC:0.984375\n",
      "Training iteration 196 loss: 0.069738008081913, ACC:0.96875\n",
      "Training iteration 197 loss: 0.019026126712560654, ACC:1.0\n",
      "Training iteration 198 loss: 0.005303248297423124, ACC:1.0\n",
      "Training iteration 199 loss: 0.03705303370952606, ACC:0.96875\n",
      "Training iteration 200 loss: 0.011590286158025265, ACC:1.0\n",
      "Training iteration 201 loss: 0.06041234731674194, ACC:0.984375\n",
      "Training iteration 202 loss: 0.00869006384164095, ACC:1.0\n",
      "Training iteration 203 loss: 0.02119165100157261, ACC:0.984375\n",
      "Training iteration 204 loss: 0.3676915466785431, ACC:0.953125\n",
      "Training iteration 205 loss: 0.04441346228122711, ACC:0.984375\n",
      "Training iteration 206 loss: 0.036037903279066086, ACC:0.984375\n",
      "Training iteration 207 loss: 0.008587116375565529, ACC:1.0\n",
      "Training iteration 208 loss: 0.0074319541454315186, ACC:1.0\n",
      "Training iteration 209 loss: 0.0762496367096901, ACC:0.984375\n",
      "Training iteration 210 loss: 0.047812316566705704, ACC:0.96875\n",
      "Training iteration 211 loss: 0.04235970973968506, ACC:0.984375\n",
      "Training iteration 212 loss: 0.08149619400501251, ACC:0.96875\n",
      "Training iteration 213 loss: 0.029623759910464287, ACC:0.984375\n",
      "Training iteration 214 loss: 0.03268401324748993, ACC:0.984375\n",
      "Training iteration 215 loss: 0.009265461005270481, ACC:1.0\n",
      "Training iteration 216 loss: 0.31484267115592957, ACC:0.953125\n",
      "Training iteration 217 loss: 0.025523988530039787, ACC:0.984375\n",
      "Training iteration 218 loss: 0.030611177906394005, ACC:1.0\n",
      "Training iteration 219 loss: 0.0804351344704628, ACC:0.984375\n",
      "Training iteration 220 loss: 0.0623762346804142, ACC:1.0\n",
      "Training iteration 221 loss: 0.027469366788864136, ACC:1.0\n",
      "Training iteration 222 loss: 0.021532880142331123, ACC:1.0\n",
      "Training iteration 223 loss: 0.034902676939964294, ACC:1.0\n",
      "Training iteration 224 loss: 0.03476328030228615, ACC:0.984375\n",
      "Training iteration 225 loss: 0.017538009211421013, ACC:1.0\n",
      "Training iteration 226 loss: 0.014926892705261707, ACC:1.0\n",
      "Training iteration 227 loss: 0.048450443893671036, ACC:0.984375\n",
      "Training iteration 228 loss: 0.20246998965740204, ACC:0.921875\n",
      "Training iteration 229 loss: 0.08437003940343857, ACC:0.96875\n",
      "Training iteration 230 loss: 0.07095823436975479, ACC:0.96875\n",
      "Training iteration 231 loss: 0.04426340386271477, ACC:0.984375\n",
      "Training iteration 232 loss: 0.10617709159851074, ACC:0.984375\n",
      "Training iteration 233 loss: 0.04827401787042618, ACC:0.984375\n",
      "Training iteration 234 loss: 0.4312226176261902, ACC:0.96875\n",
      "Training iteration 235 loss: 0.009352704510092735, ACC:1.0\n",
      "Training iteration 236 loss: 0.12647294998168945, ACC:0.953125\n",
      "Training iteration 237 loss: 0.008277570828795433, ACC:1.0\n",
      "Training iteration 238 loss: 0.07185850292444229, ACC:0.984375\n",
      "Training iteration 239 loss: 0.009590127505362034, ACC:1.0\n",
      "Training iteration 240 loss: 0.009350018575787544, ACC:1.0\n",
      "Training iteration 241 loss: 0.04028361663222313, ACC:0.96875\n",
      "Training iteration 242 loss: 0.058266375213861465, ACC:0.984375\n",
      "Training iteration 243 loss: 0.08493804931640625, ACC:0.984375\n",
      "Training iteration 244 loss: 0.03752356022596359, ACC:0.984375\n",
      "Training iteration 245 loss: 0.012889845296740532, ACC:1.0\n",
      "Training iteration 246 loss: 0.008391184732317924, ACC:1.0\n",
      "Training iteration 247 loss: 0.06678639352321625, ACC:0.984375\n",
      "Training iteration 248 loss: 0.04877396672964096, ACC:0.984375\n",
      "Training iteration 249 loss: 0.013793417252600193, ACC:1.0\n",
      "Training iteration 250 loss: 0.0076383971609175205, ACC:1.0\n",
      "Training iteration 251 loss: 0.014934122562408447, ACC:0.984375\n",
      "Training iteration 252 loss: 0.008726435713469982, ACC:1.0\n",
      "Training iteration 253 loss: 0.02463448978960514, ACC:0.984375\n",
      "Training iteration 254 loss: 0.07580552995204926, ACC:0.984375\n",
      "Training iteration 255 loss: 0.040765851736068726, ACC:0.984375\n",
      "Training iteration 256 loss: 0.020995432510972023, ACC:1.0\n",
      "Training iteration 257 loss: 0.01433507725596428, ACC:1.0\n",
      "Training iteration 258 loss: 0.13737048208713531, ACC:0.984375\n",
      "Training iteration 259 loss: 0.06011663004755974, ACC:0.984375\n",
      "Training iteration 260 loss: 0.24381399154663086, ACC:0.984375\n",
      "Training iteration 261 loss: 0.06557139754295349, ACC:0.96875\n",
      "Training iteration 262 loss: 0.02600845694541931, ACC:0.984375\n",
      "Training iteration 263 loss: 0.03845911845564842, ACC:0.984375\n",
      "Training iteration 264 loss: 0.14282086491584778, ACC:0.96875\n",
      "Training iteration 265 loss: 0.22206196188926697, ACC:0.921875\n",
      "Training iteration 266 loss: 0.030538028106093407, ACC:0.984375\n",
      "Training iteration 267 loss: 0.008959013968706131, ACC:1.0\n",
      "Training iteration 268 loss: 0.015994610264897346, ACC:1.0\n",
      "Training iteration 269 loss: 0.014841722324490547, ACC:1.0\n",
      "Training iteration 270 loss: 0.032020844519138336, ACC:0.984375\n",
      "Training iteration 271 loss: 0.036012664437294006, ACC:0.984375\n",
      "Training iteration 272 loss: 0.012674960307776928, ACC:1.0\n",
      "Training iteration 273 loss: 0.03344014286994934, ACC:1.0\n",
      "Training iteration 274 loss: 0.013145320117473602, ACC:1.0\n",
      "Training iteration 275 loss: 0.01884547993540764, ACC:1.0\n",
      "Training iteration 276 loss: 0.01292774360626936, ACC:1.0\n",
      "Training iteration 277 loss: 0.06571076810359955, ACC:0.96875\n",
      "Training iteration 278 loss: 0.02579815685749054, ACC:0.984375\n",
      "Training iteration 279 loss: 0.012000981718301773, ACC:1.0\n",
      "Training iteration 280 loss: 0.09039434790611267, ACC:0.96875\n",
      "Training iteration 281 loss: 0.05752115324139595, ACC:0.984375\n",
      "Training iteration 282 loss: 0.05228833854198456, ACC:0.984375\n",
      "Training iteration 283 loss: 0.00471113296225667, ACC:1.0\n",
      "Training iteration 284 loss: 0.00579928420484066, ACC:1.0\n",
      "Training iteration 285 loss: 0.07789633423089981, ACC:0.984375\n",
      "Training iteration 286 loss: 0.07505947351455688, ACC:0.96875\n",
      "Training iteration 287 loss: 0.05352718010544777, ACC:0.984375\n",
      "Training iteration 288 loss: 0.013968151994049549, ACC:1.0\n",
      "Training iteration 289 loss: 0.009810823015868664, ACC:1.0\n",
      "Training iteration 290 loss: 0.006745649501681328, ACC:1.0\n",
      "Training iteration 291 loss: 0.008512618020176888, ACC:1.0\n",
      "Training iteration 292 loss: 0.011136600747704506, ACC:1.0\n",
      "Training iteration 293 loss: 0.0070675997994840145, ACC:1.0\n",
      "Training iteration 294 loss: 0.3073956072330475, ACC:0.96875\n",
      "Training iteration 295 loss: 0.004385249223560095, ACC:1.0\n",
      "Training iteration 296 loss: 0.11982718855142593, ACC:0.96875\n",
      "Training iteration 297 loss: 0.15467508137226105, ACC:0.984375\n",
      "Training iteration 298 loss: 0.03115300089120865, ACC:0.984375\n",
      "Training iteration 299 loss: 0.2904312014579773, ACC:0.96875\n",
      "Training iteration 300 loss: 0.004283558577299118, ACC:1.0\n",
      "Training iteration 301 loss: 0.10310806334018707, ACC:0.96875\n",
      "Training iteration 302 loss: 0.017900336533784866, ACC:1.0\n",
      "Training iteration 303 loss: 0.03683474287390709, ACC:0.984375\n",
      "Training iteration 304 loss: 0.047460537403821945, ACC:0.96875\n",
      "Training iteration 305 loss: 0.07069990038871765, ACC:0.96875\n",
      "Training iteration 306 loss: 0.02977251447737217, ACC:1.0\n",
      "Training iteration 307 loss: 0.07807088643312454, ACC:0.96875\n",
      "Training iteration 308 loss: 0.2225313037633896, ACC:0.984375\n",
      "Training iteration 309 loss: 0.05756205692887306, ACC:0.984375\n",
      "Training iteration 310 loss: 0.029869405552744865, ACC:1.0\n",
      "Training iteration 311 loss: 0.06595870107412338, ACC:0.984375\n",
      "Training iteration 312 loss: 0.06173228845000267, ACC:0.984375\n",
      "Training iteration 313 loss: 0.04460404813289642, ACC:1.0\n",
      "Training iteration 314 loss: 0.029405148699879646, ACC:1.0\n",
      "Training iteration 315 loss: 0.052208077162504196, ACC:0.984375\n",
      "Training iteration 316 loss: 0.056861214339733124, ACC:0.96875\n",
      "Training iteration 317 loss: 0.040703631937503815, ACC:0.984375\n",
      "Training iteration 318 loss: 0.031551748514175415, ACC:0.984375\n",
      "Training iteration 319 loss: 0.023402787744998932, ACC:1.0\n",
      "Training iteration 320 loss: 0.0627347081899643, ACC:0.984375\n",
      "Training iteration 321 loss: 0.025311563163995743, ACC:1.0\n",
      "Training iteration 322 loss: 0.022852104157209396, ACC:1.0\n",
      "Training iteration 323 loss: 0.0255013108253479, ACC:1.0\n",
      "Training iteration 324 loss: 0.0354812890291214, ACC:0.984375\n",
      "Training iteration 325 loss: 0.05690499395132065, ACC:0.984375\n",
      "Training iteration 326 loss: 0.03769758716225624, ACC:0.984375\n",
      "Training iteration 327 loss: 0.004047217778861523, ACC:1.0\n",
      "Training iteration 328 loss: 0.03275242820382118, ACC:0.984375\n",
      "Training iteration 329 loss: 0.13181360065937042, ACC:0.96875\n",
      "Training iteration 330 loss: 0.05075763165950775, ACC:0.96875\n",
      "Training iteration 331 loss: 0.05289120599627495, ACC:0.96875\n",
      "Training iteration 332 loss: 0.0019388726213946939, ACC:1.0\n",
      "Training iteration 333 loss: 0.08840805292129517, ACC:0.984375\n",
      "Training iteration 334 loss: 0.14793048799037933, ACC:0.96875\n",
      "Training iteration 335 loss: 0.03601287677884102, ACC:0.984375\n",
      "Training iteration 336 loss: 0.035943709313869476, ACC:0.984375\n",
      "Training iteration 337 loss: 0.007996469736099243, ACC:1.0\n",
      "Training iteration 338 loss: 0.005894459784030914, ACC:1.0\n",
      "Training iteration 339 loss: 0.03788821026682854, ACC:0.984375\n",
      "Training iteration 340 loss: 0.006983173545449972, ACC:1.0\n",
      "Training iteration 341 loss: 0.022654108703136444, ACC:1.0\n",
      "Training iteration 342 loss: 0.07197295129299164, ACC:0.984375\n",
      "Training iteration 343 loss: 0.0286405049264431, ACC:0.984375\n",
      "Training iteration 344 loss: 0.11538335680961609, ACC:0.96875\n",
      "Training iteration 345 loss: 0.008499537594616413, ACC:1.0\n",
      "Training iteration 346 loss: 0.1369437724351883, ACC:0.984375\n",
      "Training iteration 347 loss: 0.028277508914470673, ACC:0.984375\n",
      "Training iteration 348 loss: 0.07559427618980408, ACC:0.984375\n",
      "Training iteration 349 loss: 0.00814935751259327, ACC:1.0\n",
      "Training iteration 350 loss: 0.20747999846935272, ACC:0.96875\n",
      "Training iteration 351 loss: 0.010234680026769638, ACC:1.0\n",
      "Training iteration 352 loss: 0.031105922535061836, ACC:0.984375\n",
      "Training iteration 353 loss: 0.016306351870298386, ACC:1.0\n",
      "Training iteration 354 loss: 0.0381820872426033, ACC:0.984375\n",
      "Training iteration 355 loss: 0.11951328814029694, ACC:0.96875\n",
      "Training iteration 356 loss: 0.06475599855184555, ACC:0.96875\n",
      "Training iteration 357 loss: 0.07566765695810318, ACC:0.96875\n",
      "Training iteration 358 loss: 0.06042710319161415, ACC:0.984375\n",
      "Training iteration 359 loss: 0.040496181696653366, ACC:1.0\n",
      "Training iteration 360 loss: 0.02951735630631447, ACC:1.0\n",
      "Training iteration 361 loss: 0.08738543838262558, ACC:0.953125\n",
      "Training iteration 362 loss: 0.1871999204158783, ACC:0.984375\n",
      "Training iteration 363 loss: 0.02644854038953781, ACC:1.0\n",
      "Training iteration 364 loss: 0.06504420191049576, ACC:0.984375\n",
      "Training iteration 365 loss: 0.07793223112821579, ACC:0.984375\n",
      "Training iteration 366 loss: 0.06124170124530792, ACC:0.984375\n",
      "Training iteration 367 loss: 0.018239958211779594, ACC:1.0\n",
      "Training iteration 368 loss: 0.05782538652420044, ACC:0.96875\n",
      "Training iteration 369 loss: 0.040922537446022034, ACC:0.984375\n",
      "Training iteration 370 loss: 0.006806358695030212, ACC:1.0\n",
      "Training iteration 371 loss: 0.010821843519806862, ACC:1.0\n",
      "Training iteration 372 loss: 0.02950868383049965, ACC:0.984375\n",
      "Training iteration 373 loss: 0.011960026808083057, ACC:1.0\n",
      "Training iteration 374 loss: 0.018546542152762413, ACC:0.984375\n",
      "Training iteration 375 loss: 0.006738905794918537, ACC:1.0\n",
      "Training iteration 376 loss: 0.08128977566957474, ACC:0.96875\n",
      "Training iteration 377 loss: 0.04289676994085312, ACC:0.984375\n",
      "Training iteration 378 loss: 0.023235682398080826, ACC:0.984375\n",
      "Training iteration 379 loss: 0.0057153403759002686, ACC:1.0\n",
      "Training iteration 380 loss: 0.18273477256298065, ACC:0.953125\n",
      "Training iteration 381 loss: 0.04950296878814697, ACC:0.96875\n",
      "Training iteration 382 loss: 0.005097286310046911, ACC:1.0\n",
      "Training iteration 383 loss: 0.009464147500693798, ACC:1.0\n",
      "Training iteration 384 loss: 0.0023472500033676624, ACC:1.0\n",
      "Training iteration 385 loss: 0.12099050730466843, ACC:0.984375\n",
      "Training iteration 386 loss: 0.13677909970283508, ACC:0.96875\n",
      "Training iteration 387 loss: 0.11740760505199432, ACC:0.953125\n",
      "Training iteration 388 loss: 0.09226319938898087, ACC:0.984375\n",
      "Training iteration 389 loss: 0.003910944331437349, ACC:1.0\n",
      "Training iteration 390 loss: 0.1126594990491867, ACC:0.96875\n",
      "Training iteration 391 loss: 0.01409238949418068, ACC:1.0\n",
      "Training iteration 392 loss: 0.22046519815921783, ACC:0.9375\n",
      "Training iteration 393 loss: 0.1006988063454628, ACC:0.96875\n",
      "Training iteration 394 loss: 0.15763820707798004, ACC:0.9375\n",
      "Training iteration 395 loss: 0.11946478486061096, ACC:0.953125\n",
      "Training iteration 396 loss: 0.07877465337514877, ACC:0.984375\n",
      "Training iteration 397 loss: 0.10945389419794083, ACC:1.0\n",
      "Training iteration 398 loss: 0.13918016850948334, ACC:0.984375\n",
      "Training iteration 399 loss: 0.09514392167329788, ACC:0.984375\n",
      "Training iteration 400 loss: 0.09580980241298676, ACC:0.984375\n",
      "Training iteration 401 loss: 0.04352707415819168, ACC:1.0\n",
      "Training iteration 402 loss: 0.1099611446261406, ACC:0.953125\n",
      "Training iteration 403 loss: 0.12779679894447327, ACC:0.953125\n",
      "Training iteration 404 loss: 0.08633744716644287, ACC:0.984375\n",
      "Training iteration 405 loss: 0.03831245377659798, ACC:1.0\n",
      "Training iteration 406 loss: 0.07659353315830231, ACC:0.984375\n",
      "Training iteration 407 loss: 0.03700617700815201, ACC:0.984375\n",
      "Training iteration 408 loss: 0.023025810718536377, ACC:1.0\n",
      "Training iteration 409 loss: 0.037309080362319946, ACC:0.984375\n",
      "Training iteration 410 loss: 0.013602565973997116, ACC:1.0\n",
      "Training iteration 411 loss: 0.02780858799815178, ACC:1.0\n",
      "Training iteration 412 loss: 0.03399324789643288, ACC:0.984375\n",
      "Training iteration 413 loss: 0.102287158370018, ACC:0.984375\n",
      "Training iteration 414 loss: 0.012343274429440498, ACC:1.0\n",
      "Training iteration 415 loss: 0.08044464141130447, ACC:0.953125\n",
      "Training iteration 416 loss: 0.017986265942454338, ACC:1.0\n",
      "Training iteration 417 loss: 0.08234158903360367, ACC:0.96875\n",
      "Training iteration 418 loss: 0.016989611089229584, ACC:0.984375\n",
      "Training iteration 419 loss: 0.03142565116286278, ACC:0.984375\n",
      "Training iteration 420 loss: 0.06739599257707596, ACC:0.984375\n",
      "Training iteration 421 loss: 0.038885604590177536, ACC:0.984375\n",
      "Training iteration 422 loss: 0.004128287546336651, ACC:1.0\n",
      "Training iteration 423 loss: 0.060482047498226166, ACC:0.984375\n",
      "Training iteration 424 loss: 0.03606123849749565, ACC:0.984375\n",
      "Training iteration 425 loss: 0.018681026995182037, ACC:0.984375\n",
      "Training iteration 426 loss: 0.022521555423736572, ACC:0.984375\n",
      "Training iteration 427 loss: 0.034077830612659454, ACC:0.984375\n",
      "Training iteration 428 loss: 0.013800147920846939, ACC:1.0\n",
      "Training iteration 429 loss: 0.011974520981311798, ACC:1.0\n",
      "Training iteration 430 loss: 0.04860856384038925, ACC:0.984375\n",
      "Training iteration 431 loss: 0.06808159500360489, ACC:0.96875\n",
      "Training iteration 432 loss: 0.025920527055859566, ACC:0.984375\n",
      "Training iteration 433 loss: 0.005404189229011536, ACC:1.0\n",
      "Training iteration 434 loss: 0.07349836081266403, ACC:0.984375\n",
      "Training iteration 435 loss: 0.020249612629413605, ACC:0.984375\n",
      "Training iteration 436 loss: 0.010462600737810135, ACC:1.0\n",
      "Training iteration 437 loss: 0.029605213552713394, ACC:0.984375\n",
      "Training iteration 438 loss: 0.05249295011162758, ACC:0.984375\n",
      "Training iteration 439 loss: 0.037425387650728226, ACC:0.984375\n",
      "Training iteration 440 loss: 0.0068680159747600555, ACC:1.0\n",
      "Training iteration 441 loss: 0.011573080904781818, ACC:1.0\n",
      "Training iteration 442 loss: 0.006863574963063002, ACC:1.0\n",
      "Training iteration 443 loss: 0.05075551196932793, ACC:0.984375\n",
      "Training iteration 444 loss: 0.07029101252555847, ACC:0.984375\n",
      "Training iteration 445 loss: 0.011297131888568401, ACC:1.0\n",
      "Training iteration 446 loss: 0.020562047138810158, ACC:0.984375\n",
      "Training iteration 447 loss: 0.09158305823802948, ACC:0.96875\n",
      "Training iteration 448 loss: 0.01754705049097538, ACC:1.0\n",
      "Training iteration 449 loss: 0.09144007414579391, ACC:0.96875\n",
      "Training iteration 450 loss: 0.048892997205257416, ACC:0.984375\n",
      "Validation iteration 451 loss: 0.06982129067182541, ACC: 0.96875\n",
      "Validation iteration 452 loss: 0.01812903955578804, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.02418605051934719, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.2031392753124237, ACC: 0.96875\n",
      "Validation iteration 455 loss: 0.015173044987022877, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.042326100170612335, ACC: 0.984375\n",
      "Validation iteration 457 loss: 0.05712898075580597, ACC: 0.984375\n",
      "Validation iteration 458 loss: 0.049764979630708694, ACC: 0.96875\n",
      "Validation iteration 459 loss: 0.024941079318523407, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.0346689410507679, ACC: 0.984375\n",
      "Validation iteration 461 loss: 0.023290107026696205, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.05543561279773712, ACC: 0.96875\n",
      "Validation iteration 463 loss: 0.0631161779165268, ACC: 0.984375\n",
      "Validation iteration 464 loss: 0.02051626518368721, ACC: 0.984375\n",
      "Validation iteration 465 loss: 0.04641025885939598, ACC: 0.984375\n",
      "Validation iteration 466 loss: 0.2952881455421448, ACC: 0.984375\n",
      "Validation iteration 467 loss: 0.02479867823421955, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.015365418046712875, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.012615895830094814, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.012995682656764984, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.06770408153533936, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.05962837487459183, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.013157780282199383, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.08048363775014877, ACC: 0.96875\n",
      "Validation iteration 475 loss: 0.16278745234012604, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.04053133353590965, ACC: 0.984375\n",
      "Validation iteration 477 loss: 0.09840178489685059, ACC: 0.96875\n",
      "Validation iteration 478 loss: 0.03500591591000557, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.029561106115579605, ACC: 0.984375\n",
      "Validation iteration 480 loss: 0.019391335546970367, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.07314766943454742, ACC: 0.96875\n",
      "Validation iteration 482 loss: 0.020162101835012436, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.08809652179479599, ACC: 0.96875\n",
      "Validation iteration 484 loss: 0.08568096905946732, ACC: 0.953125\n",
      "Validation iteration 485 loss: 0.01327099371701479, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.02037123218178749, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.07416263967752457, ACC: 0.984375\n",
      "Validation iteration 488 loss: 0.0127566521987319, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.03372606262564659, ACC: 0.984375\n",
      "Validation iteration 490 loss: 0.018607940524816513, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.04262784868478775, ACC: 0.984375\n",
      "Validation iteration 492 loss: 0.04624127224087715, ACC: 0.984375\n",
      "Validation iteration 493 loss: 0.0671580508351326, ACC: 0.96875\n",
      "Validation iteration 494 loss: 0.01967601291835308, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.10328454524278641, ACC: 0.984375\n",
      "Validation iteration 496 loss: 0.02433617226779461, ACC: 0.984375\n",
      "Validation iteration 497 loss: 0.020500585436820984, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.036626700311899185, ACC: 0.984375\n",
      "Validation iteration 499 loss: 0.013297220692038536, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.01903996244072914, ACC: 1.0\n",
      "-- Epoch 2 done -- Train loss: 0.07943484055214665, train ACC: 0.9791319444444444, val loss: 0.050970699619501826, val ACC: 0.9871875\n",
      "<--- 18261.87142586708 seconds --->\n",
      "Training iteration 1 loss: 0.11689862608909607, ACC:0.984375\n",
      "Training iteration 2 loss: 0.010872137732803822, ACC:1.0\n",
      "Training iteration 3 loss: 0.008848094381392002, ACC:1.0\n",
      "Training iteration 4 loss: 0.07962513715028763, ACC:0.96875\n",
      "Training iteration 5 loss: 0.024280395358800888, ACC:0.984375\n",
      "Training iteration 6 loss: 0.005558699835091829, ACC:1.0\n",
      "Training iteration 7 loss: 0.027096303179860115, ACC:0.984375\n",
      "Training iteration 8 loss: 0.01368809211999178, ACC:1.0\n",
      "Training iteration 9 loss: 0.004251514095813036, ACC:1.0\n",
      "Training iteration 10 loss: 0.0058966949582099915, ACC:1.0\n",
      "Training iteration 11 loss: 0.008907550945878029, ACC:1.0\n",
      "Training iteration 12 loss: 0.005963384173810482, ACC:1.0\n",
      "Training iteration 13 loss: 0.015862802043557167, ACC:0.984375\n",
      "Training iteration 14 loss: 0.0024741243105381727, ACC:1.0\n",
      "Training iteration 15 loss: 0.004556788131594658, ACC:1.0\n",
      "Training iteration 16 loss: 0.005549931433051825, ACC:1.0\n",
      "Training iteration 17 loss: 0.1544731706380844, ACC:0.953125\n",
      "Training iteration 18 loss: 0.018923411145806313, ACC:0.984375\n",
      "Training iteration 19 loss: 0.0019801047164946795, ACC:1.0\n",
      "Training iteration 20 loss: 0.0800759494304657, ACC:0.984375\n",
      "Training iteration 21 loss: 0.0358615517616272, ACC:0.984375\n",
      "Training iteration 22 loss: 0.011710556223988533, ACC:1.0\n",
      "Training iteration 23 loss: 0.17537178099155426, ACC:0.984375\n",
      "Training iteration 24 loss: 0.005109676159918308, ACC:1.0\n",
      "Training iteration 25 loss: 0.026761488988995552, ACC:0.984375\n",
      "Training iteration 26 loss: 0.008926834911108017, ACC:1.0\n",
      "Training iteration 27 loss: 0.024354707449674606, ACC:0.984375\n",
      "Training iteration 28 loss: 0.010407996363937855, ACC:1.0\n",
      "Training iteration 29 loss: 0.05214018002152443, ACC:0.984375\n",
      "Training iteration 30 loss: 0.002261287532746792, ACC:1.0\n",
      "Training iteration 31 loss: 0.0393439382314682, ACC:0.984375\n",
      "Training iteration 32 loss: 0.00438074953854084, ACC:1.0\n",
      "Training iteration 33 loss: 0.010366128757596016, ACC:1.0\n",
      "Training iteration 34 loss: 0.008075708523392677, ACC:1.0\n",
      "Training iteration 35 loss: 0.014544925652444363, ACC:1.0\n",
      "Training iteration 36 loss: 0.0054955133236944675, ACC:1.0\n",
      "Training iteration 37 loss: 0.011239249259233475, ACC:1.0\n",
      "Training iteration 38 loss: 0.05770409107208252, ACC:0.984375\n",
      "Training iteration 39 loss: 0.0058037294074893, ACC:1.0\n",
      "Training iteration 40 loss: 0.003863160964101553, ACC:1.0\n",
      "Training iteration 41 loss: 0.00389717030338943, ACC:1.0\n",
      "Training iteration 42 loss: 0.019084760919213295, ACC:0.984375\n",
      "Training iteration 43 loss: 0.005917080212384462, ACC:1.0\n",
      "Training iteration 44 loss: 0.007583726663142443, ACC:1.0\n",
      "Training iteration 45 loss: 0.005004412028938532, ACC:1.0\n",
      "Training iteration 46 loss: 0.03501205891370773, ACC:0.984375\n",
      "Training iteration 47 loss: 0.03242504969239235, ACC:0.96875\n",
      "Training iteration 48 loss: 0.013523575849831104, ACC:1.0\n",
      "Training iteration 49 loss: 0.015627464279532433, ACC:1.0\n",
      "Training iteration 50 loss: 0.05765509605407715, ACC:0.96875\n",
      "Training iteration 51 loss: 0.007082507014274597, ACC:1.0\n",
      "Training iteration 52 loss: 0.007203053683042526, ACC:1.0\n",
      "Training iteration 53 loss: 0.05595071613788605, ACC:0.984375\n",
      "Training iteration 54 loss: 0.016613703221082687, ACC:0.984375\n",
      "Training iteration 55 loss: 0.005046958103775978, ACC:1.0\n",
      "Training iteration 56 loss: 0.010866010561585426, ACC:1.0\n",
      "Training iteration 57 loss: 0.002183475997298956, ACC:1.0\n",
      "Training iteration 58 loss: 0.030279045924544334, ACC:0.984375\n",
      "Training iteration 59 loss: 0.0159835796803236, ACC:0.984375\n",
      "Training iteration 60 loss: 0.012277434580028057, ACC:0.984375\n",
      "Training iteration 61 loss: 0.0025272569619119167, ACC:1.0\n",
      "Training iteration 62 loss: 0.017976421862840652, ACC:0.984375\n",
      "Training iteration 63 loss: 0.002442258410155773, ACC:1.0\n",
      "Training iteration 64 loss: 0.01732686348259449, ACC:0.984375\n",
      "Training iteration 65 loss: 0.039239782840013504, ACC:0.984375\n",
      "Training iteration 66 loss: 0.002028504852205515, ACC:1.0\n",
      "Training iteration 67 loss: 0.026525847613811493, ACC:0.984375\n",
      "Training iteration 68 loss: 0.14361165463924408, ACC:0.953125\n",
      "Training iteration 69 loss: 0.062395647168159485, ACC:0.984375\n",
      "Training iteration 70 loss: 0.08072681725025177, ACC:0.96875\n",
      "Training iteration 71 loss: 0.019587181508541107, ACC:0.984375\n",
      "Training iteration 72 loss: 0.029692841693758965, ACC:0.96875\n",
      "Training iteration 73 loss: 0.033972740173339844, ACC:0.984375\n",
      "Training iteration 74 loss: 0.0681699737906456, ACC:0.984375\n",
      "Training iteration 75 loss: 0.04494572430849075, ACC:0.984375\n",
      "Training iteration 76 loss: 0.0021665473468601704, ACC:1.0\n",
      "Training iteration 77 loss: 0.004298547748476267, ACC:1.0\n",
      "Training iteration 78 loss: 0.01871570199728012, ACC:0.984375\n",
      "Training iteration 79 loss: 0.0035287528298795223, ACC:1.0\n",
      "Training iteration 80 loss: 0.11487375944852829, ACC:0.96875\n",
      "Training iteration 81 loss: 0.029487106949090958, ACC:0.984375\n",
      "Training iteration 82 loss: 0.019920658320188522, ACC:0.984375\n",
      "Training iteration 83 loss: 0.011218785308301449, ACC:1.0\n",
      "Training iteration 84 loss: 0.057189345359802246, ACC:0.984375\n",
      "Training iteration 85 loss: 0.04026103392243385, ACC:0.984375\n",
      "Training iteration 86 loss: 0.03680825233459473, ACC:0.984375\n",
      "Training iteration 87 loss: 0.027531808242201805, ACC:1.0\n",
      "Training iteration 88 loss: 0.06706364452838898, ACC:0.984375\n",
      "Training iteration 89 loss: 0.07711014151573181, ACC:0.984375\n",
      "Training iteration 90 loss: 0.05899694934487343, ACC:0.984375\n",
      "Training iteration 91 loss: 0.042860399931669235, ACC:0.984375\n",
      "Training iteration 92 loss: 0.01671445555984974, ACC:1.0\n",
      "Training iteration 93 loss: 0.0717875137925148, ACC:0.984375\n",
      "Training iteration 94 loss: 0.025373168289661407, ACC:1.0\n",
      "Training iteration 95 loss: 0.038472775369882584, ACC:0.984375\n",
      "Training iteration 96 loss: 0.02854941599071026, ACC:1.0\n",
      "Training iteration 97 loss: 0.09607089310884476, ACC:0.96875\n",
      "Training iteration 98 loss: 0.0267968587577343, ACC:1.0\n",
      "Training iteration 99 loss: 0.06269736588001251, ACC:0.984375\n",
      "Training iteration 100 loss: 0.019764279946684837, ACC:1.0\n",
      "Training iteration 101 loss: 0.0355251245200634, ACC:0.984375\n",
      "Training iteration 102 loss: 0.010020025074481964, ACC:1.0\n",
      "Training iteration 103 loss: 0.1473473310470581, ACC:0.984375\n",
      "Training iteration 104 loss: 0.07291392236948013, ACC:0.984375\n",
      "Training iteration 105 loss: 0.006706956308335066, ACC:1.0\n",
      "Training iteration 106 loss: 0.00990891270339489, ACC:1.0\n",
      "Training iteration 107 loss: 0.018209099769592285, ACC:0.984375\n",
      "Training iteration 108 loss: 0.08105836063623428, ACC:0.984375\n",
      "Training iteration 109 loss: 0.054796766489744186, ACC:0.953125\n",
      "Training iteration 110 loss: 0.00455453572794795, ACC:1.0\n",
      "Training iteration 111 loss: 0.04475012049078941, ACC:0.984375\n",
      "Training iteration 112 loss: 0.0059469230473041534, ACC:1.0\n",
      "Training iteration 113 loss: 0.00785058829933405, ACC:1.0\n",
      "Training iteration 114 loss: 0.01628737337887287, ACC:1.0\n",
      "Training iteration 115 loss: 0.04000005125999451, ACC:0.984375\n",
      "Training iteration 116 loss: 0.009660646319389343, ACC:1.0\n",
      "Training iteration 117 loss: 0.006051766686141491, ACC:1.0\n",
      "Training iteration 118 loss: 0.02441011369228363, ACC:1.0\n",
      "Training iteration 119 loss: 0.04689561575651169, ACC:0.96875\n",
      "Training iteration 120 loss: 0.011720079928636551, ACC:1.0\n",
      "Training iteration 121 loss: 0.019082600250840187, ACC:0.984375\n",
      "Training iteration 122 loss: 0.03289416804909706, ACC:0.984375\n",
      "Training iteration 123 loss: 0.005299430340528488, ACC:1.0\n",
      "Training iteration 124 loss: 0.022853663191199303, ACC:0.984375\n",
      "Training iteration 125 loss: 0.0034752637147903442, ACC:1.0\n",
      "Training iteration 126 loss: 0.009694555774331093, ACC:1.0\n",
      "Training iteration 127 loss: 0.008440425619482994, ACC:1.0\n",
      "Training iteration 128 loss: 0.002159334719181061, ACC:1.0\n",
      "Training iteration 129 loss: 0.09023753553628922, ACC:0.984375\n",
      "Training iteration 130 loss: 0.03743530809879303, ACC:0.984375\n",
      "Training iteration 131 loss: 0.0064508384093642235, ACC:1.0\n",
      "Training iteration 132 loss: 0.0023934056516736746, ACC:1.0\n",
      "Training iteration 133 loss: 0.03235157951712608, ACC:0.984375\n",
      "Training iteration 134 loss: 0.0028356933034956455, ACC:1.0\n",
      "Training iteration 135 loss: 0.1474803388118744, ACC:0.953125\n",
      "Training iteration 136 loss: 0.0016058767214417458, ACC:1.0\n",
      "Training iteration 137 loss: 0.004179808311164379, ACC:1.0\n",
      "Training iteration 138 loss: 0.19391752779483795, ACC:0.9375\n",
      "Training iteration 139 loss: 0.06295367330312729, ACC:0.984375\n",
      "Training iteration 140 loss: 0.054736435413360596, ACC:0.984375\n",
      "Training iteration 141 loss: 0.003869502106681466, ACC:1.0\n",
      "Training iteration 142 loss: 0.34207504987716675, ACC:0.953125\n",
      "Training iteration 143 loss: 0.0033811875618994236, ACC:1.0\n",
      "Training iteration 144 loss: 0.012666243128478527, ACC:1.0\n",
      "Training iteration 145 loss: 0.003045239020138979, ACC:1.0\n",
      "Training iteration 146 loss: 0.016176167875528336, ACC:1.0\n",
      "Training iteration 147 loss: 0.018784580752253532, ACC:0.984375\n",
      "Training iteration 148 loss: 0.004641985986381769, ACC:1.0\n",
      "Training iteration 149 loss: 0.0026070463936775923, ACC:1.0\n",
      "Training iteration 150 loss: 0.14380879700183868, ACC:0.96875\n",
      "Training iteration 151 loss: 0.007492734119296074, ACC:1.0\n",
      "Training iteration 152 loss: 0.004919374827295542, ACC:1.0\n",
      "Training iteration 153 loss: 0.003874104470014572, ACC:1.0\n",
      "Training iteration 154 loss: 0.019865896552801132, ACC:1.0\n",
      "Training iteration 155 loss: 0.014151697978377342, ACC:1.0\n",
      "Training iteration 156 loss: 0.010437539778649807, ACC:1.0\n",
      "Training iteration 157 loss: 0.005160684231668711, ACC:1.0\n",
      "Training iteration 158 loss: 0.14077718555927277, ACC:0.96875\n",
      "Training iteration 159 loss: 0.003054720815271139, ACC:1.0\n",
      "Training iteration 160 loss: 0.01261529978364706, ACC:1.0\n",
      "Training iteration 161 loss: 0.05160105228424072, ACC:0.96875\n",
      "Training iteration 162 loss: 0.005663672927767038, ACC:1.0\n",
      "Training iteration 163 loss: 0.042951181530952454, ACC:0.984375\n",
      "Training iteration 164 loss: 0.04461808502674103, ACC:0.984375\n",
      "Training iteration 165 loss: 0.09718761593103409, ACC:0.96875\n",
      "Training iteration 166 loss: 0.02936297468841076, ACC:0.984375\n",
      "Training iteration 167 loss: 0.02622051164507866, ACC:1.0\n",
      "Training iteration 168 loss: 0.05567048117518425, ACC:0.984375\n",
      "Training iteration 169 loss: 0.07018550485372543, ACC:0.96875\n",
      "Training iteration 170 loss: 0.103718101978302, ACC:0.953125\n",
      "Training iteration 171 loss: 0.02035149186849594, ACC:1.0\n",
      "Training iteration 172 loss: 0.053903184831142426, ACC:0.984375\n",
      "Training iteration 173 loss: 0.042325351387262344, ACC:0.984375\n",
      "Training iteration 174 loss: 0.0277844350785017, ACC:1.0\n",
      "Training iteration 175 loss: 0.037287387996912, ACC:1.0\n",
      "Training iteration 176 loss: 0.0916547030210495, ACC:0.96875\n",
      "Training iteration 177 loss: 0.06957671791315079, ACC:0.96875\n",
      "Training iteration 178 loss: 0.04255979508161545, ACC:1.0\n",
      "Training iteration 179 loss: 0.09525148570537567, ACC:0.96875\n",
      "Training iteration 180 loss: 0.09311149269342422, ACC:0.96875\n",
      "Training iteration 181 loss: 0.04055294021964073, ACC:0.984375\n",
      "Training iteration 182 loss: 0.026905961334705353, ACC:1.0\n",
      "Training iteration 183 loss: 0.22529470920562744, ACC:0.984375\n",
      "Training iteration 184 loss: 0.030848033726215363, ACC:1.0\n",
      "Training iteration 185 loss: 0.014522118493914604, ACC:1.0\n",
      "Training iteration 186 loss: 0.1504545360803604, ACC:0.9375\n",
      "Training iteration 187 loss: 0.027621988207101822, ACC:0.984375\n",
      "Training iteration 188 loss: 0.009582051075994968, ACC:1.0\n",
      "Training iteration 189 loss: 0.007910976186394691, ACC:1.0\n",
      "Training iteration 190 loss: 0.059186894446611404, ACC:0.984375\n",
      "Training iteration 191 loss: 0.0035956548526883125, ACC:1.0\n",
      "Training iteration 192 loss: 0.010707770474255085, ACC:1.0\n",
      "Training iteration 193 loss: 0.1452023983001709, ACC:0.953125\n",
      "Training iteration 194 loss: 0.0069943685084581375, ACC:1.0\n",
      "Training iteration 195 loss: 0.01251404732465744, ACC:1.0\n",
      "Training iteration 196 loss: 0.01358498353511095, ACC:1.0\n",
      "Training iteration 197 loss: 0.0074265324510633945, ACC:1.0\n",
      "Training iteration 198 loss: 0.08360419422388077, ACC:0.953125\n",
      "Training iteration 199 loss: 0.01175734493881464, ACC:1.0\n",
      "Training iteration 200 loss: 0.017102308571338654, ACC:1.0\n",
      "Training iteration 201 loss: 0.014638413675129414, ACC:1.0\n",
      "Training iteration 202 loss: 0.045942522585392, ACC:0.96875\n",
      "Training iteration 203 loss: 0.005167960189282894, ACC:1.0\n",
      "Training iteration 204 loss: 0.12524539232254028, ACC:0.96875\n",
      "Training iteration 205 loss: 0.005038917995989323, ACC:1.0\n",
      "Training iteration 206 loss: 0.032235708087682724, ACC:0.984375\n",
      "Training iteration 207 loss: 0.17884206771850586, ACC:0.9375\n",
      "Training iteration 208 loss: 0.09522140771150589, ACC:0.96875\n",
      "Training iteration 209 loss: 0.009035165421664715, ACC:1.0\n",
      "Training iteration 210 loss: 0.030462440103292465, ACC:0.984375\n",
      "Training iteration 211 loss: 0.04743911325931549, ACC:0.984375\n",
      "Training iteration 212 loss: 0.13257266581058502, ACC:0.953125\n",
      "Training iteration 213 loss: 0.06702929735183716, ACC:0.984375\n",
      "Training iteration 214 loss: 0.015245364978909492, ACC:1.0\n",
      "Training iteration 215 loss: 0.01351109892129898, ACC:1.0\n",
      "Training iteration 216 loss: 0.01786819100379944, ACC:1.0\n",
      "Training iteration 217 loss: 0.03070269525051117, ACC:0.984375\n",
      "Training iteration 218 loss: 0.0221233032643795, ACC:1.0\n",
      "Training iteration 219 loss: 0.018137434497475624, ACC:1.0\n",
      "Training iteration 220 loss: 0.022599004209041595, ACC:1.0\n",
      "Training iteration 221 loss: 0.004988093860447407, ACC:1.0\n",
      "Training iteration 222 loss: 0.023247579112648964, ACC:0.984375\n",
      "Training iteration 223 loss: 0.03384657949209213, ACC:0.984375\n",
      "Training iteration 224 loss: 0.01950884237885475, ACC:0.984375\n",
      "Training iteration 225 loss: 0.08680100739002228, ACC:0.96875\n",
      "Training iteration 226 loss: 0.0454777255654335, ACC:0.953125\n",
      "Training iteration 227 loss: 0.026563245803117752, ACC:0.984375\n",
      "Training iteration 228 loss: 0.0430356040596962, ACC:0.984375\n",
      "Training iteration 229 loss: 0.01610269583761692, ACC:0.984375\n",
      "Training iteration 230 loss: 0.02350088395178318, ACC:0.984375\n",
      "Training iteration 231 loss: 0.015445303171873093, ACC:1.0\n",
      "Training iteration 232 loss: 0.0035820233169943094, ACC:1.0\n",
      "Training iteration 233 loss: 0.053963083773851395, ACC:0.96875\n",
      "Training iteration 234 loss: 0.05347195267677307, ACC:0.984375\n",
      "Training iteration 235 loss: 0.017527274787425995, ACC:1.0\n",
      "Training iteration 236 loss: 0.002308407099917531, ACC:1.0\n",
      "Training iteration 237 loss: 0.0023260898888111115, ACC:1.0\n",
      "Training iteration 238 loss: 0.016050754114985466, ACC:0.984375\n",
      "Training iteration 239 loss: 0.04452934488654137, ACC:0.984375\n",
      "Training iteration 240 loss: 0.008649914525449276, ACC:1.0\n",
      "Training iteration 241 loss: 0.0024682816583663225, ACC:1.0\n",
      "Training iteration 242 loss: 0.03660206496715546, ACC:0.984375\n",
      "Training iteration 243 loss: 0.001357578905299306, ACC:1.0\n",
      "Training iteration 244 loss: 0.048474352806806564, ACC:0.984375\n",
      "Training iteration 245 loss: 0.002029386116191745, ACC:1.0\n",
      "Training iteration 246 loss: 0.0037780862767249346, ACC:1.0\n",
      "Training iteration 247 loss: 0.024805286899209023, ACC:0.984375\n",
      "Training iteration 248 loss: 0.013240576721727848, ACC:1.0\n",
      "Training iteration 249 loss: 0.03060685656964779, ACC:0.984375\n",
      "Training iteration 250 loss: 0.03546219319105148, ACC:0.984375\n",
      "Training iteration 251 loss: 0.010027024894952774, ACC:1.0\n",
      "Training iteration 252 loss: 0.10009044408798218, ACC:0.96875\n",
      "Training iteration 253 loss: 0.01832972839474678, ACC:1.0\n",
      "Training iteration 254 loss: 0.01095128059387207, ACC:1.0\n",
      "Training iteration 255 loss: 0.02257099375128746, ACC:0.984375\n",
      "Training iteration 256 loss: 0.01086339633911848, ACC:1.0\n",
      "Training iteration 257 loss: 0.006404426414519548, ACC:1.0\n",
      "Training iteration 258 loss: 0.011704516597092152, ACC:1.0\n",
      "Training iteration 259 loss: 0.0179282296448946, ACC:0.984375\n",
      "Training iteration 260 loss: 0.00445818156003952, ACC:1.0\n",
      "Training iteration 261 loss: 0.002428849460557103, ACC:1.0\n",
      "Training iteration 262 loss: 0.009957832284271717, ACC:1.0\n",
      "Training iteration 263 loss: 0.006650866940617561, ACC:1.0\n",
      "Training iteration 264 loss: 0.012398681603372097, ACC:1.0\n",
      "Training iteration 265 loss: 0.0027781715616583824, ACC:1.0\n",
      "Training iteration 266 loss: 0.010683759115636349, ACC:1.0\n",
      "Training iteration 267 loss: 0.01052097324281931, ACC:1.0\n",
      "Training iteration 268 loss: 0.03511407971382141, ACC:0.984375\n",
      "Training iteration 269 loss: 0.0013498177286237478, ACC:1.0\n",
      "Training iteration 270 loss: 0.02200113609433174, ACC:0.984375\n",
      "Training iteration 271 loss: 0.0015625195810571313, ACC:1.0\n",
      "Training iteration 272 loss: 0.0038346590008586645, ACC:1.0\n",
      "Training iteration 273 loss: 0.0008951417985372245, ACC:1.0\n",
      "Training iteration 274 loss: 0.0005030059837736189, ACC:1.0\n",
      "Training iteration 275 loss: 0.036142364144325256, ACC:0.984375\n",
      "Training iteration 276 loss: 0.12532515823841095, ACC:0.984375\n",
      "Training iteration 277 loss: 0.010700264945626259, ACC:1.0\n",
      "Training iteration 278 loss: 0.07422643899917603, ACC:0.984375\n",
      "Training iteration 279 loss: 0.053195200860500336, ACC:0.984375\n",
      "Training iteration 280 loss: 0.07367526739835739, ACC:0.96875\n",
      "Training iteration 281 loss: 0.022003700956702232, ACC:0.984375\n",
      "Training iteration 282 loss: 0.006806476507335901, ACC:1.0\n",
      "Training iteration 283 loss: 0.012089096009731293, ACC:1.0\n",
      "Training iteration 284 loss: 0.09115603566169739, ACC:0.953125\n",
      "Training iteration 285 loss: 0.1419905424118042, ACC:0.96875\n",
      "Training iteration 286 loss: 0.002648023422807455, ACC:1.0\n",
      "Training iteration 287 loss: 0.0237154271453619, ACC:0.984375\n",
      "Training iteration 288 loss: 0.01336329523473978, ACC:1.0\n",
      "Training iteration 289 loss: 0.04774872958660126, ACC:0.984375\n",
      "Training iteration 290 loss: 0.017085636034607887, ACC:1.0\n",
      "Training iteration 291 loss: 0.1459418535232544, ACC:0.96875\n",
      "Training iteration 292 loss: 0.014028997160494328, ACC:1.0\n",
      "Training iteration 293 loss: 0.008939857594668865, ACC:1.0\n",
      "Training iteration 294 loss: 0.011488988995552063, ACC:1.0\n",
      "Training iteration 295 loss: 0.017158256843686104, ACC:1.0\n",
      "Training iteration 296 loss: 0.09312749654054642, ACC:0.953125\n",
      "Training iteration 297 loss: 0.016329575330018997, ACC:1.0\n",
      "Training iteration 298 loss: 0.01113761868327856, ACC:1.0\n",
      "Training iteration 299 loss: 0.029140887781977654, ACC:1.0\n",
      "Training iteration 300 loss: 0.120950847864151, ACC:0.953125\n",
      "Training iteration 301 loss: 0.030066821724176407, ACC:0.984375\n",
      "Training iteration 302 loss: 0.012900074943900108, ACC:1.0\n",
      "Training iteration 303 loss: 0.03930282965302467, ACC:0.984375\n",
      "Training iteration 304 loss: 0.02813153713941574, ACC:0.984375\n",
      "Training iteration 305 loss: 0.020106501877307892, ACC:1.0\n",
      "Training iteration 306 loss: 0.01595989428460598, ACC:1.0\n",
      "Training iteration 307 loss: 0.07302568852901459, ACC:0.984375\n",
      "Training iteration 308 loss: 0.006494888104498386, ACC:1.0\n",
      "Training iteration 309 loss: 0.008641008287668228, ACC:1.0\n",
      "Training iteration 310 loss: 0.015199472196400166, ACC:1.0\n",
      "Training iteration 311 loss: 0.0063395327888429165, ACC:1.0\n",
      "Training iteration 312 loss: 0.16147489845752716, ACC:0.953125\n",
      "Training iteration 313 loss: 0.002167706610634923, ACC:1.0\n",
      "Training iteration 314 loss: 0.003967397380620241, ACC:1.0\n",
      "Training iteration 315 loss: 0.0018112321849912405, ACC:1.0\n",
      "Training iteration 316 loss: 0.0016005694633349776, ACC:1.0\n",
      "Training iteration 317 loss: 0.005586336366832256, ACC:1.0\n",
      "Training iteration 318 loss: 0.03357599675655365, ACC:0.96875\n",
      "Training iteration 319 loss: 0.007379615679383278, ACC:1.0\n",
      "Training iteration 320 loss: 0.0031331090722233057, ACC:1.0\n",
      "Training iteration 321 loss: 0.02419094555079937, ACC:0.984375\n",
      "Training iteration 322 loss: 0.009139817208051682, ACC:1.0\n",
      "Training iteration 323 loss: 0.003938538022339344, ACC:1.0\n",
      "Training iteration 324 loss: 0.025149710476398468, ACC:0.984375\n",
      "Training iteration 325 loss: 0.0024990886449813843, ACC:1.0\n",
      "Training iteration 326 loss: 0.0010910606943070889, ACC:1.0\n",
      "Training iteration 327 loss: 0.002191839274019003, ACC:1.0\n",
      "Training iteration 328 loss: 0.06905178725719452, ACC:0.984375\n",
      "Training iteration 329 loss: 0.09173198789358139, ACC:0.984375\n",
      "Training iteration 330 loss: 0.003624930512160063, ACC:1.0\n",
      "Training iteration 331 loss: 0.022169746458530426, ACC:0.984375\n",
      "Training iteration 332 loss: 0.009920481592416763, ACC:1.0\n",
      "Training iteration 333 loss: 0.0014314544387161732, ACC:1.0\n",
      "Training iteration 334 loss: 0.004332262556999922, ACC:1.0\n",
      "Training iteration 335 loss: 0.006079406477510929, ACC:1.0\n",
      "Training iteration 336 loss: 0.04880806803703308, ACC:0.984375\n",
      "Training iteration 337 loss: 0.0014488272136077285, ACC:1.0\n",
      "Training iteration 338 loss: 0.00793333351612091, ACC:1.0\n",
      "Training iteration 339 loss: 0.015113502740859985, ACC:0.984375\n",
      "Training iteration 340 loss: 0.0009919462027028203, ACC:1.0\n",
      "Training iteration 341 loss: 0.1350199282169342, ACC:0.96875\n",
      "Training iteration 342 loss: 0.0012368891621008515, ACC:1.0\n",
      "Training iteration 343 loss: 0.009694097563624382, ACC:1.0\n",
      "Training iteration 344 loss: 0.001705025089904666, ACC:1.0\n",
      "Training iteration 345 loss: 0.0028110698331147432, ACC:1.0\n",
      "Training iteration 346 loss: 0.01848524808883667, ACC:0.984375\n",
      "Training iteration 347 loss: 0.029052317142486572, ACC:0.984375\n",
      "Training iteration 348 loss: 0.039590176194906235, ACC:0.984375\n",
      "Training iteration 349 loss: 0.016238238662481308, ACC:1.0\n",
      "Training iteration 350 loss: 0.08396361023187637, ACC:0.984375\n",
      "Training iteration 351 loss: 0.06439008563756943, ACC:0.96875\n",
      "Training iteration 352 loss: 0.08644188940525055, ACC:0.96875\n",
      "Training iteration 353 loss: 0.032578762620687485, ACC:1.0\n",
      "Training iteration 354 loss: 0.050388287752866745, ACC:1.0\n",
      "Training iteration 355 loss: 0.04467232897877693, ACC:0.984375\n",
      "Training iteration 356 loss: 0.06395381689071655, ACC:0.984375\n",
      "Training iteration 357 loss: 0.029476627707481384, ACC:1.0\n",
      "Training iteration 358 loss: 0.020937083289027214, ACC:1.0\n",
      "Training iteration 359 loss: 0.07675567269325256, ACC:0.96875\n",
      "Training iteration 360 loss: 0.011006598360836506, ACC:1.0\n",
      "Training iteration 361 loss: 0.018245195969939232, ACC:1.0\n",
      "Training iteration 362 loss: 0.008587762713432312, ACC:1.0\n",
      "Training iteration 363 loss: 0.014236023649573326, ACC:1.0\n",
      "Training iteration 364 loss: 0.053653839975595474, ACC:0.984375\n",
      "Training iteration 365 loss: 0.0033727779518812895, ACC:1.0\n",
      "Training iteration 366 loss: 0.0047051627188920975, ACC:1.0\n",
      "Training iteration 367 loss: 0.005453119985759258, ACC:1.0\n",
      "Training iteration 368 loss: 0.004610390402376652, ACC:1.0\n",
      "Training iteration 369 loss: 0.06913325935602188, ACC:0.984375\n",
      "Training iteration 370 loss: 0.013077741488814354, ACC:0.984375\n",
      "Training iteration 371 loss: 0.0020084164571017027, ACC:1.0\n",
      "Training iteration 372 loss: 0.018914224579930305, ACC:0.984375\n",
      "Training iteration 373 loss: 0.002435195492580533, ACC:1.0\n",
      "Training iteration 374 loss: 0.012420336715877056, ACC:1.0\n",
      "Training iteration 375 loss: 0.17062333226203918, ACC:0.96875\n",
      "Training iteration 376 loss: 0.0040834033861756325, ACC:1.0\n",
      "Training iteration 377 loss: 0.057586003094911575, ACC:0.984375\n",
      "Training iteration 378 loss: 0.00396515429019928, ACC:1.0\n",
      "Training iteration 379 loss: 0.09376024454832077, ACC:0.984375\n",
      "Training iteration 380 loss: 0.008231502957642078, ACC:1.0\n",
      "Training iteration 381 loss: 0.026735201478004456, ACC:0.984375\n",
      "Training iteration 382 loss: 0.027609635144472122, ACC:0.984375\n",
      "Training iteration 383 loss: 0.11208783090114594, ACC:0.96875\n",
      "Training iteration 384 loss: 0.003431770484894514, ACC:1.0\n",
      "Training iteration 385 loss: 0.07090386748313904, ACC:0.96875\n",
      "Training iteration 386 loss: 0.0028290271293371916, ACC:1.0\n",
      "Training iteration 387 loss: 0.06718537956476212, ACC:0.984375\n",
      "Training iteration 388 loss: 0.06319515407085419, ACC:0.984375\n",
      "Training iteration 389 loss: 0.012926379218697548, ACC:1.0\n",
      "Training iteration 390 loss: 0.05081057548522949, ACC:0.984375\n",
      "Training iteration 391 loss: 0.008764378726482391, ACC:1.0\n",
      "Training iteration 392 loss: 0.021369554102420807, ACC:1.0\n",
      "Training iteration 393 loss: 0.00784476101398468, ACC:1.0\n",
      "Training iteration 394 loss: 0.014349310658872128, ACC:1.0\n",
      "Training iteration 395 loss: 0.01046421192586422, ACC:1.0\n",
      "Training iteration 396 loss: 0.01151807326823473, ACC:1.0\n",
      "Training iteration 397 loss: 0.011490647681057453, ACC:1.0\n",
      "Training iteration 398 loss: 0.004445540718734264, ACC:1.0\n",
      "Training iteration 399 loss: 0.013645820319652557, ACC:1.0\n",
      "Training iteration 400 loss: 0.0646272748708725, ACC:0.984375\n",
      "Training iteration 401 loss: 0.008314058184623718, ACC:1.0\n",
      "Training iteration 402 loss: 0.003285212442278862, ACC:1.0\n",
      "Training iteration 403 loss: 0.003986803349107504, ACC:1.0\n",
      "Training iteration 404 loss: 0.0018110745586454868, ACC:1.0\n",
      "Training iteration 405 loss: 0.0020601230207830667, ACC:1.0\n",
      "Training iteration 406 loss: 0.04797406122088432, ACC:0.984375\n",
      "Training iteration 407 loss: 0.057945366948843, ACC:0.984375\n",
      "Training iteration 408 loss: 0.032924700528383255, ACC:0.984375\n",
      "Training iteration 409 loss: 0.07753569632768631, ACC:0.984375\n",
      "Training iteration 410 loss: 0.04242372512817383, ACC:0.96875\n",
      "Training iteration 411 loss: 0.16853395104408264, ACC:0.953125\n",
      "Training iteration 412 loss: 0.005593399051576853, ACC:1.0\n",
      "Training iteration 413 loss: 0.01219302136451006, ACC:1.0\n",
      "Training iteration 414 loss: 0.18721969425678253, ACC:0.984375\n",
      "Training iteration 415 loss: 0.05582311004400253, ACC:0.984375\n",
      "Training iteration 416 loss: 0.005922774318605661, ACC:1.0\n",
      "Training iteration 417 loss: 0.17370669543743134, ACC:0.953125\n",
      "Training iteration 418 loss: 0.005226061679422855, ACC:1.0\n",
      "Training iteration 419 loss: 0.04122903198003769, ACC:0.984375\n",
      "Training iteration 420 loss: 0.15041089057922363, ACC:0.953125\n",
      "Training iteration 421 loss: 0.05677475780248642, ACC:0.953125\n",
      "Training iteration 422 loss: 0.0016522742807865143, ACC:1.0\n",
      "Training iteration 423 loss: 0.0972023606300354, ACC:0.96875\n",
      "Training iteration 424 loss: 0.024597687646746635, ACC:0.984375\n",
      "Training iteration 425 loss: 0.06806472688913345, ACC:0.984375\n",
      "Training iteration 426 loss: 0.03571769967675209, ACC:0.984375\n",
      "Training iteration 427 loss: 0.019019607454538345, ACC:1.0\n",
      "Training iteration 428 loss: 0.05983155593276024, ACC:0.96875\n",
      "Training iteration 429 loss: 0.036572400480508804, ACC:0.984375\n",
      "Training iteration 430 loss: 0.02227245643734932, ACC:1.0\n",
      "Training iteration 431 loss: 0.02204330638051033, ACC:1.0\n",
      "Training iteration 432 loss: 0.04110454395413399, ACC:0.984375\n",
      "Training iteration 433 loss: 0.05727526545524597, ACC:0.984375\n",
      "Training iteration 434 loss: 0.029754506424069405, ACC:1.0\n",
      "Training iteration 435 loss: 0.01823854073882103, ACC:1.0\n",
      "Training iteration 436 loss: 0.022819606587290764, ACC:1.0\n",
      "Training iteration 437 loss: 0.010174985975027084, ACC:1.0\n",
      "Training iteration 438 loss: 0.03055800125002861, ACC:0.984375\n",
      "Training iteration 439 loss: 0.038309451192617416, ACC:0.984375\n",
      "Training iteration 440 loss: 0.014637039043009281, ACC:1.0\n",
      "Training iteration 441 loss: 0.015388705767691135, ACC:1.0\n",
      "Training iteration 442 loss: 0.009097805246710777, ACC:1.0\n",
      "Training iteration 443 loss: 0.0070161293260753155, ACC:1.0\n",
      "Training iteration 444 loss: 0.1963225156068802, ACC:0.984375\n",
      "Training iteration 445 loss: 0.019830241799354553, ACC:0.984375\n",
      "Training iteration 446 loss: 0.0043196226470172405, ACC:1.0\n",
      "Training iteration 447 loss: 0.011752942577004433, ACC:1.0\n",
      "Training iteration 448 loss: 0.014272632077336311, ACC:0.984375\n",
      "Training iteration 449 loss: 0.06338393688201904, ACC:0.984375\n",
      "Training iteration 450 loss: 0.17670567333698273, ACC:0.96875\n",
      "Validation iteration 451 loss: 0.13074423372745514, ACC: 0.96875\n",
      "Validation iteration 452 loss: 0.008609033189713955, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.007728321012109518, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.14753443002700806, ACC: 0.953125\n",
      "Validation iteration 455 loss: 0.05156903713941574, ACC: 0.984375\n",
      "Validation iteration 456 loss: 0.010622822679579258, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.008008785545825958, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.09147533774375916, ACC: 0.984375\n",
      "Validation iteration 459 loss: 0.001592014916241169, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.04262245446443558, ACC: 0.984375\n",
      "Validation iteration 461 loss: 0.0015637181932106614, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.00951062235981226, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.0027294119354337454, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.0034416276030242443, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.092811219394207, ACC: 0.984375\n",
      "Validation iteration 466 loss: 0.03218182921409607, ACC: 0.984375\n",
      "Validation iteration 467 loss: 0.06933995336294174, ACC: 0.984375\n",
      "Validation iteration 468 loss: 0.00276114116422832, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.10301228612661362, ACC: 0.953125\n",
      "Validation iteration 470 loss: 0.04224003478884697, ACC: 0.984375\n",
      "Validation iteration 471 loss: 0.0016733653610572219, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.09616055339574814, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.19874612987041473, ACC: 0.953125\n",
      "Validation iteration 474 loss: 0.13781531155109406, ACC: 0.96875\n",
      "Validation iteration 475 loss: 0.047377247363328934, ACC: 0.984375\n",
      "Validation iteration 476 loss: 0.0739939883351326, ACC: 0.984375\n",
      "Validation iteration 477 loss: 0.1198638454079628, ACC: 0.96875\n",
      "Validation iteration 478 loss: 0.1209665909409523, ACC: 0.96875\n",
      "Validation iteration 479 loss: 0.1660842001438141, ACC: 0.953125\n",
      "Validation iteration 480 loss: 0.0037716086953878403, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.018112871795892715, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.1220787987112999, ACC: 0.96875\n",
      "Validation iteration 483 loss: 0.27436333894729614, ACC: 0.953125\n",
      "Validation iteration 484 loss: 0.056225087493658066, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.12213645875453949, ACC: 0.96875\n",
      "Validation iteration 486 loss: 0.01647266186773777, ACC: 0.984375\n",
      "Validation iteration 487 loss: 0.13895829021930695, ACC: 0.9375\n",
      "Validation iteration 488 loss: 0.03228743374347687, ACC: 0.984375\n",
      "Validation iteration 489 loss: 0.05060264468193054, ACC: 0.984375\n",
      "Validation iteration 490 loss: 0.002288577612489462, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.0028135054744780064, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.0014892678009346128, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.03421461582183838, ACC: 0.96875\n",
      "Validation iteration 494 loss: 0.0015487513737753034, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.02198297157883644, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.006521429866552353, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.06516026705503464, ACC: 0.984375\n",
      "Validation iteration 498 loss: 0.0011636002454906702, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.1472153514623642, ACC: 0.96875\n",
      "Validation iteration 500 loss: 0.07127730548381805, ACC: 0.984375\n",
      "-- Epoch 3 done -- Train loss: 0.03463601128948438, train ACC: 0.9899652777777778, val loss: 0.06026928771287203, val ACC: 0.9840625\n",
      "<--- 19080.07045841217 seconds --->\n",
      "Training iteration 1 loss: 0.08768308162689209, ACC:0.984375\n",
      "Training iteration 2 loss: 0.11776893585920334, ACC:0.96875\n",
      "Training iteration 3 loss: 0.04801614210009575, ACC:0.984375\n",
      "Training iteration 4 loss: 0.0021850578486919403, ACC:1.0\n",
      "Training iteration 5 loss: 0.008836851455271244, ACC:1.0\n",
      "Training iteration 6 loss: 0.07200954854488373, ACC:0.984375\n",
      "Training iteration 7 loss: 0.030548561364412308, ACC:0.984375\n",
      "Training iteration 8 loss: 0.011970375664532185, ACC:1.0\n",
      "Training iteration 9 loss: 0.0167333222925663, ACC:1.0\n",
      "Training iteration 10 loss: 0.010682739317417145, ACC:1.0\n",
      "Training iteration 11 loss: 0.02849314920604229, ACC:1.0\n",
      "Training iteration 12 loss: 0.020645158365368843, ACC:1.0\n",
      "Training iteration 13 loss: 0.005190310068428516, ACC:1.0\n",
      "Training iteration 14 loss: 0.023650938645005226, ACC:1.0\n",
      "Training iteration 15 loss: 0.012598042376339436, ACC:1.0\n",
      "Training iteration 16 loss: 0.004925454501062632, ACC:1.0\n",
      "Training iteration 17 loss: 0.05833232402801514, ACC:0.984375\n",
      "Training iteration 18 loss: 0.012177802622318268, ACC:1.0\n",
      "Training iteration 19 loss: 0.002423324156552553, ACC:1.0\n",
      "Training iteration 20 loss: 0.003709376323968172, ACC:1.0\n",
      "Training iteration 21 loss: 0.006172336172312498, ACC:1.0\n",
      "Training iteration 22 loss: 0.05170396715402603, ACC:0.984375\n",
      "Training iteration 23 loss: 0.02009502984583378, ACC:0.984375\n",
      "Training iteration 24 loss: 0.0016493067378178239, ACC:1.0\n",
      "Training iteration 25 loss: 0.012947753071784973, ACC:0.984375\n",
      "Training iteration 26 loss: 0.006543023511767387, ACC:1.0\n",
      "Training iteration 27 loss: 0.046212926506996155, ACC:0.96875\n",
      "Training iteration 28 loss: 0.022535208612680435, ACC:0.984375\n",
      "Training iteration 29 loss: 0.001030638930387795, ACC:1.0\n",
      "Training iteration 30 loss: 0.020056551322340965, ACC:0.984375\n",
      "Training iteration 31 loss: 0.0030358200892806053, ACC:1.0\n",
      "Training iteration 32 loss: 0.07614369690418243, ACC:0.96875\n",
      "Training iteration 33 loss: 0.004221553914248943, ACC:1.0\n",
      "Training iteration 34 loss: 0.0034853394608944654, ACC:1.0\n",
      "Training iteration 35 loss: 0.10958145558834076, ACC:0.984375\n",
      "Training iteration 36 loss: 0.04324347525835037, ACC:0.984375\n",
      "Training iteration 37 loss: 0.37453967332839966, ACC:0.953125\n",
      "Training iteration 38 loss: 0.07786782830953598, ACC:0.984375\n",
      "Training iteration 39 loss: 0.11091388016939163, ACC:0.984375\n",
      "Training iteration 40 loss: 0.24329258501529694, ACC:0.96875\n",
      "Training iteration 41 loss: 0.10140986740589142, ACC:0.96875\n",
      "Training iteration 42 loss: 0.1393710821866989, ACC:0.96875\n",
      "Training iteration 43 loss: 0.15370547771453857, ACC:0.953125\n",
      "Training iteration 44 loss: 0.04286959767341614, ACC:0.96875\n",
      "Training iteration 45 loss: 0.04609820991754532, ACC:0.984375\n",
      "Training iteration 46 loss: 0.0500212088227272, ACC:0.984375\n",
      "Training iteration 47 loss: 0.023987825959920883, ACC:0.984375\n",
      "Training iteration 48 loss: 0.01935739442706108, ACC:1.0\n",
      "Training iteration 49 loss: 0.07307035475969315, ACC:0.984375\n",
      "Training iteration 50 loss: 0.02975342608988285, ACC:1.0\n",
      "Training iteration 51 loss: 0.04967827349901199, ACC:0.984375\n",
      "Training iteration 52 loss: 0.01991957612335682, ACC:1.0\n",
      "Training iteration 53 loss: 0.025730842724442482, ACC:1.0\n",
      "Training iteration 54 loss: 0.016174906864762306, ACC:1.0\n",
      "Training iteration 55 loss: 0.034969691187143326, ACC:0.984375\n",
      "Training iteration 56 loss: 0.023961329832673073, ACC:0.984375\n",
      "Training iteration 57 loss: 0.020966939628124237, ACC:0.984375\n",
      "Training iteration 58 loss: 0.08758509904146194, ACC:0.953125\n",
      "Training iteration 59 loss: 0.04806607961654663, ACC:0.96875\n",
      "Training iteration 60 loss: 0.07102309167385101, ACC:0.96875\n",
      "Training iteration 61 loss: 0.0077875033020973206, ACC:1.0\n",
      "Training iteration 62 loss: 0.07788586616516113, ACC:0.953125\n",
      "Training iteration 63 loss: 0.05068865418434143, ACC:0.984375\n",
      "Training iteration 64 loss: 0.05977226793766022, ACC:0.984375\n",
      "Training iteration 65 loss: 0.02718205936253071, ACC:0.984375\n",
      "Training iteration 66 loss: 0.06619326770305634, ACC:0.984375\n",
      "Training iteration 67 loss: 0.030875561758875847, ACC:1.0\n",
      "Training iteration 68 loss: 0.030153658241033554, ACC:1.0\n",
      "Training iteration 69 loss: 0.020148200914263725, ACC:1.0\n",
      "Training iteration 70 loss: 0.02996342070400715, ACC:1.0\n",
      "Training iteration 71 loss: 0.06264586746692657, ACC:0.984375\n",
      "Training iteration 72 loss: 0.021710406988859177, ACC:1.0\n",
      "Training iteration 73 loss: 0.008742490783333778, ACC:1.0\n",
      "Training iteration 74 loss: 0.04102633520960808, ACC:0.984375\n",
      "Training iteration 75 loss: 0.0608617402613163, ACC:0.96875\n",
      "Training iteration 76 loss: 0.0077539836056530476, ACC:1.0\n",
      "Training iteration 77 loss: 0.006333894561976194, ACC:1.0\n",
      "Training iteration 78 loss: 0.006432794965803623, ACC:1.0\n",
      "Training iteration 79 loss: 0.02046671323478222, ACC:1.0\n",
      "Training iteration 80 loss: 0.005693749058991671, ACC:1.0\n",
      "Training iteration 81 loss: 0.03740822896361351, ACC:0.984375\n",
      "Training iteration 82 loss: 0.1538514345884323, ACC:0.984375\n",
      "Training iteration 83 loss: 0.003143200185149908, ACC:1.0\n",
      "Training iteration 84 loss: 0.004109181463718414, ACC:1.0\n",
      "Training iteration 85 loss: 0.0025973194278776646, ACC:1.0\n",
      "Training iteration 86 loss: 0.05747304856777191, ACC:0.984375\n",
      "Training iteration 87 loss: 0.07123468071222305, ACC:0.96875\n",
      "Training iteration 88 loss: 0.03918890282511711, ACC:0.984375\n",
      "Training iteration 89 loss: 0.035161618143320084, ACC:0.984375\n",
      "Training iteration 90 loss: 0.021385375410318375, ACC:0.984375\n",
      "Training iteration 91 loss: 0.08532512187957764, ACC:0.984375\n",
      "Training iteration 92 loss: 0.004559752531349659, ACC:1.0\n",
      "Training iteration 93 loss: 0.007855577394366264, ACC:1.0\n",
      "Training iteration 94 loss: 0.011589615605771542, ACC:1.0\n",
      "Training iteration 95 loss: 0.011956402100622654, ACC:1.0\n",
      "Training iteration 96 loss: 0.016618700698018074, ACC:1.0\n",
      "Training iteration 97 loss: 0.08164511620998383, ACC:0.96875\n",
      "Training iteration 98 loss: 0.014390289783477783, ACC:1.0\n",
      "Training iteration 99 loss: 0.021306585520505905, ACC:1.0\n",
      "Training iteration 100 loss: 0.17503662407398224, ACC:0.984375\n",
      "Training iteration 101 loss: 0.023826375603675842, ACC:1.0\n",
      "Training iteration 102 loss: 0.06179972365498543, ACC:0.96875\n",
      "Training iteration 103 loss: 0.006941003259271383, ACC:1.0\n",
      "Training iteration 104 loss: 0.009269426576793194, ACC:1.0\n",
      "Training iteration 105 loss: 0.020985718816518784, ACC:0.984375\n",
      "Training iteration 106 loss: 0.09680376946926117, ACC:0.953125\n",
      "Training iteration 107 loss: 0.057240549474954605, ACC:0.984375\n",
      "Training iteration 108 loss: 0.034261174499988556, ACC:0.96875\n",
      "Training iteration 109 loss: 0.06595286726951599, ACC:0.984375\n",
      "Training iteration 110 loss: 0.010471127927303314, ACC:1.0\n",
      "Training iteration 111 loss: 0.023401601240038872, ACC:0.984375\n",
      "Training iteration 112 loss: 0.01785394921898842, ACC:1.0\n",
      "Training iteration 113 loss: 0.05104661360383034, ACC:0.984375\n",
      "Training iteration 114 loss: 0.24811725318431854, ACC:0.953125\n",
      "Training iteration 115 loss: 0.009506231173872948, ACC:1.0\n",
      "Training iteration 116 loss: 0.008193274028599262, ACC:1.0\n",
      "Training iteration 117 loss: 0.008292845450341702, ACC:1.0\n",
      "Training iteration 118 loss: 0.0037593995220959187, ACC:1.0\n",
      "Training iteration 119 loss: 0.13000093400478363, ACC:0.9375\n",
      "Training iteration 120 loss: 0.12951883673667908, ACC:0.96875\n",
      "Training iteration 121 loss: 0.017887741327285767, ACC:0.984375\n",
      "Training iteration 122 loss: 0.208200603723526, ACC:0.96875\n",
      "Training iteration 123 loss: 0.027145259082317352, ACC:0.984375\n",
      "Training iteration 124 loss: 0.0282671507447958, ACC:0.984375\n",
      "Training iteration 125 loss: 0.008070380426943302, ACC:1.0\n",
      "Training iteration 126 loss: 0.005037149414420128, ACC:1.0\n",
      "Training iteration 127 loss: 0.01772763952612877, ACC:0.984375\n",
      "Training iteration 128 loss: 0.0714169517159462, ACC:0.96875\n",
      "Training iteration 129 loss: 0.0594177283346653, ACC:0.984375\n",
      "Training iteration 130 loss: 0.025099242106080055, ACC:0.984375\n",
      "Training iteration 131 loss: 0.007756631821393967, ACC:1.0\n",
      "Training iteration 132 loss: 0.01563771441578865, ACC:1.0\n",
      "Training iteration 133 loss: 0.09008938074111938, ACC:0.96875\n",
      "Training iteration 134 loss: 0.024141939356923103, ACC:1.0\n",
      "Training iteration 135 loss: 0.0618523508310318, ACC:0.984375\n",
      "Training iteration 136 loss: 0.009579948149621487, ACC:1.0\n",
      "Training iteration 137 loss: 0.009959043003618717, ACC:1.0\n",
      "Training iteration 138 loss: 0.035840895026922226, ACC:0.984375\n",
      "Training iteration 139 loss: 0.07018283009529114, ACC:0.984375\n",
      "Training iteration 140 loss: 0.002935824915766716, ACC:1.0\n",
      "Training iteration 141 loss: 0.08369270712137222, ACC:0.96875\n",
      "Training iteration 142 loss: 0.07358969748020172, ACC:0.984375\n",
      "Training iteration 143 loss: 0.002734535373747349, ACC:1.0\n",
      "Training iteration 144 loss: 0.004110070411115885, ACC:1.0\n",
      "Training iteration 145 loss: 0.004350322764366865, ACC:1.0\n",
      "Training iteration 146 loss: 0.05768498033285141, ACC:0.984375\n",
      "Training iteration 147 loss: 0.011160170659422874, ACC:1.0\n",
      "Training iteration 148 loss: 0.002173718996345997, ACC:1.0\n",
      "Training iteration 149 loss: 0.002960063051432371, ACC:1.0\n",
      "Training iteration 150 loss: 0.0024696725886315107, ACC:1.0\n",
      "Training iteration 151 loss: 0.02683340758085251, ACC:0.984375\n",
      "Training iteration 152 loss: 0.03796244040131569, ACC:0.984375\n",
      "Training iteration 153 loss: 0.021083630621433258, ACC:0.984375\n",
      "Training iteration 154 loss: 0.13718178868293762, ACC:0.953125\n",
      "Training iteration 155 loss: 0.15024495124816895, ACC:0.96875\n",
      "Training iteration 156 loss: 0.0013822055188938975, ACC:1.0\n",
      "Training iteration 157 loss: 0.031285688281059265, ACC:0.984375\n",
      "Training iteration 158 loss: 0.06196850165724754, ACC:0.984375\n",
      "Training iteration 159 loss: 0.0040511153638362885, ACC:1.0\n",
      "Training iteration 160 loss: 0.01101716235280037, ACC:1.0\n",
      "Training iteration 161 loss: 0.011786997318267822, ACC:1.0\n",
      "Training iteration 162 loss: 0.0035602885764092207, ACC:1.0\n",
      "Training iteration 163 loss: 0.01467844843864441, ACC:1.0\n",
      "Training iteration 164 loss: 0.00934040267020464, ACC:1.0\n",
      "Training iteration 165 loss: 0.0043508270755410194, ACC:1.0\n",
      "Training iteration 166 loss: 0.0023559778928756714, ACC:1.0\n",
      "Training iteration 167 loss: 0.007633923552930355, ACC:1.0\n",
      "Training iteration 168 loss: 0.0240988340228796, ACC:0.984375\n",
      "Training iteration 169 loss: 0.12788920104503632, ACC:0.953125\n",
      "Training iteration 170 loss: 0.03626915439963341, ACC:0.984375\n",
      "Training iteration 171 loss: 0.007734076119959354, ACC:1.0\n",
      "Training iteration 172 loss: 0.004722087178379297, ACC:1.0\n",
      "Training iteration 173 loss: 0.009582670405507088, ACC:1.0\n",
      "Training iteration 174 loss: 0.004168755374848843, ACC:1.0\n",
      "Training iteration 175 loss: 0.1686900109052658, ACC:0.96875\n",
      "Training iteration 176 loss: 0.05744580924510956, ACC:0.984375\n",
      "Training iteration 177 loss: 0.009121528826653957, ACC:1.0\n",
      "Training iteration 178 loss: 0.005120492540299892, ACC:1.0\n",
      "Training iteration 179 loss: 0.0026005078107118607, ACC:1.0\n",
      "Training iteration 180 loss: 0.0016221387777477503, ACC:1.0\n",
      "Training iteration 181 loss: 0.02485021948814392, ACC:0.984375\n",
      "Training iteration 182 loss: 0.0011550490744411945, ACC:1.0\n",
      "Training iteration 183 loss: 0.06409686803817749, ACC:0.984375\n",
      "Training iteration 184 loss: 0.001132373814471066, ACC:1.0\n",
      "Training iteration 185 loss: 0.0010956961195915937, ACC:1.0\n",
      "Training iteration 186 loss: 0.0006883853347972035, ACC:1.0\n",
      "Training iteration 187 loss: 0.03650935739278793, ACC:0.984375\n",
      "Training iteration 188 loss: 0.008643035776913166, ACC:1.0\n",
      "Training iteration 189 loss: 0.20325903594493866, ACC:0.96875\n",
      "Training iteration 190 loss: 0.009817310608923435, ACC:1.0\n",
      "Training iteration 191 loss: 0.09728953242301941, ACC:0.96875\n",
      "Training iteration 192 loss: 0.006624001078307629, ACC:1.0\n",
      "Training iteration 193 loss: 0.08422794938087463, ACC:0.96875\n",
      "Training iteration 194 loss: 0.01286856085062027, ACC:1.0\n",
      "Training iteration 195 loss: 0.015411799773573875, ACC:1.0\n",
      "Training iteration 196 loss: 0.05644289031624794, ACC:0.984375\n",
      "Training iteration 197 loss: 0.014498208649456501, ACC:1.0\n",
      "Training iteration 198 loss: 0.04260493069887161, ACC:0.984375\n",
      "Training iteration 199 loss: 0.025341995060443878, ACC:0.984375\n",
      "Training iteration 200 loss: 0.018471969291567802, ACC:1.0\n",
      "Training iteration 201 loss: 0.27246221899986267, ACC:0.96875\n",
      "Training iteration 202 loss: 0.016425419598817825, ACC:1.0\n",
      "Training iteration 203 loss: 0.025591351091861725, ACC:1.0\n",
      "Training iteration 204 loss: 0.04929741844534874, ACC:0.984375\n",
      "Training iteration 205 loss: 0.01562631130218506, ACC:1.0\n",
      "Training iteration 206 loss: 0.037081919610500336, ACC:0.984375\n",
      "Training iteration 207 loss: 0.0123301325365901, ACC:1.0\n",
      "Training iteration 208 loss: 0.13875705003738403, ACC:0.96875\n",
      "Training iteration 209 loss: 0.07007758319377899, ACC:0.96875\n",
      "Training iteration 210 loss: 0.015362758189439774, ACC:1.0\n",
      "Training iteration 211 loss: 0.024302640929818153, ACC:0.984375\n",
      "Training iteration 212 loss: 0.010835018940269947, ACC:1.0\n",
      "Training iteration 213 loss: 0.05658489093184471, ACC:0.96875\n",
      "Training iteration 214 loss: 0.011707219295203686, ACC:1.0\n",
      "Training iteration 215 loss: 0.008132467046380043, ACC:1.0\n",
      "Training iteration 216 loss: 0.030154090374708176, ACC:1.0\n",
      "Training iteration 217 loss: 0.04308903589844704, ACC:0.984375\n",
      "Training iteration 218 loss: 0.028887324035167694, ACC:0.984375\n",
      "Training iteration 219 loss: 0.039734113961458206, ACC:0.984375\n",
      "Training iteration 220 loss: 0.0677492618560791, ACC:0.953125\n",
      "Training iteration 221 loss: 0.008836520835757256, ACC:1.0\n",
      "Training iteration 222 loss: 0.02047796919941902, ACC:0.984375\n",
      "Training iteration 223 loss: 0.06776358932256699, ACC:0.984375\n",
      "Training iteration 224 loss: 0.019156621769070625, ACC:1.0\n",
      "Training iteration 225 loss: 0.053434062749147415, ACC:0.984375\n",
      "Training iteration 226 loss: 0.004941912367939949, ACC:1.0\n",
      "Training iteration 227 loss: 0.06368560343980789, ACC:0.984375\n",
      "Training iteration 228 loss: 0.013175049796700478, ACC:1.0\n",
      "Training iteration 229 loss: 0.006166230421513319, ACC:1.0\n",
      "Training iteration 230 loss: 0.11581361293792725, ACC:0.9375\n",
      "Training iteration 231 loss: 0.009368271566927433, ACC:1.0\n",
      "Training iteration 232 loss: 0.02068648859858513, ACC:0.984375\n",
      "Training iteration 233 loss: 0.04506707936525345, ACC:0.984375\n",
      "Training iteration 234 loss: 0.0030122422613203526, ACC:1.0\n",
      "Training iteration 235 loss: 0.06310027837753296, ACC:0.984375\n",
      "Training iteration 236 loss: 0.008476589806377888, ACC:1.0\n",
      "Training iteration 237 loss: 0.005118720233440399, ACC:1.0\n",
      "Training iteration 238 loss: 0.0022688962053507566, ACC:1.0\n",
      "Training iteration 239 loss: 0.0052596936002373695, ACC:1.0\n",
      "Training iteration 240 loss: 0.07646296173334122, ACC:0.984375\n",
      "Training iteration 241 loss: 0.027441732585430145, ACC:0.984375\n",
      "Training iteration 242 loss: 0.10139740258455276, ACC:0.953125\n",
      "Training iteration 243 loss: 0.0021844294387847185, ACC:1.0\n",
      "Training iteration 244 loss: 0.003709659446030855, ACC:1.0\n",
      "Training iteration 245 loss: 0.01229636836796999, ACC:1.0\n",
      "Training iteration 246 loss: 0.028164466843008995, ACC:0.984375\n",
      "Training iteration 247 loss: 0.0912177637219429, ACC:0.984375\n",
      "Training iteration 248 loss: 0.028781725093722343, ACC:0.984375\n",
      "Training iteration 249 loss: 0.002207284327596426, ACC:1.0\n",
      "Training iteration 250 loss: 0.09449407458305359, ACC:0.984375\n",
      "Training iteration 251 loss: 0.0462493933737278, ACC:0.96875\n",
      "Training iteration 252 loss: 0.0013259361730888486, ACC:1.0\n",
      "Training iteration 253 loss: 0.06336155533790588, ACC:0.984375\n",
      "Training iteration 254 loss: 0.0013063333462923765, ACC:1.0\n",
      "Training iteration 255 loss: 0.008993026800453663, ACC:1.0\n",
      "Training iteration 256 loss: 0.014624815434217453, ACC:0.984375\n",
      "Training iteration 257 loss: 0.018475303426384926, ACC:0.984375\n",
      "Training iteration 258 loss: 0.001535021816380322, ACC:1.0\n",
      "Training iteration 259 loss: 0.02278909832239151, ACC:0.984375\n",
      "Training iteration 260 loss: 0.018552226945757866, ACC:0.984375\n",
      "Training iteration 261 loss: 0.030107678845524788, ACC:0.984375\n",
      "Training iteration 262 loss: 0.0013957167975604534, ACC:1.0\n",
      "Training iteration 263 loss: 0.016936169937253, ACC:1.0\n",
      "Training iteration 264 loss: 0.0009953364497050643, ACC:1.0\n",
      "Training iteration 265 loss: 0.002330947434529662, ACC:1.0\n",
      "Training iteration 266 loss: 0.01012959610670805, ACC:1.0\n",
      "Training iteration 267 loss: 0.0814618393778801, ACC:0.984375\n",
      "Training iteration 268 loss: 0.005379968788474798, ACC:1.0\n",
      "Training iteration 269 loss: 0.0060280803591012955, ACC:1.0\n",
      "Training iteration 270 loss: 0.0037689439486712217, ACC:1.0\n",
      "Training iteration 271 loss: 0.021847322583198547, ACC:0.984375\n",
      "Training iteration 272 loss: 0.005581500008702278, ACC:1.0\n",
      "Training iteration 273 loss: 0.05980033054947853, ACC:0.984375\n",
      "Training iteration 274 loss: 0.04103407636284828, ACC:0.984375\n",
      "Training iteration 275 loss: 0.03584365174174309, ACC:0.984375\n",
      "Training iteration 276 loss: 0.019140297546982765, ACC:0.984375\n",
      "Training iteration 277 loss: 0.009417785331606865, ACC:1.0\n",
      "Training iteration 278 loss: 0.010474649257957935, ACC:1.0\n",
      "Training iteration 279 loss: 0.06327478587627411, ACC:0.96875\n",
      "Training iteration 280 loss: 0.07252973318099976, ACC:0.984375\n",
      "Training iteration 281 loss: 0.026206789538264275, ACC:1.0\n",
      "Training iteration 282 loss: 0.010984905064105988, ACC:1.0\n",
      "Training iteration 283 loss: 0.010555803775787354, ACC:1.0\n",
      "Training iteration 284 loss: 0.10978551208972931, ACC:0.96875\n",
      "Training iteration 285 loss: 0.011769461445510387, ACC:1.0\n",
      "Training iteration 286 loss: 0.028758859261870384, ACC:0.984375\n",
      "Training iteration 287 loss: 0.02567702904343605, ACC:1.0\n",
      "Training iteration 288 loss: 0.014385414309799671, ACC:1.0\n",
      "Training iteration 289 loss: 0.008764329366385937, ACC:1.0\n",
      "Training iteration 290 loss: 0.0286455899477005, ACC:0.984375\n",
      "Training iteration 291 loss: 0.054567452520132065, ACC:0.96875\n",
      "Training iteration 292 loss: 0.006391668692231178, ACC:1.0\n",
      "Training iteration 293 loss: 0.006435037590563297, ACC:1.0\n",
      "Training iteration 294 loss: 0.008141998201608658, ACC:1.0\n",
      "Training iteration 295 loss: 0.01329933013767004, ACC:1.0\n",
      "Training iteration 296 loss: 0.03866792470216751, ACC:0.984375\n",
      "Training iteration 297 loss: 0.010512814857065678, ACC:1.0\n",
      "Training iteration 298 loss: 0.0374961793422699, ACC:0.984375\n",
      "Training iteration 299 loss: 0.01662452518939972, ACC:0.984375\n",
      "Training iteration 300 loss: 0.006284796167165041, ACC:1.0\n",
      "Training iteration 301 loss: 0.0856168121099472, ACC:0.984375\n",
      "Training iteration 302 loss: 0.003408574964851141, ACC:1.0\n",
      "Training iteration 303 loss: 0.004659619648009539, ACC:1.0\n",
      "Training iteration 304 loss: 0.004318470135331154, ACC:1.0\n",
      "Training iteration 305 loss: 0.002919310238212347, ACC:1.0\n",
      "Training iteration 306 loss: 0.01666252873837948, ACC:1.0\n",
      "Training iteration 307 loss: 0.02495812252163887, ACC:0.984375\n",
      "Training iteration 308 loss: 0.0069139981642365456, ACC:1.0\n",
      "Training iteration 309 loss: 0.03489528223872185, ACC:0.96875\n",
      "Training iteration 310 loss: 0.00827754195779562, ACC:1.0\n",
      "Training iteration 311 loss: 0.002131861401721835, ACC:1.0\n",
      "Training iteration 312 loss: 0.05032210797071457, ACC:0.984375\n",
      "Training iteration 313 loss: 0.07878835499286652, ACC:0.953125\n",
      "Training iteration 314 loss: 0.026750870048999786, ACC:0.984375\n",
      "Training iteration 315 loss: 0.018085457384586334, ACC:0.984375\n",
      "Training iteration 316 loss: 0.00257204775698483, ACC:1.0\n",
      "Training iteration 317 loss: 0.005015529692173004, ACC:1.0\n",
      "Training iteration 318 loss: 0.07972004264593124, ACC:0.984375\n",
      "Training iteration 319 loss: 0.006246098317205906, ACC:1.0\n",
      "Training iteration 320 loss: 0.018986547365784645, ACC:1.0\n",
      "Training iteration 321 loss: 0.004407527856528759, ACC:1.0\n",
      "Training iteration 322 loss: 0.11611601710319519, ACC:0.984375\n",
      "Training iteration 323 loss: 0.2207816243171692, ACC:0.96875\n",
      "Training iteration 324 loss: 0.035562217235565186, ACC:0.984375\n",
      "Training iteration 325 loss: 0.06404565274715424, ACC:0.96875\n",
      "Training iteration 326 loss: 0.000744528544601053, ACC:1.0\n",
      "Training iteration 327 loss: 0.023582858964800835, ACC:0.984375\n",
      "Training iteration 328 loss: 0.061452705413103104, ACC:0.984375\n",
      "Training iteration 329 loss: 0.03273780271410942, ACC:0.984375\n",
      "Training iteration 330 loss: 0.0024725724942982197, ACC:1.0\n",
      "Training iteration 331 loss: 0.004054853692650795, ACC:1.0\n",
      "Training iteration 332 loss: 0.011069164611399174, ACC:1.0\n",
      "Training iteration 333 loss: 0.07154359668493271, ACC:0.984375\n",
      "Training iteration 334 loss: 0.05061037838459015, ACC:0.96875\n",
      "Training iteration 335 loss: 0.02531960979104042, ACC:1.0\n",
      "Training iteration 336 loss: 0.11313224583864212, ACC:0.96875\n",
      "Training iteration 337 loss: 0.04298526793718338, ACC:1.0\n",
      "Training iteration 338 loss: 0.07113362848758698, ACC:0.984375\n",
      "Training iteration 339 loss: 0.09025929868221283, ACC:0.953125\n",
      "Training iteration 340 loss: 0.029250506311655045, ACC:1.0\n",
      "Training iteration 341 loss: 0.03186730295419693, ACC:0.984375\n",
      "Training iteration 342 loss: 0.022132130339741707, ACC:1.0\n",
      "Training iteration 343 loss: 0.041915640234947205, ACC:0.984375\n",
      "Training iteration 344 loss: 0.05040431022644043, ACC:0.984375\n",
      "Training iteration 345 loss: 0.07396858185529709, ACC:0.984375\n",
      "Training iteration 346 loss: 0.01268150843679905, ACC:1.0\n",
      "Training iteration 347 loss: 0.015903528779745102, ACC:1.0\n",
      "Training iteration 348 loss: 0.016265302896499634, ACC:1.0\n",
      "Training iteration 349 loss: 0.007375639397650957, ACC:1.0\n",
      "Training iteration 350 loss: 0.013248415663838387, ACC:1.0\n",
      "Training iteration 351 loss: 0.06397717446088791, ACC:0.984375\n",
      "Training iteration 352 loss: 0.03137660771608353, ACC:0.984375\n",
      "Training iteration 353 loss: 0.010626906529068947, ACC:1.0\n",
      "Training iteration 354 loss: 0.0484587736427784, ACC:0.984375\n",
      "Training iteration 355 loss: 0.0864807665348053, ACC:0.953125\n",
      "Training iteration 356 loss: 0.042370911687612534, ACC:0.984375\n",
      "Training iteration 357 loss: 0.004116788972169161, ACC:1.0\n",
      "Training iteration 358 loss: 0.0030881287530064583, ACC:1.0\n",
      "Training iteration 359 loss: 0.010164681822061539, ACC:1.0\n",
      "Training iteration 360 loss: 0.010786481201648712, ACC:1.0\n",
      "Training iteration 361 loss: 0.002739341463893652, ACC:1.0\n",
      "Training iteration 362 loss: 0.007508631329983473, ACC:1.0\n",
      "Training iteration 363 loss: 0.0036549395881593227, ACC:1.0\n",
      "Training iteration 364 loss: 0.06164606288075447, ACC:0.984375\n",
      "Training iteration 365 loss: 0.01816224493086338, ACC:0.984375\n",
      "Training iteration 366 loss: 0.004741418641060591, ACC:1.0\n",
      "Training iteration 367 loss: 0.020208898931741714, ACC:0.984375\n",
      "Training iteration 368 loss: 0.016759060323238373, ACC:0.984375\n",
      "Training iteration 369 loss: 0.03605141118168831, ACC:0.984375\n",
      "Training iteration 370 loss: 0.003784401109442115, ACC:1.0\n",
      "Training iteration 371 loss: 0.011500094085931778, ACC:0.984375\n",
      "Training iteration 372 loss: 0.0004231624770909548, ACC:1.0\n",
      "Training iteration 373 loss: 0.00023886605049483478, ACC:1.0\n",
      "Training iteration 374 loss: 0.0014659729786217213, ACC:1.0\n",
      "Training iteration 375 loss: 0.06680759787559509, ACC:0.984375\n",
      "Training iteration 376 loss: 0.0065115634351968765, ACC:1.0\n",
      "Training iteration 377 loss: 0.002381313359364867, ACC:1.0\n",
      "Training iteration 378 loss: 0.019498050212860107, ACC:0.984375\n",
      "Training iteration 379 loss: 0.2897571325302124, ACC:0.953125\n",
      "Training iteration 380 loss: 0.042792338877916336, ACC:0.96875\n",
      "Training iteration 381 loss: 0.03539315611124039, ACC:0.984375\n",
      "Training iteration 382 loss: 0.0023363959044218063, ACC:1.0\n",
      "Training iteration 383 loss: 0.29349231719970703, ACC:0.9375\n",
      "Training iteration 384 loss: 0.00039162454777397215, ACC:1.0\n",
      "Training iteration 385 loss: 0.001821034587919712, ACC:1.0\n",
      "Training iteration 386 loss: 0.0009407850448042154, ACC:1.0\n",
      "Training iteration 387 loss: 0.014731884934008121, ACC:0.984375\n",
      "Training iteration 388 loss: 0.07154641300439835, ACC:0.96875\n",
      "Training iteration 389 loss: 0.030987560749053955, ACC:0.96875\n",
      "Training iteration 390 loss: 0.00048474117647856474, ACC:1.0\n",
      "Training iteration 391 loss: 0.022373011335730553, ACC:0.984375\n",
      "Training iteration 392 loss: 0.005635816603899002, ACC:1.0\n",
      "Training iteration 393 loss: 0.007153663318604231, ACC:1.0\n",
      "Training iteration 394 loss: 0.01008597668260336, ACC:1.0\n",
      "Training iteration 395 loss: 0.0086332056671381, ACC:1.0\n",
      "Training iteration 396 loss: 0.01980379968881607, ACC:1.0\n",
      "Training iteration 397 loss: 0.04664713889360428, ACC:0.96875\n",
      "Training iteration 398 loss: 0.052475351840257645, ACC:0.984375\n",
      "Training iteration 399 loss: 0.008590883575379848, ACC:1.0\n",
      "Training iteration 400 loss: 0.04128627851605415, ACC:0.984375\n",
      "Training iteration 401 loss: 0.01210385374724865, ACC:1.0\n",
      "Training iteration 402 loss: 0.12279464304447174, ACC:0.953125\n",
      "Training iteration 403 loss: 0.18438558280467987, ACC:0.9375\n",
      "Training iteration 404 loss: 0.016645140945911407, ACC:1.0\n",
      "Training iteration 405 loss: 0.01634812541306019, ACC:1.0\n",
      "Training iteration 406 loss: 0.01687929406762123, ACC:1.0\n",
      "Training iteration 407 loss: 0.030710121616721153, ACC:1.0\n",
      "Training iteration 408 loss: 0.02884203940629959, ACC:1.0\n",
      "Training iteration 409 loss: 0.11606630682945251, ACC:0.984375\n",
      "Training iteration 410 loss: 0.014615550637245178, ACC:1.0\n",
      "Training iteration 411 loss: 0.016439314931631088, ACC:1.0\n",
      "Training iteration 412 loss: 0.009519555605947971, ACC:1.0\n",
      "Training iteration 413 loss: 0.020959928631782532, ACC:1.0\n",
      "Training iteration 414 loss: 0.026575898751616478, ACC:0.984375\n",
      "Training iteration 415 loss: 0.011214327067136765, ACC:1.0\n",
      "Training iteration 416 loss: 0.006734676659107208, ACC:1.0\n",
      "Training iteration 417 loss: 0.03629826754331589, ACC:0.984375\n",
      "Training iteration 418 loss: 0.006278637330979109, ACC:1.0\n",
      "Training iteration 419 loss: 0.021251864731311798, ACC:0.984375\n",
      "Training iteration 420 loss: 0.004709799308329821, ACC:1.0\n",
      "Training iteration 421 loss: 0.02202971838414669, ACC:0.984375\n",
      "Training iteration 422 loss: 0.021276278421282768, ACC:1.0\n",
      "Training iteration 423 loss: 0.017233967781066895, ACC:0.984375\n",
      "Training iteration 424 loss: 0.0034426036290824413, ACC:1.0\n",
      "Training iteration 425 loss: 0.007739931344985962, ACC:1.0\n",
      "Training iteration 426 loss: 0.01848640665411949, ACC:1.0\n",
      "Training iteration 427 loss: 0.006696098484098911, ACC:1.0\n",
      "Training iteration 428 loss: 0.09642428159713745, ACC:0.96875\n",
      "Training iteration 429 loss: 0.013004165142774582, ACC:0.984375\n",
      "Training iteration 430 loss: 0.016854839399456978, ACC:0.984375\n",
      "Training iteration 431 loss: 0.0014124419540166855, ACC:1.0\n",
      "Training iteration 432 loss: 0.017157021909952164, ACC:0.984375\n",
      "Training iteration 433 loss: 0.0013152864994481206, ACC:1.0\n",
      "Training iteration 434 loss: 0.005818897858262062, ACC:1.0\n",
      "Training iteration 435 loss: 0.007079224102199078, ACC:1.0\n",
      "Training iteration 436 loss: 0.08186052739620209, ACC:0.984375\n",
      "Training iteration 437 loss: 0.0002457182854413986, ACC:1.0\n",
      "Training iteration 438 loss: 0.0008752864087000489, ACC:1.0\n",
      "Training iteration 439 loss: 0.00029049598379060626, ACC:1.0\n",
      "Training iteration 440 loss: 0.002541006775572896, ACC:1.0\n",
      "Training iteration 441 loss: 0.0003855643735732883, ACC:1.0\n",
      "Training iteration 442 loss: 0.238946795463562, ACC:0.984375\n",
      "Training iteration 443 loss: 0.2001018077135086, ACC:0.96875\n",
      "Training iteration 444 loss: 0.0013469145633280277, ACC:1.0\n",
      "Training iteration 445 loss: 0.00022683307179249823, ACC:1.0\n",
      "Training iteration 446 loss: 0.0016347578493878245, ACC:1.0\n",
      "Training iteration 447 loss: 0.0005714257713407278, ACC:1.0\n",
      "Training iteration 448 loss: 0.0008089455077424645, ACC:1.0\n",
      "Training iteration 449 loss: 0.00802034791558981, ACC:1.0\n",
      "Training iteration 450 loss: 0.073198102414608, ACC:0.96875\n",
      "Validation iteration 451 loss: 0.035793133080005646, ACC: 0.984375\n",
      "Validation iteration 452 loss: 0.0729045569896698, ACC: 0.96875\n",
      "Validation iteration 453 loss: 0.010825312696397305, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.004153548274189234, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.014283942990005016, ACC: 0.984375\n",
      "Validation iteration 456 loss: 0.028221942484378815, ACC: 0.984375\n",
      "Validation iteration 457 loss: 0.10801520198583603, ACC: 0.953125\n",
      "Validation iteration 458 loss: 0.044649917632341385, ACC: 0.984375\n",
      "Validation iteration 459 loss: 0.0012966481735929847, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.037803247570991516, ACC: 0.984375\n",
      "Validation iteration 461 loss: 0.07371476292610168, ACC: 0.96875\n",
      "Validation iteration 462 loss: 0.012051139026880264, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.005603749305009842, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.1328994631767273, ACC: 0.96875\n",
      "Validation iteration 465 loss: 0.011236459016799927, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.0031323411967605352, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.07753954827785492, ACC: 0.96875\n",
      "Validation iteration 468 loss: 0.0009542265324853361, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.04363355413079262, ACC: 0.984375\n",
      "Validation iteration 470 loss: 0.021609874442219734, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.042438045144081116, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.0010990829905495048, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.017950348556041718, ACC: 0.984375\n",
      "Validation iteration 474 loss: 0.008029015734791756, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.0008171704248525202, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.0010274754604324698, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.004953855648636818, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.03500627726316452, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.015885474160313606, ACC: 0.984375\n",
      "Validation iteration 480 loss: 0.02841976098716259, ACC: 0.96875\n",
      "Validation iteration 481 loss: 0.04146039858460426, ACC: 0.984375\n",
      "Validation iteration 482 loss: 0.0010706102475523949, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.0014382980298250914, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.09413350373506546, ACC: 0.96875\n",
      "Validation iteration 485 loss: 0.006125700660049915, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.03935982286930084, ACC: 0.96875\n",
      "Validation iteration 487 loss: 0.0022355213295668364, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.02060863748192787, ACC: 0.984375\n",
      "Validation iteration 489 loss: 0.01979619450867176, ACC: 0.984375\n",
      "Validation iteration 490 loss: 0.0861029401421547, ACC: 0.96875\n",
      "Validation iteration 491 loss: 0.0786118283867836, ACC: 0.984375\n",
      "Validation iteration 492 loss: 0.011469078250229359, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.10161666572093964, ACC: 0.96875\n",
      "Validation iteration 494 loss: 0.0022113851737231016, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.03601636365056038, ACC: 0.984375\n",
      "Validation iteration 496 loss: 0.003941346891224384, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.10557214170694351, ACC: 0.984375\n",
      "Validation iteration 498 loss: 0.003883928060531616, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.06547341495752335, ACC: 0.984375\n",
      "Validation iteration 500 loss: 0.020540375262498856, ACC: 0.984375\n",
      "-- Epoch 4 done -- Train loss: 0.036318933994043616, train ACC: 0.9896875, val loss: 0.03275234463857487, val ACC: 0.9878125\n",
      "<--- 19465.937071800232 seconds --->\n",
      "Training iteration 1 loss: 0.004058452323079109, ACC:1.0\n",
      "Training iteration 2 loss: 0.007226217538118362, ACC:1.0\n",
      "Training iteration 3 loss: 0.010824504308402538, ACC:1.0\n",
      "Training iteration 4 loss: 0.07551437616348267, ACC:0.96875\n",
      "Training iteration 5 loss: 0.11762493848800659, ACC:0.96875\n",
      "Training iteration 6 loss: 0.049390580505132675, ACC:0.984375\n",
      "Training iteration 7 loss: 0.04427779093384743, ACC:0.984375\n",
      "Training iteration 8 loss: 0.025461914017796516, ACC:0.984375\n",
      "Training iteration 9 loss: 0.022501425817608833, ACC:1.0\n",
      "Training iteration 10 loss: 0.04237953573465347, ACC:0.984375\n",
      "Training iteration 11 loss: 0.10285534709692001, ACC:0.984375\n",
      "Training iteration 12 loss: 0.03710462152957916, ACC:1.0\n",
      "Training iteration 13 loss: 0.1048206239938736, ACC:0.96875\n",
      "Training iteration 14 loss: 0.008487651124596596, ACC:1.0\n",
      "Training iteration 15 loss: 0.033093664795160294, ACC:0.984375\n",
      "Training iteration 16 loss: 0.036152224987745285, ACC:0.96875\n",
      "Training iteration 17 loss: 0.01784871146082878, ACC:0.984375\n",
      "Training iteration 18 loss: 0.023191245272755623, ACC:0.984375\n",
      "Training iteration 19 loss: 0.08852517604827881, ACC:0.921875\n",
      "Training iteration 20 loss: 0.007796930149197578, ACC:1.0\n",
      "Training iteration 21 loss: 0.028672166168689728, ACC:0.984375\n",
      "Training iteration 22 loss: 0.013195023871958256, ACC:1.0\n",
      "Training iteration 23 loss: 0.006013349164277315, ACC:1.0\n",
      "Training iteration 24 loss: 0.1391979455947876, ACC:0.984375\n",
      "Training iteration 25 loss: 0.015688424929976463, ACC:1.0\n",
      "Training iteration 26 loss: 0.0038961132522672415, ACC:1.0\n",
      "Training iteration 27 loss: 0.06407248973846436, ACC:0.96875\n",
      "Training iteration 28 loss: 0.004443199373781681, ACC:1.0\n",
      "Training iteration 29 loss: 0.17934872210025787, ACC:0.984375\n",
      "Training iteration 30 loss: 0.0032898865174502134, ACC:1.0\n",
      "Training iteration 31 loss: 0.004238354042172432, ACC:1.0\n",
      "Training iteration 32 loss: 0.04374239593744278, ACC:0.984375\n",
      "Training iteration 33 loss: 0.11636365205049515, ACC:0.96875\n",
      "Training iteration 34 loss: 0.07284823060035706, ACC:0.96875\n",
      "Training iteration 35 loss: 0.029258931055665016, ACC:0.984375\n",
      "Training iteration 36 loss: 0.055707793682813644, ACC:0.96875\n",
      "Training iteration 37 loss: 0.0019208190497010946, ACC:1.0\n",
      "Training iteration 38 loss: 0.0021842308342456818, ACC:1.0\n",
      "Training iteration 39 loss: 0.01153857633471489, ACC:1.0\n",
      "Training iteration 40 loss: 0.002424540463835001, ACC:1.0\n",
      "Training iteration 41 loss: 0.005037064664065838, ACC:1.0\n",
      "Training iteration 42 loss: 0.13302387297153473, ACC:0.96875\n",
      "Training iteration 43 loss: 0.0035250845830887556, ACC:1.0\n",
      "Training iteration 44 loss: 0.008568232879042625, ACC:1.0\n",
      "Training iteration 45 loss: 0.005423336289823055, ACC:1.0\n",
      "Training iteration 46 loss: 0.0050277188420295715, ACC:1.0\n",
      "Training iteration 47 loss: 0.005302143283188343, ACC:1.0\n",
      "Training iteration 48 loss: 0.009483492001891136, ACC:1.0\n",
      "Training iteration 49 loss: 0.04297785088419914, ACC:0.984375\n",
      "Training iteration 50 loss: 0.0637049451470375, ACC:0.96875\n",
      "Training iteration 51 loss: 0.01198663655668497, ACC:1.0\n",
      "Training iteration 52 loss: 0.01268102042376995, ACC:1.0\n",
      "Training iteration 53 loss: 0.11869361996650696, ACC:0.953125\n",
      "Training iteration 54 loss: 0.03644205257296562, ACC:0.984375\n",
      "Training iteration 55 loss: 0.011538021266460419, ACC:1.0\n",
      "Training iteration 56 loss: 0.031553205102682114, ACC:0.984375\n",
      "Training iteration 57 loss: 0.023174792528152466, ACC:0.984375\n",
      "Training iteration 58 loss: 0.06299270689487457, ACC:0.96875\n",
      "Training iteration 59 loss: 0.006789179984480143, ACC:1.0\n",
      "Training iteration 60 loss: 0.005211151670664549, ACC:1.0\n",
      "Training iteration 61 loss: 0.005787605419754982, ACC:1.0\n",
      "Training iteration 62 loss: 0.005252927076071501, ACC:1.0\n",
      "Training iteration 63 loss: 0.07454900443553925, ACC:0.984375\n",
      "Training iteration 64 loss: 0.03245106711983681, ACC:0.984375\n",
      "Training iteration 65 loss: 0.17584900557994843, ACC:0.984375\n",
      "Training iteration 66 loss: 0.06998196989297867, ACC:0.96875\n",
      "Training iteration 67 loss: 0.010080082342028618, ACC:1.0\n",
      "Training iteration 68 loss: 0.02234182506799698, ACC:0.984375\n",
      "Training iteration 69 loss: 0.02790888026356697, ACC:0.96875\n",
      "Training iteration 70 loss: 0.07798022776842117, ACC:0.984375\n",
      "Training iteration 71 loss: 0.08491384238004684, ACC:0.953125\n",
      "Training iteration 72 loss: 0.07546126842498779, ACC:0.96875\n",
      "Training iteration 73 loss: 0.012434465810656548, ACC:1.0\n",
      "Training iteration 74 loss: 0.06813269108533859, ACC:0.984375\n",
      "Training iteration 75 loss: 0.03846673667430878, ACC:0.984375\n",
      "Training iteration 76 loss: 0.03414863348007202, ACC:0.984375\n",
      "Training iteration 77 loss: 0.05116569250822067, ACC:0.96875\n",
      "Training iteration 78 loss: 0.015828179195523262, ACC:1.0\n",
      "Training iteration 79 loss: 0.036794211715459824, ACC:0.984375\n",
      "Training iteration 80 loss: 0.01242869719862938, ACC:1.0\n",
      "Training iteration 81 loss: 0.01626531034708023, ACC:1.0\n",
      "Training iteration 82 loss: 0.020801465958356857, ACC:1.0\n",
      "Training iteration 83 loss: 0.03488127142190933, ACC:0.984375\n",
      "Training iteration 84 loss: 0.0036499877460300922, ACC:1.0\n",
      "Training iteration 85 loss: 0.0040283119305968285, ACC:1.0\n",
      "Training iteration 86 loss: 0.02302544377744198, ACC:0.984375\n",
      "Training iteration 87 loss: 0.010807986371219158, ACC:1.0\n",
      "Training iteration 88 loss: 0.022917037829756737, ACC:0.984375\n",
      "Training iteration 89 loss: 0.01372436247766018, ACC:1.0\n",
      "Training iteration 90 loss: 0.002716564340516925, ACC:1.0\n",
      "Training iteration 91 loss: 0.002147387247532606, ACC:1.0\n",
      "Training iteration 92 loss: 0.0037040587048977613, ACC:1.0\n",
      "Training iteration 93 loss: 0.005113629624247551, ACC:1.0\n",
      "Training iteration 94 loss: 0.01605219393968582, ACC:0.984375\n",
      "Training iteration 95 loss: 0.0008931068005040288, ACC:1.0\n",
      "Training iteration 96 loss: 0.01825028285384178, ACC:1.0\n",
      "Training iteration 97 loss: 0.010703299194574356, ACC:1.0\n",
      "Training iteration 98 loss: 0.06595928221940994, ACC:0.96875\n",
      "Training iteration 99 loss: 0.045663777738809586, ACC:0.984375\n",
      "Training iteration 100 loss: 0.004947765730321407, ACC:1.0\n",
      "Training iteration 101 loss: 0.07036338001489639, ACC:0.96875\n",
      "Training iteration 102 loss: 0.0030737067572772503, ACC:1.0\n",
      "Training iteration 103 loss: 0.028314275667071342, ACC:0.984375\n",
      "Training iteration 104 loss: 0.000637067249044776, ACC:1.0\n",
      "Training iteration 105 loss: 0.00044809665996581316, ACC:1.0\n",
      "Training iteration 106 loss: 0.0005111235077492893, ACC:1.0\n",
      "Training iteration 107 loss: 0.0008274003630504012, ACC:1.0\n",
      "Training iteration 108 loss: 0.0003604032099246979, ACC:1.0\n",
      "Training iteration 109 loss: 0.04620171710848808, ACC:0.984375\n",
      "Training iteration 110 loss: 0.0018107787473127246, ACC:1.0\n",
      "Training iteration 111 loss: 0.02748032659292221, ACC:0.984375\n",
      "Training iteration 112 loss: 0.05644664168357849, ACC:0.96875\n",
      "Training iteration 113 loss: 0.0036405441351234913, ACC:1.0\n",
      "Training iteration 114 loss: 0.012099461629986763, ACC:1.0\n",
      "Training iteration 115 loss: 0.02921137399971485, ACC:0.984375\n",
      "Training iteration 116 loss: 0.006674665957689285, ACC:1.0\n",
      "Training iteration 117 loss: 0.003969254903495312, ACC:1.0\n",
      "Training iteration 118 loss: 0.000850990298204124, ACC:1.0\n",
      "Training iteration 119 loss: 0.007738190237432718, ACC:1.0\n",
      "Training iteration 120 loss: 0.04011831805109978, ACC:0.984375\n",
      "Training iteration 121 loss: 0.00282663363032043, ACC:1.0\n",
      "Training iteration 122 loss: 0.005152497440576553, ACC:1.0\n",
      "Training iteration 123 loss: 0.01043999195098877, ACC:1.0\n",
      "Training iteration 124 loss: 0.0019020074978470802, ACC:1.0\n",
      "Training iteration 125 loss: 0.0021330828312784433, ACC:1.0\n",
      "Training iteration 126 loss: 0.04245024546980858, ACC:0.96875\n",
      "Training iteration 127 loss: 0.007931063883006573, ACC:1.0\n",
      "Training iteration 128 loss: 0.011822111904621124, ACC:1.0\n",
      "Training iteration 129 loss: 0.0023527706507593393, ACC:1.0\n",
      "Training iteration 130 loss: 0.00350749958306551, ACC:1.0\n",
      "Training iteration 131 loss: 0.0017212678212672472, ACC:1.0\n",
      "Training iteration 132 loss: 0.16966727375984192, ACC:0.96875\n",
      "Training iteration 133 loss: 0.005055918823927641, ACC:1.0\n",
      "Training iteration 134 loss: 0.0010059208143502474, ACC:1.0\n",
      "Training iteration 135 loss: 0.12510527670383453, ACC:0.96875\n",
      "Training iteration 136 loss: 0.004048505797982216, ACC:1.0\n",
      "Training iteration 137 loss: 0.0023203627206385136, ACC:1.0\n",
      "Training iteration 138 loss: 0.008024785667657852, ACC:1.0\n",
      "Training iteration 139 loss: 0.052626438438892365, ACC:0.984375\n",
      "Training iteration 140 loss: 0.1542089283466339, ACC:0.921875\n",
      "Training iteration 141 loss: 0.10892142355442047, ACC:0.953125\n",
      "Training iteration 142 loss: 0.009067891165614128, ACC:1.0\n",
      "Training iteration 143 loss: 0.010648190043866634, ACC:1.0\n",
      "Training iteration 144 loss: 0.05611925572156906, ACC:0.96875\n",
      "Training iteration 145 loss: 0.011629952117800713, ACC:1.0\n",
      "Training iteration 146 loss: 0.06981377303600311, ACC:0.96875\n",
      "Training iteration 147 loss: 0.020930202677845955, ACC:1.0\n",
      "Training iteration 148 loss: 0.02152305841445923, ACC:1.0\n",
      "Training iteration 149 loss: 0.027847260236740112, ACC:1.0\n",
      "Training iteration 150 loss: 0.15393486618995667, ACC:0.953125\n",
      "Training iteration 151 loss: 0.08108944445848465, ACC:0.96875\n",
      "Training iteration 152 loss: 0.020831899717450142, ACC:0.984375\n",
      "Training iteration 153 loss: 0.009405761957168579, ACC:1.0\n",
      "Training iteration 154 loss: 0.11021699011325836, ACC:0.953125\n",
      "Training iteration 155 loss: 0.13888484239578247, ACC:0.96875\n",
      "Training iteration 156 loss: 0.23723995685577393, ACC:0.9375\n",
      "Training iteration 157 loss: 0.06241443008184433, ACC:0.984375\n",
      "Training iteration 158 loss: 0.09940654039382935, ACC:0.984375\n",
      "Training iteration 159 loss: 0.05140964686870575, ACC:1.0\n",
      "Training iteration 160 loss: 0.09998967498540878, ACC:0.96875\n",
      "Training iteration 161 loss: 0.08600562810897827, ACC:0.96875\n",
      "Training iteration 162 loss: 0.043889567255973816, ACC:0.984375\n",
      "Training iteration 163 loss: 0.005670415237545967, ACC:1.0\n",
      "Training iteration 164 loss: 0.042527638375759125, ACC:0.984375\n",
      "Training iteration 165 loss: 0.02818736433982849, ACC:0.984375\n",
      "Training iteration 166 loss: 0.006620513275265694, ACC:1.0\n",
      "Training iteration 167 loss: 0.059508711099624634, ACC:0.984375\n",
      "Training iteration 168 loss: 0.0342482328414917, ACC:0.96875\n",
      "Training iteration 169 loss: 0.013475140556693077, ACC:1.0\n",
      "Training iteration 170 loss: 0.013728640973567963, ACC:1.0\n",
      "Training iteration 171 loss: 0.016502732411026955, ACC:0.984375\n",
      "Training iteration 172 loss: 0.0195381511002779, ACC:1.0\n",
      "Training iteration 173 loss: 0.01784661039710045, ACC:0.984375\n",
      "Training iteration 174 loss: 0.0015929046785458922, ACC:1.0\n",
      "Training iteration 175 loss: 0.08286604285240173, ACC:0.984375\n",
      "Training iteration 176 loss: 0.0012238830095157027, ACC:1.0\n",
      "Training iteration 177 loss: 0.003352469066157937, ACC:1.0\n",
      "Training iteration 178 loss: 0.1271674931049347, ACC:0.96875\n",
      "Training iteration 179 loss: 0.023072220385074615, ACC:0.984375\n",
      "Training iteration 180 loss: 0.2940128743648529, ACC:0.96875\n",
      "Training iteration 181 loss: 0.032444801181554794, ACC:0.984375\n",
      "Training iteration 182 loss: 0.022078411653637886, ACC:0.984375\n",
      "Training iteration 183 loss: 0.028576385229825974, ACC:0.96875\n",
      "Training iteration 184 loss: 0.02340906485915184, ACC:0.984375\n",
      "Training iteration 185 loss: 0.008924187161028385, ACC:1.0\n",
      "Training iteration 186 loss: 0.0021840373519808054, ACC:1.0\n",
      "Training iteration 187 loss: 0.017552673816680908, ACC:0.984375\n",
      "Training iteration 188 loss: 0.01948080025613308, ACC:0.984375\n",
      "Training iteration 189 loss: 0.00636095879599452, ACC:1.0\n",
      "Training iteration 190 loss: 0.006921693217009306, ACC:1.0\n",
      "Training iteration 191 loss: 0.009440906345844269, ACC:1.0\n",
      "Training iteration 192 loss: 0.02501196600496769, ACC:0.984375\n",
      "Training iteration 193 loss: 0.018527060747146606, ACC:1.0\n",
      "Training iteration 194 loss: 0.012363491579890251, ACC:1.0\n",
      "Training iteration 195 loss: 0.006718906108289957, ACC:1.0\n",
      "Training iteration 196 loss: 0.0019000291358679533, ACC:1.0\n",
      "Training iteration 197 loss: 0.006071630399674177, ACC:1.0\n",
      "Training iteration 198 loss: 0.02491336315870285, ACC:0.984375\n",
      "Training iteration 199 loss: 0.0062303743325173855, ACC:1.0\n",
      "Training iteration 200 loss: 0.0007511127623729408, ACC:1.0\n",
      "Training iteration 201 loss: 0.002978414297103882, ACC:1.0\n",
      "Training iteration 202 loss: 0.027356412261724472, ACC:0.984375\n",
      "Training iteration 203 loss: 0.001168314483948052, ACC:1.0\n",
      "Training iteration 204 loss: 0.01730598881840706, ACC:0.984375\n",
      "Training iteration 205 loss: 0.006303080357611179, ACC:1.0\n",
      "Training iteration 206 loss: 0.04247520491480827, ACC:0.984375\n",
      "Training iteration 207 loss: 0.0026303520426154137, ACC:1.0\n",
      "Training iteration 208 loss: 0.02663325145840645, ACC:0.984375\n",
      "Training iteration 209 loss: 0.006214629393070936, ACC:1.0\n",
      "Training iteration 210 loss: 0.004946449771523476, ACC:1.0\n",
      "Training iteration 211 loss: 0.013622622936964035, ACC:1.0\n",
      "Training iteration 212 loss: 0.01807388663291931, ACC:0.984375\n",
      "Training iteration 213 loss: 0.03096114844083786, ACC:0.984375\n",
      "Training iteration 214 loss: 0.21237483620643616, ACC:0.984375\n",
      "Training iteration 215 loss: 0.006127519998699427, ACC:1.0\n",
      "Training iteration 216 loss: 0.0033826211001724005, ACC:1.0\n",
      "Training iteration 217 loss: 0.005983227398246527, ACC:1.0\n",
      "Training iteration 218 loss: 0.001163366949185729, ACC:1.0\n",
      "Training iteration 219 loss: 0.0007872235728427768, ACC:1.0\n",
      "Training iteration 220 loss: 0.045009322464466095, ACC:0.984375\n",
      "Training iteration 221 loss: 0.011236420832574368, ACC:1.0\n",
      "Training iteration 222 loss: 0.022965051233768463, ACC:0.984375\n",
      "Training iteration 223 loss: 0.09253588318824768, ACC:0.96875\n",
      "Training iteration 224 loss: 0.001792266732081771, ACC:1.0\n",
      "Training iteration 225 loss: 0.01377071999013424, ACC:1.0\n",
      "Training iteration 226 loss: 0.0696708932518959, ACC:0.984375\n",
      "Training iteration 227 loss: 0.024790212512016296, ACC:0.984375\n",
      "Training iteration 228 loss: 0.1522919237613678, ACC:0.96875\n",
      "Training iteration 229 loss: 0.048520270735025406, ACC:0.984375\n",
      "Training iteration 230 loss: 0.010127429850399494, ACC:1.0\n",
      "Training iteration 231 loss: 0.010616508312523365, ACC:1.0\n",
      "Training iteration 232 loss: 0.006542670540511608, ACC:1.0\n",
      "Training iteration 233 loss: 0.1033778265118599, ACC:0.96875\n",
      "Training iteration 234 loss: 0.014327584765851498, ACC:0.984375\n",
      "Training iteration 235 loss: 0.11502982676029205, ACC:0.953125\n",
      "Training iteration 236 loss: 0.0058614639565348625, ACC:1.0\n",
      "Training iteration 237 loss: 0.009664086624979973, ACC:1.0\n",
      "Training iteration 238 loss: 0.031077228486537933, ACC:1.0\n",
      "Training iteration 239 loss: 0.05042487010359764, ACC:0.96875\n",
      "Training iteration 240 loss: 0.05368291586637497, ACC:0.984375\n",
      "Training iteration 241 loss: 0.03972718119621277, ACC:0.984375\n",
      "Training iteration 242 loss: 0.014245586469769478, ACC:1.0\n",
      "Training iteration 243 loss: 0.04253121092915535, ACC:0.984375\n",
      "Training iteration 244 loss: 0.0033188650850206614, ACC:1.0\n",
      "Training iteration 245 loss: 0.03778286278247833, ACC:0.96875\n",
      "Training iteration 246 loss: 0.10757346451282501, ACC:0.953125\n",
      "Training iteration 247 loss: 0.004633763339370489, ACC:1.0\n",
      "Training iteration 248 loss: 0.033360667526721954, ACC:0.984375\n",
      "Training iteration 249 loss: 0.0011187188792973757, ACC:1.0\n",
      "Training iteration 250 loss: 0.00244677672162652, ACC:1.0\n",
      "Training iteration 251 loss: 0.25205209851264954, ACC:0.984375\n",
      "Training iteration 252 loss: 0.06729674339294434, ACC:0.984375\n",
      "Training iteration 253 loss: 0.015393725596368313, ACC:0.984375\n",
      "Training iteration 254 loss: 0.002565242350101471, ACC:1.0\n",
      "Training iteration 255 loss: 0.06105649843811989, ACC:0.984375\n",
      "Training iteration 256 loss: 0.28375566005706787, ACC:0.96875\n",
      "Training iteration 257 loss: 0.04344436526298523, ACC:0.984375\n",
      "Training iteration 258 loss: 0.0016366075724363327, ACC:1.0\n",
      "Training iteration 259 loss: 0.15085573494434357, ACC:0.96875\n",
      "Training iteration 260 loss: 0.0018971296958625317, ACC:1.0\n",
      "Training iteration 261 loss: 0.01316650491207838, ACC:0.984375\n",
      "Training iteration 262 loss: 0.10514520108699799, ACC:0.984375\n",
      "Training iteration 263 loss: 0.00210557016544044, ACC:1.0\n",
      "Training iteration 264 loss: 0.13186292350292206, ACC:0.96875\n",
      "Training iteration 265 loss: 0.003158208914101124, ACC:1.0\n",
      "Training iteration 266 loss: 0.0056128473952412605, ACC:1.0\n",
      "Training iteration 267 loss: 0.007184293586760759, ACC:1.0\n",
      "Training iteration 268 loss: 0.028281323611736298, ACC:0.984375\n",
      "Training iteration 269 loss: 0.007098276633769274, ACC:1.0\n",
      "Training iteration 270 loss: 0.0022928158286958933, ACC:1.0\n",
      "Training iteration 271 loss: 0.032815154641866684, ACC:0.984375\n",
      "Training iteration 272 loss: 0.01426663901656866, ACC:1.0\n",
      "Training iteration 273 loss: 0.038314007222652435, ACC:0.984375\n",
      "Training iteration 274 loss: 0.008172834292054176, ACC:1.0\n",
      "Training iteration 275 loss: 0.012141849845647812, ACC:1.0\n",
      "Training iteration 276 loss: 0.015091149136424065, ACC:1.0\n",
      "Training iteration 277 loss: 0.023805493488907814, ACC:1.0\n",
      "Training iteration 278 loss: 0.005590776912868023, ACC:1.0\n",
      "Training iteration 279 loss: 0.06115306541323662, ACC:0.984375\n",
      "Training iteration 280 loss: 0.019940650090575218, ACC:0.984375\n",
      "Training iteration 281 loss: 0.006503741722553968, ACC:1.0\n",
      "Training iteration 282 loss: 0.021645206958055496, ACC:0.984375\n",
      "Training iteration 283 loss: 0.0012375724036246538, ACC:1.0\n",
      "Training iteration 284 loss: 0.0011630887165665627, ACC:1.0\n",
      "Training iteration 285 loss: 0.011138640344142914, ACC:1.0\n",
      "Training iteration 286 loss: 0.0006273437174968421, ACC:1.0\n",
      "Training iteration 287 loss: 0.07794592529535294, ACC:0.984375\n",
      "Training iteration 288 loss: 0.024534298107028008, ACC:0.96875\n",
      "Training iteration 289 loss: 0.06011364981532097, ACC:0.984375\n",
      "Training iteration 290 loss: 0.0030131249222904444, ACC:1.0\n",
      "Training iteration 291 loss: 0.06385038048028946, ACC:0.984375\n",
      "Training iteration 292 loss: 0.12272223830223083, ACC:0.984375\n",
      "Training iteration 293 loss: 0.0016690200427547097, ACC:1.0\n",
      "Training iteration 294 loss: 0.08247741311788559, ACC:0.984375\n",
      "Training iteration 295 loss: 0.0016872440464794636, ACC:1.0\n",
      "Training iteration 296 loss: 0.001684533548541367, ACC:1.0\n",
      "Training iteration 297 loss: 0.016579966992139816, ACC:0.984375\n",
      "Training iteration 298 loss: 0.07500091940164566, ACC:0.984375\n",
      "Training iteration 299 loss: 0.007693255785852671, ACC:1.0\n",
      "Training iteration 300 loss: 0.0010606124997138977, ACC:1.0\n",
      "Training iteration 301 loss: 0.0028428195510059595, ACC:1.0\n",
      "Training iteration 302 loss: 0.02141045592725277, ACC:0.984375\n",
      "Training iteration 303 loss: 0.004535023123025894, ACC:1.0\n",
      "Training iteration 304 loss: 0.013669049367308617, ACC:1.0\n",
      "Training iteration 305 loss: 0.002853481797501445, ACC:1.0\n",
      "Training iteration 306 loss: 0.021265534684062004, ACC:0.984375\n",
      "Training iteration 307 loss: 0.010201424360275269, ACC:1.0\n",
      "Training iteration 308 loss: 0.0029448072891682386, ACC:1.0\n",
      "Training iteration 309 loss: 0.0036254078149795532, ACC:1.0\n",
      "Training iteration 310 loss: 0.041653066873550415, ACC:0.984375\n",
      "Training iteration 311 loss: 0.0034743568394333124, ACC:1.0\n",
      "Training iteration 312 loss: 0.04911341518163681, ACC:0.984375\n",
      "Training iteration 313 loss: 0.035567231476306915, ACC:0.984375\n",
      "Training iteration 314 loss: 0.001871318556368351, ACC:1.0\n",
      "Training iteration 315 loss: 0.004082805942744017, ACC:1.0\n",
      "Training iteration 316 loss: 0.001347213052213192, ACC:1.0\n",
      "Training iteration 317 loss: 0.02901759371161461, ACC:0.984375\n",
      "Training iteration 318 loss: 0.0010533155873417854, ACC:1.0\n",
      "Training iteration 319 loss: 0.041962798684835434, ACC:0.984375\n",
      "Training iteration 320 loss: 0.0019215195206925273, ACC:1.0\n",
      "Training iteration 321 loss: 0.023888006806373596, ACC:0.984375\n",
      "Training iteration 322 loss: 0.037004224956035614, ACC:0.984375\n",
      "Training iteration 323 loss: 0.12951898574829102, ACC:0.984375\n",
      "Training iteration 324 loss: 0.0008396821795031428, ACC:1.0\n",
      "Training iteration 325 loss: 0.0006097692530602217, ACC:1.0\n",
      "Training iteration 326 loss: 0.0046569062396883965, ACC:1.0\n",
      "Training iteration 327 loss: 0.32296621799468994, ACC:0.921875\n",
      "Training iteration 328 loss: 0.0016134586185216904, ACC:1.0\n",
      "Training iteration 329 loss: 0.002494804561138153, ACC:1.0\n",
      "Training iteration 330 loss: 0.09065555036067963, ACC:0.984375\n",
      "Training iteration 331 loss: 0.0882081687450409, ACC:0.984375\n",
      "Training iteration 332 loss: 0.004141384735703468, ACC:1.0\n",
      "Training iteration 333 loss: 0.024577580392360687, ACC:0.984375\n",
      "Training iteration 334 loss: 0.008684518747031689, ACC:1.0\n",
      "Training iteration 335 loss: 0.062149934470653534, ACC:0.984375\n",
      "Training iteration 336 loss: 0.02701013907790184, ACC:1.0\n",
      "Training iteration 337 loss: 0.010460891760885715, ACC:1.0\n",
      "Training iteration 338 loss: 0.028707489371299744, ACC:1.0\n",
      "Training iteration 339 loss: 0.0256437286734581, ACC:1.0\n",
      "Training iteration 340 loss: 0.006873297039419413, ACC:1.0\n",
      "Training iteration 341 loss: 0.011971460655331612, ACC:1.0\n",
      "Training iteration 342 loss: 0.015064781531691551, ACC:1.0\n",
      "Training iteration 343 loss: 0.006466127000749111, ACC:1.0\n",
      "Training iteration 344 loss: 0.016956184059381485, ACC:1.0\n",
      "Training iteration 345 loss: 0.03676225617527962, ACC:0.984375\n",
      "Training iteration 346 loss: 0.0019217680674046278, ACC:1.0\n",
      "Training iteration 347 loss: 0.0058953408151865005, ACC:1.0\n",
      "Training iteration 348 loss: 0.016770608723163605, ACC:0.984375\n",
      "Training iteration 349 loss: 0.0009156232117675245, ACC:1.0\n",
      "Training iteration 350 loss: 0.0016834744019433856, ACC:1.0\n",
      "Training iteration 351 loss: 0.03994075581431389, ACC:0.96875\n",
      "Training iteration 352 loss: 0.0005415586638264358, ACC:1.0\n",
      "Training iteration 353 loss: 0.03568967804312706, ACC:0.984375\n",
      "Training iteration 354 loss: 0.006115392316132784, ACC:1.0\n",
      "Training iteration 355 loss: 0.0005663458141498268, ACC:1.0\n",
      "Training iteration 356 loss: 0.0006362740532495081, ACC:1.0\n",
      "Training iteration 357 loss: 0.050865042954683304, ACC:0.984375\n",
      "Training iteration 358 loss: 0.0024036760441958904, ACC:1.0\n",
      "Training iteration 359 loss: 0.0012004680465906858, ACC:1.0\n",
      "Training iteration 360 loss: 0.052340906113386154, ACC:0.984375\n",
      "Training iteration 361 loss: 0.0010548800928518176, ACC:1.0\n",
      "Training iteration 362 loss: 0.0020499355159699917, ACC:1.0\n",
      "Training iteration 363 loss: 0.011497484520077705, ACC:1.0\n",
      "Training iteration 364 loss: 0.00754206907004118, ACC:1.0\n",
      "Training iteration 365 loss: 0.009968557395040989, ACC:1.0\n",
      "Training iteration 366 loss: 0.02094394341111183, ACC:0.984375\n",
      "Training iteration 367 loss: 0.004379771184176207, ACC:1.0\n",
      "Training iteration 368 loss: 0.0013827166985720396, ACC:1.0\n",
      "Training iteration 369 loss: 0.0018020498100668192, ACC:1.0\n",
      "Training iteration 370 loss: 0.05035221576690674, ACC:0.984375\n",
      "Training iteration 371 loss: 0.07942690700292587, ACC:0.96875\n",
      "Training iteration 372 loss: 0.002713767345994711, ACC:1.0\n",
      "Training iteration 373 loss: 0.02412739023566246, ACC:0.984375\n",
      "Training iteration 374 loss: 0.02579127997159958, ACC:0.984375\n",
      "Training iteration 375 loss: 0.0043605417013168335, ACC:1.0\n",
      "Training iteration 376 loss: 0.0033419709652662277, ACC:1.0\n",
      "Training iteration 377 loss: 0.0024947524070739746, ACC:1.0\n",
      "Training iteration 378 loss: 0.004706549923866987, ACC:1.0\n",
      "Training iteration 379 loss: 0.008281556889414787, ACC:1.0\n",
      "Training iteration 380 loss: 0.0038722166791558266, ACC:1.0\n",
      "Training iteration 381 loss: 0.005760778672993183, ACC:1.0\n",
      "Training iteration 382 loss: 0.06573375314474106, ACC:0.984375\n",
      "Training iteration 383 loss: 0.0795760378241539, ACC:0.984375\n",
      "Training iteration 384 loss: 0.0033703106455504894, ACC:1.0\n",
      "Training iteration 385 loss: 0.007141596637666225, ACC:1.0\n",
      "Training iteration 386 loss: 0.00727332616224885, ACC:1.0\n",
      "Training iteration 387 loss: 0.004666964989155531, ACC:1.0\n",
      "Training iteration 388 loss: 0.0038903297390788794, ACC:1.0\n",
      "Training iteration 389 loss: 0.010952110402286053, ACC:1.0\n",
      "Training iteration 390 loss: 0.008640484884381294, ACC:1.0\n",
      "Training iteration 391 loss: 0.023068835958838463, ACC:1.0\n",
      "Training iteration 392 loss: 0.09944372624158859, ACC:0.96875\n",
      "Training iteration 393 loss: 0.0014313000719994307, ACC:1.0\n",
      "Training iteration 394 loss: 0.10379938036203384, ACC:0.984375\n",
      "Training iteration 395 loss: 0.04138578474521637, ACC:0.96875\n",
      "Training iteration 396 loss: 0.03833527863025665, ACC:0.984375\n",
      "Training iteration 397 loss: 0.09248318523168564, ACC:0.96875\n",
      "Training iteration 398 loss: 0.036247629672288895, ACC:0.96875\n",
      "Training iteration 399 loss: 0.004938420839607716, ACC:1.0\n",
      "Training iteration 400 loss: 0.00472263153642416, ACC:1.0\n",
      "Training iteration 401 loss: 0.16615262627601624, ACC:0.96875\n",
      "Training iteration 402 loss: 0.004569554701447487, ACC:1.0\n",
      "Training iteration 403 loss: 0.006505412980914116, ACC:1.0\n",
      "Training iteration 404 loss: 0.07131105661392212, ACC:0.984375\n",
      "Training iteration 405 loss: 0.0031833057291805744, ACC:1.0\n",
      "Training iteration 406 loss: 0.03320644795894623, ACC:0.984375\n",
      "Training iteration 407 loss: 0.006437121890485287, ACC:1.0\n",
      "Training iteration 408 loss: 0.010944017209112644, ACC:1.0\n",
      "Training iteration 409 loss: 0.025707552209496498, ACC:0.984375\n",
      "Training iteration 410 loss: 0.007704318501055241, ACC:1.0\n",
      "Training iteration 411 loss: 0.0071950978599488735, ACC:1.0\n",
      "Training iteration 412 loss: 0.009587951004505157, ACC:1.0\n",
      "Training iteration 413 loss: 0.005730756092816591, ACC:1.0\n",
      "Training iteration 414 loss: 0.005931563675403595, ACC:1.0\n",
      "Training iteration 415 loss: 0.010268425568938255, ACC:1.0\n",
      "Training iteration 416 loss: 0.03104012832045555, ACC:1.0\n",
      "Training iteration 417 loss: 0.014449095353484154, ACC:1.0\n",
      "Training iteration 418 loss: 0.07083532959222794, ACC:0.984375\n",
      "Training iteration 419 loss: 0.025691188871860504, ACC:0.984375\n",
      "Training iteration 420 loss: 0.004616447724401951, ACC:1.0\n",
      "Training iteration 421 loss: 0.001938490429893136, ACC:1.0\n",
      "Training iteration 422 loss: 0.0015549712115898728, ACC:1.0\n",
      "Training iteration 423 loss: 0.002174332970753312, ACC:1.0\n",
      "Training iteration 424 loss: 0.0027794751804322004, ACC:1.0\n",
      "Training iteration 425 loss: 0.001613655942492187, ACC:1.0\n",
      "Training iteration 426 loss: 0.0012994525022804737, ACC:1.0\n",
      "Training iteration 427 loss: 0.008668294176459312, ACC:1.0\n",
      "Training iteration 428 loss: 0.010575921274721622, ACC:1.0\n",
      "Training iteration 429 loss: 0.050717584788799286, ACC:0.984375\n",
      "Training iteration 430 loss: 0.0061980197206139565, ACC:1.0\n",
      "Training iteration 431 loss: 0.0010900363558903337, ACC:1.0\n",
      "Training iteration 432 loss: 0.006866153795272112, ACC:1.0\n",
      "Training iteration 433 loss: 0.006779999006539583, ACC:1.0\n",
      "Training iteration 434 loss: 0.028706185519695282, ACC:0.984375\n",
      "Training iteration 435 loss: 0.07547206431627274, ACC:0.984375\n",
      "Training iteration 436 loss: 0.00630957493558526, ACC:1.0\n",
      "Training iteration 437 loss: 0.0015300160739570856, ACC:1.0\n",
      "Training iteration 438 loss: 0.0012902469607070088, ACC:1.0\n",
      "Training iteration 439 loss: 0.001624839729629457, ACC:1.0\n",
      "Training iteration 440 loss: 0.0032085725106298923, ACC:1.0\n",
      "Training iteration 441 loss: 0.001982893096283078, ACC:1.0\n",
      "Training iteration 442 loss: 0.0012478113640099764, ACC:1.0\n",
      "Training iteration 443 loss: 0.021906036883592606, ACC:0.984375\n",
      "Training iteration 444 loss: 0.20890876650810242, ACC:0.984375\n",
      "Training iteration 445 loss: 0.01671745255589485, ACC:0.984375\n",
      "Training iteration 446 loss: 0.005455224309116602, ACC:1.0\n",
      "Training iteration 447 loss: 0.10644885897636414, ACC:0.984375\n",
      "Training iteration 448 loss: 0.0758572518825531, ACC:0.984375\n",
      "Training iteration 449 loss: 0.00513745890930295, ACC:1.0\n",
      "Training iteration 450 loss: 0.002262698719277978, ACC:1.0\n",
      "Validation iteration 451 loss: 0.0016587042482569814, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.13569523394107819, ACC: 0.984375\n",
      "Validation iteration 453 loss: 0.012140031903982162, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.00108239334076643, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.1749674677848816, ACC: 0.96875\n",
      "Validation iteration 456 loss: 0.08198435604572296, ACC: 0.984375\n",
      "Validation iteration 457 loss: 0.003731076605618, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.0038219098933041096, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.03093721903860569, ACC: 0.984375\n",
      "Validation iteration 460 loss: 0.0070576476864516735, ACC: 1.0\n",
      "Validation iteration 461 loss: 0.0014215890550985932, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.0556013248860836, ACC: 0.984375\n",
      "Validation iteration 463 loss: 0.0010667401365935802, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.0007493725861422718, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.0027397789526730776, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.08700619637966156, ACC: 0.96875\n",
      "Validation iteration 467 loss: 0.12842878699302673, ACC: 0.96875\n",
      "Validation iteration 468 loss: 0.0740068331360817, ACC: 0.96875\n",
      "Validation iteration 469 loss: 0.017491323873400688, ACC: 0.984375\n",
      "Validation iteration 470 loss: 0.060146719217300415, ACC: 0.984375\n",
      "Validation iteration 471 loss: 0.002324030501767993, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.0010947129921987653, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.07338370382785797, ACC: 0.96875\n",
      "Validation iteration 474 loss: 0.03056412935256958, ACC: 0.984375\n",
      "Validation iteration 475 loss: 0.005137507803738117, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.0010535514447838068, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.014781741425395012, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.006211022846400738, ACC: 1.0\n",
      "Validation iteration 479 loss: 0.0008359693456441164, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.0055633122101426125, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.03466279059648514, ACC: 0.984375\n",
      "Validation iteration 482 loss: 0.14531871676445007, ACC: 0.96875\n",
      "Validation iteration 483 loss: 0.0016430664109066129, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.07245969027280807, ACC: 0.96875\n",
      "Validation iteration 485 loss: 0.015423704870045185, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.0025470389518886805, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.0008576218388043344, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.06098911911249161, ACC: 0.96875\n",
      "Validation iteration 489 loss: 0.0009354287758469582, ACC: 1.0\n",
      "Validation iteration 490 loss: 0.007891133427619934, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.0022593485191464424, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.04212896153330803, ACC: 0.984375\n",
      "Validation iteration 493 loss: 0.09887818992137909, ACC: 0.96875\n",
      "Validation iteration 494 loss: 0.02018510177731514, ACC: 0.984375\n",
      "Validation iteration 495 loss: 0.024666201323270798, ACC: 0.984375\n",
      "Validation iteration 496 loss: 0.006598222069442272, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.0009449956123717129, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.0010576143395155668, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.00197493564337492, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.018343618139624596, ACC: 0.984375\n",
      "-- Epoch 5 done -- Train loss: 0.03174676166730933, train ACC: 0.9907638888888889, val loss: 0.03164899774710648, val ACC: 0.990625\n",
      "<--- 19763.800918102264 seconds --->\n",
      "Training iteration 1 loss: 0.10596027225255966, ACC:0.984375\n",
      "Training iteration 2 loss: 0.0022369208745658398, ACC:1.0\n",
      "Training iteration 3 loss: 0.0025573698803782463, ACC:1.0\n",
      "Training iteration 4 loss: 0.013274090364575386, ACC:0.984375\n",
      "Training iteration 5 loss: 0.01107647456228733, ACC:1.0\n",
      "Training iteration 6 loss: 0.005786273628473282, ACC:1.0\n",
      "Training iteration 7 loss: 0.02005535364151001, ACC:1.0\n",
      "Training iteration 8 loss: 0.00600126339122653, ACC:1.0\n",
      "Training iteration 9 loss: 0.0052603245712816715, ACC:1.0\n",
      "Training iteration 10 loss: 0.003443994792178273, ACC:1.0\n",
      "Training iteration 11 loss: 0.012876112014055252, ACC:1.0\n",
      "Training iteration 12 loss: 0.007295199669897556, ACC:1.0\n",
      "Training iteration 13 loss: 0.055207110941410065, ACC:0.984375\n",
      "Training iteration 14 loss: 0.03540641814470291, ACC:0.984375\n",
      "Training iteration 15 loss: 0.05530478432774544, ACC:0.984375\n",
      "Training iteration 16 loss: 0.007595429662615061, ACC:1.0\n",
      "Training iteration 17 loss: 0.00970249529927969, ACC:1.0\n",
      "Training iteration 18 loss: 0.0906679555773735, ACC:0.96875\n",
      "Training iteration 19 loss: 0.044985316693782806, ACC:0.984375\n",
      "Training iteration 20 loss: 0.006146223284304142, ACC:1.0\n",
      "Training iteration 21 loss: 0.060795653611421585, ACC:0.96875\n",
      "Training iteration 22 loss: 0.0086682653054595, ACC:1.0\n",
      "Training iteration 23 loss: 0.026469450443983078, ACC:0.984375\n",
      "Training iteration 24 loss: 0.016367517411708832, ACC:1.0\n",
      "Training iteration 25 loss: 0.018657146021723747, ACC:1.0\n",
      "Training iteration 26 loss: 0.021050814539194107, ACC:1.0\n",
      "Training iteration 27 loss: 0.04126862436532974, ACC:0.984375\n",
      "Training iteration 28 loss: 0.0300618726760149, ACC:1.0\n",
      "Training iteration 29 loss: 0.07509225606918335, ACC:0.96875\n",
      "Training iteration 30 loss: 0.022770214825868607, ACC:0.984375\n",
      "Training iteration 31 loss: 0.010985387489199638, ACC:1.0\n",
      "Training iteration 32 loss: 0.012883026152849197, ACC:1.0\n",
      "Training iteration 33 loss: 0.011805719695985317, ACC:1.0\n",
      "Training iteration 34 loss: 0.023439699783921242, ACC:1.0\n",
      "Training iteration 35 loss: 0.04328285902738571, ACC:0.984375\n",
      "Training iteration 36 loss: 0.016085045412182808, ACC:1.0\n",
      "Training iteration 37 loss: 0.008424853906035423, ACC:1.0\n",
      "Training iteration 38 loss: 0.004600848071277142, ACC:1.0\n",
      "Training iteration 39 loss: 0.005760926753282547, ACC:1.0\n",
      "Training iteration 40 loss: 0.004722737707197666, ACC:1.0\n",
      "Training iteration 41 loss: 0.04818594083189964, ACC:0.984375\n",
      "Training iteration 42 loss: 0.004370612557977438, ACC:1.0\n",
      "Training iteration 43 loss: 0.013019593432545662, ACC:0.984375\n",
      "Training iteration 44 loss: 0.019850073382258415, ACC:0.984375\n",
      "Training iteration 45 loss: 0.08482237905263901, ACC:0.984375\n",
      "Training iteration 46 loss: 0.0005786736728623509, ACC:1.0\n",
      "Training iteration 47 loss: 0.015545356087386608, ACC:0.984375\n",
      "Training iteration 48 loss: 0.007619949523359537, ACC:1.0\n",
      "Training iteration 49 loss: 0.043167226016521454, ACC:0.953125\n",
      "Training iteration 50 loss: 0.0016376757994294167, ACC:1.0\n",
      "Training iteration 51 loss: 0.0005092265200801194, ACC:1.0\n",
      "Training iteration 52 loss: 0.0002318346523679793, ACC:1.0\n",
      "Training iteration 53 loss: 0.0786038413643837, ACC:0.984375\n",
      "Training iteration 54 loss: 0.0022626202553510666, ACC:1.0\n",
      "Training iteration 55 loss: 0.001345824683085084, ACC:1.0\n",
      "Training iteration 56 loss: 0.025308329612016678, ACC:0.984375\n",
      "Training iteration 57 loss: 0.11951952427625656, ACC:0.953125\n",
      "Training iteration 58 loss: 0.0007469411939382553, ACC:1.0\n",
      "Training iteration 59 loss: 0.00373472785577178, ACC:1.0\n",
      "Training iteration 60 loss: 0.0019389549270272255, ACC:1.0\n",
      "Training iteration 61 loss: 0.06142633035778999, ACC:0.984375\n",
      "Training iteration 62 loss: 0.04515952616930008, ACC:0.96875\n",
      "Training iteration 63 loss: 0.026153521612286568, ACC:0.984375\n",
      "Training iteration 64 loss: 0.05165904760360718, ACC:0.984375\n",
      "Training iteration 65 loss: 0.01259255688637495, ACC:1.0\n",
      "Training iteration 66 loss: 0.030505048111081123, ACC:0.984375\n",
      "Training iteration 67 loss: 0.012879819609224796, ACC:1.0\n",
      "Training iteration 68 loss: 0.006725354585796595, ACC:1.0\n",
      "Training iteration 69 loss: 0.02051067352294922, ACC:1.0\n",
      "Training iteration 70 loss: 0.04668224975466728, ACC:0.96875\n",
      "Training iteration 71 loss: 0.03329015523195267, ACC:0.984375\n",
      "Training iteration 72 loss: 0.010950712487101555, ACC:1.0\n",
      "Training iteration 73 loss: 0.0029870974831283092, ACC:1.0\n",
      "Training iteration 74 loss: 0.07951420545578003, ACC:0.96875\n",
      "Training iteration 75 loss: 0.003894075285643339, ACC:1.0\n",
      "Training iteration 76 loss: 0.04292599484324455, ACC:0.984375\n",
      "Training iteration 77 loss: 0.02727988362312317, ACC:0.984375\n",
      "Training iteration 78 loss: 0.003917010501027107, ACC:1.0\n",
      "Training iteration 79 loss: 0.08563781529664993, ACC:0.96875\n",
      "Training iteration 80 loss: 0.026836471632122993, ACC:0.984375\n",
      "Training iteration 81 loss: 0.032056957483291626, ACC:0.984375\n",
      "Training iteration 82 loss: 0.009061532095074654, ACC:1.0\n",
      "Training iteration 83 loss: 0.04767690226435661, ACC:0.984375\n",
      "Training iteration 84 loss: 0.004069444723427296, ACC:1.0\n",
      "Training iteration 85 loss: 0.0074650803580880165, ACC:1.0\n",
      "Training iteration 86 loss: 0.04851025715470314, ACC:0.984375\n",
      "Training iteration 87 loss: 0.0038370429538190365, ACC:1.0\n",
      "Training iteration 88 loss: 0.0479154996573925, ACC:0.984375\n",
      "Training iteration 89 loss: 0.008464667946100235, ACC:1.0\n",
      "Training iteration 90 loss: 0.0025887470692396164, ACC:1.0\n",
      "Training iteration 91 loss: 0.001131739467382431, ACC:1.0\n",
      "Training iteration 92 loss: 0.001352100633084774, ACC:1.0\n",
      "Training iteration 93 loss: 0.008655541576445103, ACC:1.0\n",
      "Training iteration 94 loss: 0.010248507373034954, ACC:1.0\n",
      "Training iteration 95 loss: 0.0564429946243763, ACC:0.984375\n",
      "Training iteration 96 loss: 0.03562198951840401, ACC:0.984375\n",
      "Training iteration 97 loss: 0.0008659226004965603, ACC:1.0\n",
      "Training iteration 98 loss: 0.0017743675271049142, ACC:1.0\n",
      "Training iteration 99 loss: 0.0018153937999159098, ACC:1.0\n",
      "Training iteration 100 loss: 0.003801539074629545, ACC:1.0\n",
      "Training iteration 101 loss: 0.011295012198388577, ACC:1.0\n",
      "Training iteration 102 loss: 0.0015133984852582216, ACC:1.0\n",
      "Training iteration 103 loss: 0.005361189600080252, ACC:1.0\n",
      "Training iteration 104 loss: 0.0016188722802326083, ACC:1.0\n",
      "Training iteration 105 loss: 0.021136725321412086, ACC:0.984375\n",
      "Training iteration 106 loss: 0.002656239317730069, ACC:1.0\n",
      "Training iteration 107 loss: 0.09706754982471466, ACC:0.96875\n",
      "Training iteration 108 loss: 0.03653290495276451, ACC:0.984375\n",
      "Training iteration 109 loss: 0.0027263702359050512, ACC:1.0\n",
      "Training iteration 110 loss: 0.007524517830461264, ACC:1.0\n",
      "Training iteration 111 loss: 0.012978053651750088, ACC:1.0\n",
      "Training iteration 112 loss: 0.02150810696184635, ACC:1.0\n",
      "Training iteration 113 loss: 0.017096493393182755, ACC:0.984375\n",
      "Training iteration 114 loss: 0.013982593081891537, ACC:1.0\n",
      "Training iteration 115 loss: 0.0353241004049778, ACC:0.984375\n",
      "Training iteration 116 loss: 0.0044845654629170895, ACC:1.0\n",
      "Training iteration 117 loss: 0.008954844437539577, ACC:1.0\n",
      "Training iteration 118 loss: 0.005584106780588627, ACC:1.0\n",
      "Training iteration 119 loss: 0.0038544246926903725, ACC:1.0\n",
      "Training iteration 120 loss: 0.005559972021728754, ACC:1.0\n",
      "Training iteration 121 loss: 0.01132919266819954, ACC:1.0\n",
      "Training iteration 122 loss: 0.0025382586754858494, ACC:1.0\n",
      "Training iteration 123 loss: 0.013282767497003078, ACC:1.0\n",
      "Training iteration 124 loss: 0.0964955985546112, ACC:0.96875\n",
      "Training iteration 125 loss: 0.01700144074857235, ACC:0.984375\n",
      "Training iteration 126 loss: 0.004489965736865997, ACC:1.0\n",
      "Training iteration 127 loss: 0.008415097370743752, ACC:1.0\n",
      "Training iteration 128 loss: 0.0332745760679245, ACC:0.984375\n",
      "Training iteration 129 loss: 0.03077489510178566, ACC:0.984375\n",
      "Training iteration 130 loss: 0.001415916602127254, ACC:1.0\n",
      "Training iteration 131 loss: 0.001817592536099255, ACC:1.0\n",
      "Training iteration 132 loss: 0.0016465478111058474, ACC:1.0\n",
      "Training iteration 133 loss: 0.0560590997338295, ACC:0.984375\n",
      "Training iteration 134 loss: 0.0013210750184953213, ACC:1.0\n",
      "Training iteration 135 loss: 0.02614402212202549, ACC:0.984375\n",
      "Training iteration 136 loss: 0.001090698060579598, ACC:1.0\n",
      "Training iteration 137 loss: 0.06532900035381317, ACC:0.953125\n",
      "Training iteration 138 loss: 0.002846992341801524, ACC:1.0\n",
      "Training iteration 139 loss: 0.030053548514842987, ACC:0.984375\n",
      "Training iteration 140 loss: 0.0004380795289762318, ACC:1.0\n",
      "Training iteration 141 loss: 0.0014447936555370688, ACC:1.0\n",
      "Training iteration 142 loss: 0.04428831487894058, ACC:0.984375\n",
      "Training iteration 143 loss: 0.0021520231384783983, ACC:1.0\n",
      "Training iteration 144 loss: 0.000466469005914405, ACC:1.0\n",
      "Training iteration 145 loss: 0.0009685777476988733, ACC:1.0\n",
      "Training iteration 146 loss: 0.0016729942290112376, ACC:1.0\n",
      "Training iteration 147 loss: 0.002237204695120454, ACC:1.0\n",
      "Training iteration 148 loss: 0.0007917514885775745, ACC:1.0\n",
      "Training iteration 149 loss: 0.0005396295455284417, ACC:1.0\n",
      "Training iteration 150 loss: 0.002365042455494404, ACC:1.0\n",
      "Training iteration 151 loss: 0.0015422702999785542, ACC:1.0\n",
      "Training iteration 152 loss: 0.00201314315199852, ACC:1.0\n",
      "Training iteration 153 loss: 0.013251093216240406, ACC:0.984375\n",
      "Training iteration 154 loss: 0.008709119632840157, ACC:1.0\n",
      "Training iteration 155 loss: 0.0005861567915417254, ACC:1.0\n",
      "Training iteration 156 loss: 0.11981361359357834, ACC:0.984375\n",
      "Training iteration 157 loss: 0.00032408128026872873, ACC:1.0\n",
      "Training iteration 158 loss: 0.020172445103526115, ACC:0.984375\n",
      "Training iteration 159 loss: 0.000598001352045685, ACC:1.0\n",
      "Training iteration 160 loss: 0.0034215471241623163, ACC:1.0\n",
      "Training iteration 161 loss: 0.062482286244630814, ACC:0.984375\n",
      "Training iteration 162 loss: 0.13788101077079773, ACC:0.9375\n",
      "Training iteration 163 loss: 0.01761728525161743, ACC:0.984375\n",
      "Training iteration 164 loss: 0.06899493932723999, ACC:0.984375\n",
      "Training iteration 165 loss: 0.004820710979402065, ACC:1.0\n",
      "Training iteration 166 loss: 0.011152378283441067, ACC:1.0\n",
      "Training iteration 167 loss: 0.007697182707488537, ACC:1.0\n",
      "Training iteration 168 loss: 0.016362817957997322, ACC:1.0\n",
      "Training iteration 169 loss: 0.017230408266186714, ACC:1.0\n",
      "Training iteration 170 loss: 0.0217600017786026, ACC:0.984375\n",
      "Training iteration 171 loss: 0.08275972306728363, ACC:0.9375\n",
      "Training iteration 172 loss: 0.17432758212089539, ACC:0.96875\n",
      "Training iteration 173 loss: 0.037109024822711945, ACC:0.984375\n",
      "Training iteration 174 loss: 0.005698222666978836, ACC:1.0\n",
      "Training iteration 175 loss: 0.0022926267702132463, ACC:1.0\n",
      "Training iteration 176 loss: 0.02254898101091385, ACC:0.984375\n",
      "Training iteration 177 loss: 0.07825116068124771, ACC:0.96875\n",
      "Training iteration 178 loss: 0.006257942412048578, ACC:1.0\n",
      "Training iteration 179 loss: 0.05085420608520508, ACC:0.984375\n",
      "Training iteration 180 loss: 0.07099471986293793, ACC:0.96875\n",
      "Training iteration 181 loss: 0.04717040807008743, ACC:0.96875\n",
      "Training iteration 182 loss: 0.05128054320812225, ACC:0.96875\n",
      "Training iteration 183 loss: 0.0061339279636740685, ACC:1.0\n",
      "Training iteration 184 loss: 0.013882149010896683, ACC:1.0\n",
      "Training iteration 185 loss: 0.008240223862230778, ACC:1.0\n",
      "Training iteration 186 loss: 0.008426263928413391, ACC:1.0\n",
      "Training iteration 187 loss: 0.032230131328105927, ACC:0.984375\n",
      "Training iteration 188 loss: 0.008412444964051247, ACC:1.0\n",
      "Training iteration 189 loss: 0.02748989872634411, ACC:0.984375\n",
      "Training iteration 190 loss: 0.009680639021098614, ACC:1.0\n",
      "Training iteration 191 loss: 0.026176204904913902, ACC:0.984375\n",
      "Training iteration 192 loss: 0.23878762125968933, ACC:0.96875\n",
      "Training iteration 193 loss: 0.005245150066912174, ACC:1.0\n",
      "Training iteration 194 loss: 0.00659398827701807, ACC:1.0\n",
      "Training iteration 195 loss: 0.002537118038162589, ACC:1.0\n",
      "Training iteration 196 loss: 0.008571387268602848, ACC:1.0\n",
      "Training iteration 197 loss: 0.034565720707178116, ACC:0.984375\n",
      "Training iteration 198 loss: 0.0032455124892294407, ACC:1.0\n",
      "Training iteration 199 loss: 0.0016444785287603736, ACC:1.0\n",
      "Training iteration 200 loss: 0.007675912231206894, ACC:1.0\n",
      "Training iteration 201 loss: 0.021799426525831223, ACC:0.984375\n",
      "Training iteration 202 loss: 0.011294337920844555, ACC:1.0\n",
      "Training iteration 203 loss: 0.04217632859945297, ACC:0.984375\n",
      "Training iteration 204 loss: 0.005902369972318411, ACC:1.0\n",
      "Training iteration 205 loss: 0.0014464480336755514, ACC:1.0\n",
      "Training iteration 206 loss: 0.0018455423414707184, ACC:1.0\n",
      "Training iteration 207 loss: 0.03394263982772827, ACC:0.984375\n",
      "Training iteration 208 loss: 0.031091606244444847, ACC:0.984375\n",
      "Training iteration 209 loss: 0.06326987594366074, ACC:0.96875\n",
      "Training iteration 210 loss: 0.013008969835937023, ACC:0.984375\n",
      "Training iteration 211 loss: 0.018931711092591286, ACC:1.0\n",
      "Training iteration 212 loss: 0.013463125564157963, ACC:0.984375\n",
      "Training iteration 213 loss: 0.005912903230637312, ACC:1.0\n",
      "Training iteration 214 loss: 0.0014245091006159782, ACC:1.0\n",
      "Training iteration 215 loss: 0.003250213572755456, ACC:1.0\n",
      "Training iteration 216 loss: 0.0028294208459556103, ACC:1.0\n",
      "Training iteration 217 loss: 0.004324738867580891, ACC:1.0\n",
      "Training iteration 218 loss: 0.0032628593035042286, ACC:1.0\n",
      "Training iteration 219 loss: 0.009794323705136776, ACC:1.0\n",
      "Training iteration 220 loss: 0.003238269593566656, ACC:1.0\n",
      "Training iteration 221 loss: 0.041510503739118576, ACC:0.984375\n",
      "Training iteration 222 loss: 0.00889729242771864, ACC:1.0\n",
      "Training iteration 223 loss: 0.0028920418117195368, ACC:1.0\n",
      "Training iteration 224 loss: 0.00210773479193449, ACC:1.0\n",
      "Training iteration 225 loss: 0.0016174325719475746, ACC:1.0\n",
      "Training iteration 226 loss: 0.1692829430103302, ACC:0.96875\n",
      "Training iteration 227 loss: 0.03356834873557091, ACC:0.984375\n",
      "Training iteration 228 loss: 0.04822956398129463, ACC:0.984375\n",
      "Training iteration 229 loss: 0.006990675814449787, ACC:1.0\n",
      "Training iteration 230 loss: 0.009395909495651722, ACC:1.0\n",
      "Training iteration 231 loss: 0.0079627251252532, ACC:1.0\n",
      "Training iteration 232 loss: 0.0052591352723538876, ACC:1.0\n",
      "Training iteration 233 loss: 0.024811221286654472, ACC:0.984375\n",
      "Training iteration 234 loss: 0.009528649970889091, ACC:1.0\n",
      "Training iteration 235 loss: 0.06967467069625854, ACC:0.984375\n",
      "Training iteration 236 loss: 0.009753050282597542, ACC:1.0\n",
      "Training iteration 237 loss: 0.020071664825081825, ACC:1.0\n",
      "Training iteration 238 loss: 0.014462316408753395, ACC:1.0\n",
      "Training iteration 239 loss: 0.017467450350522995, ACC:1.0\n",
      "Training iteration 240 loss: 0.0531582348048687, ACC:0.984375\n",
      "Training iteration 241 loss: 0.029327835887670517, ACC:0.984375\n",
      "Training iteration 242 loss: 0.012093515135347843, ACC:1.0\n",
      "Training iteration 243 loss: 0.013434642925858498, ACC:1.0\n",
      "Training iteration 244 loss: 0.030966345220804214, ACC:0.984375\n",
      "Training iteration 245 loss: 0.025341399013996124, ACC:0.984375\n",
      "Training iteration 246 loss: 0.015829967334866524, ACC:1.0\n",
      "Training iteration 247 loss: 0.006253090687096119, ACC:1.0\n",
      "Training iteration 248 loss: 0.014862315729260445, ACC:1.0\n",
      "Training iteration 249 loss: 0.004738919902592897, ACC:1.0\n",
      "Training iteration 250 loss: 0.0048251673579216, ACC:1.0\n",
      "Training iteration 251 loss: 0.008833047933876514, ACC:1.0\n",
      "Training iteration 252 loss: 0.03179045021533966, ACC:1.0\n",
      "Training iteration 253 loss: 0.06669704616069794, ACC:0.984375\n",
      "Training iteration 254 loss: 0.008312394842505455, ACC:1.0\n",
      "Training iteration 255 loss: 0.0032985233701765537, ACC:1.0\n",
      "Training iteration 256 loss: 0.008784454315900803, ACC:1.0\n",
      "Training iteration 257 loss: 0.004620490130037069, ACC:1.0\n",
      "Training iteration 258 loss: 0.0030740455258637667, ACC:1.0\n",
      "Training iteration 259 loss: 0.007658420596271753, ACC:1.0\n",
      "Training iteration 260 loss: 0.003965800162404776, ACC:1.0\n",
      "Training iteration 261 loss: 0.0020361212082207203, ACC:1.0\n",
      "Training iteration 262 loss: 0.00376414624042809, ACC:1.0\n",
      "Training iteration 263 loss: 0.06904008239507675, ACC:0.953125\n",
      "Training iteration 264 loss: 0.002322528511285782, ACC:1.0\n",
      "Training iteration 265 loss: 0.0037486166693270206, ACC:1.0\n",
      "Training iteration 266 loss: 0.11494071781635284, ACC:0.984375\n",
      "Training iteration 267 loss: 0.05079040303826332, ACC:0.984375\n",
      "Training iteration 268 loss: 0.07726183533668518, ACC:0.984375\n",
      "Training iteration 269 loss: 0.02241096831858158, ACC:0.984375\n",
      "Training iteration 270 loss: 0.007960374467074871, ACC:1.0\n",
      "Training iteration 271 loss: 0.07967609167098999, ACC:0.96875\n",
      "Training iteration 272 loss: 0.056657444685697556, ACC:0.96875\n",
      "Training iteration 273 loss: 0.03162751719355583, ACC:0.984375\n",
      "Training iteration 274 loss: 0.0352792963385582, ACC:0.984375\n",
      "Training iteration 275 loss: 0.008173015899956226, ACC:1.0\n",
      "Training iteration 276 loss: 0.11132136732339859, ACC:0.984375\n",
      "Training iteration 277 loss: 0.027679473161697388, ACC:0.984375\n",
      "Training iteration 278 loss: 0.00913278479129076, ACC:1.0\n",
      "Training iteration 279 loss: 0.012026938609778881, ACC:1.0\n",
      "Training iteration 280 loss: 0.16760866343975067, ACC:0.984375\n",
      "Training iteration 281 loss: 0.004741085693240166, ACC:1.0\n",
      "Training iteration 282 loss: 0.006062007509171963, ACC:1.0\n",
      "Training iteration 283 loss: 0.004594134632498026, ACC:1.0\n",
      "Training iteration 284 loss: 0.028888339176774025, ACC:0.984375\n",
      "Training iteration 285 loss: 0.04876508563756943, ACC:0.984375\n",
      "Training iteration 286 loss: 0.010560483671724796, ACC:1.0\n",
      "Training iteration 287 loss: 0.06048428267240524, ACC:0.96875\n",
      "Training iteration 288 loss: 0.02833242528140545, ACC:0.984375\n",
      "Training iteration 289 loss: 0.0074649592861533165, ACC:1.0\n",
      "Training iteration 290 loss: 0.012655137106776237, ACC:1.0\n",
      "Training iteration 291 loss: 0.007863074541091919, ACC:1.0\n",
      "Training iteration 292 loss: 0.004129691515117884, ACC:1.0\n",
      "Training iteration 293 loss: 0.1288449466228485, ACC:0.953125\n",
      "Training iteration 294 loss: 0.054789911955595016, ACC:0.96875\n",
      "Training iteration 295 loss: 0.016968442127108574, ACC:1.0\n",
      "Training iteration 296 loss: 0.0826173648238182, ACC:0.96875\n",
      "Training iteration 297 loss: 0.03333232179284096, ACC:0.984375\n",
      "Training iteration 298 loss: 0.010599018074572086, ACC:1.0\n",
      "Training iteration 299 loss: 0.08257323503494263, ACC:0.953125\n",
      "Training iteration 300 loss: 0.007655435241758823, ACC:1.0\n",
      "Training iteration 301 loss: 0.026124436408281326, ACC:0.984375\n",
      "Training iteration 302 loss: 0.00881141796708107, ACC:1.0\n",
      "Training iteration 303 loss: 0.0044658067636191845, ACC:1.0\n",
      "Training iteration 304 loss: 0.07250986993312836, ACC:0.984375\n",
      "Training iteration 305 loss: 0.08434838801622391, ACC:0.96875\n",
      "Training iteration 306 loss: 0.01987435296177864, ACC:1.0\n",
      "Training iteration 307 loss: 0.04759438335895538, ACC:0.984375\n",
      "Training iteration 308 loss: 0.0766320526599884, ACC:0.984375\n",
      "Training iteration 309 loss: 0.014259446412324905, ACC:1.0\n",
      "Training iteration 310 loss: 0.0023105735890567303, ACC:1.0\n",
      "Training iteration 311 loss: 0.004703504033386707, ACC:1.0\n",
      "Training iteration 312 loss: 0.015325434505939484, ACC:0.984375\n",
      "Training iteration 313 loss: 0.002889223862439394, ACC:1.0\n",
      "Training iteration 314 loss: 0.010688794776797295, ACC:1.0\n",
      "Training iteration 315 loss: 0.01569465547800064, ACC:0.984375\n",
      "Training iteration 316 loss: 0.11039486527442932, ACC:0.984375\n",
      "Training iteration 317 loss: 0.022752175107598305, ACC:0.984375\n",
      "Training iteration 318 loss: 0.0016275906236842275, ACC:1.0\n",
      "Training iteration 319 loss: 0.0018397337989881635, ACC:1.0\n",
      "Training iteration 320 loss: 0.0013580011436715722, ACC:1.0\n",
      "Training iteration 321 loss: 0.08794406801462173, ACC:0.984375\n",
      "Training iteration 322 loss: 0.0018122846959158778, ACC:1.0\n",
      "Training iteration 323 loss: 0.14063885807991028, ACC:0.953125\n",
      "Training iteration 324 loss: 0.03950006514787674, ACC:0.984375\n",
      "Training iteration 325 loss: 0.002039233921095729, ACC:1.0\n",
      "Training iteration 326 loss: 0.01131649874150753, ACC:1.0\n",
      "Training iteration 327 loss: 0.044556714594364166, ACC:0.96875\n",
      "Training iteration 328 loss: 0.006252729333937168, ACC:1.0\n",
      "Training iteration 329 loss: 0.003345583798363805, ACC:1.0\n",
      "Training iteration 330 loss: 0.017560211941599846, ACC:1.0\n",
      "Training iteration 331 loss: 0.008673558011651039, ACC:1.0\n",
      "Training iteration 332 loss: 0.016673550009727478, ACC:1.0\n",
      "Training iteration 333 loss: 0.02783682756125927, ACC:0.984375\n",
      "Training iteration 334 loss: 0.04161859676241875, ACC:0.984375\n",
      "Training iteration 335 loss: 0.027607016265392303, ACC:1.0\n",
      "Training iteration 336 loss: 0.01293017715215683, ACC:1.0\n",
      "Training iteration 337 loss: 0.0033560884185135365, ACC:1.0\n",
      "Training iteration 338 loss: 0.011415695771574974, ACC:1.0\n",
      "Training iteration 339 loss: 0.0021315719932317734, ACC:1.0\n",
      "Training iteration 340 loss: 0.04830044135451317, ACC:0.984375\n",
      "Training iteration 341 loss: 0.003403902519494295, ACC:1.0\n",
      "Training iteration 342 loss: 0.16951997578144073, ACC:0.96875\n",
      "Training iteration 343 loss: 0.02678406797349453, ACC:0.984375\n",
      "Training iteration 344 loss: 0.06992192566394806, ACC:0.984375\n",
      "Training iteration 345 loss: 0.012556067667901516, ACC:1.0\n",
      "Training iteration 346 loss: 0.003092106431722641, ACC:1.0\n",
      "Training iteration 347 loss: 0.0022545307874679565, ACC:1.0\n",
      "Training iteration 348 loss: 0.028576040640473366, ACC:0.984375\n",
      "Training iteration 349 loss: 0.03224886581301689, ACC:0.96875\n",
      "Training iteration 350 loss: 0.010928353294730186, ACC:1.0\n",
      "Training iteration 351 loss: 0.003489758586511016, ACC:1.0\n",
      "Training iteration 352 loss: 0.0033909890335053205, ACC:1.0\n",
      "Training iteration 353 loss: 0.01864033006131649, ACC:0.984375\n",
      "Training iteration 354 loss: 0.006194998510181904, ACC:1.0\n",
      "Training iteration 355 loss: 0.0051918854005634785, ACC:1.0\n",
      "Training iteration 356 loss: 0.0035710977390408516, ACC:1.0\n",
      "Training iteration 357 loss: 0.012167913839221, ACC:1.0\n",
      "Training iteration 358 loss: 0.010300686582922935, ACC:1.0\n",
      "Training iteration 359 loss: 0.0017515679355710745, ACC:1.0\n",
      "Training iteration 360 loss: 0.0015356866642832756, ACC:1.0\n",
      "Training iteration 361 loss: 0.009828468784689903, ACC:1.0\n",
      "Training iteration 362 loss: 0.032309506088495255, ACC:0.984375\n",
      "Training iteration 363 loss: 0.08972416073083878, ACC:0.984375\n",
      "Training iteration 364 loss: 0.0019021204207092524, ACC:1.0\n",
      "Training iteration 365 loss: 0.0011380247306078672, ACC:1.0\n",
      "Training iteration 366 loss: 0.002423627534881234, ACC:1.0\n",
      "Training iteration 367 loss: 0.003894917666912079, ACC:1.0\n",
      "Training iteration 368 loss: 0.029067371040582657, ACC:0.984375\n",
      "Training iteration 369 loss: 0.06669507920742035, ACC:0.984375\n",
      "Training iteration 370 loss: 0.0021991755347698927, ACC:1.0\n",
      "Training iteration 371 loss: 0.002022144151851535, ACC:1.0\n",
      "Training iteration 372 loss: 0.04868476837873459, ACC:0.984375\n",
      "Training iteration 373 loss: 0.04192674160003662, ACC:0.984375\n",
      "Training iteration 374 loss: 0.019552292302250862, ACC:0.984375\n",
      "Training iteration 375 loss: 0.010211825370788574, ACC:1.0\n",
      "Training iteration 376 loss: 0.0016890504630282521, ACC:1.0\n",
      "Training iteration 377 loss: 0.005795505363494158, ACC:1.0\n",
      "Training iteration 378 loss: 0.001786675420589745, ACC:1.0\n",
      "Training iteration 379 loss: 0.004131949041038752, ACC:1.0\n",
      "Training iteration 380 loss: 0.008964921347796917, ACC:1.0\n",
      "Training iteration 381 loss: 0.010109584778547287, ACC:1.0\n",
      "Training iteration 382 loss: 0.017577076330780983, ACC:0.984375\n",
      "Training iteration 383 loss: 0.060426805168390274, ACC:0.984375\n",
      "Training iteration 384 loss: 0.004907320719212294, ACC:1.0\n",
      "Training iteration 385 loss: 0.0049811797216534615, ACC:1.0\n",
      "Training iteration 386 loss: 0.010583862662315369, ACC:1.0\n",
      "Training iteration 387 loss: 0.12387438863515854, ACC:0.953125\n",
      "Training iteration 388 loss: 0.0018304401310160756, ACC:1.0\n",
      "Training iteration 389 loss: 0.02675413154065609, ACC:0.984375\n",
      "Training iteration 390 loss: 0.05890963599085808, ACC:0.984375\n",
      "Training iteration 391 loss: 0.01973610371351242, ACC:0.984375\n",
      "Training iteration 392 loss: 0.016483204439282417, ACC:0.984375\n",
      "Training iteration 393 loss: 0.005032737739384174, ACC:1.0\n",
      "Training iteration 394 loss: 0.022763384506106377, ACC:0.984375\n",
      "Training iteration 395 loss: 0.005425586830824614, ACC:1.0\n",
      "Training iteration 396 loss: 0.009821435436606407, ACC:1.0\n",
      "Training iteration 397 loss: 0.031765781342983246, ACC:0.984375\n",
      "Training iteration 398 loss: 0.01058960985392332, ACC:1.0\n",
      "Training iteration 399 loss: 0.024754764512181282, ACC:0.984375\n",
      "Training iteration 400 loss: 0.03450648486614227, ACC:0.984375\n",
      "Training iteration 401 loss: 0.006093970965594053, ACC:1.0\n",
      "Training iteration 402 loss: 0.004552527330815792, ACC:1.0\n",
      "Training iteration 403 loss: 0.02609875611960888, ACC:0.984375\n",
      "Training iteration 404 loss: 0.026766439899802208, ACC:0.984375\n",
      "Training iteration 405 loss: 0.04722916707396507, ACC:0.984375\n",
      "Training iteration 406 loss: 0.04240492731332779, ACC:0.984375\n",
      "Training iteration 407 loss: 0.013988408260047436, ACC:1.0\n",
      "Training iteration 408 loss: 0.033412836492061615, ACC:0.984375\n",
      "Training iteration 409 loss: 0.007729858160018921, ACC:1.0\n",
      "Training iteration 410 loss: 0.004221152979880571, ACC:1.0\n",
      "Training iteration 411 loss: 0.004436901304870844, ACC:1.0\n",
      "Training iteration 412 loss: 0.02958458475768566, ACC:0.984375\n",
      "Training iteration 413 loss: 0.011737470515072346, ACC:1.0\n",
      "Training iteration 414 loss: 0.05268338695168495, ACC:0.96875\n",
      "Training iteration 415 loss: 0.004257156513631344, ACC:1.0\n",
      "Training iteration 416 loss: 0.004346857778728008, ACC:1.0\n",
      "Training iteration 417 loss: 0.01890195906162262, ACC:0.984375\n",
      "Training iteration 418 loss: 0.003513043513521552, ACC:1.0\n",
      "Training iteration 419 loss: 0.05499224737286568, ACC:0.984375\n",
      "Training iteration 420 loss: 0.09770139306783676, ACC:0.984375\n",
      "Training iteration 421 loss: 0.03942851349711418, ACC:0.984375\n",
      "Training iteration 422 loss: 0.11471645534038544, ACC:0.953125\n",
      "Training iteration 423 loss: 0.020309990271925926, ACC:0.984375\n",
      "Training iteration 424 loss: 0.0056163109838962555, ACC:1.0\n",
      "Training iteration 425 loss: 0.04532930254936218, ACC:0.984375\n",
      "Training iteration 426 loss: 0.010080107487738132, ACC:1.0\n",
      "Training iteration 427 loss: 0.029957501217722893, ACC:0.984375\n",
      "Training iteration 428 loss: 0.014103589579463005, ACC:1.0\n",
      "Training iteration 429 loss: 0.047446634620428085, ACC:0.96875\n",
      "Training iteration 430 loss: 0.04687713086605072, ACC:0.984375\n",
      "Training iteration 431 loss: 0.0031016182620078325, ACC:1.0\n",
      "Training iteration 432 loss: 0.011805146001279354, ACC:1.0\n",
      "Training iteration 433 loss: 0.00791830662637949, ACC:1.0\n",
      "Training iteration 434 loss: 0.008924773894250393, ACC:1.0\n",
      "Training iteration 435 loss: 0.007444689515978098, ACC:1.0\n",
      "Training iteration 436 loss: 0.126192107796669, ACC:0.953125\n",
      "Training iteration 437 loss: 0.003503472311422229, ACC:1.0\n",
      "Training iteration 438 loss: 0.0074281892739236355, ACC:1.0\n",
      "Training iteration 439 loss: 0.003343014745041728, ACC:1.0\n",
      "Training iteration 440 loss: 0.004660464823246002, ACC:1.0\n",
      "Training iteration 441 loss: 0.01646004430949688, ACC:0.984375\n",
      "Training iteration 442 loss: 0.0032810198608785868, ACC:1.0\n",
      "Training iteration 443 loss: 0.003083400893956423, ACC:1.0\n",
      "Training iteration 444 loss: 0.0075484830886125565, ACC:1.0\n",
      "Training iteration 445 loss: 0.07727059721946716, ACC:0.984375\n",
      "Training iteration 446 loss: 0.006280196364969015, ACC:1.0\n",
      "Training iteration 447 loss: 0.08410736918449402, ACC:0.984375\n",
      "Training iteration 448 loss: 0.011871537193655968, ACC:1.0\n",
      "Training iteration 449 loss: 0.19977878034114838, ACC:0.984375\n",
      "Training iteration 450 loss: 0.05966183543205261, ACC:0.96875\n",
      "Validation iteration 451 loss: 0.008370447903871536, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.0032399834599345922, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.015376027673482895, ACC: 0.984375\n",
      "Validation iteration 454 loss: 0.0020585788879543543, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.04841514676809311, ACC: 0.984375\n",
      "Validation iteration 456 loss: 0.14925862848758698, ACC: 0.984375\n",
      "Validation iteration 457 loss: 0.000931742019020021, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.0012828906765207648, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.10898110270500183, ACC: 0.96875\n",
      "Validation iteration 460 loss: 0.026376761496067047, ACC: 0.984375\n",
      "Validation iteration 461 loss: 0.0035570377949625254, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.0026824099477380514, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.0017936443910002708, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.005856032948940992, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.002880267333239317, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.09597843140363693, ACC: 0.984375\n",
      "Validation iteration 467 loss: 0.0029653944075107574, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.0019064157968387008, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.018243277445435524, ACC: 0.984375\n",
      "Validation iteration 470 loss: 0.06489628553390503, ACC: 0.984375\n",
      "Validation iteration 471 loss: 0.1516886204481125, ACC: 0.953125\n",
      "Validation iteration 472 loss: 0.056620728224515915, ACC: 0.96875\n",
      "Validation iteration 473 loss: 0.0064194826409220695, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.003981348592787981, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.0019785978365689516, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.04013126343488693, ACC: 0.984375\n",
      "Validation iteration 477 loss: 0.003116600215435028, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.02109512872993946, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.0022020149044692516, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.0014856242341920733, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.01069616712629795, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.0017365170642733574, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.0017105313017964363, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.0032881838269531727, ACC: 1.0\n",
      "Validation iteration 485 loss: 0.01946198381483555, ACC: 0.984375\n",
      "Validation iteration 486 loss: 0.003103548428043723, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.0014669059310108423, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.018485333770513535, ACC: 0.984375\n",
      "Validation iteration 489 loss: 0.00378781626932323, ACC: 1.0\n",
      "Validation iteration 490 loss: 0.007741577923297882, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.004543189890682697, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.025325482711195946, ACC: 0.984375\n",
      "Validation iteration 493 loss: 0.014246582984924316, ACC: 0.984375\n",
      "Validation iteration 494 loss: 0.047276295721530914, ACC: 0.984375\n",
      "Validation iteration 495 loss: 0.0013164078118279576, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.001977794338017702, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.001706574228592217, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.0013625912833958864, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.0019942515064030886, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.026238886639475822, ACC: 0.984375\n",
      "-- Epoch 6 done -- Train loss: 0.025373378954504408, train ACC: 0.9919097222222222, val loss: 0.02102473077829927, val ACC: 0.993125\n",
      "<--- 20043.028519392014 seconds --->\n",
      "Training iteration 1 loss: 0.004672036971896887, ACC:1.0\n",
      "Training iteration 2 loss: 0.0033804792910814285, ACC:1.0\n",
      "Training iteration 3 loss: 0.042857054620981216, ACC:0.984375\n",
      "Training iteration 4 loss: 0.007467455230653286, ACC:1.0\n",
      "Training iteration 5 loss: 0.0014767121756449342, ACC:1.0\n",
      "Training iteration 6 loss: 0.0014423466054722667, ACC:1.0\n",
      "Training iteration 7 loss: 0.08895310014486313, ACC:0.984375\n",
      "Training iteration 8 loss: 0.0006760444375686347, ACC:1.0\n",
      "Training iteration 9 loss: 0.12631575763225555, ACC:0.96875\n",
      "Training iteration 10 loss: 0.0054938942193984985, ACC:1.0\n",
      "Training iteration 11 loss: 0.010356699116528034, ACC:1.0\n",
      "Training iteration 12 loss: 0.0006365843582898378, ACC:1.0\n",
      "Training iteration 13 loss: 0.036776378750801086, ACC:0.984375\n",
      "Training iteration 14 loss: 0.0034299520775675774, ACC:1.0\n",
      "Training iteration 15 loss: 0.003410744946449995, ACC:1.0\n",
      "Training iteration 16 loss: 0.012111241929233074, ACC:1.0\n",
      "Training iteration 17 loss: 0.14037004113197327, ACC:0.984375\n",
      "Training iteration 18 loss: 0.004841192159801722, ACC:1.0\n",
      "Training iteration 19 loss: 0.011387394741177559, ACC:1.0\n",
      "Training iteration 20 loss: 0.005938566755503416, ACC:1.0\n",
      "Training iteration 21 loss: 0.02874511294066906, ACC:0.984375\n",
      "Training iteration 22 loss: 0.020264096558094025, ACC:0.984375\n",
      "Training iteration 23 loss: 0.02504235878586769, ACC:0.984375\n",
      "Training iteration 24 loss: 0.0035088800359517336, ACC:1.0\n",
      "Training iteration 25 loss: 0.08002390712499619, ACC:0.984375\n",
      "Training iteration 26 loss: 0.0058173867873847485, ACC:1.0\n",
      "Training iteration 27 loss: 0.01055221538990736, ACC:1.0\n",
      "Training iteration 28 loss: 0.005725180730223656, ACC:1.0\n",
      "Training iteration 29 loss: 0.003587838029488921, ACC:1.0\n",
      "Training iteration 30 loss: 0.004198189824819565, ACC:1.0\n",
      "Training iteration 31 loss: 0.0036553151439875364, ACC:1.0\n",
      "Training iteration 32 loss: 0.003670282429084182, ACC:1.0\n",
      "Training iteration 33 loss: 0.0044830297119915485, ACC:1.0\n",
      "Training iteration 34 loss: 0.02809716947376728, ACC:0.984375\n",
      "Training iteration 35 loss: 0.00653438875451684, ACC:1.0\n",
      "Training iteration 36 loss: 0.054930634796619415, ACC:0.984375\n",
      "Training iteration 37 loss: 0.025909224525094032, ACC:1.0\n",
      "Training iteration 38 loss: 0.039897505193948746, ACC:0.984375\n",
      "Training iteration 39 loss: 0.003861732315272093, ACC:1.0\n",
      "Training iteration 40 loss: 0.0024583949707448483, ACC:1.0\n",
      "Training iteration 41 loss: 0.08491602540016174, ACC:0.96875\n",
      "Training iteration 42 loss: 0.0022258907556533813, ACC:1.0\n",
      "Training iteration 43 loss: 0.07692550122737885, ACC:0.984375\n",
      "Training iteration 44 loss: 0.005022342316806316, ACC:1.0\n",
      "Training iteration 45 loss: 0.002462005475535989, ACC:1.0\n",
      "Training iteration 46 loss: 0.0034559855703264475, ACC:1.0\n",
      "Training iteration 47 loss: 0.002503001131117344, ACC:1.0\n",
      "Training iteration 48 loss: 0.004523094743490219, ACC:1.0\n",
      "Training iteration 49 loss: 0.0026560372207313776, ACC:1.0\n",
      "Training iteration 50 loss: 0.005129157565534115, ACC:1.0\n",
      "Training iteration 51 loss: 0.005581125617027283, ACC:1.0\n",
      "Training iteration 52 loss: 0.06123132258653641, ACC:0.96875\n",
      "Training iteration 53 loss: 0.014495179057121277, ACC:0.984375\n",
      "Training iteration 54 loss: 0.0011003256076946855, ACC:1.0\n",
      "Training iteration 55 loss: 0.012319263070821762, ACC:0.984375\n",
      "Training iteration 56 loss: 0.0030520844738930464, ACC:1.0\n",
      "Training iteration 57 loss: 0.004524981137365103, ACC:1.0\n",
      "Training iteration 58 loss: 0.03467165306210518, ACC:0.984375\n",
      "Training iteration 59 loss: 0.14027918875217438, ACC:0.96875\n",
      "Training iteration 60 loss: 0.0006098814192228019, ACC:1.0\n",
      "Training iteration 61 loss: 0.00041173279169015586, ACC:1.0\n",
      "Training iteration 62 loss: 0.09028401225805283, ACC:0.96875\n",
      "Training iteration 63 loss: 0.004792794585227966, ACC:1.0\n",
      "Training iteration 64 loss: 0.0014090174809098244, ACC:1.0\n",
      "Training iteration 65 loss: 0.004933842923492193, ACC:1.0\n",
      "Training iteration 66 loss: 0.11674217879772186, ACC:0.96875\n",
      "Training iteration 67 loss: 0.025885192677378654, ACC:0.984375\n",
      "Training iteration 68 loss: 0.011846919544041157, ACC:1.0\n",
      "Training iteration 69 loss: 0.04469633847475052, ACC:0.984375\n",
      "Training iteration 70 loss: 0.012466670013964176, ACC:1.0\n",
      "Training iteration 71 loss: 0.02472580038011074, ACC:1.0\n",
      "Training iteration 72 loss: 0.3469494581222534, ACC:0.984375\n",
      "Training iteration 73 loss: 0.04492149129509926, ACC:0.984375\n",
      "Training iteration 74 loss: 0.007097977679222822, ACC:1.0\n",
      "Training iteration 75 loss: 0.04540811479091644, ACC:0.984375\n",
      "Training iteration 76 loss: 0.0096840625628829, ACC:1.0\n",
      "Training iteration 77 loss: 0.03244464471936226, ACC:0.984375\n",
      "Training iteration 78 loss: 0.033332567662000656, ACC:0.984375\n",
      "Training iteration 79 loss: 0.001507044886238873, ACC:1.0\n",
      "Training iteration 80 loss: 0.001403415109962225, ACC:1.0\n",
      "Training iteration 81 loss: 0.0011760825291275978, ACC:1.0\n",
      "Training iteration 82 loss: 0.0015377379022538662, ACC:1.0\n",
      "Training iteration 83 loss: 0.0029352703131735325, ACC:1.0\n",
      "Training iteration 84 loss: 0.04000188782811165, ACC:0.984375\n",
      "Training iteration 85 loss: 0.011890292167663574, ACC:0.984375\n",
      "Training iteration 86 loss: 0.09989738464355469, ACC:0.984375\n",
      "Training iteration 87 loss: 0.012614677660167217, ACC:0.984375\n",
      "Training iteration 88 loss: 0.08238276094198227, ACC:0.984375\n",
      "Training iteration 89 loss: 0.002153021516278386, ACC:1.0\n",
      "Training iteration 90 loss: 0.008361217565834522, ACC:1.0\n",
      "Training iteration 91 loss: 0.022874779999256134, ACC:0.984375\n",
      "Training iteration 92 loss: 0.0020905439741909504, ACC:1.0\n",
      "Training iteration 93 loss: 0.0033710238058120012, ACC:1.0\n",
      "Training iteration 94 loss: 0.0072658248245716095, ACC:1.0\n",
      "Training iteration 95 loss: 0.023508626967668533, ACC:0.984375\n",
      "Training iteration 96 loss: 0.00873881671577692, ACC:1.0\n",
      "Training iteration 97 loss: 0.04372734576463699, ACC:0.984375\n",
      "Training iteration 98 loss: 0.0028459460008889437, ACC:1.0\n",
      "Training iteration 99 loss: 0.003616930451244116, ACC:1.0\n",
      "Training iteration 100 loss: 0.026898346841335297, ACC:0.984375\n",
      "Training iteration 101 loss: 0.013271927833557129, ACC:1.0\n",
      "Training iteration 102 loss: 0.0030059129931032658, ACC:1.0\n",
      "Training iteration 103 loss: 0.05213051289319992, ACC:0.984375\n",
      "Training iteration 104 loss: 0.006959153804928064, ACC:1.0\n",
      "Training iteration 105 loss: 0.010706420987844467, ACC:1.0\n",
      "Training iteration 106 loss: 0.0069944653660058975, ACC:1.0\n",
      "Training iteration 107 loss: 0.005028244107961655, ACC:1.0\n",
      "Training iteration 108 loss: 0.05361204966902733, ACC:0.984375\n",
      "Training iteration 109 loss: 0.00415033521130681, ACC:1.0\n",
      "Training iteration 110 loss: 0.15131275355815887, ACC:0.96875\n",
      "Training iteration 111 loss: 0.0399821400642395, ACC:0.984375\n",
      "Training iteration 112 loss: 0.008967028930783272, ACC:1.0\n",
      "Training iteration 113 loss: 0.003884181845933199, ACC:1.0\n",
      "Training iteration 114 loss: 0.0036286073736846447, ACC:1.0\n",
      "Training iteration 115 loss: 0.008033820427954197, ACC:1.0\n",
      "Training iteration 116 loss: 0.002159148920327425, ACC:1.0\n",
      "Training iteration 117 loss: 0.006590850651264191, ACC:1.0\n",
      "Training iteration 118 loss: 0.014199362136423588, ACC:0.984375\n",
      "Training iteration 119 loss: 0.07190350443124771, ACC:0.984375\n",
      "Training iteration 120 loss: 0.012496650218963623, ACC:1.0\n",
      "Training iteration 121 loss: 0.008316273801028728, ACC:1.0\n",
      "Training iteration 122 loss: 0.006323268637061119, ACC:1.0\n",
      "Training iteration 123 loss: 0.005988922901451588, ACC:1.0\n",
      "Training iteration 124 loss: 0.0034684869460761547, ACC:1.0\n",
      "Training iteration 125 loss: 0.002391361864283681, ACC:1.0\n",
      "Training iteration 126 loss: 0.07353565841913223, ACC:0.984375\n",
      "Training iteration 127 loss: 0.0038932799361646175, ACC:1.0\n",
      "Training iteration 128 loss: 0.0041471198201179504, ACC:1.0\n",
      "Training iteration 129 loss: 0.006146728526800871, ACC:1.0\n",
      "Training iteration 130 loss: 0.006133908871561289, ACC:1.0\n",
      "Training iteration 131 loss: 0.012300968170166016, ACC:1.0\n",
      "Training iteration 132 loss: 0.0037729181349277496, ACC:1.0\n",
      "Training iteration 133 loss: 0.006569713354110718, ACC:1.0\n",
      "Training iteration 134 loss: 0.004516346380114555, ACC:1.0\n",
      "Training iteration 135 loss: 0.004814618267118931, ACC:1.0\n",
      "Training iteration 136 loss: 0.010490340180695057, ACC:1.0\n",
      "Training iteration 137 loss: 0.10165992379188538, ACC:0.984375\n",
      "Training iteration 138 loss: 0.0014027211582288146, ACC:1.0\n",
      "Training iteration 139 loss: 0.07624294608831406, ACC:0.984375\n",
      "Training iteration 140 loss: 0.004600696265697479, ACC:1.0\n",
      "Training iteration 141 loss: 0.029719971120357513, ACC:0.984375\n",
      "Training iteration 142 loss: 0.002873543184250593, ACC:1.0\n",
      "Training iteration 143 loss: 0.006367701105773449, ACC:1.0\n",
      "Training iteration 144 loss: 0.0021319929510354996, ACC:1.0\n",
      "Training iteration 145 loss: 0.007528695743530989, ACC:1.0\n",
      "Training iteration 146 loss: 0.006964385975152254, ACC:1.0\n",
      "Training iteration 147 loss: 0.004574892111122608, ACC:1.0\n",
      "Training iteration 148 loss: 0.002876236103475094, ACC:1.0\n",
      "Training iteration 149 loss: 0.005727698560804129, ACC:1.0\n",
      "Training iteration 150 loss: 0.0056520006619393826, ACC:1.0\n",
      "Training iteration 151 loss: 0.002640334190800786, ACC:1.0\n",
      "Training iteration 152 loss: 0.003935403656214476, ACC:1.0\n",
      "Training iteration 153 loss: 0.005565122235566378, ACC:1.0\n",
      "Training iteration 154 loss: 0.07194407284259796, ACC:0.96875\n",
      "Training iteration 155 loss: 0.006629293318837881, ACC:1.0\n",
      "Training iteration 156 loss: 0.006535741500556469, ACC:1.0\n",
      "Training iteration 157 loss: 0.002004876732826233, ACC:1.0\n",
      "Training iteration 158 loss: 0.013935649767518044, ACC:1.0\n",
      "Training iteration 159 loss: 0.0023879080545157194, ACC:1.0\n",
      "Training iteration 160 loss: 0.014639862813055515, ACC:0.984375\n",
      "Training iteration 161 loss: 0.000624144624453038, ACC:1.0\n",
      "Training iteration 162 loss: 0.0017315952572971582, ACC:1.0\n",
      "Training iteration 163 loss: 0.050986941903829575, ACC:0.984375\n",
      "Training iteration 164 loss: 0.0008614090038463473, ACC:1.0\n",
      "Training iteration 165 loss: 0.0006454647518694401, ACC:1.0\n",
      "Training iteration 166 loss: 0.0006405551102943718, ACC:1.0\n",
      "Training iteration 167 loss: 0.0007160424720495939, ACC:1.0\n",
      "Training iteration 168 loss: 0.007302102167159319, ACC:1.0\n",
      "Training iteration 169 loss: 0.014047578908503056, ACC:0.984375\n",
      "Training iteration 170 loss: 0.017209960147738457, ACC:1.0\n",
      "Training iteration 171 loss: 0.050650469958782196, ACC:0.96875\n",
      "Training iteration 172 loss: 0.06043248996138573, ACC:0.96875\n",
      "Training iteration 173 loss: 0.0011861068196594715, ACC:1.0\n",
      "Training iteration 174 loss: 0.001687478506937623, ACC:1.0\n",
      "Training iteration 175 loss: 0.005507158115506172, ACC:1.0\n",
      "Training iteration 176 loss: 0.0035154325887560844, ACC:1.0\n",
      "Training iteration 177 loss: 0.008397068828344345, ACC:1.0\n",
      "Training iteration 178 loss: 0.016082409769296646, ACC:1.0\n",
      "Training iteration 179 loss: 0.006380697712302208, ACC:1.0\n",
      "Training iteration 180 loss: 0.06644754856824875, ACC:0.984375\n",
      "Training iteration 181 loss: 0.04320859536528587, ACC:0.984375\n",
      "Training iteration 182 loss: 0.04101648926734924, ACC:0.984375\n",
      "Training iteration 183 loss: 0.0010892851278185844, ACC:1.0\n",
      "Training iteration 184 loss: 0.001044027041643858, ACC:1.0\n",
      "Training iteration 185 loss: 0.0020918415393680334, ACC:1.0\n",
      "Training iteration 186 loss: 0.04508274048566818, ACC:0.984375\n",
      "Training iteration 187 loss: 0.001018195296637714, ACC:1.0\n",
      "Training iteration 188 loss: 0.00424438389018178, ACC:1.0\n",
      "Training iteration 189 loss: 0.003281746990978718, ACC:1.0\n",
      "Training iteration 190 loss: 0.00208953651599586, ACC:1.0\n",
      "Training iteration 191 loss: 0.0015556425787508488, ACC:1.0\n",
      "Training iteration 192 loss: 0.019151050597429276, ACC:1.0\n",
      "Training iteration 193 loss: 0.0022714505903422832, ACC:1.0\n",
      "Training iteration 194 loss: 0.0003886785707436502, ACC:1.0\n",
      "Training iteration 195 loss: 0.0020998602267354727, ACC:1.0\n",
      "Training iteration 196 loss: 0.012270309031009674, ACC:0.984375\n",
      "Training iteration 197 loss: 0.07404355704784393, ACC:0.96875\n",
      "Training iteration 198 loss: 0.009302517399191856, ACC:1.0\n",
      "Training iteration 199 loss: 0.009172486141324043, ACC:1.0\n",
      "Training iteration 200 loss: 0.002852552104741335, ACC:1.0\n",
      "Training iteration 201 loss: 0.0012003553565591574, ACC:1.0\n",
      "Training iteration 202 loss: 0.06451018899679184, ACC:0.984375\n",
      "Training iteration 203 loss: 0.026948627084493637, ACC:0.984375\n",
      "Training iteration 204 loss: 0.026602046564221382, ACC:0.984375\n",
      "Training iteration 205 loss: 0.0034337597899138927, ACC:1.0\n",
      "Training iteration 206 loss: 0.0023267650976777077, ACC:1.0\n",
      "Training iteration 207 loss: 0.0698288157582283, ACC:0.984375\n",
      "Training iteration 208 loss: 0.003858348121866584, ACC:1.0\n",
      "Training iteration 209 loss: 0.00502695981413126, ACC:1.0\n",
      "Training iteration 210 loss: 0.0038641798309981823, ACC:1.0\n",
      "Training iteration 211 loss: 0.04432346671819687, ACC:0.984375\n",
      "Training iteration 212 loss: 0.006714192684739828, ACC:1.0\n",
      "Training iteration 213 loss: 0.004729295149445534, ACC:1.0\n",
      "Training iteration 214 loss: 0.015016048215329647, ACC:0.984375\n",
      "Training iteration 215 loss: 0.008178283460438251, ACC:1.0\n",
      "Training iteration 216 loss: 0.005319996736943722, ACC:1.0\n",
      "Training iteration 217 loss: 0.0047759441658854485, ACC:1.0\n",
      "Training iteration 218 loss: 0.007107298821210861, ACC:1.0\n",
      "Training iteration 219 loss: 0.026813924312591553, ACC:0.984375\n",
      "Training iteration 220 loss: 0.0037625120021402836, ACC:1.0\n",
      "Training iteration 221 loss: 0.0035582755226641893, ACC:1.0\n",
      "Training iteration 222 loss: 0.004942170809954405, ACC:1.0\n",
      "Training iteration 223 loss: 0.002733299508690834, ACC:1.0\n",
      "Training iteration 224 loss: 0.003618794959038496, ACC:1.0\n",
      "Training iteration 225 loss: 0.04186057671904564, ACC:0.96875\n",
      "Training iteration 226 loss: 0.003535608295351267, ACC:1.0\n",
      "Training iteration 227 loss: 0.03233940526843071, ACC:0.984375\n",
      "Training iteration 228 loss: 0.007730448618531227, ACC:1.0\n",
      "Training iteration 229 loss: 0.004183644894510508, ACC:1.0\n",
      "Training iteration 230 loss: 0.0038894398603588343, ACC:1.0\n",
      "Training iteration 231 loss: 0.004468507133424282, ACC:1.0\n",
      "Training iteration 232 loss: 0.0030718331690877676, ACC:1.0\n",
      "Training iteration 233 loss: 0.0025886069051921368, ACC:1.0\n",
      "Training iteration 234 loss: 0.038503874093294144, ACC:0.96875\n",
      "Training iteration 235 loss: 0.0075232842937111855, ACC:1.0\n",
      "Training iteration 236 loss: 0.041564323008060455, ACC:0.984375\n",
      "Training iteration 237 loss: 0.003234383650124073, ACC:1.0\n",
      "Training iteration 238 loss: 0.0054232459515333176, ACC:1.0\n",
      "Training iteration 239 loss: 0.010203719139099121, ACC:1.0\n",
      "Training iteration 240 loss: 0.005754109472036362, ACC:1.0\n",
      "Training iteration 241 loss: 0.004834434483200312, ACC:1.0\n",
      "Training iteration 242 loss: 0.009069914929568768, ACC:1.0\n",
      "Training iteration 243 loss: 0.05424031242728233, ACC:0.984375\n",
      "Training iteration 244 loss: 0.007662034127861261, ACC:1.0\n",
      "Training iteration 245 loss: 0.002176092006266117, ACC:1.0\n",
      "Training iteration 246 loss: 0.005979839712381363, ACC:1.0\n",
      "Training iteration 247 loss: 0.0028068930841982365, ACC:1.0\n",
      "Training iteration 248 loss: 0.07695987820625305, ACC:0.96875\n",
      "Training iteration 249 loss: 0.003731326898559928, ACC:1.0\n",
      "Training iteration 250 loss: 0.05641802400350571, ACC:0.984375\n",
      "Training iteration 251 loss: 0.0030506907496601343, ACC:1.0\n",
      "Training iteration 252 loss: 0.0431504026055336, ACC:0.984375\n",
      "Training iteration 253 loss: 0.0053576757200062275, ACC:1.0\n",
      "Training iteration 254 loss: 0.04887019842863083, ACC:0.984375\n",
      "Training iteration 255 loss: 0.005747276823967695, ACC:1.0\n",
      "Training iteration 256 loss: 0.0214817076921463, ACC:1.0\n",
      "Training iteration 257 loss: 0.030591709539294243, ACC:0.984375\n",
      "Training iteration 258 loss: 0.005556522402912378, ACC:1.0\n",
      "Training iteration 259 loss: 0.015385892242193222, ACC:1.0\n",
      "Training iteration 260 loss: 0.17005258798599243, ACC:0.984375\n",
      "Training iteration 261 loss: 0.1542433202266693, ACC:0.9375\n",
      "Training iteration 262 loss: 0.004208640661090612, ACC:1.0\n",
      "Training iteration 263 loss: 0.002250740770250559, ACC:1.0\n",
      "Training iteration 264 loss: 0.06195073574781418, ACC:0.984375\n",
      "Training iteration 265 loss: 0.003381946822628379, ACC:1.0\n",
      "Training iteration 266 loss: 0.002869890071451664, ACC:1.0\n",
      "Training iteration 267 loss: 0.025545045733451843, ACC:0.984375\n",
      "Training iteration 268 loss: 0.0031037279404699802, ACC:1.0\n",
      "Training iteration 269 loss: 0.0036476750392466784, ACC:1.0\n",
      "Training iteration 270 loss: 0.08040399104356766, ACC:0.984375\n",
      "Training iteration 271 loss: 0.010552355088293552, ACC:1.0\n",
      "Training iteration 272 loss: 0.008069157600402832, ACC:1.0\n",
      "Training iteration 273 loss: 0.09481586515903473, ACC:0.96875\n",
      "Training iteration 274 loss: 0.03176324814558029, ACC:0.984375\n",
      "Training iteration 275 loss: 0.05116928741335869, ACC:0.96875\n",
      "Training iteration 276 loss: 0.0023684422485530376, ACC:1.0\n",
      "Training iteration 277 loss: 0.00613054633140564, ACC:1.0\n",
      "Training iteration 278 loss: 0.007689743302762508, ACC:1.0\n",
      "Training iteration 279 loss: 0.0032694044057279825, ACC:1.0\n",
      "Training iteration 280 loss: 0.03673742339015007, ACC:0.984375\n",
      "Training iteration 281 loss: 0.005369038786739111, ACC:1.0\n",
      "Training iteration 282 loss: 0.004046906251460314, ACC:1.0\n",
      "Training iteration 283 loss: 0.00349879520945251, ACC:1.0\n",
      "Training iteration 284 loss: 0.0037273799534887075, ACC:1.0\n",
      "Training iteration 285 loss: 0.022688131779432297, ACC:0.984375\n",
      "Training iteration 286 loss: 0.006995853036642075, ACC:1.0\n",
      "Training iteration 287 loss: 0.005202201195061207, ACC:1.0\n",
      "Training iteration 288 loss: 0.013785802759230137, ACC:1.0\n",
      "Training iteration 289 loss: 0.010304293595254421, ACC:1.0\n",
      "Training iteration 290 loss: 0.013926221989095211, ACC:1.0\n",
      "Training iteration 291 loss: 0.1861899346113205, ACC:0.984375\n",
      "Training iteration 292 loss: 0.0017142388969659805, ACC:1.0\n",
      "Training iteration 293 loss: 0.005795369856059551, ACC:1.0\n",
      "Training iteration 294 loss: 0.0010454491712152958, ACC:1.0\n",
      "Training iteration 295 loss: 0.0023074839264154434, ACC:1.0\n",
      "Training iteration 296 loss: 0.012700016610324383, ACC:1.0\n",
      "Training iteration 297 loss: 0.0018918924033641815, ACC:1.0\n",
      "Training iteration 298 loss: 0.004679592326283455, ACC:1.0\n",
      "Training iteration 299 loss: 0.0007315354887396097, ACC:1.0\n",
      "Training iteration 300 loss: 0.0011705405777320266, ACC:1.0\n",
      "Training iteration 301 loss: 0.04611358046531677, ACC:0.984375\n",
      "Training iteration 302 loss: 0.07025836408138275, ACC:0.984375\n",
      "Training iteration 303 loss: 0.0102317463606596, ACC:1.0\n",
      "Training iteration 304 loss: 0.000566707516554743, ACC:1.0\n",
      "Training iteration 305 loss: 0.0036592260003089905, ACC:1.0\n",
      "Training iteration 306 loss: 0.0015795762883499265, ACC:1.0\n",
      "Training iteration 307 loss: 0.0012592785060405731, ACC:1.0\n",
      "Training iteration 308 loss: 0.006726081017404795, ACC:1.0\n",
      "Training iteration 309 loss: 0.0007495853351429105, ACC:1.0\n",
      "Training iteration 310 loss: 0.0013744971947744489, ACC:1.0\n",
      "Training iteration 311 loss: 0.004121783189475536, ACC:1.0\n",
      "Training iteration 312 loss: 0.0012377663515508175, ACC:1.0\n",
      "Training iteration 313 loss: 0.000758855720050633, ACC:1.0\n",
      "Training iteration 314 loss: 0.000953622511588037, ACC:1.0\n",
      "Training iteration 315 loss: 0.0052988575771451, ACC:1.0\n",
      "Training iteration 316 loss: 0.029425859451293945, ACC:0.984375\n",
      "Training iteration 317 loss: 0.002911527408286929, ACC:1.0\n",
      "Training iteration 318 loss: 0.0017913930350914598, ACC:1.0\n",
      "Training iteration 319 loss: 0.0010662018321454525, ACC:1.0\n",
      "Training iteration 320 loss: 0.2123987376689911, ACC:0.953125\n",
      "Training iteration 321 loss: 0.0006890412769280374, ACC:1.0\n",
      "Training iteration 322 loss: 0.04192008823156357, ACC:0.984375\n",
      "Training iteration 323 loss: 0.0021339564118534327, ACC:1.0\n",
      "Training iteration 324 loss: 0.0009110327810049057, ACC:1.0\n",
      "Training iteration 325 loss: 0.06638064980506897, ACC:0.984375\n",
      "Training iteration 326 loss: 0.0007947449339553714, ACC:1.0\n",
      "Training iteration 327 loss: 0.01779521256685257, ACC:0.984375\n",
      "Training iteration 328 loss: 0.0011360165663063526, ACC:1.0\n",
      "Training iteration 329 loss: 0.0013061849167570472, ACC:1.0\n",
      "Training iteration 330 loss: 0.003142016474157572, ACC:1.0\n",
      "Training iteration 331 loss: 0.009477590210735798, ACC:1.0\n",
      "Training iteration 332 loss: 0.019691700115799904, ACC:1.0\n",
      "Training iteration 333 loss: 0.0025908113457262516, ACC:1.0\n",
      "Training iteration 334 loss: 0.010657292790710926, ACC:1.0\n",
      "Training iteration 335 loss: 0.008365432731807232, ACC:1.0\n",
      "Training iteration 336 loss: 0.0036825197748839855, ACC:1.0\n",
      "Training iteration 337 loss: 0.009512720629572868, ACC:1.0\n",
      "Training iteration 338 loss: 0.004083547275513411, ACC:1.0\n",
      "Training iteration 339 loss: 0.04470358416438103, ACC:0.984375\n",
      "Training iteration 340 loss: 0.004627092741429806, ACC:1.0\n",
      "Training iteration 341 loss: 0.001255420851521194, ACC:1.0\n",
      "Training iteration 342 loss: 0.02457542158663273, ACC:0.984375\n",
      "Training iteration 343 loss: 0.0016287325415760279, ACC:1.0\n",
      "Training iteration 344 loss: 0.03494298458099365, ACC:0.984375\n",
      "Training iteration 345 loss: 0.002993551082909107, ACC:1.0\n",
      "Training iteration 346 loss: 0.002365345833823085, ACC:1.0\n",
      "Training iteration 347 loss: 0.015005200169980526, ACC:1.0\n",
      "Training iteration 348 loss: 0.0029725090134888887, ACC:1.0\n",
      "Training iteration 349 loss: 0.012211919762194157, ACC:1.0\n",
      "Training iteration 350 loss: 0.0010956361657008529, ACC:1.0\n",
      "Training iteration 351 loss: 0.003559984266757965, ACC:1.0\n",
      "Training iteration 352 loss: 0.0429457388818264, ACC:0.984375\n",
      "Training iteration 353 loss: 0.004591098055243492, ACC:1.0\n",
      "Training iteration 354 loss: 0.01047474890947342, ACC:1.0\n",
      "Training iteration 355 loss: 0.013126439414918423, ACC:1.0\n",
      "Training iteration 356 loss: 0.04030168801546097, ACC:0.984375\n",
      "Training iteration 357 loss: 0.03014511615037918, ACC:0.984375\n",
      "Training iteration 358 loss: 0.05513336881995201, ACC:0.96875\n",
      "Training iteration 359 loss: 0.015879863873124123, ACC:0.984375\n",
      "Training iteration 360 loss: 0.009198385290801525, ACC:1.0\n",
      "Training iteration 361 loss: 0.06208739057183266, ACC:0.984375\n",
      "Training iteration 362 loss: 0.03756188973784447, ACC:0.984375\n",
      "Training iteration 363 loss: 0.011780163273215294, ACC:1.0\n",
      "Training iteration 364 loss: 0.014586972072720528, ACC:0.984375\n",
      "Training iteration 365 loss: 0.20397649705410004, ACC:0.96875\n",
      "Training iteration 366 loss: 0.0022517251782119274, ACC:1.0\n",
      "Training iteration 367 loss: 0.0033760364167392254, ACC:1.0\n",
      "Training iteration 368 loss: 0.006076335441321135, ACC:1.0\n",
      "Training iteration 369 loss: 0.0027530265506356955, ACC:1.0\n",
      "Training iteration 370 loss: 0.0060707153752446175, ACC:1.0\n",
      "Training iteration 371 loss: 0.0823836550116539, ACC:0.984375\n",
      "Training iteration 372 loss: 0.0013723473530262709, ACC:1.0\n",
      "Training iteration 373 loss: 0.003019602969288826, ACC:1.0\n",
      "Training iteration 374 loss: 0.0048392461612820625, ACC:1.0\n",
      "Training iteration 375 loss: 0.059031400829553604, ACC:0.984375\n",
      "Training iteration 376 loss: 0.0018312680767849088, ACC:1.0\n",
      "Training iteration 377 loss: 0.008320504799485207, ACC:1.0\n",
      "Training iteration 378 loss: 0.016019567847251892, ACC:1.0\n",
      "Training iteration 379 loss: 0.00941307470202446, ACC:1.0\n",
      "Training iteration 380 loss: 0.013024174608290195, ACC:1.0\n",
      "Training iteration 381 loss: 0.0262929517775774, ACC:0.984375\n",
      "Training iteration 382 loss: 0.005199539475142956, ACC:1.0\n",
      "Training iteration 383 loss: 0.08146138489246368, ACC:0.984375\n",
      "Training iteration 384 loss: 0.014600054360926151, ACC:1.0\n",
      "Training iteration 385 loss: 0.008386963978409767, ACC:1.0\n",
      "Training iteration 386 loss: 0.06517992913722992, ACC:0.96875\n",
      "Training iteration 387 loss: 0.00912313163280487, ACC:1.0\n",
      "Training iteration 388 loss: 0.025936201214790344, ACC:0.984375\n",
      "Training iteration 389 loss: 0.004902961663901806, ACC:1.0\n",
      "Training iteration 390 loss: 0.05913791060447693, ACC:0.984375\n",
      "Training iteration 391 loss: 0.025157758966088295, ACC:0.984375\n",
      "Training iteration 392 loss: 0.05261370539665222, ACC:0.96875\n",
      "Training iteration 393 loss: 0.004278385080397129, ACC:1.0\n",
      "Training iteration 394 loss: 0.024245403707027435, ACC:0.984375\n",
      "Training iteration 395 loss: 0.01657232828438282, ACC:1.0\n",
      "Training iteration 396 loss: 0.038955893367528915, ACC:0.984375\n",
      "Training iteration 397 loss: 0.01494633685797453, ACC:1.0\n",
      "Training iteration 398 loss: 0.014107592403888702, ACC:1.0\n",
      "Training iteration 399 loss: 0.0056343842297792435, ACC:1.0\n",
      "Training iteration 400 loss: 0.0053494274616241455, ACC:1.0\n",
      "Training iteration 401 loss: 0.008454833179712296, ACC:1.0\n",
      "Training iteration 402 loss: 0.017345042899250984, ACC:0.984375\n",
      "Training iteration 403 loss: 0.005272675305604935, ACC:1.0\n",
      "Training iteration 404 loss: 0.08338688313961029, ACC:0.984375\n",
      "Training iteration 405 loss: 0.00867035984992981, ACC:1.0\n",
      "Training iteration 406 loss: 0.05610226094722748, ACC:0.984375\n",
      "Training iteration 407 loss: 0.0013959878124296665, ACC:1.0\n",
      "Training iteration 408 loss: 0.006113096605986357, ACC:1.0\n",
      "Training iteration 409 loss: 0.001250830595381558, ACC:1.0\n",
      "Training iteration 410 loss: 0.0019049235852435231, ACC:1.0\n",
      "Training iteration 411 loss: 0.0036231670528650284, ACC:1.0\n",
      "Training iteration 412 loss: 0.004501651972532272, ACC:1.0\n",
      "Training iteration 413 loss: 0.006220960523933172, ACC:1.0\n",
      "Training iteration 414 loss: 0.00487359007820487, ACC:1.0\n",
      "Training iteration 415 loss: 0.0075148483738303185, ACC:1.0\n",
      "Training iteration 416 loss: 0.009400444105267525, ACC:1.0\n",
      "Training iteration 417 loss: 0.0829046368598938, ACC:0.984375\n",
      "Training iteration 418 loss: 0.0016054067527875304, ACC:1.0\n",
      "Training iteration 419 loss: 0.0031250750180333853, ACC:1.0\n",
      "Training iteration 420 loss: 0.0013213623315095901, ACC:1.0\n",
      "Training iteration 421 loss: 0.06216359883546829, ACC:0.984375\n",
      "Training iteration 422 loss: 0.0012527755461633205, ACC:1.0\n",
      "Training iteration 423 loss: 0.021899744868278503, ACC:0.984375\n",
      "Training iteration 424 loss: 0.008132975548505783, ACC:1.0\n",
      "Training iteration 425 loss: 0.058735452592372894, ACC:0.984375\n",
      "Training iteration 426 loss: 0.004406350199133158, ACC:1.0\n",
      "Training iteration 427 loss: 0.001612626714631915, ACC:1.0\n",
      "Training iteration 428 loss: 0.0013950754655525088, ACC:1.0\n",
      "Training iteration 429 loss: 0.004746846854686737, ACC:1.0\n",
      "Training iteration 430 loss: 0.001974675804376602, ACC:1.0\n",
      "Training iteration 431 loss: 0.0009559880127198994, ACC:1.0\n",
      "Training iteration 432 loss: 0.00516203697770834, ACC:1.0\n",
      "Training iteration 433 loss: 0.21407073736190796, ACC:0.953125\n",
      "Training iteration 434 loss: 0.001083026290871203, ACC:1.0\n",
      "Training iteration 435 loss: 0.012754259631037712, ACC:1.0\n",
      "Training iteration 436 loss: 0.002846744377166033, ACC:1.0\n",
      "Training iteration 437 loss: 0.002036383142694831, ACC:1.0\n",
      "Training iteration 438 loss: 0.0026719416491687298, ACC:1.0\n",
      "Training iteration 439 loss: 0.04876583069562912, ACC:0.984375\n",
      "Training iteration 440 loss: 0.001565176178701222, ACC:1.0\n",
      "Training iteration 441 loss: 0.007986432872712612, ACC:1.0\n",
      "Training iteration 442 loss: 0.0019621949177235365, ACC:1.0\n",
      "Training iteration 443 loss: 0.02650480717420578, ACC:0.984375\n",
      "Training iteration 444 loss: 0.0015435548266395926, ACC:1.0\n",
      "Training iteration 445 loss: 0.01090520340949297, ACC:1.0\n",
      "Training iteration 446 loss: 0.004119697492569685, ACC:1.0\n",
      "Training iteration 447 loss: 0.02244088053703308, ACC:1.0\n",
      "Training iteration 448 loss: 0.022465914487838745, ACC:0.984375\n",
      "Training iteration 449 loss: 0.021607711911201477, ACC:0.984375\n",
      "Training iteration 450 loss: 0.0018377213273197412, ACC:1.0\n",
      "Validation iteration 451 loss: 0.002546729752793908, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.001277226023375988, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.027069412171840668, ACC: 0.984375\n",
      "Validation iteration 454 loss: 0.0012997656594961882, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.0021363229025155306, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.004132202826440334, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.003598293289542198, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.0011346123646944761, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.08344170451164246, ACC: 0.96875\n",
      "Validation iteration 460 loss: 0.00618608295917511, ACC: 1.0\n",
      "Validation iteration 461 loss: 0.0016816923161968589, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.001412788755260408, ACC: 1.0\n",
      "Validation iteration 463 loss: 0.011501770466566086, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.0014738633763045073, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.027846865355968475, ACC: 0.984375\n",
      "Validation iteration 466 loss: 0.10704070329666138, ACC: 0.984375\n",
      "Validation iteration 467 loss: 0.0009335114737041295, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.0013593750772997737, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.0024168710224330425, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.08602216839790344, ACC: 0.984375\n",
      "Validation iteration 471 loss: 0.0007919276831671596, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.0023762574419379234, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.02580498717725277, ACC: 0.984375\n",
      "Validation iteration 474 loss: 0.0024234172888100147, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.002420185599476099, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.004807503893971443, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.001326977158896625, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.048457443714141846, ACC: 0.984375\n",
      "Validation iteration 479 loss: 0.003459111088886857, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.06008165702223778, ACC: 0.984375\n",
      "Validation iteration 481 loss: 0.0116681894287467, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.04401497170329094, ACC: 0.984375\n",
      "Validation iteration 483 loss: 0.016832929104566574, ACC: 0.984375\n",
      "Validation iteration 484 loss: 0.0011373928282409906, ACC: 1.0\n",
      "Validation iteration 485 loss: 0.00819921214133501, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.002314263489097357, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.027422413229942322, ACC: 0.984375\n",
      "Validation iteration 488 loss: 0.004701402969658375, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.02826748602092266, ACC: 0.984375\n",
      "Validation iteration 490 loss: 0.05520515888929367, ACC: 0.984375\n",
      "Validation iteration 491 loss: 0.03154522925615311, ACC: 0.96875\n",
      "Validation iteration 492 loss: 0.01842341385781765, ACC: 0.984375\n",
      "Validation iteration 493 loss: 0.030261367559432983, ACC: 0.984375\n",
      "Validation iteration 494 loss: 0.002625707071274519, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.0012409049086272717, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.001636655768379569, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.0009149049874395132, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.012201730161905289, ACC: 0.984375\n",
      "Validation iteration 499 loss: 0.04569613188505173, ACC: 0.984375\n",
      "Validation iteration 500 loss: 0.013940464705228806, ACC: 1.0\n",
      "-- Epoch 7 done -- Train loss: 0.020442760746809653, train ACC: 0.9945833333333334, val loss: 0.01769422720069997, val ACC: 0.99375\n",
      "<--- 20279.254807710648 seconds --->\n",
      "Training iteration 1 loss: 0.0028751930221915245, ACC:1.0\n",
      "Training iteration 2 loss: 0.0011789596173912287, ACC:1.0\n",
      "Training iteration 3 loss: 0.0016167769208550453, ACC:1.0\n",
      "Training iteration 4 loss: 0.0064429109916090965, ACC:1.0\n",
      "Training iteration 5 loss: 0.0031911744736135006, ACC:1.0\n",
      "Training iteration 6 loss: 0.0013605026761069894, ACC:1.0\n",
      "Training iteration 7 loss: 0.016309542581439018, ACC:0.984375\n",
      "Training iteration 8 loss: 0.0012367991730570793, ACC:1.0\n",
      "Training iteration 9 loss: 0.0022103236988186836, ACC:1.0\n",
      "Training iteration 10 loss: 0.0016532718436792493, ACC:1.0\n",
      "Training iteration 11 loss: 0.0018104850314557552, ACC:1.0\n",
      "Training iteration 12 loss: 0.003409961936995387, ACC:1.0\n",
      "Training iteration 13 loss: 0.004914166405797005, ACC:1.0\n",
      "Training iteration 14 loss: 0.04123242199420929, ACC:0.984375\n",
      "Training iteration 15 loss: 0.005449152551591396, ACC:1.0\n",
      "Training iteration 16 loss: 0.001165522262454033, ACC:1.0\n",
      "Training iteration 17 loss: 0.013600748032331467, ACC:0.984375\n",
      "Training iteration 18 loss: 0.013834179379045963, ACC:0.984375\n",
      "Training iteration 19 loss: 0.0004276687395758927, ACC:1.0\n",
      "Training iteration 20 loss: 0.09738203883171082, ACC:0.984375\n",
      "Training iteration 21 loss: 0.004994336515665054, ACC:1.0\n",
      "Training iteration 22 loss: 0.01937350071966648, ACC:0.984375\n",
      "Training iteration 23 loss: 0.010660471394658089, ACC:1.0\n",
      "Training iteration 24 loss: 0.10403444617986679, ACC:0.984375\n",
      "Training iteration 25 loss: 0.039336565881967545, ACC:0.96875\n",
      "Training iteration 26 loss: 0.0009869042551144958, ACC:1.0\n",
      "Training iteration 27 loss: 0.0014123984146863222, ACC:1.0\n",
      "Training iteration 28 loss: 0.0010997396893799305, ACC:1.0\n",
      "Training iteration 29 loss: 0.0013045266969129443, ACC:1.0\n",
      "Training iteration 30 loss: 0.01648220606148243, ACC:0.984375\n",
      "Training iteration 31 loss: 0.0007920669158920646, ACC:1.0\n",
      "Training iteration 32 loss: 0.0047611030749976635, ACC:1.0\n",
      "Training iteration 33 loss: 0.07577565312385559, ACC:0.984375\n",
      "Training iteration 34 loss: 0.07811470329761505, ACC:0.984375\n",
      "Training iteration 35 loss: 0.0020703813061118126, ACC:1.0\n",
      "Training iteration 36 loss: 0.019296692684292793, ACC:0.984375\n",
      "Training iteration 37 loss: 0.0022251177579164505, ACC:1.0\n",
      "Training iteration 38 loss: 0.0029581773560494184, ACC:1.0\n",
      "Training iteration 39 loss: 0.00802633073180914, ACC:1.0\n",
      "Training iteration 40 loss: 0.0014890060992911458, ACC:1.0\n",
      "Training iteration 41 loss: 0.01531421672552824, ACC:1.0\n",
      "Training iteration 42 loss: 0.010430044494569302, ACC:1.0\n",
      "Training iteration 43 loss: 0.0934557095170021, ACC:0.984375\n",
      "Training iteration 44 loss: 0.0029050754383206367, ACC:1.0\n",
      "Training iteration 45 loss: 0.0015524385962635279, ACC:1.0\n",
      "Training iteration 46 loss: 0.004764392040669918, ACC:1.0\n",
      "Training iteration 47 loss: 0.002323215128853917, ACC:1.0\n",
      "Training iteration 48 loss: 0.0021368740126490593, ACC:1.0\n",
      "Training iteration 49 loss: 0.01291021890938282, ACC:1.0\n",
      "Training iteration 50 loss: 0.0015476590488106012, ACC:1.0\n",
      "Training iteration 51 loss: 0.0014826948754489422, ACC:1.0\n",
      "Training iteration 52 loss: 0.0010675110388547182, ACC:1.0\n",
      "Training iteration 53 loss: 0.02723718248307705, ACC:0.984375\n",
      "Training iteration 54 loss: 0.040094923228025436, ACC:0.984375\n",
      "Training iteration 55 loss: 0.002594495192170143, ACC:1.0\n",
      "Training iteration 56 loss: 0.022626671940088272, ACC:0.984375\n",
      "Training iteration 57 loss: 0.005512297619134188, ACC:1.0\n",
      "Training iteration 58 loss: 0.004527116660028696, ACC:1.0\n",
      "Training iteration 59 loss: 0.011384408921003342, ACC:1.0\n",
      "Training iteration 60 loss: 0.01345077995210886, ACC:0.984375\n",
      "Training iteration 61 loss: 0.025973612442612648, ACC:0.984375\n",
      "Training iteration 62 loss: 0.0034693372435867786, ACC:1.0\n",
      "Training iteration 63 loss: 0.03614425286650658, ACC:0.984375\n",
      "Training iteration 64 loss: 0.0017180709401145577, ACC:1.0\n",
      "Training iteration 65 loss: 0.002170872176066041, ACC:1.0\n",
      "Training iteration 66 loss: 0.00964856706559658, ACC:1.0\n",
      "Training iteration 67 loss: 0.0015662231016904116, ACC:1.0\n",
      "Training iteration 68 loss: 0.001368698780424893, ACC:1.0\n",
      "Training iteration 69 loss: 0.0015034022508189082, ACC:1.0\n",
      "Training iteration 70 loss: 0.0011440414236858487, ACC:1.0\n",
      "Training iteration 71 loss: 0.0021621445193886757, ACC:1.0\n",
      "Training iteration 72 loss: 0.01300276443362236, ACC:0.984375\n",
      "Training iteration 73 loss: 0.03965425491333008, ACC:0.984375\n",
      "Training iteration 74 loss: 0.0008193753892555833, ACC:1.0\n",
      "Training iteration 75 loss: 0.1505664885044098, ACC:0.96875\n",
      "Training iteration 76 loss: 0.04004259780049324, ACC:0.96875\n",
      "Training iteration 77 loss: 0.05405087396502495, ACC:0.984375\n",
      "Training iteration 78 loss: 0.006257341243326664, ACC:1.0\n",
      "Training iteration 79 loss: 0.012957467697560787, ACC:0.984375\n",
      "Training iteration 80 loss: 0.021358206868171692, ACC:0.984375\n",
      "Training iteration 81 loss: 0.003494999837130308, ACC:1.0\n",
      "Training iteration 82 loss: 0.03721829503774643, ACC:0.96875\n",
      "Training iteration 83 loss: 0.013396186754107475, ACC:1.0\n",
      "Training iteration 84 loss: 0.004886272829025984, ACC:1.0\n",
      "Training iteration 85 loss: 0.00720135448500514, ACC:1.0\n",
      "Training iteration 86 loss: 0.02779400162398815, ACC:0.984375\n",
      "Training iteration 87 loss: 0.002255284460261464, ACC:1.0\n",
      "Training iteration 88 loss: 0.0011129111517220736, ACC:1.0\n",
      "Training iteration 89 loss: 0.004586612340062857, ACC:1.0\n",
      "Training iteration 90 loss: 0.002483634976670146, ACC:1.0\n",
      "Training iteration 91 loss: 0.004626049660146236, ACC:1.0\n",
      "Training iteration 92 loss: 0.08999153971672058, ACC:0.984375\n",
      "Training iteration 93 loss: 0.09568328410387039, ACC:0.953125\n",
      "Training iteration 94 loss: 0.14897151291370392, ACC:0.953125\n",
      "Training iteration 95 loss: 0.19215285778045654, ACC:0.953125\n",
      "Training iteration 96 loss: 0.005830835551023483, ACC:1.0\n",
      "Training iteration 97 loss: 0.008391030132770538, ACC:1.0\n",
      "Training iteration 98 loss: 0.020646028220653534, ACC:1.0\n",
      "Training iteration 99 loss: 0.118461012840271, ACC:0.96875\n",
      "Training iteration 100 loss: 0.026127126067876816, ACC:0.984375\n",
      "Training iteration 101 loss: 0.09210225194692612, ACC:0.953125\n",
      "Training iteration 102 loss: 0.02376292273402214, ACC:0.984375\n",
      "Training iteration 103 loss: 0.02469518780708313, ACC:0.984375\n",
      "Training iteration 104 loss: 0.004695643205195665, ACC:1.0\n",
      "Training iteration 105 loss: 0.01491138432174921, ACC:1.0\n",
      "Training iteration 106 loss: 0.02571692131459713, ACC:0.984375\n",
      "Training iteration 107 loss: 0.002401140285655856, ACC:1.0\n",
      "Training iteration 108 loss: 0.002213832689449191, ACC:1.0\n",
      "Training iteration 109 loss: 0.0020687889773398638, ACC:1.0\n",
      "Training iteration 110 loss: 0.003033038694411516, ACC:1.0\n",
      "Training iteration 111 loss: 0.0013907476095482707, ACC:1.0\n",
      "Training iteration 112 loss: 0.0018436296377331018, ACC:1.0\n",
      "Training iteration 113 loss: 0.021122867241501808, ACC:0.984375\n",
      "Training iteration 114 loss: 0.1824226975440979, ACC:0.953125\n",
      "Training iteration 115 loss: 0.009541069157421589, ACC:1.0\n",
      "Training iteration 116 loss: 0.0671628937125206, ACC:0.984375\n",
      "Training iteration 117 loss: 0.11340583860874176, ACC:0.96875\n",
      "Training iteration 118 loss: 0.0018718073843047023, ACC:1.0\n",
      "Training iteration 119 loss: 0.0034289932809770107, ACC:1.0\n",
      "Training iteration 120 loss: 0.05967845767736435, ACC:0.96875\n",
      "Training iteration 121 loss: 0.005990390665829182, ACC:1.0\n",
      "Training iteration 122 loss: 0.10361940413713455, ACC:0.984375\n",
      "Training iteration 123 loss: 0.03738927096128464, ACC:0.984375\n",
      "Training iteration 124 loss: 0.01622842811048031, ACC:1.0\n",
      "Training iteration 125 loss: 0.057012900710105896, ACC:0.984375\n",
      "Training iteration 126 loss: 0.14177249372005463, ACC:0.953125\n",
      "Training iteration 127 loss: 0.004790252074599266, ACC:1.0\n",
      "Training iteration 128 loss: 0.006074100732803345, ACC:1.0\n",
      "Training iteration 129 loss: 0.03350825607776642, ACC:0.984375\n",
      "Training iteration 130 loss: 0.008094876073300838, ACC:1.0\n",
      "Training iteration 131 loss: 0.07369755208492279, ACC:0.96875\n",
      "Training iteration 132 loss: 0.021935127675533295, ACC:1.0\n",
      "Training iteration 133 loss: 0.0066939061507582664, ACC:1.0\n",
      "Training iteration 134 loss: 0.0037822662852704525, ACC:1.0\n",
      "Training iteration 135 loss: 0.013905074447393417, ACC:0.984375\n",
      "Training iteration 136 loss: 0.025065632537007332, ACC:0.984375\n",
      "Training iteration 137 loss: 0.00841176975518465, ACC:1.0\n",
      "Training iteration 138 loss: 0.08905581384897232, ACC:0.984375\n",
      "Training iteration 139 loss: 0.003162478096783161, ACC:1.0\n",
      "Training iteration 140 loss: 0.0028395764529705048, ACC:1.0\n",
      "Training iteration 141 loss: 0.005005617626011372, ACC:1.0\n",
      "Training iteration 142 loss: 0.005172767676413059, ACC:1.0\n",
      "Training iteration 143 loss: 0.0020327316597104073, ACC:1.0\n",
      "Training iteration 144 loss: 0.1247153952717781, ACC:0.96875\n",
      "Training iteration 145 loss: 0.002407043008133769, ACC:1.0\n",
      "Training iteration 146 loss: 0.0697183683514595, ACC:0.984375\n",
      "Training iteration 147 loss: 0.0020634576212614775, ACC:1.0\n",
      "Training iteration 148 loss: 0.002144445898011327, ACC:1.0\n",
      "Training iteration 149 loss: 0.004149871878325939, ACC:1.0\n",
      "Training iteration 150 loss: 0.002518697874620557, ACC:1.0\n",
      "Training iteration 151 loss: 0.007083149626851082, ACC:1.0\n",
      "Training iteration 152 loss: 0.009902837686240673, ACC:1.0\n",
      "Training iteration 153 loss: 0.0064020911231637, ACC:1.0\n",
      "Training iteration 154 loss: 0.0022108943667262793, ACC:1.0\n",
      "Training iteration 155 loss: 0.0406452976167202, ACC:0.953125\n",
      "Training iteration 156 loss: 0.0018592963460832834, ACC:1.0\n",
      "Training iteration 157 loss: 0.0010714124655351043, ACC:1.0\n",
      "Training iteration 158 loss: 0.0019179318333044648, ACC:1.0\n",
      "Training iteration 159 loss: 0.003256808966398239, ACC:1.0\n",
      "Training iteration 160 loss: 0.0033783908002078533, ACC:1.0\n",
      "Training iteration 161 loss: 0.004934882745146751, ACC:1.0\n",
      "Training iteration 162 loss: 0.05469045415520668, ACC:0.96875\n",
      "Training iteration 163 loss: 0.0027611851692199707, ACC:1.0\n",
      "Training iteration 164 loss: 0.007894064299762249, ACC:1.0\n",
      "Training iteration 165 loss: 0.018831295892596245, ACC:1.0\n",
      "Training iteration 166 loss: 0.010041666217148304, ACC:1.0\n",
      "Training iteration 167 loss: 0.09295308589935303, ACC:0.984375\n",
      "Training iteration 168 loss: 0.0009183579823002219, ACC:1.0\n",
      "Training iteration 169 loss: 0.011670244857668877, ACC:1.0\n",
      "Training iteration 170 loss: 0.0046035670675337315, ACC:1.0\n",
      "Training iteration 171 loss: 0.0041374447755515575, ACC:1.0\n",
      "Training iteration 172 loss: 0.0029090959578752518, ACC:1.0\n",
      "Training iteration 173 loss: 0.04509418457746506, ACC:0.984375\n",
      "Training iteration 174 loss: 0.11350853741168976, ACC:0.953125\n",
      "Training iteration 175 loss: 0.005052217282354832, ACC:1.0\n",
      "Training iteration 176 loss: 0.03806515783071518, ACC:0.984375\n",
      "Training iteration 177 loss: 0.0035974488127976656, ACC:1.0\n",
      "Training iteration 178 loss: 0.0037947793025523424, ACC:1.0\n",
      "Training iteration 179 loss: 0.003792842384427786, ACC:1.0\n",
      "Training iteration 180 loss: 0.005440021399408579, ACC:1.0\n",
      "Training iteration 181 loss: 0.02104523405432701, ACC:0.984375\n",
      "Training iteration 182 loss: 0.01952611468732357, ACC:1.0\n",
      "Training iteration 183 loss: 0.02242211438715458, ACC:0.984375\n",
      "Training iteration 184 loss: 0.02193133346736431, ACC:0.984375\n",
      "Training iteration 185 loss: 0.009929506108164787, ACC:1.0\n",
      "Training iteration 186 loss: 0.012399554252624512, ACC:1.0\n",
      "Training iteration 187 loss: 0.005208035930991173, ACC:1.0\n",
      "Training iteration 188 loss: 0.008396691642701626, ACC:1.0\n",
      "Training iteration 189 loss: 0.07394803315401077, ACC:0.984375\n",
      "Training iteration 190 loss: 0.0631084069609642, ACC:0.96875\n",
      "Training iteration 191 loss: 0.01326699648052454, ACC:1.0\n",
      "Training iteration 192 loss: 0.012154145166277885, ACC:1.0\n",
      "Training iteration 193 loss: 0.028714658692479134, ACC:0.984375\n",
      "Training iteration 194 loss: 0.012090593576431274, ACC:1.0\n",
      "Training iteration 195 loss: 0.003306108061224222, ACC:1.0\n",
      "Training iteration 196 loss: 0.001370675046928227, ACC:1.0\n",
      "Training iteration 197 loss: 0.013668805360794067, ACC:1.0\n",
      "Training iteration 198 loss: 0.0031359673012048006, ACC:1.0\n",
      "Training iteration 199 loss: 0.07272890955209732, ACC:0.984375\n",
      "Training iteration 200 loss: 0.005700975190848112, ACC:1.0\n",
      "Training iteration 201 loss: 0.019646214321255684, ACC:0.984375\n",
      "Training iteration 202 loss: 0.0021526413038372993, ACC:1.0\n",
      "Training iteration 203 loss: 0.0036031343042850494, ACC:1.0\n",
      "Training iteration 204 loss: 0.06595545262098312, ACC:0.984375\n",
      "Training iteration 205 loss: 0.0019785554613918066, ACC:1.0\n",
      "Training iteration 206 loss: 0.028662094846367836, ACC:0.984375\n",
      "Training iteration 207 loss: 0.06954293698072433, ACC:0.984375\n",
      "Training iteration 208 loss: 0.009668895974755287, ACC:1.0\n",
      "Training iteration 209 loss: 0.009076506830751896, ACC:1.0\n",
      "Training iteration 210 loss: 0.003936109133064747, ACC:1.0\n",
      "Training iteration 211 loss: 0.00330945011228323, ACC:1.0\n",
      "Training iteration 212 loss: 0.003361361101269722, ACC:1.0\n",
      "Training iteration 213 loss: 0.007511042524129152, ACC:1.0\n",
      "Training iteration 214 loss: 0.03528444468975067, ACC:0.984375\n",
      "Training iteration 215 loss: 0.003528465051203966, ACC:1.0\n",
      "Training iteration 216 loss: 0.0020464188419282436, ACC:1.0\n",
      "Training iteration 217 loss: 0.014461079612374306, ACC:0.984375\n",
      "Training iteration 218 loss: 0.057192690670490265, ACC:0.984375\n",
      "Training iteration 219 loss: 0.002847162773832679, ACC:1.0\n",
      "Training iteration 220 loss: 0.01860995404422283, ACC:0.984375\n",
      "Training iteration 221 loss: 0.0037458911538124084, ACC:1.0\n",
      "Training iteration 222 loss: 0.08200984448194504, ACC:0.984375\n",
      "Training iteration 223 loss: 0.0022815661504864693, ACC:1.0\n",
      "Training iteration 224 loss: 0.019877534359693527, ACC:0.984375\n",
      "Training iteration 225 loss: 0.005977057386189699, ACC:1.0\n",
      "Training iteration 226 loss: 0.010289291851222515, ACC:1.0\n",
      "Training iteration 227 loss: 0.1099090501666069, ACC:0.96875\n",
      "Training iteration 228 loss: 0.017826516181230545, ACC:0.984375\n",
      "Training iteration 229 loss: 0.01040171179920435, ACC:1.0\n",
      "Training iteration 230 loss: 0.0029156131204217672, ACC:1.0\n",
      "Training iteration 231 loss: 0.004039356019347906, ACC:1.0\n",
      "Training iteration 232 loss: 0.04694361984729767, ACC:0.984375\n",
      "Training iteration 233 loss: 0.001355376560240984, ACC:1.0\n",
      "Training iteration 234 loss: 0.0016066314419731498, ACC:1.0\n",
      "Training iteration 235 loss: 0.08947040885686874, ACC:0.96875\n",
      "Training iteration 236 loss: 0.07001230120658875, ACC:0.984375\n",
      "Training iteration 237 loss: 0.0025892024859786034, ACC:1.0\n",
      "Training iteration 238 loss: 0.003161392640322447, ACC:1.0\n",
      "Training iteration 239 loss: 0.001562313875183463, ACC:1.0\n",
      "Training iteration 240 loss: 0.0012001636205241084, ACC:1.0\n",
      "Training iteration 241 loss: 0.003061805386096239, ACC:1.0\n",
      "Training iteration 242 loss: 0.02234288677573204, ACC:0.984375\n",
      "Training iteration 243 loss: 0.06708743423223495, ACC:0.984375\n",
      "Training iteration 244 loss: 0.010546598583459854, ACC:1.0\n",
      "Training iteration 245 loss: 0.00120366713963449, ACC:1.0\n",
      "Training iteration 246 loss: 0.034948866814374924, ACC:0.96875\n",
      "Training iteration 247 loss: 0.013181219808757305, ACC:0.984375\n",
      "Training iteration 248 loss: 0.019065290689468384, ACC:0.984375\n",
      "Training iteration 249 loss: 0.002023848704993725, ACC:1.0\n",
      "Training iteration 250 loss: 0.002652667462825775, ACC:1.0\n",
      "Training iteration 251 loss: 0.025669699534773827, ACC:0.984375\n",
      "Training iteration 252 loss: 0.001575030735693872, ACC:1.0\n",
      "Training iteration 253 loss: 0.07026390731334686, ACC:0.984375\n",
      "Training iteration 254 loss: 0.11892052739858627, ACC:0.953125\n",
      "Training iteration 255 loss: 0.0009924699552357197, ACC:1.0\n",
      "Training iteration 256 loss: 0.0012574620777741075, ACC:1.0\n",
      "Training iteration 257 loss: 0.07879550755023956, ACC:0.984375\n",
      "Training iteration 258 loss: 0.016017938032746315, ACC:0.984375\n",
      "Training iteration 259 loss: 0.004405723884701729, ACC:1.0\n",
      "Training iteration 260 loss: 0.01273979153484106, ACC:1.0\n",
      "Training iteration 261 loss: 0.1006467267870903, ACC:0.96875\n",
      "Training iteration 262 loss: 0.008084239438176155, ACC:1.0\n",
      "Training iteration 263 loss: 0.005486533511430025, ACC:1.0\n",
      "Training iteration 264 loss: 0.006502676289528608, ACC:1.0\n",
      "Training iteration 265 loss: 0.02383645437657833, ACC:0.984375\n",
      "Training iteration 266 loss: 0.002964470535516739, ACC:1.0\n",
      "Training iteration 267 loss: 0.008696502074599266, ACC:1.0\n",
      "Training iteration 268 loss: 0.04863309487700462, ACC:0.96875\n",
      "Training iteration 269 loss: 0.0034309253096580505, ACC:1.0\n",
      "Training iteration 270 loss: 0.0016254314687103033, ACC:1.0\n",
      "Training iteration 271 loss: 0.05814516916871071, ACC:0.96875\n",
      "Training iteration 272 loss: 0.002022606786340475, ACC:1.0\n",
      "Training iteration 273 loss: 0.0037269247695803642, ACC:1.0\n",
      "Training iteration 274 loss: 0.010844722390174866, ACC:1.0\n",
      "Training iteration 275 loss: 0.0018865910824388266, ACC:1.0\n",
      "Training iteration 276 loss: 0.011471322737634182, ACC:1.0\n",
      "Training iteration 277 loss: 0.0014525693841278553, ACC:1.0\n",
      "Training iteration 278 loss: 0.016576215624809265, ACC:0.984375\n",
      "Training iteration 279 loss: 0.0017964199651032686, ACC:1.0\n",
      "Training iteration 280 loss: 0.0029161430429667234, ACC:1.0\n",
      "Training iteration 281 loss: 0.012129372917115688, ACC:1.0\n",
      "Training iteration 282 loss: 0.004748919978737831, ACC:1.0\n",
      "Training iteration 283 loss: 0.05364229530096054, ACC:0.96875\n",
      "Training iteration 284 loss: 0.004484930075705051, ACC:1.0\n",
      "Training iteration 285 loss: 0.0009833312360569835, ACC:1.0\n",
      "Training iteration 286 loss: 0.0482158400118351, ACC:0.96875\n",
      "Training iteration 287 loss: 0.004625094123184681, ACC:1.0\n",
      "Training iteration 288 loss: 0.0035950567107647657, ACC:1.0\n",
      "Training iteration 289 loss: 0.0688813254237175, ACC:0.984375\n",
      "Training iteration 290 loss: 0.0015210515120998025, ACC:1.0\n",
      "Training iteration 291 loss: 0.0011302550556138158, ACC:1.0\n",
      "Training iteration 292 loss: 0.00958344154059887, ACC:1.0\n",
      "Training iteration 293 loss: 0.003483904991298914, ACC:1.0\n",
      "Training iteration 294 loss: 0.002972712740302086, ACC:1.0\n",
      "Training iteration 295 loss: 0.0015525335911661386, ACC:1.0\n",
      "Training iteration 296 loss: 0.0016029434045776725, ACC:1.0\n",
      "Training iteration 297 loss: 0.003092221450060606, ACC:1.0\n",
      "Training iteration 298 loss: 0.001576059265062213, ACC:1.0\n",
      "Training iteration 299 loss: 0.004521054215729237, ACC:1.0\n",
      "Training iteration 300 loss: 0.003383076284080744, ACC:1.0\n",
      "Training iteration 301 loss: 0.0037278414238244295, ACC:1.0\n",
      "Training iteration 302 loss: 0.002511790953576565, ACC:1.0\n",
      "Training iteration 303 loss: 0.08411487936973572, ACC:0.984375\n",
      "Training iteration 304 loss: 0.0018273349851369858, ACC:1.0\n",
      "Training iteration 305 loss: 0.0008714971481822431, ACC:1.0\n",
      "Training iteration 306 loss: 0.0010574357584118843, ACC:1.0\n",
      "Training iteration 307 loss: 0.025136040523648262, ACC:0.984375\n",
      "Training iteration 308 loss: 0.0015632177237421274, ACC:1.0\n",
      "Training iteration 309 loss: 0.018361041322350502, ACC:0.984375\n",
      "Training iteration 310 loss: 0.001249791355803609, ACC:1.0\n",
      "Training iteration 311 loss: 0.09718254208564758, ACC:0.984375\n",
      "Training iteration 312 loss: 0.010752946138381958, ACC:1.0\n",
      "Training iteration 313 loss: 0.0009388535981997848, ACC:1.0\n",
      "Training iteration 314 loss: 0.001079866080544889, ACC:1.0\n",
      "Training iteration 315 loss: 0.03772967308759689, ACC:0.984375\n",
      "Training iteration 316 loss: 0.0009034089162014425, ACC:1.0\n",
      "Training iteration 317 loss: 0.10669969767332077, ACC:0.953125\n",
      "Training iteration 318 loss: 0.013314277864992619, ACC:0.984375\n",
      "Training iteration 319 loss: 0.011643699370324612, ACC:1.0\n",
      "Training iteration 320 loss: 0.0022153074387460947, ACC:1.0\n",
      "Training iteration 321 loss: 0.12348560988903046, ACC:0.984375\n",
      "Training iteration 322 loss: 0.00863430742174387, ACC:1.0\n",
      "Training iteration 323 loss: 0.005132168065756559, ACC:1.0\n",
      "Training iteration 324 loss: 0.0024912848602980375, ACC:1.0\n",
      "Training iteration 325 loss: 0.010022792033851147, ACC:1.0\n",
      "Training iteration 326 loss: 0.07891325652599335, ACC:0.984375\n",
      "Training iteration 327 loss: 0.0014218183932825923, ACC:1.0\n",
      "Training iteration 328 loss: 0.0013458410976454616, ACC:1.0\n",
      "Training iteration 329 loss: 0.0015894364332780242, ACC:1.0\n",
      "Training iteration 330 loss: 0.020968666300177574, ACC:0.984375\n",
      "Training iteration 331 loss: 0.016060946509242058, ACC:1.0\n",
      "Training iteration 332 loss: 0.01870903931558132, ACC:0.984375\n",
      "Training iteration 333 loss: 0.0013399911113083363, ACC:1.0\n",
      "Training iteration 334 loss: 0.002793386112898588, ACC:1.0\n",
      "Training iteration 335 loss: 0.007003071717917919, ACC:1.0\n",
      "Training iteration 336 loss: 0.001989274052903056, ACC:1.0\n",
      "Training iteration 337 loss: 0.06223062425851822, ACC:0.984375\n",
      "Training iteration 338 loss: 0.0010418735910207033, ACC:1.0\n",
      "Training iteration 339 loss: 0.026737499982118607, ACC:0.984375\n",
      "Training iteration 340 loss: 0.0015522304456681013, ACC:1.0\n",
      "Training iteration 341 loss: 0.0012204510858282447, ACC:1.0\n",
      "Training iteration 342 loss: 0.033048130571842194, ACC:0.984375\n",
      "Training iteration 343 loss: 0.0021626653615385294, ACC:1.0\n",
      "Training iteration 344 loss: 0.004585720598697662, ACC:1.0\n",
      "Training iteration 345 loss: 0.008771251887083054, ACC:1.0\n",
      "Training iteration 346 loss: 0.00301155517809093, ACC:1.0\n",
      "Training iteration 347 loss: 0.00413794070482254, ACC:1.0\n",
      "Training iteration 348 loss: 0.04269656911492348, ACC:0.984375\n",
      "Training iteration 349 loss: 0.09491276741027832, ACC:0.984375\n",
      "Training iteration 350 loss: 0.0012452846858650446, ACC:1.0\n",
      "Training iteration 351 loss: 0.007077945861965418, ACC:1.0\n",
      "Training iteration 352 loss: 0.009339933283627033, ACC:1.0\n",
      "Training iteration 353 loss: 0.005150072742253542, ACC:1.0\n",
      "Training iteration 354 loss: 0.07464063167572021, ACC:0.984375\n",
      "Training iteration 355 loss: 0.00365238543599844, ACC:1.0\n",
      "Training iteration 356 loss: 0.05644175037741661, ACC:0.984375\n",
      "Training iteration 357 loss: 0.0034473882988095284, ACC:1.0\n",
      "Training iteration 358 loss: 0.0008952068164944649, ACC:1.0\n",
      "Training iteration 359 loss: 0.003935681190341711, ACC:1.0\n",
      "Training iteration 360 loss: 0.0013853030977770686, ACC:1.0\n",
      "Training iteration 361 loss: 0.002874372061342001, ACC:1.0\n",
      "Training iteration 362 loss: 0.004777562338858843, ACC:1.0\n",
      "Training iteration 363 loss: 0.00408317381516099, ACC:1.0\n",
      "Training iteration 364 loss: 0.00242971652187407, ACC:1.0\n",
      "Training iteration 365 loss: 0.0011849227594211698, ACC:1.0\n",
      "Training iteration 366 loss: 0.018549522385001183, ACC:0.984375\n",
      "Training iteration 367 loss: 0.0070958007127046585, ACC:1.0\n",
      "Training iteration 368 loss: 0.09458684176206589, ACC:0.96875\n",
      "Training iteration 369 loss: 0.0014885507989674807, ACC:1.0\n",
      "Training iteration 370 loss: 0.0006934021366760135, ACC:1.0\n",
      "Training iteration 371 loss: 0.0008413532050326467, ACC:1.0\n",
      "Training iteration 372 loss: 0.04019337147474289, ACC:0.984375\n",
      "Training iteration 373 loss: 0.01659368723630905, ACC:0.984375\n",
      "Training iteration 374 loss: 0.00154787371866405, ACC:1.0\n",
      "Training iteration 375 loss: 0.0037536148447543383, ACC:1.0\n",
      "Training iteration 376 loss: 0.0008691851980984211, ACC:1.0\n",
      "Training iteration 377 loss: 0.002447039820253849, ACC:1.0\n",
      "Training iteration 378 loss: 0.0018190671689808369, ACC:1.0\n",
      "Training iteration 379 loss: 0.0007669798796996474, ACC:1.0\n",
      "Training iteration 380 loss: 0.0008540997514501214, ACC:1.0\n",
      "Training iteration 381 loss: 0.0015609747497364879, ACC:1.0\n",
      "Training iteration 382 loss: 0.03766312822699547, ACC:0.96875\n",
      "Training iteration 383 loss: 0.001463532680645585, ACC:1.0\n",
      "Training iteration 384 loss: 0.0008412901079282165, ACC:1.0\n",
      "Training iteration 385 loss: 0.0019969751592725515, ACC:1.0\n",
      "Training iteration 386 loss: 0.008217647671699524, ACC:1.0\n",
      "Training iteration 387 loss: 0.002314292825758457, ACC:1.0\n",
      "Training iteration 388 loss: 0.0025185570120811462, ACC:1.0\n",
      "Training iteration 389 loss: 0.011590835638344288, ACC:1.0\n",
      "Training iteration 390 loss: 0.0013503788504749537, ACC:1.0\n",
      "Training iteration 391 loss: 0.0247433353215456, ACC:0.984375\n",
      "Training iteration 392 loss: 0.06825342029333115, ACC:0.984375\n",
      "Training iteration 393 loss: 0.001130111631937325, ACC:1.0\n",
      "Training iteration 394 loss: 0.0034533219877630472, ACC:1.0\n",
      "Training iteration 395 loss: 0.0009636532049626112, ACC:1.0\n",
      "Training iteration 396 loss: 0.0019140945514664054, ACC:1.0\n",
      "Training iteration 397 loss: 0.0008193710818886757, ACC:1.0\n",
      "Training iteration 398 loss: 0.0023129917681217194, ACC:1.0\n",
      "Training iteration 399 loss: 0.0008601889712736011, ACC:1.0\n",
      "Training iteration 400 loss: 0.07274572551250458, ACC:0.96875\n",
      "Training iteration 401 loss: 0.0023542139679193497, ACC:1.0\n",
      "Training iteration 402 loss: 0.025112638249993324, ACC:0.984375\n",
      "Training iteration 403 loss: 0.0017488441662862897, ACC:1.0\n",
      "Training iteration 404 loss: 0.02474167011678219, ACC:0.984375\n",
      "Training iteration 405 loss: 0.002906091045588255, ACC:1.0\n",
      "Training iteration 406 loss: 0.0015957264695316553, ACC:1.0\n",
      "Training iteration 407 loss: 0.0016958728665485978, ACC:1.0\n",
      "Training iteration 408 loss: 0.001047576661221683, ACC:1.0\n",
      "Training iteration 409 loss: 0.01401027012616396, ACC:0.984375\n",
      "Training iteration 410 loss: 0.013570433482527733, ACC:1.0\n",
      "Training iteration 411 loss: 0.004629090428352356, ACC:1.0\n",
      "Training iteration 412 loss: 0.0022267450112849474, ACC:1.0\n",
      "Training iteration 413 loss: 0.005897198803722858, ACC:1.0\n",
      "Training iteration 414 loss: 0.0023431433364748955, ACC:1.0\n",
      "Training iteration 415 loss: 0.018309472128748894, ACC:0.984375\n",
      "Training iteration 416 loss: 0.005824151448905468, ACC:1.0\n",
      "Training iteration 417 loss: 0.0919075533747673, ACC:0.984375\n",
      "Training iteration 418 loss: 0.0031314303632825613, ACC:1.0\n",
      "Training iteration 419 loss: 0.00424579530954361, ACC:1.0\n",
      "Training iteration 420 loss: 0.03417215496301651, ACC:0.984375\n",
      "Training iteration 421 loss: 0.010944873094558716, ACC:1.0\n",
      "Training iteration 422 loss: 0.003390761325135827, ACC:1.0\n",
      "Training iteration 423 loss: 0.020251978188753128, ACC:0.984375\n",
      "Training iteration 424 loss: 0.01322021335363388, ACC:1.0\n",
      "Training iteration 425 loss: 0.1253538578748703, ACC:0.96875\n",
      "Training iteration 426 loss: 0.01811802200973034, ACC:0.984375\n",
      "Training iteration 427 loss: 0.001778673380613327, ACC:1.0\n",
      "Training iteration 428 loss: 0.004575177561491728, ACC:1.0\n",
      "Training iteration 429 loss: 0.053196102380752563, ACC:0.96875\n",
      "Training iteration 430 loss: 0.010797621682286263, ACC:1.0\n",
      "Training iteration 431 loss: 0.007606517057865858, ACC:1.0\n",
      "Training iteration 432 loss: 0.002449345774948597, ACC:1.0\n",
      "Training iteration 433 loss: 0.031103037297725677, ACC:0.984375\n",
      "Training iteration 434 loss: 0.011520445346832275, ACC:1.0\n",
      "Training iteration 435 loss: 0.004915339406579733, ACC:1.0\n",
      "Training iteration 436 loss: 0.004091834183782339, ACC:1.0\n",
      "Training iteration 437 loss: 0.009496932849287987, ACC:1.0\n",
      "Training iteration 438 loss: 0.027560921385884285, ACC:0.984375\n",
      "Training iteration 439 loss: 0.06210293620824814, ACC:0.984375\n",
      "Training iteration 440 loss: 0.012009612284600735, ACC:1.0\n",
      "Training iteration 441 loss: 0.019275762140750885, ACC:1.0\n",
      "Training iteration 442 loss: 0.16060134768486023, ACC:0.984375\n",
      "Training iteration 443 loss: 0.0024810510221868753, ACC:1.0\n",
      "Training iteration 444 loss: 0.005840244237333536, ACC:1.0\n",
      "Training iteration 445 loss: 0.08166708052158356, ACC:0.984375\n",
      "Training iteration 446 loss: 0.001013916335068643, ACC:1.0\n",
      "Training iteration 447 loss: 0.0016731913201510906, ACC:1.0\n",
      "Training iteration 448 loss: 0.0009719838853925467, ACC:1.0\n",
      "Training iteration 449 loss: 0.001678431173786521, ACC:1.0\n",
      "Training iteration 450 loss: 0.041016317903995514, ACC:0.984375\n",
      "Validation iteration 451 loss: 0.00029037095373496413, ACC: 1.0\n",
      "Validation iteration 452 loss: 0.02088765613734722, ACC: 0.984375\n",
      "Validation iteration 453 loss: 0.010439582169055939, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.005341432522982359, ACC: 1.0\n",
      "Validation iteration 455 loss: 0.043366119265556335, ACC: 0.984375\n",
      "Validation iteration 456 loss: 0.002030236879363656, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.021497342735528946, ACC: 0.984375\n",
      "Validation iteration 458 loss: 0.013965819031000137, ACC: 0.984375\n",
      "Validation iteration 459 loss: 0.0018432853976264596, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.0250005591660738, ACC: 0.984375\n",
      "Validation iteration 461 loss: 0.03226861730217934, ACC: 0.984375\n",
      "Validation iteration 462 loss: 0.02028118446469307, ACC: 0.984375\n",
      "Validation iteration 463 loss: 0.0007954207831062376, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.03877006843686104, ACC: 0.984375\n",
      "Validation iteration 465 loss: 0.0015856740064918995, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.3187398314476013, ACC: 0.921875\n",
      "Validation iteration 467 loss: 0.00042021204717457294, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.08089657872915268, ACC: 0.984375\n",
      "Validation iteration 469 loss: 0.0009490336524322629, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.001090292353183031, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.06037271022796631, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.03267069160938263, ACC: 0.984375\n",
      "Validation iteration 473 loss: 0.01022691372781992, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.035583723336458206, ACC: 0.96875\n",
      "Validation iteration 475 loss: 0.0007429668330587447, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.0014869434526190162, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.022767696529626846, ACC: 0.984375\n",
      "Validation iteration 478 loss: 0.00680322851985693, ACC: 1.0\n",
      "Validation iteration 479 loss: 0.024853041395545006, ACC: 0.984375\n",
      "Validation iteration 480 loss: 0.0009399392292834818, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.00038127778680063784, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.0013571659801527858, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.0041363053023815155, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.0010199528187513351, ACC: 1.0\n",
      "Validation iteration 485 loss: 0.09705883264541626, ACC: 0.96875\n",
      "Validation iteration 486 loss: 0.014770448207855225, ACC: 0.984375\n",
      "Validation iteration 487 loss: 0.0019858423620462418, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.0003466094203758985, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.0021056702826172113, ACC: 1.0\n",
      "Validation iteration 490 loss: 0.014535211957991123, ACC: 0.984375\n",
      "Validation iteration 491 loss: 0.0015252038137987256, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.10600477457046509, ACC: 0.984375\n",
      "Validation iteration 493 loss: 0.006225204560905695, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.03068554401397705, ACC: 0.984375\n",
      "Validation iteration 495 loss: 0.0006987648666836321, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.00037532622809521854, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.021328242495656013, ACC: 0.984375\n",
      "Validation iteration 498 loss: 0.036072708666324615, ACC: 0.984375\n",
      "Validation iteration 499 loss: 0.06059868261218071, ACC: 0.96875\n",
      "Validation iteration 500 loss: 0.04592503607273102, ACC: 0.984375\n",
      "-- Epoch 8 done -- Train loss: 0.020375147740641194, train ACC: 0.9935416666666667, val loss: 0.025680879540159365, val ACC: 0.9903125\n",
      "<--- 20508.192663908005 seconds --->\n",
      "Training iteration 1 loss: 0.031864020973443985, ACC:0.984375\n",
      "Training iteration 2 loss: 0.06825847178697586, ACC:0.96875\n",
      "Training iteration 3 loss: 0.00914103351533413, ACC:1.0\n",
      "Training iteration 4 loss: 0.0015591534320265055, ACC:1.0\n",
      "Training iteration 5 loss: 0.0002607660135254264, ACC:1.0\n",
      "Training iteration 6 loss: 0.0062484280206263065, ACC:1.0\n",
      "Training iteration 7 loss: 0.07024727761745453, ACC:0.984375\n",
      "Training iteration 8 loss: 0.000810518569778651, ACC:1.0\n",
      "Training iteration 9 loss: 0.002569408155977726, ACC:1.0\n",
      "Training iteration 10 loss: 0.0017806128598749638, ACC:1.0\n",
      "Training iteration 11 loss: 0.007974480278789997, ACC:1.0\n",
      "Training iteration 12 loss: 0.07385217398405075, ACC:0.984375\n",
      "Training iteration 13 loss: 0.005502455867826939, ACC:1.0\n",
      "Training iteration 14 loss: 0.013250845484435558, ACC:1.0\n",
      "Training iteration 15 loss: 0.006835131905972958, ACC:1.0\n",
      "Training iteration 16 loss: 0.11000420153141022, ACC:0.96875\n",
      "Training iteration 17 loss: 0.00901604350656271, ACC:1.0\n",
      "Training iteration 18 loss: 0.016249340027570724, ACC:0.984375\n",
      "Training iteration 19 loss: 0.006308705545961857, ACC:1.0\n",
      "Training iteration 20 loss: 0.003803225699812174, ACC:1.0\n",
      "Training iteration 21 loss: 0.010012486949563026, ACC:1.0\n",
      "Training iteration 22 loss: 0.014781626872718334, ACC:1.0\n",
      "Training iteration 23 loss: 0.0017578069819137454, ACC:1.0\n",
      "Training iteration 24 loss: 0.005000874400138855, ACC:1.0\n",
      "Training iteration 25 loss: 0.004128163680434227, ACC:1.0\n",
      "Training iteration 26 loss: 0.020664529874920845, ACC:0.984375\n",
      "Training iteration 27 loss: 0.04157911241054535, ACC:0.984375\n",
      "Training iteration 28 loss: 0.0046783676370978355, ACC:1.0\n",
      "Training iteration 29 loss: 0.0030685127712786198, ACC:1.0\n",
      "Training iteration 30 loss: 0.031174566596746445, ACC:0.96875\n",
      "Training iteration 31 loss: 0.0027486460749059916, ACC:1.0\n",
      "Training iteration 32 loss: 0.004373830743134022, ACC:1.0\n",
      "Training iteration 33 loss: 0.030263492837548256, ACC:0.984375\n",
      "Training iteration 34 loss: 0.006518784444779158, ACC:1.0\n",
      "Training iteration 35 loss: 0.014783390797674656, ACC:1.0\n",
      "Training iteration 36 loss: 0.009461799636483192, ACC:1.0\n",
      "Training iteration 37 loss: 0.002062070183455944, ACC:1.0\n",
      "Training iteration 38 loss: 0.019310271367430687, ACC:0.984375\n",
      "Training iteration 39 loss: 0.03475907817482948, ACC:0.984375\n",
      "Training iteration 40 loss: 0.0007652128697372973, ACC:1.0\n",
      "Training iteration 41 loss: 0.005865753628313541, ACC:1.0\n",
      "Training iteration 42 loss: 0.01128101535141468, ACC:1.0\n",
      "Training iteration 43 loss: 0.016255754977464676, ACC:0.984375\n",
      "Training iteration 44 loss: 0.0024731142912060022, ACC:1.0\n",
      "Training iteration 45 loss: 0.005737612955272198, ACC:1.0\n",
      "Training iteration 46 loss: 0.011683625169098377, ACC:1.0\n",
      "Training iteration 47 loss: 0.003197664860635996, ACC:1.0\n",
      "Training iteration 48 loss: 0.01080579124391079, ACC:1.0\n",
      "Training iteration 49 loss: 0.001713459612801671, ACC:1.0\n",
      "Training iteration 50 loss: 0.004417437128722668, ACC:1.0\n",
      "Training iteration 51 loss: 0.02180923894047737, ACC:0.984375\n",
      "Training iteration 52 loss: 0.010313737206161022, ACC:1.0\n",
      "Training iteration 53 loss: 0.00458885682746768, ACC:1.0\n",
      "Training iteration 54 loss: 0.0011453910265117884, ACC:1.0\n",
      "Training iteration 55 loss: 0.0012982405023649335, ACC:1.0\n",
      "Training iteration 56 loss: 0.015719257295131683, ACC:0.984375\n",
      "Training iteration 57 loss: 0.0003090600948780775, ACC:1.0\n",
      "Training iteration 58 loss: 0.0005255123833194375, ACC:1.0\n",
      "Training iteration 59 loss: 0.0018110740929841995, ACC:1.0\n",
      "Training iteration 60 loss: 0.0003091535472776741, ACC:1.0\n",
      "Training iteration 61 loss: 0.0017755832523107529, ACC:1.0\n",
      "Training iteration 62 loss: 0.0009119025780819356, ACC:1.0\n",
      "Training iteration 63 loss: 0.032295338809490204, ACC:0.984375\n",
      "Training iteration 64 loss: 0.0002446572470944375, ACC:1.0\n",
      "Training iteration 65 loss: 0.0011748645920306444, ACC:1.0\n",
      "Training iteration 66 loss: 0.0015720021910965443, ACC:1.0\n",
      "Training iteration 67 loss: 0.002477633533999324, ACC:1.0\n",
      "Training iteration 68 loss: 0.0023387635592371225, ACC:1.0\n",
      "Training iteration 69 loss: 0.002267364878207445, ACC:1.0\n",
      "Training iteration 70 loss: 0.000587534043006599, ACC:1.0\n",
      "Training iteration 71 loss: 0.0006561882328242064, ACC:1.0\n",
      "Training iteration 72 loss: 0.004285093862563372, ACC:1.0\n",
      "Training iteration 73 loss: 0.0012473315000534058, ACC:1.0\n",
      "Training iteration 74 loss: 0.0039229257963597775, ACC:1.0\n",
      "Training iteration 75 loss: 0.00040385237662121654, ACC:1.0\n",
      "Training iteration 76 loss: 0.0009627824765630066, ACC:1.0\n",
      "Training iteration 77 loss: 0.0016185801941901445, ACC:1.0\n",
      "Training iteration 78 loss: 0.009284834377467632, ACC:1.0\n",
      "Training iteration 79 loss: 0.000761095667257905, ACC:1.0\n",
      "Training iteration 80 loss: 0.0005432483158074319, ACC:1.0\n",
      "Training iteration 81 loss: 0.0009687652345746756, ACC:1.0\n",
      "Training iteration 82 loss: 0.0010704559972509742, ACC:1.0\n",
      "Training iteration 83 loss: 0.001070330967195332, ACC:1.0\n",
      "Training iteration 84 loss: 0.001248425804078579, ACC:1.0\n",
      "Training iteration 85 loss: 0.009395943023264408, ACC:1.0\n",
      "Training iteration 86 loss: 0.006761346478015184, ACC:1.0\n",
      "Training iteration 87 loss: 0.006296723149716854, ACC:1.0\n",
      "Training iteration 88 loss: 0.009150665253400803, ACC:1.0\n",
      "Training iteration 89 loss: 0.050237029790878296, ACC:0.984375\n",
      "Training iteration 90 loss: 0.010317106731235981, ACC:1.0\n",
      "Training iteration 91 loss: 0.0003951887192670256, ACC:1.0\n",
      "Training iteration 92 loss: 0.00048732198774814606, ACC:1.0\n",
      "Training iteration 93 loss: 0.06535802036523819, ACC:0.984375\n",
      "Training iteration 94 loss: 0.031142959371209145, ACC:0.984375\n",
      "Training iteration 95 loss: 0.0001480572682339698, ACC:1.0\n",
      "Training iteration 96 loss: 0.03355424106121063, ACC:0.984375\n",
      "Training iteration 97 loss: 0.000882207415997982, ACC:1.0\n",
      "Training iteration 98 loss: 0.001143821980804205, ACC:1.0\n",
      "Training iteration 99 loss: 0.0019368473440408707, ACC:1.0\n",
      "Training iteration 100 loss: 0.0009047685307450593, ACC:1.0\n",
      "Training iteration 101 loss: 0.00016538469935767353, ACC:1.0\n",
      "Training iteration 102 loss: 0.001526143285445869, ACC:1.0\n",
      "Training iteration 103 loss: 0.002065020613372326, ACC:1.0\n",
      "Training iteration 104 loss: 0.00041576664079912007, ACC:1.0\n",
      "Training iteration 105 loss: 0.00035250926157459617, ACC:1.0\n",
      "Training iteration 106 loss: 0.0005719180917367339, ACC:1.0\n",
      "Training iteration 107 loss: 0.0002758395567070693, ACC:1.0\n",
      "Training iteration 108 loss: 0.002946101129055023, ACC:1.0\n",
      "Training iteration 109 loss: 0.0009741137619130313, ACC:1.0\n",
      "Training iteration 110 loss: 0.01659572124481201, ACC:1.0\n",
      "Training iteration 111 loss: 0.0014722857158631086, ACC:1.0\n",
      "Training iteration 112 loss: 0.08565123379230499, ACC:0.984375\n",
      "Training iteration 113 loss: 0.0011431291932240129, ACC:1.0\n",
      "Training iteration 114 loss: 0.055820874869823456, ACC:0.984375\n",
      "Training iteration 115 loss: 0.0005661176983267069, ACC:1.0\n",
      "Training iteration 116 loss: 0.011041466146707535, ACC:1.0\n",
      "Training iteration 117 loss: 0.0025520212948322296, ACC:1.0\n",
      "Training iteration 118 loss: 0.005113965831696987, ACC:1.0\n",
      "Training iteration 119 loss: 0.009183155372738838, ACC:1.0\n",
      "Training iteration 120 loss: 0.057692158967256546, ACC:0.984375\n",
      "Training iteration 121 loss: 0.06064795330166817, ACC:0.984375\n",
      "Training iteration 122 loss: 0.00423278845846653, ACC:1.0\n",
      "Training iteration 123 loss: 0.009688422083854675, ACC:1.0\n",
      "Training iteration 124 loss: 0.00823559332638979, ACC:1.0\n",
      "Training iteration 125 loss: 0.07071281224489212, ACC:0.96875\n",
      "Training iteration 126 loss: 0.052376631647348404, ACC:0.984375\n",
      "Training iteration 127 loss: 0.006173810921609402, ACC:1.0\n",
      "Training iteration 128 loss: 0.06002387776970863, ACC:0.984375\n",
      "Training iteration 129 loss: 0.0018461945001035929, ACC:1.0\n",
      "Training iteration 130 loss: 0.007662925869226456, ACC:1.0\n",
      "Training iteration 131 loss: 0.002448913874104619, ACC:1.0\n",
      "Training iteration 132 loss: 0.005850760731846094, ACC:1.0\n",
      "Training iteration 133 loss: 0.11052361875772476, ACC:0.953125\n",
      "Training iteration 134 loss: 0.008909654803574085, ACC:1.0\n",
      "Training iteration 135 loss: 0.004335954785346985, ACC:1.0\n",
      "Training iteration 136 loss: 0.0026893694885075092, ACC:1.0\n",
      "Training iteration 137 loss: 0.0016809700755402446, ACC:1.0\n",
      "Training iteration 138 loss: 0.0015671519795432687, ACC:1.0\n",
      "Training iteration 139 loss: 0.002753014676272869, ACC:1.0\n",
      "Training iteration 140 loss: 0.00935932993888855, ACC:1.0\n",
      "Training iteration 141 loss: 0.002651620889082551, ACC:1.0\n",
      "Training iteration 142 loss: 0.0011073146015405655, ACC:1.0\n",
      "Training iteration 143 loss: 0.0006646941183134913, ACC:1.0\n",
      "Training iteration 144 loss: 0.0006867743213661015, ACC:1.0\n",
      "Training iteration 145 loss: 0.0007249033660627902, ACC:1.0\n",
      "Training iteration 146 loss: 0.001835712231695652, ACC:1.0\n",
      "Training iteration 147 loss: 0.0027275450993329287, ACC:1.0\n",
      "Training iteration 148 loss: 0.00037613854510709643, ACC:1.0\n",
      "Training iteration 149 loss: 0.08575967699289322, ACC:0.984375\n",
      "Training iteration 150 loss: 0.0016678213141858578, ACC:1.0\n",
      "Training iteration 151 loss: 0.00046762567944824696, ACC:1.0\n",
      "Training iteration 152 loss: 0.0028252003248780966, ACC:1.0\n",
      "Training iteration 153 loss: 0.0009879410499706864, ACC:1.0\n",
      "Training iteration 154 loss: 0.0011469569290056825, ACC:1.0\n",
      "Training iteration 155 loss: 0.0005405148258432746, ACC:1.0\n",
      "Training iteration 156 loss: 0.0012226789258420467, ACC:1.0\n",
      "Training iteration 157 loss: 0.0010416182922199368, ACC:1.0\n",
      "Training iteration 158 loss: 0.01066967286169529, ACC:1.0\n",
      "Training iteration 159 loss: 0.0005561878788284957, ACC:1.0\n",
      "Training iteration 160 loss: 0.0010414334246888757, ACC:1.0\n",
      "Training iteration 161 loss: 0.00432259775698185, ACC:1.0\n",
      "Training iteration 162 loss: 0.007228629197925329, ACC:1.0\n",
      "Training iteration 163 loss: 0.003440782893449068, ACC:1.0\n",
      "Training iteration 164 loss: 0.006255598273128271, ACC:1.0\n",
      "Training iteration 165 loss: 0.013660057447850704, ACC:1.0\n",
      "Training iteration 166 loss: 0.0009641401120461524, ACC:1.0\n",
      "Training iteration 167 loss: 0.03407997265458107, ACC:0.984375\n",
      "Training iteration 168 loss: 0.028441667556762695, ACC:0.984375\n",
      "Training iteration 169 loss: 0.00957820937037468, ACC:1.0\n",
      "Training iteration 170 loss: 0.003957757260650396, ACC:1.0\n",
      "Training iteration 171 loss: 0.007616060320287943, ACC:1.0\n",
      "Training iteration 172 loss: 0.010996688157320023, ACC:1.0\n",
      "Training iteration 173 loss: 0.010547179728746414, ACC:1.0\n",
      "Training iteration 174 loss: 0.008909697644412518, ACC:1.0\n",
      "Training iteration 175 loss: 0.012400061823427677, ACC:1.0\n",
      "Training iteration 176 loss: 0.027049465104937553, ACC:0.984375\n",
      "Training iteration 177 loss: 0.004456616938114166, ACC:1.0\n",
      "Training iteration 178 loss: 0.006226004101336002, ACC:1.0\n",
      "Training iteration 179 loss: 0.0016911450074985623, ACC:1.0\n",
      "Training iteration 180 loss: 0.0214398056268692, ACC:0.984375\n",
      "Training iteration 181 loss: 0.005363146774470806, ACC:1.0\n",
      "Training iteration 182 loss: 0.1790788322687149, ACC:0.96875\n",
      "Training iteration 183 loss: 0.0041466280817985535, ACC:1.0\n",
      "Training iteration 184 loss: 0.11871452629566193, ACC:0.96875\n",
      "Training iteration 185 loss: 0.029604587703943253, ACC:0.984375\n",
      "Training iteration 186 loss: 0.07851452380418777, ACC:0.984375\n",
      "Training iteration 187 loss: 0.00763603113591671, ACC:1.0\n",
      "Training iteration 188 loss: 0.031057165935635567, ACC:0.984375\n",
      "Training iteration 189 loss: 0.004596008453518152, ACC:1.0\n",
      "Training iteration 190 loss: 0.05263514071702957, ACC:0.984375\n",
      "Training iteration 191 loss: 0.004417683929204941, ACC:1.0\n",
      "Training iteration 192 loss: 0.008822789415717125, ACC:1.0\n",
      "Training iteration 193 loss: 0.0043859477154910564, ACC:1.0\n",
      "Training iteration 194 loss: 0.10731755197048187, ACC:0.953125\n",
      "Training iteration 195 loss: 0.03551027923822403, ACC:0.96875\n",
      "Training iteration 196 loss: 0.01041301991790533, ACC:1.0\n",
      "Training iteration 197 loss: 0.06793036311864853, ACC:0.984375\n",
      "Training iteration 198 loss: 0.0683184564113617, ACC:0.984375\n",
      "Training iteration 199 loss: 0.006675572134554386, ACC:1.0\n",
      "Training iteration 200 loss: 0.010483920574188232, ACC:1.0\n",
      "Training iteration 201 loss: 0.013431286439299583, ACC:1.0\n",
      "Training iteration 202 loss: 0.014626216143369675, ACC:1.0\n",
      "Training iteration 203 loss: 0.04693948104977608, ACC:0.984375\n",
      "Training iteration 204 loss: 0.010458688251674175, ACC:1.0\n",
      "Training iteration 205 loss: 0.008987506851553917, ACC:1.0\n",
      "Training iteration 206 loss: 0.005185581743717194, ACC:1.0\n",
      "Training iteration 207 loss: 0.0036487970501184464, ACC:1.0\n",
      "Training iteration 208 loss: 0.019580604508519173, ACC:0.984375\n",
      "Training iteration 209 loss: 0.004155924543738365, ACC:1.0\n",
      "Training iteration 210 loss: 0.027111496776342392, ACC:0.984375\n",
      "Training iteration 211 loss: 0.00344264879822731, ACC:1.0\n",
      "Training iteration 212 loss: 0.008424192667007446, ACC:1.0\n",
      "Training iteration 213 loss: 0.0033769418951123953, ACC:1.0\n",
      "Training iteration 214 loss: 0.0029448047280311584, ACC:1.0\n",
      "Training iteration 215 loss: 0.03133068233728409, ACC:0.984375\n",
      "Training iteration 216 loss: 0.003029364161193371, ACC:1.0\n",
      "Training iteration 217 loss: 0.002011121017858386, ACC:1.0\n",
      "Training iteration 218 loss: 0.0010759253054857254, ACC:1.0\n",
      "Training iteration 219 loss: 0.021544823423027992, ACC:0.984375\n",
      "Training iteration 220 loss: 0.0052809808403253555, ACC:1.0\n",
      "Training iteration 221 loss: 0.001037299050949514, ACC:1.0\n",
      "Training iteration 222 loss: 0.03582102805376053, ACC:0.984375\n",
      "Training iteration 223 loss: 0.06348226964473724, ACC:0.984375\n",
      "Training iteration 224 loss: 0.03360868617892265, ACC:0.984375\n",
      "Training iteration 225 loss: 0.006168635096400976, ACC:1.0\n",
      "Training iteration 226 loss: 0.08669958263635635, ACC:0.984375\n",
      "Training iteration 227 loss: 0.003553707618266344, ACC:1.0\n",
      "Training iteration 228 loss: 0.09030350297689438, ACC:0.984375\n",
      "Training iteration 229 loss: 0.01961912214756012, ACC:0.984375\n",
      "Training iteration 230 loss: 0.010316663421690464, ACC:1.0\n",
      "Training iteration 231 loss: 0.008351604454219341, ACC:1.0\n",
      "Training iteration 232 loss: 0.04076408967375755, ACC:0.984375\n",
      "Training iteration 233 loss: 0.001293762936256826, ACC:1.0\n",
      "Training iteration 234 loss: 0.0016132349846884608, ACC:1.0\n",
      "Training iteration 235 loss: 0.001318819005973637, ACC:1.0\n",
      "Training iteration 236 loss: 0.0007429606630466878, ACC:1.0\n",
      "Training iteration 237 loss: 0.021049469709396362, ACC:0.984375\n",
      "Training iteration 238 loss: 0.0010162506951019168, ACC:1.0\n",
      "Training iteration 239 loss: 0.01643585041165352, ACC:0.984375\n",
      "Training iteration 240 loss: 0.0050407820381224155, ACC:1.0\n",
      "Training iteration 241 loss: 0.0008811706793494523, ACC:1.0\n",
      "Training iteration 242 loss: 0.1597192883491516, ACC:0.96875\n",
      "Training iteration 243 loss: 0.0010306988842785358, ACC:1.0\n",
      "Training iteration 244 loss: 0.0023968233726918697, ACC:1.0\n",
      "Training iteration 245 loss: 0.004774381406605244, ACC:1.0\n",
      "Training iteration 246 loss: 0.3121001422405243, ACC:0.90625\n",
      "Training iteration 247 loss: 0.044033657759428024, ACC:0.984375\n",
      "Training iteration 248 loss: 0.0008359582861885428, ACC:1.0\n",
      "Training iteration 249 loss: 0.005608214996755123, ACC:1.0\n",
      "Training iteration 250 loss: 0.0008002997492440045, ACC:1.0\n",
      "Training iteration 251 loss: 0.035186439752578735, ACC:0.96875\n",
      "Training iteration 252 loss: 0.006617982871830463, ACC:1.0\n",
      "Training iteration 253 loss: 0.005134972743690014, ACC:1.0\n",
      "Training iteration 254 loss: 0.020111501216888428, ACC:0.984375\n",
      "Training iteration 255 loss: 0.0017891760217025876, ACC:1.0\n",
      "Training iteration 256 loss: 0.004055646248161793, ACC:1.0\n",
      "Training iteration 257 loss: 0.00486035505309701, ACC:1.0\n",
      "Training iteration 258 loss: 0.0036717397160828114, ACC:1.0\n",
      "Training iteration 259 loss: 0.10357081890106201, ACC:0.96875\n",
      "Training iteration 260 loss: 0.015330969356000423, ACC:0.984375\n",
      "Training iteration 261 loss: 0.06274063140153885, ACC:0.96875\n",
      "Training iteration 262 loss: 0.002592479344457388, ACC:1.0\n",
      "Training iteration 263 loss: 0.0029490056913346052, ACC:1.0\n",
      "Training iteration 264 loss: 0.01153978519141674, ACC:1.0\n",
      "Training iteration 265 loss: 0.010562226176261902, ACC:1.0\n",
      "Training iteration 266 loss: 0.009113866835832596, ACC:1.0\n",
      "Training iteration 267 loss: 0.005003312602639198, ACC:1.0\n",
      "Training iteration 268 loss: 0.008977910503745079, ACC:1.0\n",
      "Training iteration 269 loss: 0.0027593013364821672, ACC:1.0\n",
      "Training iteration 270 loss: 0.00903694611042738, ACC:1.0\n",
      "Training iteration 271 loss: 0.001267001498490572, ACC:1.0\n",
      "Training iteration 272 loss: 0.03958786278963089, ACC:0.984375\n",
      "Training iteration 273 loss: 0.004552920814603567, ACC:1.0\n",
      "Training iteration 274 loss: 0.0038411859422922134, ACC:1.0\n",
      "Training iteration 275 loss: 0.014994222670793533, ACC:1.0\n",
      "Training iteration 276 loss: 0.0034186444245278835, ACC:1.0\n",
      "Training iteration 277 loss: 0.0026647034101188183, ACC:1.0\n",
      "Training iteration 278 loss: 0.0011624082690104842, ACC:1.0\n",
      "Training iteration 279 loss: 0.03255542367696762, ACC:0.984375\n",
      "Training iteration 280 loss: 0.001895432942546904, ACC:1.0\n",
      "Training iteration 281 loss: 0.003115881234407425, ACC:1.0\n",
      "Training iteration 282 loss: 0.0007808243972249329, ACC:1.0\n",
      "Training iteration 283 loss: 0.051787830889225006, ACC:0.984375\n",
      "Training iteration 284 loss: 0.01145881786942482, ACC:1.0\n",
      "Training iteration 285 loss: 0.0018282602541148663, ACC:1.0\n",
      "Training iteration 286 loss: 0.02112349309027195, ACC:0.984375\n",
      "Training iteration 287 loss: 0.005414026323705912, ACC:1.0\n",
      "Training iteration 288 loss: 0.0013320475118234754, ACC:1.0\n",
      "Training iteration 289 loss: 0.0088035324588418, ACC:1.0\n",
      "Training iteration 290 loss: 0.06611984968185425, ACC:0.984375\n",
      "Training iteration 291 loss: 0.02855420485138893, ACC:0.984375\n",
      "Training iteration 292 loss: 0.008465549908578396, ACC:1.0\n",
      "Training iteration 293 loss: 0.038696471601724625, ACC:0.984375\n",
      "Training iteration 294 loss: 0.0598125234246254, ACC:0.984375\n",
      "Training iteration 295 loss: 0.0005016576033085585, ACC:1.0\n",
      "Training iteration 296 loss: 0.0020919498056173325, ACC:1.0\n",
      "Training iteration 297 loss: 0.03488124534487724, ACC:0.984375\n",
      "Training iteration 298 loss: 0.0031251246109604836, ACC:1.0\n",
      "Training iteration 299 loss: 0.008871599100530148, ACC:1.0\n",
      "Training iteration 300 loss: 0.001192543306387961, ACC:1.0\n",
      "Training iteration 301 loss: 0.03828850015997887, ACC:0.96875\n",
      "Training iteration 302 loss: 0.017366502434015274, ACC:0.984375\n",
      "Training iteration 303 loss: 0.007173316087573767, ACC:1.0\n",
      "Training iteration 304 loss: 0.01613842509686947, ACC:0.984375\n",
      "Training iteration 305 loss: 0.07027950137853622, ACC:0.984375\n",
      "Training iteration 306 loss: 0.021805496886372566, ACC:0.984375\n",
      "Training iteration 307 loss: 0.003324960358440876, ACC:1.0\n",
      "Training iteration 308 loss: 0.01022801548242569, ACC:1.0\n",
      "Training iteration 309 loss: 0.04645197093486786, ACC:0.984375\n",
      "Training iteration 310 loss: 0.007424216717481613, ACC:1.0\n",
      "Training iteration 311 loss: 0.0010843242052942514, ACC:1.0\n",
      "Training iteration 312 loss: 0.005403431132435799, ACC:1.0\n",
      "Training iteration 313 loss: 0.007528388407081366, ACC:1.0\n",
      "Training iteration 314 loss: 0.006820475682616234, ACC:1.0\n",
      "Training iteration 315 loss: 0.005361602175980806, ACC:1.0\n",
      "Training iteration 316 loss: 0.001294247806072235, ACC:1.0\n",
      "Training iteration 317 loss: 0.04351090267300606, ACC:0.984375\n",
      "Training iteration 318 loss: 0.005122805945575237, ACC:1.0\n",
      "Training iteration 319 loss: 0.03269214183092117, ACC:0.984375\n",
      "Training iteration 320 loss: 0.011049443855881691, ACC:1.0\n",
      "Training iteration 321 loss: 0.0007077641785144806, ACC:1.0\n",
      "Training iteration 322 loss: 0.000567128648981452, ACC:1.0\n",
      "Training iteration 323 loss: 0.005240354686975479, ACC:1.0\n",
      "Training iteration 324 loss: 0.08042768388986588, ACC:0.984375\n",
      "Training iteration 325 loss: 0.027182770892977715, ACC:0.984375\n",
      "Training iteration 326 loss: 0.036309294402599335, ACC:0.984375\n",
      "Training iteration 327 loss: 0.019203010946512222, ACC:0.984375\n",
      "Training iteration 328 loss: 0.0008515217341482639, ACC:1.0\n",
      "Training iteration 329 loss: 0.04884298890829086, ACC:0.984375\n",
      "Training iteration 330 loss: 0.0021144142374396324, ACC:1.0\n",
      "Training iteration 331 loss: 0.0014596421970054507, ACC:1.0\n",
      "Training iteration 332 loss: 0.003177391365170479, ACC:1.0\n",
      "Training iteration 333 loss: 0.006872703321278095, ACC:1.0\n",
      "Training iteration 334 loss: 0.0035041484516113997, ACC:1.0\n",
      "Training iteration 335 loss: 0.08956968039274216, ACC:0.96875\n",
      "Training iteration 336 loss: 0.024429459124803543, ACC:0.984375\n",
      "Training iteration 337 loss: 0.0016495460877195, ACC:1.0\n",
      "Training iteration 338 loss: 0.007995478808879852, ACC:1.0\n",
      "Training iteration 339 loss: 0.0035926352720707655, ACC:1.0\n",
      "Training iteration 340 loss: 0.0018079631263390183, ACC:1.0\n",
      "Training iteration 341 loss: 0.005580530501902103, ACC:1.0\n",
      "Training iteration 342 loss: 0.0021350993774831295, ACC:1.0\n",
      "Training iteration 343 loss: 0.012336964718997478, ACC:1.0\n",
      "Training iteration 344 loss: 0.009918825700879097, ACC:1.0\n",
      "Training iteration 345 loss: 0.0033679381012916565, ACC:1.0\n",
      "Training iteration 346 loss: 0.07910905033349991, ACC:0.984375\n",
      "Training iteration 347 loss: 0.0024451608769595623, ACC:1.0\n",
      "Training iteration 348 loss: 0.015289245173335075, ACC:1.0\n",
      "Training iteration 349 loss: 0.002424926497042179, ACC:1.0\n",
      "Training iteration 350 loss: 0.001523516490124166, ACC:1.0\n",
      "Training iteration 351 loss: 0.004624339286237955, ACC:1.0\n",
      "Training iteration 352 loss: 0.0020459676161408424, ACC:1.0\n",
      "Training iteration 353 loss: 0.05330577865242958, ACC:0.984375\n",
      "Training iteration 354 loss: 0.04476907476782799, ACC:0.96875\n",
      "Training iteration 355 loss: 0.0017782093491405249, ACC:1.0\n",
      "Training iteration 356 loss: 0.017481127753853798, ACC:0.984375\n",
      "Training iteration 357 loss: 0.00585478451102972, ACC:1.0\n",
      "Training iteration 358 loss: 0.002441400894895196, ACC:1.0\n",
      "Training iteration 359 loss: 0.0026623981539160013, ACC:1.0\n",
      "Training iteration 360 loss: 0.1203799843788147, ACC:0.984375\n",
      "Training iteration 361 loss: 0.003906653728336096, ACC:1.0\n",
      "Training iteration 362 loss: 0.002644824795424938, ACC:1.0\n",
      "Training iteration 363 loss: 0.0026880239602178335, ACC:1.0\n",
      "Training iteration 364 loss: 0.0037173633463680744, ACC:1.0\n",
      "Training iteration 365 loss: 0.003779337741434574, ACC:1.0\n",
      "Training iteration 366 loss: 0.010720220394432545, ACC:1.0\n",
      "Training iteration 367 loss: 0.006003661546856165, ACC:1.0\n",
      "Training iteration 368 loss: 0.002312034135684371, ACC:1.0\n",
      "Training iteration 369 loss: 0.00136641098652035, ACC:1.0\n",
      "Training iteration 370 loss: 0.0014486114960163832, ACC:1.0\n",
      "Training iteration 371 loss: 0.022820057347416878, ACC:0.984375\n",
      "Training iteration 372 loss: 0.0621308870613575, ACC:0.953125\n",
      "Training iteration 373 loss: 0.026246117427945137, ACC:0.984375\n",
      "Training iteration 374 loss: 0.0027727060951292515, ACC:1.0\n",
      "Training iteration 375 loss: 0.0061560822650790215, ACC:1.0\n",
      "Training iteration 376 loss: 0.00757911242544651, ACC:1.0\n",
      "Training iteration 377 loss: 0.04944687709212303, ACC:0.984375\n",
      "Training iteration 378 loss: 0.001583311939612031, ACC:1.0\n",
      "Training iteration 379 loss: 0.0017554194200783968, ACC:1.0\n",
      "Training iteration 380 loss: 0.0004852176352869719, ACC:1.0\n",
      "Training iteration 381 loss: 0.011278290301561356, ACC:1.0\n",
      "Training iteration 382 loss: 0.0009055619593709707, ACC:1.0\n",
      "Training iteration 383 loss: 0.0017369219567626715, ACC:1.0\n",
      "Training iteration 384 loss: 0.0005502087296918035, ACC:1.0\n",
      "Training iteration 385 loss: 0.0005916770896874368, ACC:1.0\n",
      "Training iteration 386 loss: 0.0005681234179064631, ACC:1.0\n",
      "Training iteration 387 loss: 0.045210644602775574, ACC:0.984375\n",
      "Training iteration 388 loss: 0.05155397206544876, ACC:0.96875\n",
      "Training iteration 389 loss: 0.0007653802167624235, ACC:1.0\n",
      "Training iteration 390 loss: 0.05466115474700928, ACC:0.984375\n",
      "Training iteration 391 loss: 0.000471520732389763, ACC:1.0\n",
      "Training iteration 392 loss: 0.0003218142956029624, ACC:1.0\n",
      "Training iteration 393 loss: 0.03843008354306221, ACC:0.984375\n",
      "Training iteration 394 loss: 0.015463755466043949, ACC:0.984375\n",
      "Training iteration 395 loss: 0.0718279704451561, ACC:0.96875\n",
      "Training iteration 396 loss: 0.0017531689954921603, ACC:1.0\n",
      "Training iteration 397 loss: 0.041739240288734436, ACC:0.984375\n",
      "Training iteration 398 loss: 0.0038450846914201975, ACC:1.0\n",
      "Training iteration 399 loss: 0.0043779886327683926, ACC:1.0\n",
      "Training iteration 400 loss: 0.05659906566143036, ACC:0.984375\n",
      "Training iteration 401 loss: 0.021628204733133316, ACC:0.984375\n",
      "Training iteration 402 loss: 0.00593436136841774, ACC:1.0\n",
      "Training iteration 403 loss: 0.009037709794938564, ACC:1.0\n",
      "Training iteration 404 loss: 0.017917754128575325, ACC:0.984375\n",
      "Training iteration 405 loss: 0.009198712185025215, ACC:1.0\n",
      "Training iteration 406 loss: 0.006870156154036522, ACC:1.0\n",
      "Training iteration 407 loss: 0.013250584714114666, ACC:1.0\n",
      "Training iteration 408 loss: 0.0085267573595047, ACC:1.0\n",
      "Training iteration 409 loss: 0.002689523622393608, ACC:1.0\n",
      "Training iteration 410 loss: 0.005483908113092184, ACC:1.0\n",
      "Training iteration 411 loss: 0.0013752938248217106, ACC:1.0\n",
      "Training iteration 412 loss: 0.006693100556731224, ACC:1.0\n",
      "Training iteration 413 loss: 0.022659240290522575, ACC:0.984375\n",
      "Training iteration 414 loss: 0.007654400076717138, ACC:1.0\n",
      "Training iteration 415 loss: 0.03430173918604851, ACC:0.96875\n",
      "Training iteration 416 loss: 0.002942680846899748, ACC:1.0\n",
      "Training iteration 417 loss: 0.014403864741325378, ACC:1.0\n",
      "Training iteration 418 loss: 0.008806178346276283, ACC:1.0\n",
      "Training iteration 419 loss: 0.01507093571126461, ACC:1.0\n",
      "Training iteration 420 loss: 0.02811519056558609, ACC:0.984375\n",
      "Training iteration 421 loss: 0.006221487186849117, ACC:1.0\n",
      "Training iteration 422 loss: 0.008575594052672386, ACC:1.0\n",
      "Training iteration 423 loss: 0.025957580655813217, ACC:0.984375\n",
      "Training iteration 424 loss: 0.028627794235944748, ACC:0.984375\n",
      "Training iteration 425 loss: 0.041165027767419815, ACC:0.984375\n",
      "Training iteration 426 loss: 0.0020295565482228994, ACC:1.0\n",
      "Training iteration 427 loss: 0.02491641789674759, ACC:0.984375\n",
      "Training iteration 428 loss: 0.002682158024981618, ACC:1.0\n",
      "Training iteration 429 loss: 0.01944512315094471, ACC:0.984375\n",
      "Training iteration 430 loss: 0.0019142527598887682, ACC:1.0\n",
      "Training iteration 431 loss: 0.010660122148692608, ACC:1.0\n",
      "Training iteration 432 loss: 0.02859523519873619, ACC:0.984375\n",
      "Training iteration 433 loss: 0.00381852057762444, ACC:1.0\n",
      "Training iteration 434 loss: 0.0020360955968499184, ACC:1.0\n",
      "Training iteration 435 loss: 0.0019329219358041883, ACC:1.0\n",
      "Training iteration 436 loss: 0.025316255167126656, ACC:0.984375\n",
      "Training iteration 437 loss: 0.015082553029060364, ACC:0.984375\n",
      "Training iteration 438 loss: 0.003860719036310911, ACC:1.0\n",
      "Training iteration 439 loss: 0.0431140698492527, ACC:0.984375\n",
      "Training iteration 440 loss: 0.0005879477830603719, ACC:1.0\n",
      "Training iteration 441 loss: 0.005706033203750849, ACC:1.0\n",
      "Training iteration 442 loss: 0.022705713286995888, ACC:0.984375\n",
      "Training iteration 443 loss: 0.017727985978126526, ACC:0.984375\n",
      "Training iteration 444 loss: 0.04585166648030281, ACC:0.984375\n",
      "Training iteration 445 loss: 0.003677884815260768, ACC:1.0\n",
      "Training iteration 446 loss: 0.02490387111902237, ACC:0.984375\n",
      "Training iteration 447 loss: 0.05715315043926239, ACC:0.984375\n",
      "Training iteration 448 loss: 0.03309091925621033, ACC:0.984375\n",
      "Training iteration 449 loss: 0.0013243511784821749, ACC:1.0\n",
      "Training iteration 450 loss: 0.0037268702872097492, ACC:1.0\n",
      "Validation iteration 451 loss: 0.03277180343866348, ACC: 0.984375\n",
      "Validation iteration 452 loss: 0.0008154080715030432, ACC: 1.0\n",
      "Validation iteration 453 loss: 0.011113467626273632, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.015941351652145386, ACC: 0.984375\n",
      "Validation iteration 455 loss: 0.011221949942409992, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.0017177358968183398, ACC: 1.0\n",
      "Validation iteration 457 loss: 0.0142151964828372, ACC: 0.984375\n",
      "Validation iteration 458 loss: 0.005085375159978867, ACC: 1.0\n",
      "Validation iteration 459 loss: 0.003281752346083522, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.005747372750192881, ACC: 1.0\n",
      "Validation iteration 461 loss: 0.0009799061808735132, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.01932639256119728, ACC: 0.984375\n",
      "Validation iteration 463 loss: 0.001105730189010501, ACC: 1.0\n",
      "Validation iteration 464 loss: 0.0006871884106658399, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.0029000670183449984, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.015035700052976608, ACC: 0.984375\n",
      "Validation iteration 467 loss: 0.0004303287423681468, ACC: 1.0\n",
      "Validation iteration 468 loss: 0.0006384949665516615, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.05121762678027153, ACC: 0.984375\n",
      "Validation iteration 470 loss: 0.01905437745153904, ACC: 0.984375\n",
      "Validation iteration 471 loss: 0.01529969647526741, ACC: 0.984375\n",
      "Validation iteration 472 loss: 0.0025727751199156046, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.0005524531006813049, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.0047219302505254745, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.0014864425174891949, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.0013788588112220168, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.0012282422976568341, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.0963272750377655, ACC: 0.96875\n",
      "Validation iteration 479 loss: 0.002100925659760833, ACC: 1.0\n",
      "Validation iteration 480 loss: 0.0011280677281320095, ACC: 1.0\n",
      "Validation iteration 481 loss: 0.010108099319040775, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.0008524455479346216, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.00815313495695591, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.021055296063423157, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.003110403660684824, ACC: 1.0\n",
      "Validation iteration 486 loss: 0.0005870976019650698, ACC: 1.0\n",
      "Validation iteration 487 loss: 0.006239169277250767, ACC: 1.0\n",
      "Validation iteration 488 loss: 0.00311649008654058, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.005926273297518492, ACC: 1.0\n",
      "Validation iteration 490 loss: 0.0013492723228409886, ACC: 1.0\n",
      "Validation iteration 491 loss: 0.0011884637642651796, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.00044290052028372884, ACC: 1.0\n",
      "Validation iteration 493 loss: 0.07254234701395035, ACC: 0.96875\n",
      "Validation iteration 494 loss: 0.0044268835335969925, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.0012267677811905742, ACC: 1.0\n",
      "Validation iteration 496 loss: 0.0009371496853418648, ACC: 1.0\n",
      "Validation iteration 497 loss: 0.0009370983461849391, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.0009574353462085128, ACC: 1.0\n",
      "Validation iteration 499 loss: 0.0100912656635046, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.0017609462374821305, ACC: 1.0\n",
      "-- Epoch 9 done -- Train loss: 0.016673202522150758, train ACC: 0.9946527777777778, val loss: 0.009901856654905714, val ACC: 0.9959375\n",
      "<--- 20741.835569620132 seconds --->\n",
      "Training iteration 1 loss: 0.018883001059293747, ACC:0.984375\n",
      "Training iteration 2 loss: 0.0006571292178705335, ACC:1.0\n",
      "Training iteration 3 loss: 0.000272873614449054, ACC:1.0\n",
      "Training iteration 4 loss: 0.004538845270872116, ACC:1.0\n",
      "Training iteration 5 loss: 0.0021531868260353804, ACC:1.0\n",
      "Training iteration 6 loss: 0.00282044755294919, ACC:1.0\n",
      "Training iteration 7 loss: 0.0011567776091396809, ACC:1.0\n",
      "Training iteration 8 loss: 0.003382078604772687, ACC:1.0\n",
      "Training iteration 9 loss: 0.03769885003566742, ACC:0.984375\n",
      "Training iteration 10 loss: 0.02462773211300373, ACC:0.984375\n",
      "Training iteration 11 loss: 0.00224279030226171, ACC:1.0\n",
      "Training iteration 12 loss: 0.0029710924718528986, ACC:1.0\n",
      "Training iteration 13 loss: 0.0009287144057452679, ACC:1.0\n",
      "Training iteration 14 loss: 0.0004964486579410732, ACC:1.0\n",
      "Training iteration 15 loss: 0.005061609670519829, ACC:1.0\n",
      "Training iteration 16 loss: 0.00842711515724659, ACC:1.0\n",
      "Training iteration 17 loss: 0.005155292339622974, ACC:1.0\n",
      "Training iteration 18 loss: 0.000977883581072092, ACC:1.0\n",
      "Training iteration 19 loss: 0.001095414161682129, ACC:1.0\n",
      "Training iteration 20 loss: 0.003533438313752413, ACC:1.0\n",
      "Training iteration 21 loss: 0.00016001828771550208, ACC:1.0\n",
      "Training iteration 22 loss: 0.004993876907974482, ACC:1.0\n",
      "Training iteration 23 loss: 0.0009333331836387515, ACC:1.0\n",
      "Training iteration 24 loss: 0.0003380576672498137, ACC:1.0\n",
      "Training iteration 25 loss: 0.00676826061680913, ACC:1.0\n",
      "Training iteration 26 loss: 0.0017789432313293219, ACC:1.0\n",
      "Training iteration 27 loss: 0.005663939286023378, ACC:1.0\n",
      "Training iteration 28 loss: 0.057115621864795685, ACC:0.96875\n",
      "Training iteration 29 loss: 0.004735222551971674, ACC:1.0\n",
      "Training iteration 30 loss: 0.0030867222230881453, ACC:1.0\n",
      "Training iteration 31 loss: 0.014243459329009056, ACC:1.0\n",
      "Training iteration 32 loss: 0.0022126748226583004, ACC:1.0\n",
      "Training iteration 33 loss: 0.0035330774262547493, ACC:1.0\n",
      "Training iteration 34 loss: 0.12082197517156601, ACC:0.96875\n",
      "Training iteration 35 loss: 0.009566144086420536, ACC:1.0\n",
      "Training iteration 36 loss: 0.001563076744787395, ACC:1.0\n",
      "Training iteration 37 loss: 0.00012122187035856768, ACC:1.0\n",
      "Training iteration 38 loss: 0.000273213314358145, ACC:1.0\n",
      "Training iteration 39 loss: 0.04494570940732956, ACC:0.984375\n",
      "Training iteration 40 loss: 0.001503900857642293, ACC:1.0\n",
      "Training iteration 41 loss: 0.016285402700304985, ACC:0.984375\n",
      "Training iteration 42 loss: 0.007364228833466768, ACC:1.0\n",
      "Training iteration 43 loss: 0.007370533421635628, ACC:1.0\n",
      "Training iteration 44 loss: 0.09065420925617218, ACC:0.984375\n",
      "Training iteration 45 loss: 0.00888852495700121, ACC:1.0\n",
      "Training iteration 46 loss: 0.005421735811978579, ACC:1.0\n",
      "Training iteration 47 loss: 0.012637108564376831, ACC:1.0\n",
      "Training iteration 48 loss: 0.014856220223009586, ACC:0.984375\n",
      "Training iteration 49 loss: 0.01729421503841877, ACC:0.984375\n",
      "Training iteration 50 loss: 0.010437553748488426, ACC:1.0\n",
      "Training iteration 51 loss: 0.013615225441753864, ACC:1.0\n",
      "Training iteration 52 loss: 0.001173086347989738, ACC:1.0\n",
      "Training iteration 53 loss: 0.0002718260802794248, ACC:1.0\n",
      "Training iteration 54 loss: 0.03046027570962906, ACC:0.984375\n",
      "Training iteration 55 loss: 0.05575604736804962, ACC:0.984375\n",
      "Training iteration 56 loss: 0.002851528115570545, ACC:1.0\n",
      "Training iteration 57 loss: 0.0024155958089977503, ACC:1.0\n",
      "Training iteration 58 loss: 0.003493084106594324, ACC:1.0\n",
      "Training iteration 59 loss: 0.0038017542101442814, ACC:1.0\n",
      "Training iteration 60 loss: 0.002753725042566657, ACC:1.0\n",
      "Training iteration 61 loss: 0.006998741067945957, ACC:1.0\n",
      "Training iteration 62 loss: 0.005489964969456196, ACC:1.0\n",
      "Training iteration 63 loss: 0.0017575641395524144, ACC:1.0\n",
      "Training iteration 64 loss: 0.021298177540302277, ACC:1.0\n",
      "Training iteration 65 loss: 0.10061201453208923, ACC:0.96875\n",
      "Training iteration 66 loss: 0.007262533530592918, ACC:1.0\n",
      "Training iteration 67 loss: 0.07619881629943848, ACC:0.96875\n",
      "Training iteration 68 loss: 0.017274290323257446, ACC:0.984375\n",
      "Training iteration 69 loss: 0.05405491963028908, ACC:0.984375\n",
      "Training iteration 70 loss: 0.007223868742585182, ACC:1.0\n",
      "Training iteration 71 loss: 0.003110999008640647, ACC:1.0\n",
      "Training iteration 72 loss: 0.08592850714921951, ACC:0.984375\n",
      "Training iteration 73 loss: 0.009414256550371647, ACC:1.0\n",
      "Training iteration 74 loss: 0.06733489036560059, ACC:0.96875\n",
      "Training iteration 75 loss: 0.05914108082652092, ACC:0.96875\n",
      "Training iteration 76 loss: 0.08138171583414078, ACC:0.984375\n",
      "Training iteration 77 loss: 0.009036119095981121, ACC:1.0\n",
      "Training iteration 78 loss: 0.014822116121649742, ACC:1.0\n",
      "Training iteration 79 loss: 0.06124328449368477, ACC:0.984375\n",
      "Training iteration 80 loss: 0.017582161352038383, ACC:1.0\n",
      "Training iteration 81 loss: 0.005227030720561743, ACC:1.0\n",
      "Training iteration 82 loss: 0.015165634453296661, ACC:0.984375\n",
      "Training iteration 83 loss: 0.01688428409397602, ACC:0.984375\n",
      "Training iteration 84 loss: 0.05696236714720726, ACC:0.984375\n",
      "Training iteration 85 loss: 0.016923021525144577, ACC:0.984375\n",
      "Training iteration 86 loss: 0.00840584933757782, ACC:1.0\n",
      "Training iteration 87 loss: 0.0029205228202044964, ACC:1.0\n",
      "Training iteration 88 loss: 0.016983328387141228, ACC:1.0\n",
      "Training iteration 89 loss: 0.0013564411783590913, ACC:1.0\n",
      "Training iteration 90 loss: 0.0032072756439447403, ACC:1.0\n",
      "Training iteration 91 loss: 0.007500617764890194, ACC:1.0\n",
      "Training iteration 92 loss: 0.0010062817018479109, ACC:1.0\n",
      "Training iteration 93 loss: 0.000986680039204657, ACC:1.0\n",
      "Training iteration 94 loss: 0.0004604368004947901, ACC:1.0\n",
      "Training iteration 95 loss: 0.0013171728933230042, ACC:1.0\n",
      "Training iteration 96 loss: 0.0028454854618757963, ACC:1.0\n",
      "Training iteration 97 loss: 0.0011536974925547838, ACC:1.0\n",
      "Training iteration 98 loss: 0.0006334620993584394, ACC:1.0\n",
      "Training iteration 99 loss: 0.0005666446522809565, ACC:1.0\n",
      "Training iteration 100 loss: 0.0853782370686531, ACC:0.984375\n",
      "Training iteration 101 loss: 0.00041590468026697636, ACC:1.0\n",
      "Training iteration 102 loss: 0.0010838728630915284, ACC:1.0\n",
      "Training iteration 103 loss: 0.07812581956386566, ACC:0.953125\n",
      "Training iteration 104 loss: 0.0463179275393486, ACC:0.984375\n",
      "Training iteration 105 loss: 0.001188111025840044, ACC:1.0\n",
      "Training iteration 106 loss: 0.02484584040939808, ACC:0.984375\n",
      "Training iteration 107 loss: 0.023377927020192146, ACC:0.984375\n",
      "Training iteration 108 loss: 0.017853744328022003, ACC:1.0\n",
      "Training iteration 109 loss: 0.004254721570760012, ACC:1.0\n",
      "Training iteration 110 loss: 0.001649454701691866, ACC:1.0\n",
      "Training iteration 111 loss: 0.00637228786945343, ACC:1.0\n",
      "Training iteration 112 loss: 0.002529770601540804, ACC:1.0\n",
      "Training iteration 113 loss: 0.004156807437539101, ACC:1.0\n",
      "Training iteration 114 loss: 0.05388036370277405, ACC:0.984375\n",
      "Training iteration 115 loss: 0.00046521687181666493, ACC:1.0\n",
      "Training iteration 116 loss: 0.003405752358958125, ACC:1.0\n",
      "Training iteration 117 loss: 0.0002237563458038494, ACC:1.0\n",
      "Training iteration 118 loss: 0.0003371769853401929, ACC:1.0\n",
      "Training iteration 119 loss: 0.00267725414596498, ACC:1.0\n",
      "Training iteration 120 loss: 0.013060413300991058, ACC:1.0\n",
      "Training iteration 121 loss: 0.00022654843633063138, ACC:1.0\n",
      "Training iteration 122 loss: 0.005444298963993788, ACC:1.0\n",
      "Training iteration 123 loss: 0.000618816411588341, ACC:1.0\n",
      "Training iteration 124 loss: 0.001512159826233983, ACC:1.0\n",
      "Training iteration 125 loss: 0.0006529635284096003, ACC:1.0\n",
      "Training iteration 126 loss: 0.00022958836052566767, ACC:1.0\n",
      "Training iteration 127 loss: 0.0007904232479631901, ACC:1.0\n",
      "Training iteration 128 loss: 0.002650144509971142, ACC:1.0\n",
      "Training iteration 129 loss: 0.0003023601311724633, ACC:1.0\n",
      "Training iteration 130 loss: 0.0008987808250822127, ACC:1.0\n",
      "Training iteration 131 loss: 0.0004635098739527166, ACC:1.0\n",
      "Training iteration 132 loss: 0.01720423437654972, ACC:1.0\n",
      "Training iteration 133 loss: 0.003410211531445384, ACC:1.0\n",
      "Training iteration 134 loss: 0.0013599151279777288, ACC:1.0\n",
      "Training iteration 135 loss: 0.11068984121084213, ACC:0.984375\n",
      "Training iteration 136 loss: 0.022580623626708984, ACC:0.984375\n",
      "Training iteration 137 loss: 0.012943287380039692, ACC:1.0\n",
      "Training iteration 138 loss: 0.018094394356012344, ACC:1.0\n",
      "Training iteration 139 loss: 0.04373509809374809, ACC:0.96875\n",
      "Training iteration 140 loss: 0.0075928885489702225, ACC:1.0\n",
      "Training iteration 141 loss: 0.038383275270462036, ACC:0.984375\n",
      "Training iteration 142 loss: 0.004859529435634613, ACC:1.0\n",
      "Training iteration 143 loss: 0.019295383244752884, ACC:0.984375\n",
      "Training iteration 144 loss: 0.04498520866036415, ACC:0.984375\n",
      "Training iteration 145 loss: 0.000981680816039443, ACC:1.0\n",
      "Training iteration 146 loss: 0.01584753766655922, ACC:0.984375\n",
      "Training iteration 147 loss: 0.12115663290023804, ACC:0.984375\n",
      "Training iteration 148 loss: 0.0004565685521811247, ACC:1.0\n",
      "Training iteration 149 loss: 0.0007092588348314166, ACC:1.0\n",
      "Training iteration 150 loss: 0.001283146906644106, ACC:1.0\n",
      "Training iteration 151 loss: 0.0024206782691180706, ACC:1.0\n",
      "Training iteration 152 loss: 0.08158384263515472, ACC:0.984375\n",
      "Training iteration 153 loss: 0.0039087445475161076, ACC:1.0\n",
      "Training iteration 154 loss: 0.0034508751705288887, ACC:1.0\n",
      "Training iteration 155 loss: 0.002434761496260762, ACC:1.0\n",
      "Training iteration 156 loss: 0.00100072065833956, ACC:1.0\n",
      "Training iteration 157 loss: 0.0012104585766792297, ACC:1.0\n",
      "Training iteration 158 loss: 0.002008694689720869, ACC:1.0\n",
      "Training iteration 159 loss: 0.010377738624811172, ACC:1.0\n",
      "Training iteration 160 loss: 0.0019385125488042831, ACC:1.0\n",
      "Training iteration 161 loss: 0.001632463769055903, ACC:1.0\n",
      "Training iteration 162 loss: 0.001218319172039628, ACC:1.0\n",
      "Training iteration 163 loss: 0.01329461857676506, ACC:1.0\n",
      "Training iteration 164 loss: 0.006316834595054388, ACC:1.0\n",
      "Training iteration 165 loss: 0.005418311804533005, ACC:1.0\n",
      "Training iteration 166 loss: 0.053836800158023834, ACC:0.984375\n",
      "Training iteration 167 loss: 0.011170937679708004, ACC:1.0\n",
      "Training iteration 168 loss: 0.0045191338285803795, ACC:1.0\n",
      "Training iteration 169 loss: 0.00226061069406569, ACC:1.0\n",
      "Training iteration 170 loss: 0.09192617982625961, ACC:0.984375\n",
      "Training iteration 171 loss: 0.04116871953010559, ACC:0.984375\n",
      "Training iteration 172 loss: 0.0035575306974351406, ACC:1.0\n",
      "Training iteration 173 loss: 0.0029477649368345737, ACC:1.0\n",
      "Training iteration 174 loss: 0.000600624771323055, ACC:1.0\n",
      "Training iteration 175 loss: 0.0077749318443238735, ACC:1.0\n",
      "Training iteration 176 loss: 0.09678951650857925, ACC:0.953125\n",
      "Training iteration 177 loss: 0.0205429308116436, ACC:0.984375\n",
      "Training iteration 178 loss: 0.045404102653265, ACC:0.984375\n",
      "Training iteration 179 loss: 0.002757779788225889, ACC:1.0\n",
      "Training iteration 180 loss: 0.00016569528088439256, ACC:1.0\n",
      "Training iteration 181 loss: 0.0002701789198908955, ACC:1.0\n",
      "Training iteration 182 loss: 0.009634926915168762, ACC:1.0\n",
      "Training iteration 183 loss: 0.0003714030608534813, ACC:1.0\n",
      "Training iteration 184 loss: 0.0017251950921490788, ACC:1.0\n",
      "Training iteration 185 loss: 0.003817316610366106, ACC:1.0\n",
      "Training iteration 186 loss: 0.00792552251368761, ACC:1.0\n",
      "Training iteration 187 loss: 0.000983964535407722, ACC:1.0\n",
      "Training iteration 188 loss: 0.003926693927496672, ACC:1.0\n",
      "Training iteration 189 loss: 0.0010255029192194343, ACC:1.0\n",
      "Training iteration 190 loss: 0.0007662712596356869, ACC:1.0\n",
      "Training iteration 191 loss: 0.006788755767047405, ACC:1.0\n",
      "Training iteration 192 loss: 0.0002426534192636609, ACC:1.0\n",
      "Training iteration 193 loss: 0.08972736448049545, ACC:0.984375\n",
      "Training iteration 194 loss: 0.0012950800592079759, ACC:1.0\n",
      "Training iteration 195 loss: 0.0019920270424336195, ACC:1.0\n",
      "Training iteration 196 loss: 0.0007134162588045001, ACC:1.0\n",
      "Training iteration 197 loss: 0.0024839029647409916, ACC:1.0\n",
      "Training iteration 198 loss: 0.0003111513506155461, ACC:1.0\n",
      "Training iteration 199 loss: 0.004039849154651165, ACC:1.0\n",
      "Training iteration 200 loss: 0.07572231441736221, ACC:0.96875\n",
      "Training iteration 201 loss: 0.0036410281900316477, ACC:1.0\n",
      "Training iteration 202 loss: 0.00022923402138985693, ACC:1.0\n",
      "Training iteration 203 loss: 0.0006942786276340485, ACC:1.0\n",
      "Training iteration 204 loss: 0.0002691853733267635, ACC:1.0\n",
      "Training iteration 205 loss: 0.0007920566713437438, ACC:1.0\n",
      "Training iteration 206 loss: 0.09893067181110382, ACC:0.96875\n",
      "Training iteration 207 loss: 0.0005848523578606546, ACC:1.0\n",
      "Training iteration 208 loss: 0.005354205146431923, ACC:1.0\n",
      "Training iteration 209 loss: 0.0012144000502303243, ACC:1.0\n",
      "Training iteration 210 loss: 0.00233949301764369, ACC:1.0\n",
      "Training iteration 211 loss: 0.00034772095386870205, ACC:1.0\n",
      "Training iteration 212 loss: 0.0008640486630611122, ACC:1.0\n",
      "Training iteration 213 loss: 0.02107122167944908, ACC:0.984375\n",
      "Training iteration 214 loss: 0.0024396516382694244, ACC:1.0\n",
      "Training iteration 215 loss: 0.0020762751810252666, ACC:1.0\n",
      "Training iteration 216 loss: 0.012759457342326641, ACC:0.984375\n",
      "Training iteration 217 loss: 0.060928866267204285, ACC:0.984375\n",
      "Training iteration 218 loss: 0.0009008708293549716, ACC:1.0\n",
      "Training iteration 219 loss: 0.0017359756166115403, ACC:1.0\n",
      "Training iteration 220 loss: 0.0009547167574055493, ACC:1.0\n",
      "Training iteration 221 loss: 0.02057168446481228, ACC:1.0\n",
      "Training iteration 222 loss: 0.0012243425007909536, ACC:1.0\n",
      "Training iteration 223 loss: 0.00232395576313138, ACC:1.0\n",
      "Training iteration 224 loss: 0.0007253859657794237, ACC:1.0\n",
      "Training iteration 225 loss: 0.007849868386983871, ACC:1.0\n",
      "Training iteration 226 loss: 0.05974045395851135, ACC:0.984375\n",
      "Training iteration 227 loss: 0.02554173208773136, ACC:0.984375\n",
      "Training iteration 228 loss: 0.0025827609933912754, ACC:1.0\n",
      "Training iteration 229 loss: 0.000811665435321629, ACC:1.0\n",
      "Training iteration 230 loss: 0.0034587213303893805, ACC:1.0\n",
      "Training iteration 231 loss: 0.0008167388150468469, ACC:1.0\n",
      "Training iteration 232 loss: 0.0005865469574928284, ACC:1.0\n",
      "Training iteration 233 loss: 0.0012230142019689083, ACC:1.0\n",
      "Training iteration 234 loss: 0.000785204058047384, ACC:1.0\n",
      "Training iteration 235 loss: 0.037743981927633286, ACC:0.984375\n",
      "Training iteration 236 loss: 0.0019292564829811454, ACC:1.0\n",
      "Training iteration 237 loss: 0.0011314759030938148, ACC:1.0\n",
      "Training iteration 238 loss: 0.024830728769302368, ACC:0.984375\n",
      "Training iteration 239 loss: 0.007713967468589544, ACC:1.0\n",
      "Training iteration 240 loss: 0.034316111356019974, ACC:0.96875\n",
      "Training iteration 241 loss: 0.09016615897417068, ACC:0.984375\n",
      "Training iteration 242 loss: 0.0013393027475103736, ACC:1.0\n",
      "Training iteration 243 loss: 0.01947876811027527, ACC:0.984375\n",
      "Training iteration 244 loss: 0.004300564061850309, ACC:1.0\n",
      "Training iteration 245 loss: 0.0029772422276437283, ACC:1.0\n",
      "Training iteration 246 loss: 0.03858722746372223, ACC:0.984375\n",
      "Training iteration 247 loss: 0.04073622450232506, ACC:0.96875\n",
      "Training iteration 248 loss: 0.0128013351932168, ACC:1.0\n",
      "Training iteration 249 loss: 0.011489065364003181, ACC:1.0\n",
      "Training iteration 250 loss: 0.0014765036758035421, ACC:1.0\n",
      "Training iteration 251 loss: 0.004973996430635452, ACC:1.0\n",
      "Training iteration 252 loss: 0.004201620351523161, ACC:1.0\n",
      "Training iteration 253 loss: 0.008593491278588772, ACC:1.0\n",
      "Training iteration 254 loss: 0.0007921859505586326, ACC:1.0\n",
      "Training iteration 255 loss: 0.01489584892988205, ACC:0.984375\n",
      "Training iteration 256 loss: 0.0011717459419742227, ACC:1.0\n",
      "Training iteration 257 loss: 0.0007627426530234516, ACC:1.0\n",
      "Training iteration 258 loss: 0.007489405572414398, ACC:1.0\n",
      "Training iteration 259 loss: 0.015849344432353973, ACC:0.984375\n",
      "Training iteration 260 loss: 0.007061867043375969, ACC:1.0\n",
      "Training iteration 261 loss: 0.0006320319371297956, ACC:1.0\n",
      "Training iteration 262 loss: 0.00697134668007493, ACC:1.0\n",
      "Training iteration 263 loss: 0.011484994553029537, ACC:1.0\n",
      "Training iteration 264 loss: 0.017502732574939728, ACC:0.984375\n",
      "Training iteration 265 loss: 0.0005182318855077028, ACC:1.0\n",
      "Training iteration 266 loss: 0.12251105159521103, ACC:0.984375\n",
      "Training iteration 267 loss: 0.0008971578790806234, ACC:1.0\n",
      "Training iteration 268 loss: 0.010561274364590645, ACC:1.0\n",
      "Training iteration 269 loss: 0.00036342311068437994, ACC:1.0\n",
      "Training iteration 270 loss: 0.0006103747873567045, ACC:1.0\n",
      "Training iteration 271 loss: 0.00036783062387257814, ACC:1.0\n",
      "Training iteration 272 loss: 0.002529524965211749, ACC:1.0\n",
      "Training iteration 273 loss: 0.0006612896104343235, ACC:1.0\n",
      "Training iteration 274 loss: 0.000355764088453725, ACC:1.0\n",
      "Training iteration 275 loss: 0.0003945978241972625, ACC:1.0\n",
      "Training iteration 276 loss: 0.0003025906626135111, ACC:1.0\n",
      "Training iteration 277 loss: 0.09253302216529846, ACC:0.984375\n",
      "Training iteration 278 loss: 0.002795281121507287, ACC:1.0\n",
      "Training iteration 279 loss: 0.060322508215904236, ACC:0.984375\n",
      "Training iteration 280 loss: 0.0013779659057036042, ACC:1.0\n",
      "Training iteration 281 loss: 0.0005273858550935984, ACC:1.0\n",
      "Training iteration 282 loss: 0.024996832013130188, ACC:0.984375\n",
      "Training iteration 283 loss: 0.0014477333752438426, ACC:1.0\n",
      "Training iteration 284 loss: 0.03197459504008293, ACC:0.984375\n",
      "Training iteration 285 loss: 0.009501457214355469, ACC:1.0\n",
      "Training iteration 286 loss: 0.0009253253228962421, ACC:1.0\n",
      "Training iteration 287 loss: 0.0017775521846488118, ACC:1.0\n",
      "Training iteration 288 loss: 0.005675668362528086, ACC:1.0\n",
      "Training iteration 289 loss: 0.0015339519595727324, ACC:1.0\n",
      "Training iteration 290 loss: 0.001458465470932424, ACC:1.0\n",
      "Training iteration 291 loss: 0.0006109751993790269, ACC:1.0\n",
      "Training iteration 292 loss: 0.002297099446877837, ACC:1.0\n",
      "Training iteration 293 loss: 0.012077344581484795, ACC:0.984375\n",
      "Training iteration 294 loss: 0.0007432279526256025, ACC:1.0\n",
      "Training iteration 295 loss: 0.011628435924649239, ACC:1.0\n",
      "Training iteration 296 loss: 0.024952443316578865, ACC:0.984375\n",
      "Training iteration 297 loss: 0.07270517945289612, ACC:0.984375\n",
      "Training iteration 298 loss: 0.014635421335697174, ACC:0.984375\n",
      "Training iteration 299 loss: 0.003636196255683899, ACC:1.0\n",
      "Training iteration 300 loss: 0.024459775537252426, ACC:1.0\n",
      "Training iteration 301 loss: 0.009605244733393192, ACC:1.0\n",
      "Training iteration 302 loss: 0.006813367363065481, ACC:1.0\n",
      "Training iteration 303 loss: 0.004483821801841259, ACC:1.0\n",
      "Training iteration 304 loss: 0.022176265716552734, ACC:0.984375\n",
      "Training iteration 305 loss: 0.004956871271133423, ACC:1.0\n",
      "Training iteration 306 loss: 0.019132861867547035, ACC:0.984375\n",
      "Training iteration 307 loss: 0.001663328381255269, ACC:1.0\n",
      "Training iteration 308 loss: 0.027530759572982788, ACC:0.984375\n",
      "Training iteration 309 loss: 0.018204854801297188, ACC:0.984375\n",
      "Training iteration 310 loss: 0.0007765801274217665, ACC:1.0\n",
      "Training iteration 311 loss: 0.028089230880141258, ACC:0.984375\n",
      "Training iteration 312 loss: 0.003684611525386572, ACC:1.0\n",
      "Training iteration 313 loss: 0.03415709733963013, ACC:0.984375\n",
      "Training iteration 314 loss: 0.0003049899241887033, ACC:1.0\n",
      "Training iteration 315 loss: 0.0004608409071806818, ACC:1.0\n",
      "Training iteration 316 loss: 0.00017678311269264668, ACC:1.0\n",
      "Training iteration 317 loss: 0.00031518758623860776, ACC:1.0\n",
      "Training iteration 318 loss: 0.00024391812621615827, ACC:1.0\n",
      "Training iteration 319 loss: 0.045256540179252625, ACC:0.984375\n",
      "Training iteration 320 loss: 0.025093412026762962, ACC:0.984375\n",
      "Training iteration 321 loss: 0.000800833513494581, ACC:1.0\n",
      "Training iteration 322 loss: 0.01640806347131729, ACC:0.984375\n",
      "Training iteration 323 loss: 0.05743258073925972, ACC:0.984375\n",
      "Training iteration 324 loss: 0.02022731304168701, ACC:0.984375\n",
      "Training iteration 325 loss: 0.006252788472920656, ACC:1.0\n",
      "Training iteration 326 loss: 0.008654305711388588, ACC:1.0\n",
      "Training iteration 327 loss: 0.002274654572829604, ACC:1.0\n",
      "Training iteration 328 loss: 0.0020694853737950325, ACC:1.0\n",
      "Training iteration 329 loss: 0.009782089851796627, ACC:1.0\n",
      "Training iteration 330 loss: 0.00846272986382246, ACC:1.0\n",
      "Training iteration 331 loss: 0.0035058921203017235, ACC:1.0\n",
      "Training iteration 332 loss: 0.007843573577702045, ACC:1.0\n",
      "Training iteration 333 loss: 0.031224340200424194, ACC:0.984375\n",
      "Training iteration 334 loss: 0.012420273385941982, ACC:1.0\n",
      "Training iteration 335 loss: 0.011105664074420929, ACC:1.0\n",
      "Training iteration 336 loss: 0.0022865966893732548, ACC:1.0\n",
      "Training iteration 337 loss: 0.007463113404810429, ACC:1.0\n",
      "Training iteration 338 loss: 0.001934662228450179, ACC:1.0\n",
      "Training iteration 339 loss: 0.0019187129801139235, ACC:1.0\n",
      "Training iteration 340 loss: 0.0026457717176526785, ACC:1.0\n",
      "Training iteration 341 loss: 0.0010627808514982462, ACC:1.0\n",
      "Training iteration 342 loss: 0.0005610624793916941, ACC:1.0\n",
      "Training iteration 343 loss: 0.004237224813550711, ACC:1.0\n",
      "Training iteration 344 loss: 0.0004241745045874268, ACC:1.0\n",
      "Training iteration 345 loss: 0.001469110487960279, ACC:1.0\n",
      "Training iteration 346 loss: 0.0006400002748705447, ACC:1.0\n",
      "Training iteration 347 loss: 0.0006867538904771209, ACC:1.0\n",
      "Training iteration 348 loss: 0.00025279365945607424, ACC:1.0\n",
      "Training iteration 349 loss: 0.011315681971609592, ACC:0.984375\n",
      "Training iteration 350 loss: 0.010782558470964432, ACC:1.0\n",
      "Training iteration 351 loss: 0.0023152304347604513, ACC:1.0\n",
      "Training iteration 352 loss: 0.000996172078885138, ACC:1.0\n",
      "Training iteration 353 loss: 0.00040647570858709514, ACC:1.0\n",
      "Training iteration 354 loss: 0.0005385370459407568, ACC:1.0\n",
      "Training iteration 355 loss: 0.045917946845293045, ACC:0.984375\n",
      "Training iteration 356 loss: 0.006676780059933662, ACC:1.0\n",
      "Training iteration 357 loss: 0.02631392702460289, ACC:0.984375\n",
      "Training iteration 358 loss: 0.0005823472165502608, ACC:1.0\n",
      "Training iteration 359 loss: 0.02547445520758629, ACC:0.984375\n",
      "Training iteration 360 loss: 0.000991143169812858, ACC:1.0\n",
      "Training iteration 361 loss: 0.0008860206580720842, ACC:1.0\n",
      "Training iteration 362 loss: 0.004099168814718723, ACC:1.0\n",
      "Training iteration 363 loss: 0.004821921233087778, ACC:1.0\n",
      "Training iteration 364 loss: 0.001016990514472127, ACC:1.0\n",
      "Training iteration 365 loss: 0.008919250220060349, ACC:1.0\n",
      "Training iteration 366 loss: 0.0018442929722368717, ACC:1.0\n",
      "Training iteration 367 loss: 0.00340809253975749, ACC:1.0\n",
      "Training iteration 368 loss: 0.0348261259496212, ACC:0.984375\n",
      "Training iteration 369 loss: 0.0019382821628823876, ACC:1.0\n",
      "Training iteration 370 loss: 0.052890125662088394, ACC:0.984375\n",
      "Training iteration 371 loss: 0.007767461705952883, ACC:1.0\n",
      "Training iteration 372 loss: 0.0058014364913105965, ACC:1.0\n",
      "Training iteration 373 loss: 0.0006057751015760005, ACC:1.0\n",
      "Training iteration 374 loss: 0.00120096979662776, ACC:1.0\n",
      "Training iteration 375 loss: 0.018468260765075684, ACC:0.984375\n",
      "Training iteration 376 loss: 0.059645507484674454, ACC:0.96875\n",
      "Training iteration 377 loss: 0.006283782888203859, ACC:1.0\n",
      "Training iteration 378 loss: 0.00040512491250410676, ACC:1.0\n",
      "Training iteration 379 loss: 0.0010103576350957155, ACC:1.0\n",
      "Training iteration 380 loss: 0.0001738643040880561, ACC:1.0\n",
      "Training iteration 381 loss: 0.0006012061494402587, ACC:1.0\n",
      "Training iteration 382 loss: 0.0055867694318294525, ACC:1.0\n",
      "Training iteration 383 loss: 0.0030454490333795547, ACC:1.0\n",
      "Training iteration 384 loss: 0.0007062567165121436, ACC:1.0\n",
      "Training iteration 385 loss: 0.0031764693558216095, ACC:1.0\n",
      "Training iteration 386 loss: 0.05383026972413063, ACC:0.984375\n",
      "Training iteration 387 loss: 0.0009515488636679947, ACC:1.0\n",
      "Training iteration 388 loss: 0.0005059557734057307, ACC:1.0\n",
      "Training iteration 389 loss: 0.0007324301404878497, ACC:1.0\n",
      "Training iteration 390 loss: 0.019381370395421982, ACC:0.984375\n",
      "Training iteration 391 loss: 0.04482463747262955, ACC:0.984375\n",
      "Training iteration 392 loss: 0.04714604839682579, ACC:0.984375\n",
      "Training iteration 393 loss: 0.016313405707478523, ACC:0.984375\n",
      "Training iteration 394 loss: 0.051575370132923126, ACC:0.984375\n",
      "Training iteration 395 loss: 0.0015689257998019457, ACC:1.0\n",
      "Training iteration 396 loss: 0.004793392959982157, ACC:1.0\n",
      "Training iteration 397 loss: 0.003559864591807127, ACC:1.0\n",
      "Training iteration 398 loss: 0.003818839555606246, ACC:1.0\n",
      "Training iteration 399 loss: 0.01059788279235363, ACC:1.0\n",
      "Training iteration 400 loss: 0.007666230667382479, ACC:1.0\n",
      "Training iteration 401 loss: 0.003580662189051509, ACC:1.0\n",
      "Training iteration 402 loss: 0.0028859975282102823, ACC:1.0\n",
      "Training iteration 403 loss: 0.010156707838177681, ACC:1.0\n",
      "Training iteration 404 loss: 0.021605851128697395, ACC:0.984375\n",
      "Training iteration 405 loss: 0.004134903196245432, ACC:1.0\n",
      "Training iteration 406 loss: 0.0030824982095509768, ACC:1.0\n",
      "Training iteration 407 loss: 0.000518630666192621, ACC:1.0\n",
      "Training iteration 408 loss: 0.0005855833296664059, ACC:1.0\n",
      "Training iteration 409 loss: 0.10351500660181046, ACC:0.953125\n",
      "Training iteration 410 loss: 0.005666874814778566, ACC:1.0\n",
      "Training iteration 411 loss: 0.003478413447737694, ACC:1.0\n",
      "Training iteration 412 loss: 0.00030182968475855887, ACC:1.0\n",
      "Training iteration 413 loss: 0.00032278933213092387, ACC:1.0\n",
      "Training iteration 414 loss: 0.011591380462050438, ACC:1.0\n",
      "Training iteration 415 loss: 0.0016049138503149152, ACC:1.0\n",
      "Training iteration 416 loss: 0.01259433850646019, ACC:1.0\n",
      "Training iteration 417 loss: 0.0004383045597933233, ACC:1.0\n",
      "Training iteration 418 loss: 0.021223844960331917, ACC:0.984375\n",
      "Training iteration 419 loss: 0.008017098531126976, ACC:1.0\n",
      "Training iteration 420 loss: 0.0035191902425140142, ACC:1.0\n",
      "Training iteration 421 loss: 0.05225346237421036, ACC:0.984375\n",
      "Training iteration 422 loss: 0.001280969474464655, ACC:1.0\n",
      "Training iteration 423 loss: 0.0007909302366897464, ACC:1.0\n",
      "Training iteration 424 loss: 0.00136798236053437, ACC:1.0\n",
      "Training iteration 425 loss: 0.0013621313264593482, ACC:1.0\n",
      "Training iteration 426 loss: 0.0011034692870453, ACC:1.0\n",
      "Training iteration 427 loss: 0.0016263885190710425, ACC:1.0\n",
      "Training iteration 428 loss: 0.003697216510772705, ACC:1.0\n",
      "Training iteration 429 loss: 0.007679312955588102, ACC:1.0\n",
      "Training iteration 430 loss: 0.07163409143686295, ACC:0.984375\n",
      "Training iteration 431 loss: 0.0580793060362339, ACC:0.984375\n",
      "Training iteration 432 loss: 0.09646300971508026, ACC:0.96875\n",
      "Training iteration 433 loss: 0.0036661745980381966, ACC:1.0\n",
      "Training iteration 434 loss: 0.006917779333889484, ACC:1.0\n",
      "Training iteration 435 loss: 0.012096501886844635, ACC:1.0\n",
      "Training iteration 436 loss: 0.019886085763573647, ACC:1.0\n",
      "Training iteration 437 loss: 0.052518684417009354, ACC:0.984375\n",
      "Training iteration 438 loss: 0.03884579613804817, ACC:0.984375\n",
      "Training iteration 439 loss: 0.0027960832230746746, ACC:1.0\n",
      "Training iteration 440 loss: 0.005021346732974052, ACC:1.0\n",
      "Training iteration 441 loss: 0.005885356105864048, ACC:1.0\n",
      "Training iteration 442 loss: 0.0052211773581802845, ACC:1.0\n",
      "Training iteration 443 loss: 0.017426304519176483, ACC:0.984375\n",
      "Training iteration 444 loss: 0.006461096461862326, ACC:1.0\n",
      "Training iteration 445 loss: 0.006794716231524944, ACC:1.0\n",
      "Training iteration 446 loss: 0.014310781843960285, ACC:1.0\n",
      "Training iteration 447 loss: 0.04021671786904335, ACC:0.984375\n",
      "Training iteration 448 loss: 0.0019101712387055159, ACC:1.0\n",
      "Training iteration 449 loss: 0.006158843170851469, ACC:1.0\n",
      "Training iteration 450 loss: 0.008494372479617596, ACC:1.0\n",
      "Validation iteration 451 loss: 0.020981816574931145, ACC: 0.984375\n",
      "Validation iteration 452 loss: 0.09130691736936569, ACC: 0.96875\n",
      "Validation iteration 453 loss: 0.012335581704974174, ACC: 1.0\n",
      "Validation iteration 454 loss: 0.06738255172967911, ACC: 0.984375\n",
      "Validation iteration 455 loss: 0.007002502214163542, ACC: 1.0\n",
      "Validation iteration 456 loss: 0.014850867912173271, ACC: 0.984375\n",
      "Validation iteration 457 loss: 0.0021653659641742706, ACC: 1.0\n",
      "Validation iteration 458 loss: 0.014783923514187336, ACC: 0.984375\n",
      "Validation iteration 459 loss: 0.0055690184235572815, ACC: 1.0\n",
      "Validation iteration 460 loss: 0.028705302625894547, ACC: 0.984375\n",
      "Validation iteration 461 loss: 0.0004098187491763383, ACC: 1.0\n",
      "Validation iteration 462 loss: 0.05972186475992203, ACC: 0.96875\n",
      "Validation iteration 463 loss: 0.020208057016134262, ACC: 0.984375\n",
      "Validation iteration 464 loss: 0.003983823582530022, ACC: 1.0\n",
      "Validation iteration 465 loss: 0.005277210380882025, ACC: 1.0\n",
      "Validation iteration 466 loss: 0.0005998763954266906, ACC: 1.0\n",
      "Validation iteration 467 loss: 0.04175914078950882, ACC: 0.984375\n",
      "Validation iteration 468 loss: 0.0006902868626639247, ACC: 1.0\n",
      "Validation iteration 469 loss: 0.0009524392080493271, ACC: 1.0\n",
      "Validation iteration 470 loss: 0.0009715229971334338, ACC: 1.0\n",
      "Validation iteration 471 loss: 0.016025932505726814, ACC: 1.0\n",
      "Validation iteration 472 loss: 0.0032732731197029352, ACC: 1.0\n",
      "Validation iteration 473 loss: 0.001705781789496541, ACC: 1.0\n",
      "Validation iteration 474 loss: 0.01592916250228882, ACC: 1.0\n",
      "Validation iteration 475 loss: 0.00036693812580779195, ACC: 1.0\n",
      "Validation iteration 476 loss: 0.007374100387096405, ACC: 1.0\n",
      "Validation iteration 477 loss: 0.0006055791163817048, ACC: 1.0\n",
      "Validation iteration 478 loss: 0.008593447506427765, ACC: 1.0\n",
      "Validation iteration 479 loss: 0.0821167528629303, ACC: 0.96875\n",
      "Validation iteration 480 loss: 0.05737146735191345, ACC: 0.96875\n",
      "Validation iteration 481 loss: 0.0004392282571643591, ACC: 1.0\n",
      "Validation iteration 482 loss: 0.0005410219309851527, ACC: 1.0\n",
      "Validation iteration 483 loss: 0.0036466531455516815, ACC: 1.0\n",
      "Validation iteration 484 loss: 0.05014963075518608, ACC: 0.984375\n",
      "Validation iteration 485 loss: 0.06584716588258743, ACC: 0.984375\n",
      "Validation iteration 486 loss: 0.020565710961818695, ACC: 0.984375\n",
      "Validation iteration 487 loss: 0.016342345625162125, ACC: 0.984375\n",
      "Validation iteration 488 loss: 0.0004326541384216398, ACC: 1.0\n",
      "Validation iteration 489 loss: 0.0146241569891572, ACC: 0.984375\n",
      "Validation iteration 490 loss: 0.03231588006019592, ACC: 0.984375\n",
      "Validation iteration 491 loss: 0.007380818948149681, ACC: 1.0\n",
      "Validation iteration 492 loss: 0.07928940653800964, ACC: 0.984375\n",
      "Validation iteration 493 loss: 0.009948347695171833, ACC: 1.0\n",
      "Validation iteration 494 loss: 0.0013384392950683832, ACC: 1.0\n",
      "Validation iteration 495 loss: 0.013479162938892841, ACC: 0.984375\n",
      "Validation iteration 496 loss: 0.022573791444301605, ACC: 0.984375\n",
      "Validation iteration 497 loss: 0.003858000272884965, ACC: 1.0\n",
      "Validation iteration 498 loss: 0.049101799726486206, ACC: 0.984375\n",
      "Validation iteration 499 loss: 0.0017661742167547345, ACC: 1.0\n",
      "Validation iteration 500 loss: 0.0006357553065754473, ACC: 1.0\n",
      "-- Epoch 10 done -- Train loss: 0.014214187095818084, train ACC: 0.9955208333333333, val loss: 0.019745929363416508, val ACC: 0.9921875\n",
      "<--- 20982.783660411835 seconds --->\n"
     ]
    }
   ],
   "source": [
    "# for timing model training purposes\n",
    "start_time = time.time()\n",
    "\n",
    "# input variables\n",
    "epoch_num = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# train model\n",
    "model = LeNet5()\n",
    "cost_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# to store loss for training & validation set\n",
    "jw_train_epoch = []  ## to store loss for training set by epoch (average loss of iterations)\n",
    "jw_val_epoch = []    ## to store loss for validation set by epoch (average loss of iterations)\n",
    "\n",
    "# to store accuracy of training & validation set\n",
    "acc_train_epoch = []\n",
    "acc_val_epoch = []\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "\n",
    "    # track train & validation loss & accuracy by iteration for each epoch\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    test_counter = 1\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ''' include below IF statement if want to limit number of batches '''\n",
    "        if i+1 == 501:  # should have total of 500 batches after train & val sets\n",
    "            break\n",
    "        \n",
    "        # use 80% for training, 20% for testing, and 10% for validation of the 40k training samples\n",
    "        ## batch size=64 so 625 batches total: 450 train batches, 50 val batches, and 125 test batches\n",
    "\n",
    "        if i+1 > 450:  # validate model with validation set (10% of total train data)\n",
    "            inputs, labels = data\n",
    "            \n",
    "            logits, outputs = model(inputs)\n",
    "            cost = cost_fn(logits, labels)\n",
    "            \n",
    "            jw_val = float(cost)\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "            correct = pred == labels\n",
    "            acc = sum(np.array(correct))/len(np.array(correct))\n",
    "\n",
    "            acc_val = acc\n",
    "\n",
    "            val_loss.append(jw_val)\n",
    "            val_acc.append(acc_val)\n",
    "            \n",
    "            print(f'Validation iteration {i+1} loss: {jw_val}, ACC: {acc_val}')\n",
    "            \n",
    "        else:  # train model with training set\n",
    "\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()            # zero the parameter gradients\n",
    "            logits, outputs = model(inputs)  # forward\n",
    "            cost = cost_fn(logits, labels)   # input logits prior to softmax activation into cost function\n",
    "            cost.backward()                  # backward\n",
    "            optimizer.step()                 # optimize\n",
    "\n",
    "            jw_train = float(cost)\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred = torch.Tensor.argmax(outputs, dim=1)            # get labels of prediction with highest probability\n",
    "            correct = pred == labels                              # compare to actual labels and see which was predicted correctly\n",
    "\n",
    "            acc = sum(np.array(correct))/len(np.array(correct))   # calculate accuracy\n",
    "\n",
    "            acc_train = acc\n",
    "\n",
    "            train_loss.append(jw_train)\n",
    "            train_acc.append(acc_train)\n",
    "            \n",
    "            print(f'Training iteration {i+1} loss: {jw_train}, ACC:{acc_train}')\n",
    "\n",
    "    # to save time, epoch loss = the lowest loss, epoch acc = highest acc in training\n",
    "    epoch_jw = np.mean(np.array(train_loss))\n",
    "    epoch_acc = np.mean(np.array(train_acc))\n",
    "\n",
    "    jw_train_epoch.append(epoch_jw)\n",
    "    jw_val_epoch.append(np.mean(val_loss))\n",
    "    acc_train_epoch.append(epoch_acc)\n",
    "    acc_val_epoch.append(np.mean(val_acc))\n",
    "    \n",
    "    print(f'-- Epoch {epoch+1} done -- Train loss: {epoch_jw}, train ACC: {epoch_acc}, val loss: {np.mean(val_loss)}, val ACC: {np.mean(val_acc)}')\n",
    "    \n",
    "    print(\"<--- %s seconds --->\" % (time.time() - start_time))\n",
    "\n",
    "    # save model at every epoch\n",
    "    path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/01_relu_lr0.001/lenet5_lr0.001_cpu_epoch{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwdS7oqJweUo"
   },
   "source": [
    "Plot loss and accuracy over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1636927084885,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "hZPUGYuZYBLH",
    "outputId": "9fd446bd-a496-475d-db1e-ea01df20a200"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss over epoch\n",
    "plt.plot([i+1 for i in range(len(jw_train_epoch))],jw_train_epoch)\n",
    "plt.plot([i+1 for i in range(len(jw_val_epoch))],jw_val_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Loss with learning rate {learning_rate}')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1636927084887,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "j8coUlN-YBLI",
    "outputId": "1545d21a-0430-445a-ebd1-43a0d3b11f71"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracy over epoch\n",
    "plt.plot([i+1 for i in range(len(acc_train_epoch))],acc_train_epoch)\n",
    "plt.plot([i+1 for i in range(len(acc_val_epoch))],acc_val_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Accuracy with learning rate {learning_rate}')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJQvqXWnYBLI"
   },
   "source": [
    "**Calculate test accuracy for model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 118310,
     "status": "ok",
     "timestamp": 1636928139394,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "EoO1l_KoBvaf",
    "outputId": "b56cbfcd-3713-4908-feeb-fd19fc0a33f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=5880, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      ")\n",
      "\n",
      " Testing accuracy = 99.6%\n"
     ]
    }
   ],
   "source": [
    "# load saved model \n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/01_relu_lr0.001/lenet5_lr0.001_cpu_epoch{7}.pth\"\n",
    "\n",
    "model = LeNet5()\n",
    "model.load_state_dict(torch.load(path))\n",
    "print(model.eval())\n",
    "\n",
    "# or use model in current session\n",
    "# model = model\n",
    "\n",
    "# calculate test accuracy of model at a given epoch\n",
    "\n",
    "testset_path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/00_tanh_lr0.001/lenet5_lr0.001_testset.pth\"\n",
    "inputs, labels = torch.load(testset_path)\n",
    "\n",
    "logits, outputs = model(inputs)\n",
    "\n",
    "# calculate overall accuracy across all classes\n",
    "pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "correct = pred == labels\n",
    "\n",
    "correct_cnt = sum(np.array(correct))\n",
    "data_cnt = len(np.array(correct))\n",
    "\n",
    "print(f'\\n Testing accuracy = {(correct_cnt/data_cnt)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11322,
     "status": "ok",
     "timestamp": 1638249902672,
     "user": {
      "displayName": "Dahlia Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17548577935735583956"
     },
     "user_tz": 480
    },
    "id": "L9H1t2s1S_tv",
    "outputId": "cf60ee59-46f5-41b7-f459-0986efbc084b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=5880, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      ")\n",
      "\n",
      " Testing accuracy = 87.1%\n"
     ]
    }
   ],
   "source": [
    "# test accuracy with 1000 augmented images to see if model can handle \"new\" inputs\n",
    "\n",
    "# load saved model \n",
    "path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/LeNet5_model_saves/01_relu_lr0.001/lenet5_lr0.001_cpu_epoch{7}.pth\"\n",
    "\n",
    "model = LeNet5()\n",
    "model.load_state_dict(torch.load(path))\n",
    "print(model.eval())\n",
    "\n",
    "# or use model in current session\n",
    "# model = model\n",
    "\n",
    "# calculate test accuracy of model at a given epoch\n",
    "\n",
    "testset_path = f\"/content/drive/MyDrive/SJSU MSDA/Fall 2021/DATA 255/Project/Code/data/test_sets/small_1000_set.pth\"\n",
    "inputs, labels = torch.load(testset_path)\n",
    "\n",
    "logits, outputs = model(inputs)\n",
    "\n",
    "# calculate overall accuracy across all classes\n",
    "pred = torch.Tensor.argmax(outputs, dim=1)\n",
    "correct = pred == labels\n",
    "\n",
    "correct_cnt = sum(np.array(correct))\n",
    "data_cnt = len(np.array(correct))\n",
    "\n",
    "print(f'\\n Testing accuracy = {(correct_cnt/data_cnt)*100:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LeNet5_relu_model_noAugmentation_lr0.001.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
